var store = [{
        "title": "There can be only one css stylesheet per media",
        "excerpt":"I thought it was a good idea to specify a media in my stylesheet link :   &lt;link rel=\"stylesheet\" href=\"/stylesheets/tags.css\" type=\"text/css\" name=\"tags stylesheet\" media=\"screen\" /&gt;   but when I tried to add another stylesheet, it was ignored !    &lt;link rel=\"stylesheet\" href=\"/stylesheets/common.css\" type=\"text/css\" name=\"main stylesheet\" media=\"screen\" /&gt; &lt;link rel=\"stylesheet\" href=\"/stylesheets/tags.css\" type=\"text/css\" name=\"tags stylesheet\" media=\"screen\" /&gt;   It worked by removing the media attribute. It seems that only one stylesheet per media is taken into account.  ","categories": ["css"],
        "tags": [],
        "url": "/there-can-be-only-one-css-stylesheet-per-medi/",
        "teaser": null
      },{
        "title": "Using gems instead of rails plugins on heroku",
        "excerpt":"Heroku is great, you can have a rails app running live in a few minutes ! There are still a few tricky points you’ve got to sort out a little differently. One of them is using rails plugins or extensions. The usage way is to have a submodule in your git repository, but heroku does not support submodules … You are left with 2 choices :      Expand the submodule as classical files in your main git repository   Use the extension or plugin as a gem (if possible)   When I can, I’ll go the second route. Last time, I ran into a problem. I wanted to use radiant-tags-extension gem with my radiant app, but I could not launch the install rake task   rake radiant:extensions:tags:install   I eventually found out that rake tasks from gems are not automatically available from the mail rake command. I also found this and it did the trick.   # &lt;railsapproot&gt;/lib/tasks/gems.rake  Dir[\"#Gem.searcher.find('radiant-tags-extension').full_gem_path}/lib/tasks/*.rake\"].each { |ext| load ext }  ","categories": ["gem","heroku","rake","ruby"],
        "tags": [],
        "url": "/using-gems-instead-of-rails-plugins-on-heroku/",
        "teaser": null
      },{
        "title": "Radiant v0.9.1 incompatibility with radiant-tags-extension ~> 1.6.2",
        "excerpt":"I was getting an undefined method 'find_by_path' error when using &lt;r:related_by_tags&gt;.   I rolled back to radiant-tags-extension v1.6.1 and it was fixed.  ","categories": ["radiant","radiant-tags-extension"],
        "tags": [],
        "url": "/radiant-v091-incompatibility-with-radiant-tag/",
        "teaser": null
      },{
        "title": "Making C# properties first class objects",
        "excerpt":"Functions and methods have a better status in the .Net world than they had in the Java world … They are now first class. This means that it is possible to assigne a function or an instance method to a variable, and pass it around as wished. Delegates are indeed very useful.   What about properties ? It’s a shame but they are not first class, they cannot be assigned to a variable, and if you ever used WinForms data bindings, I am sure you wrote the following quite a lot :   public event EventHandler NameChanged; private string _name; public string Name {   get   {     return _name;   }   set   {     if (_name != value)     {       _name = value;       InvokeNameChanged();      }   } }   How great would it be if we could define base property types, inherit and override them ! This is how I came up with the idea of using Property&lt;&gt; classes instances instead of raw properties.   class Presenter {   public readonly Property&lt;string&gt; Name = new Property&lt;string&gt;(); }   The first advantage that we got was that Winforw gui bindings would now be a lot more type safe, and resharper safe :   public partial class MyForm : Form {   Presenter _presenter = new Presenter();    public Form1()   {     InitializeComponent();   }    protected override void OnLoad(EventArgs e)    {     base.OnLoad(e);      AddBinding(textBox1, \"Text\", _presenter.Name, false, DataSourceUpdateMode.OnPropertyChanged);     AddBinding(textBox2, \"Text\", _presenter.Name, false, DataSourceUpdateMode.OnPropertyChanged);    }    void AddBinding&lt;T&gt;(Control control, string controlProperty, Property&lt;T&gt; property, bool formattingEnabled, DataSourceUpdateMode dataSourceUpdateMode)   {     control.DataBindings.Add(controlProperty, property, \"Value\", formattingEnabled, dataSourceUpdateMode);    } }   Creating a new AddBinding shareable function, the only presenter side property name hard coded in the gui could be “Value”, and the real property could explicitly be used to set up a binding.&lt;p /&gt; After using this kind of property in our work project, we discovered and expanded a lot more around it :      No need to test property event notifications every time, just test it once on the Property&lt;&gt; class   Ultra simple property delegation&lt;/li&gt;&lt;li&gt;Reusable read only properties   Computed properties with dependencies, able to notify changes automaticaly   Properties able to validate their value, and notify precise errors to the UI   I hope this will help someone  ","categories": ["binding","c-sharp","UI"],
        "tags": [],
        "url": "/making-c-properties-first-class-objects/",
        "teaser": null
      },{
        "title": "Collecting agile and XP best practices",
        "excerpt":"There is a huge difference between reading about agile software developpment, and being an agile developper or team. There are a lot of practical best practices that one learns only through practice.   As I am currently building up a training about “practices of an agile developper” for my company, I wanted to collect these best practices in an XP Patterns wiki  ","categories": ["agile","Best Practices","extreme programming"],
        "tags": [],
        "url": "/collecting-agile-and-xp-best-practices/",
        "teaser": null
      },{
        "title": "Automaticaly rollback static overrides when testing legacy code",
        "excerpt":"When working with legacy code, writing tests requires to exploit seams to -hack- inject custom behaviour. In Working Effectively with Legacy Code, Michael Feathers explains that singleton (the anti pattern) are often good starting seams. The idea is as follow :      create a public method to override the singleton value   make this method deprecated, obsolete or use a TEST_ONLY_ name prefix so that it is not called elsewhere in the code   Call this method from your test code to inject a mock or whatever so that you can test in isolation   This works fine … until you end up with an unexpected failing test [long debugging session] that is in fact the result of a test not restoring a singleton it had overriden. You can try hard not to forget, or you can use some kind of auto restore test class. Here is how it could look like in C#   using System; using System.Collections.Generic; using NUnit.Framework;  namespace Test.Utils {   public class CleanOverridesTest   {     private class StaticOverrider&lt;T&gt; : IDisposable     {       private readonly T _initialValue;       private readonly Action&lt;T&gt; _setter;        public StaticOverrider(T initialValue, Action&lt;T&gt; setter, T newValue)       {         _initialValue = initialValue;         _setter = setter;          _setter(newValue);       }        public void Dispose()       {         _setter(_initialValue);       }     }      private List&amp;lt;IDisposable&amp;gt; _toDisposeAfterEachTest;     private List&amp;lt;IDisposable&amp;gt; ToDisposeAfterEachTest     {       get       {         // NUnit enforces a single SetUp method, so we have to make sure the base SetUp method was called by subclasses         Assert.IsTrue(BaseSetUpCalled, \"Override SetUp and TearDown, and call base implementation.\");         return _toDisposeAfterEachTest;       }     }       [SetUp]     public virtual void SetUp()     {       _toDisposeAfterEachTest = new List&amp;lt;IDisposable&amp;gt;();     }     private bool BaseSetUpCalled     {       get { return _toDisposeAfterEachTest != null; }     }      [TearDown]     public virtual void TearDown()     {       // Overides and restores should be done in reverse order       ToDisposeAfterEachTest.Reverse();       foreach(var disposable in ToDisposeAfterEachTest)       {         try         {           disposable.Dispose();         }         catch (Exception)         { }        }       ToDisposeAfterEachTest.Clear();     }      protected void OverrideStatic&lt;T&gt;(T initialValue, Action&lt;T&gt; setter, T newValue)     {       ToDisposeAfterEachTest.Add(new StaticOverrider&lt;T&gt;(initialValue, setter, newValue));     }   } }   Later in an actual test :   [TestFixture] public class LegacyTest : CleanOverridesTest {   [Test]   public void LegacyShouldWorkWhenZingAndZang()   {     OverrideStatic(BigManager.Instance, BigManager.TestOnlySetInstance, new BigInstanceMock());      ...   } }   The singleton is automaticaly restored in the TearDown method of the base class. In C#, we are luky enough to have delegates so that we can pass the injection setter directly. In language without this feature, you could use reflection or anonymous class.   Of course, when the code is more reliably tested, you should try to move away from all this hack …  ","categories": ["c-sharp","testing"],
        "tags": [],
        "url": "/75417332/",
        "teaser": null
      },{
        "title": "Rspec messes up stubs and expectations",
        "excerpt":"Here is an example showing the issue :   before :each do   @car = stub(\"a car\")   @car.stub(:move) end it \"should be possible to mix stubbing and expectations\" do   @car.should_receive(:move).once    2.times { @car.move } end   This example should obviously fail, but it passes ! Here is a working (failing) version :   before :each do   @car = stub(\"a car\")  end it \"should be possible to mix stubbing and expectations\" do   @car.should_receive(:move).once    2.times { @car.move } end   I am using rspec 1.3.0. Did you fall into the same issues ? Is this fixed in rspec 2 ?  ","categories": ["rspec","ruby"],
        "tags": [],
        "url": "/rspec-messes-up-stubs-and-expectations/",
        "teaser": null
      },{
        "title": "radiant r:find does not find hidden pages",
        "excerpt":"Suppose you have a hidden newsletter folder in the file following file structure inside radiant:   home   /newsletters   /june-2011   /july-2011   /rss   and that you want to use the following tag in your rss file   &lt;r:find url=\"/newsletters/\"&gt; ... &lt;/r:find&gt;   r:find won’t find newsletters because it is hidden, it has to be published to be found.  ","categories": ["radiant-tags-extension"],
        "tags": [],
        "url": "/radiant-rfind-does-not-find-hidden-pages/",
        "teaser": null
      },{
        "title": "Don't forget the heroku stack",
        "excerpt":"Don’t waste your time as I did !   If ever you get a dependency gem version error when deploying to a new heroku test app, before starting to mess up your Gemfile configuration, check that the heroku stack of your new app is what you are expecting …  ","categories": ["heroku","ruby"],
        "tags": [],
        "url": "/dont-forget-the-heroku-stack/",
        "teaser": null
      },{
        "title": "external gem rails generator not found",
        "excerpt":"When trying to use devise 1.0.11 with rails 2.3.8 and bundler 1.0.10, I got the error could not find generator ... when running bundle exec script/generate devise_install ... After reading this stackoverflow Q&amp;A, I ended up copying the file manually !   I hope this is fixed in rails 3, I am looking forward to migrate.  ","categories": ["Devise","rails","ruby","workaround"],
        "tags": [],
        "url": "/external-gem-rails-generator-not-found/",
        "teaser": null
      },{
        "title": "How to test a class using an implementation helper ?",
        "excerpt":"Suppose you have some duplicated code in the Foo &amp; Bar classes. You managed to extracted this code in a helper class. Fine, the helper class can now be tested on its own, but how do I get rid of duplication in FooTest and BarTest ?   I though of injecting a mock on the helper class in Foo &amp; Bar when testing, to make sure the helper instance is correctly used, but sometimes, it just feel as if testing an implementation …   Here is what we eventually did at work : create an abstract base class to test the api of the helper class. Implement this abstract class once to test the helper class itself, and then implement it once each time it is reused.   Here is an exemple with a ListBuilder helper class that wraps a list and returns a copy of it each time it is asked for.   public class ListBuilder&lt;T&gt; {   private readonly List&lt;T&gt; _internalList = new List&lt;T&gt;();   private List&lt;T&gt; _listSnapshot = new List&lt;T&gt;();   private readonly object _locker = new object();    public List&lt;T&gt; ListSnapshot()   {     lock (_locker)     {       if (_listSnapshot.Count != _internalList.Count)       {         _listSnapshot = new List&lt;T&gt;(_internalList);       }     }      return _listSnapshot;   }    public void Add(T newItem)   {     lock (_locker)     {       _internalList.Add(newItem);     }   } }    Here is the tests for ListBuilder class itself   public abstract class ListBuilderContractTest&lt;T&gt; {   protected abstract void AddAnItem();   protected abstract void VerifyAddedItem(T addedItem);   protected abstract List&lt;T&gt; ListCopy();    [Test]   public void TheDefaultListShouldBeEmpty()   {     Assert.AreEqual(0, ListCopy().Count);   }    [Test]   public void AddShouldAddItemInList()   {     AddAnItem();     VerifyItemWasAdded();   }    protected void VerifyItemWasAdded()   {     var copy = ListCopy();     Assert.AreEqual(1, copy.Count);     VerifyAddedItem(copy[0]);   }    [Test]   public void WhenOfSameSizeNewListCopyShoulReturnSameList()   {     AddAnItem();     Assert.AreSame(ListCopy(), ListCopy());   }    [Test]   public void CopiesShouldBeSnapshots()   {     var copy = ListCopy();      AssertExecution.Of(AddAnItem)       .ShouldNotChange(() =&gt; copy.Count);   } }  [TestFixture] public class ListBuilderTest : ListBuilderContractTest&lt;string&gt; {   private const string ADDED_ITEM = \"toto\";    private ListBuilder&lt;string&gt; _appender;    [SetUp]   public void SetUp()   {     _appender = new ListBuilder&lt;string&gt;();   }    protected override void AddAnItem()   {     _appender.Add(ADDED_ITEM);   }   protected override void VerifyAddedItem(string addedItem)   {     Assert.AreEqual(ADDED_ITEM, addedItem);   }    protected override List&lt;string&gt; ListCopy()   {     return _appender.ListSnapshot();   } }    And here are tests for other class using it. First a simple one :   [TestFixture] public class OrderPresenterAsExecListBuilderTest : ListBuilderContractTest&lt;IExecRowPresenter&gt; {   private OrderPresenter _presenter;   private Mock&lt;IExec&gt; _exec;    [SetUp]   public void SetUp()   {     _exec = new Mock&lt;IExec&gt;();     _exec.Setup(x =&gt; x.Id).Returns(\"an exec\");     _exec.Setup(x =&gt; x.Price).Returns(123.456);     _exec.Setup(x =&gt; x.Quantity).Returns(36);      _presenter = new OrderPresenter();   }    protected override void AddAnItem()   {     _presenter.AddExec(_exec.Object);   }    protected override void VerifyAddedItem(IExecRowPresenter addedItem)   {     Assert.AreEqual(_exec.Object.Id, addedItem.Id);     Assert.AreEqual(_exec.Object.Price, addedItem.Price);     Assert.AreEqual(_exec.Object.Quantity, addedItem.Quantity);   }    protected override List&lt;IExecRowPresenter&gt; ListSnapshot()   {     return _presenter.GraphicalExecs;   } }    Then a more complex one :   [TestFixture] public class StrategyPresenterAsOrderPresenterListBuilderTest : ListBuilderContractTest&lt;IOrderPresenter&gt; {   private StrategyPresenter _presenter;   private Mock&lt;IStrategy&gt; _strategy;   private Mock&lt;IOrder&gt; _order;    [SetUp]   public void SetUp()   {     _strategy= new Mock&lt;IStrategy&gt; { DefaultValue = DefaultValue.Mock };      _order = new Mock&lt;IOrder&gt;();     _order.Setup(x =&gt; x.Id).Returns(123);      _presenter = new StrategyPresenter(_strategy.Object);   }    protected override void AddAnItem()   {     _strategy.Raise(x =&gt; x.SentQuantityChanged += null, new SentQuantityChangedEventArgs {Origin = _order.Object});   }    protected override void VerifyAddedItem(IOrderPresenter addedItem)   {     Assert.AreEqual(_order.Object.Id, addedItem.Id);   }    protected override List&lt;IOrderPresenter&gt; ListSnapshot()   {     return _presenter.GraphicalOrderList;   }     [Test]   public void ExecutedQuantityChangedShouldAddAnItem()   {     _strategy.Raise(x =&gt; x.ExecutedQuantityChanged += null, new ExecutedQuantityChangedEventArgs {Origin = _order.Object});      VerifyItemWasAdded();   } }   The solution really pleases me :      No code duplication   Robust tests   No artificial mocking  ","categories": ["c-sharp","testing"],
        "tags": [],
        "url": "/how-to-test-a-class-using-an-implementation-h/",
        "teaser": null
      },{
        "title": "Order of execution of before blocks in RSpec",
        "excerpt":"I discovered that before blocks in RSpec’s examples are executed in the order they are declared. There is no great deal about it, but it can be useful when using shared examples.   In rspec 1.3.8, it is possible to simulate shared examples with parameters by using instance variables instead. This is were it gets useful to know the order of evaluation of before blocks. As an example :   shared_example_for \"anything\" do   before :each do     @thing.should_not be_nil   end end  describe \"A monkey wrench\" do   it_should_behave_like \"anything\"    before :each do     @thing = MonkeyWrench.new   end end   The previous example will fail, whereas the next one will succeed.   describe \"A monkey wrench\" do   before :each do     @thing = MonkeyWrench.new   end   it_should_behave_like \"anything\" end   That is the before blocks nested in shared examples still evaluate in the order of declaration.  ","categories": ["rspec","ruby"],
        "tags": [],
        "url": "/order-of-execution-of-before-blocks-in-rspec/",
        "teaser": null
      },{
        "title": "A useful link to find free pictures",
        "excerpt":"While I was looking for photos for an article on blog for mes-courses.fr, I found this site : http://www.photo-libre.fr. There are a lot of pictures, both €-free and royalties free. All that is asked for is a linking back to http://www.photo-libre.fr.  ","categories": ["blogging","pictures"],
        "tags": [],
        "url": "/a-nice-place-to-find-free-pictures-for-my-web/",
        "teaser": null
      },{
        "title": "Scaling Agile",
        "excerpt":"I have worked in small agile teams, and it does indeed work a lot better than the classical waterfall &amp; hierarchic environment. When speaking with other people who have been through the same experience, we started to wonder how could this be scaled to a big organization ?   I read two good books on the subject from Craig Larman and Bas Vodde:      Scaling Lean &amp; Agile Development: Thinking and Organizational Tools for Large-Scale Scrum   Practices for Scaling Lean &amp; Agile Development: Large, Multisite, and Offshore Product Development with Large-Scale Scrum   There made a lot of good points and techniques, but it still sounded a bit theoretical. Later I read “Producing Open Source Software” from Karl Fogel. At first it did not seem to have any link with scaling agile, but successful open source projects have to handle distributed development and high turnover, so I thought that maybe large agile organizations have something to learn from open source development mechanics. Why wouldn’t these technics that are successful at sharing code between companies be used internally to share code between teams ?   At the same time, at work, I learnt the hard way that I should avoid to work for a “library, framework, engine …” team. It’s both frustrating and unrewarding :      You never really understand what you are working for, what is the final goal   You are always doing a poor job at prioritizing requests from different sponsors   You are doing too much on some areas, but too few on others   Agile &amp; incremental development and architecture solves this the nice way, by extract framework and libraries from running software ! Successful open source projects often start like that (Rails being the canonical example).   So why couldn’t we just let libraries and framework emerge the same way inside a company ? Why don’t we maintain these projects as we would maintain an open source project : as a side product of our main output ?   Craig Larman and Bas Vodde speak of feature teams, 20% free time, communities : all this fits nicely with this idea. Suppose we could have 20% of our time to work on what we like, any developper could take this time to extract a library from his team’s codebase and promote it as a company library, he could become dictator in chief as long as he wishes to. Other teams could reuse it and submit patches. All of the time proven open source development best practices could be used.   Edit: Github builds Github with something a lot like this !  ","categories": ["agile","enterprise","lean","scrum","extreme programming"],
        "tags": [],
        "url": "/scaling-agile/",
        "teaser": null
      },{
        "title": "Devise, SSL requirements and post data",
        "excerpt":"I am using rails 2.3.8, devise 1.0.11 and ssl_requirements 0.1.0. I am having strange behaviour when mixing devise with ssl_requirements. When a form posts to an action that is required to be ssl, the post is redirected to ssl as a get, and looses its post data !   To workaround the problem, I had to require ssl for the form itself aswell.   I hope the situation will be better once I migrate to rails 3 and to newer versions of devise and ssl_requirements.  ","categories": ["Devise","rails","SSL"],
        "tags": [],
        "url": "/devise-ssl-requirements-and-post-data/",
        "teaser": null
      },{
        "title": "Using stock ruby with rbenv",
        "excerpt":"While migrating to ruby 1.9, I started using rbenv. I was wondering how it would handle my stock ubuntu ruby 1.8. It works out of the box with the rbenv “system” version. I just have to run rbenv global system to come back to my stock ruby. Great !  ","categories": ["ruby","unix"],
        "tags": [],
        "url": "/using-stock-ruby-with-rbenv/",
        "teaser": null
      },{
        "title": "How to mock an out of process COM server with C#",
        "excerpt":"I am currently working to replace a legacy command line front end on a COM out of process server.   This server is written in C++ and communicates with the front end through COM. For our new front end project, we wanted a standalone integration test harness to run end to end tests. I thought it would have been great to use a mock library (like MOQ) to validate the interaction between the front end and the COM server.   After searching the web, I tried ServicedComponent but I did not manage to make it work … I then found How to develop and out-of-process COM component by using Visual C# in microsoft knowledge base. It is a full blown COM server completely written in C#.   After downloading it, the first thing I tried was to return a mock instead of a concrete object in the class factory   // CSSimpleObject.cs internal class CSSimpleObjectClassFactory : IClassFactory {    public int CreateInstance(IntPtr pUnkOuter, ref Guid riid, out IntPtr ppvObject)    {       ...       if (riid == new Guid(CSSimpleObject.ClassId) ||          riid == new Guid(COMNative.IID_IDispatch) ||          riid == new Guid(COMNative.IID_IUnknown))       {          // Create the instance of the .NET object          var simpleObject = new Mock&lt;CSSimpleObject&gt; {CallBase = true};          simpleObject.SetupGet(x =&gt; x.FloatProperty).Returns(666);          simpleObject.Setup(x =&gt; x.HelloWorld()).Returns(\"Hello, you got pwned !!!\");           ppvObject = Marshal.GetComInterfaceForObject(             simpleObject.Object, typeof(ICSSimpleObject));          }          ...   Launching CSExeCOMClient.vbs test program told me that it worked.   So here is how I eventually integrated this in our test harness :      I ran tlbimp.exe on the real COM server I wanted to mock   In CSSimpleObject.cs, I returned a Mock on the interface of the COM component I wanted to mock   In CSSimpleObject.cs, I removed SimpleObjectsXXX types   To skip registration and to keep access on the returned mock, I started this mock COM server in a new thread at the beginning of each test   I tested our new front end in a new process, to make sure we would go through out-of-process COM communication   Edit:   We discovered a problem with this technique when we did not manage to implement events correctly. The problem is with COM and not with the mocking framework because it does not work with a real implementation neither  ","categories": ["c-sharp","mocking"],
        "tags": [],
        "url": "/how-to-mock-an-out-of-process-com-server-with/",
        "teaser": null
      },{
        "title": "Including Rails.application.routes.url_helpers from a module",
        "excerpt":"When I migrated from rails 2.0 to rails 3.0, I had to change inclusion of ActionController::UrlWriter to Rails.application.routes.url_helpers. I started to get strange errors like undefined 'default_url_options' when running my specs. The issue was that I was including a module himself including Rails.application.routes.url_helpers.   module PathBarHelper    include Rails.application.routes.url_helpers   ...  end   As if at module definition time, url_helpers was not yet completely ready. I changed the code to include url helpers through a hook :   module PathBarHelper    def self.included(base)    base.send :include Rails.application.routes.url_helpers   end   ...  end   That did the trick, but I must admit I did not dig the issue completely. Tell me if you did ?  ","categories": ["rails","ruby"],
        "tags": [],
        "url": "/including-railsapplicationroutesurlhelpers-fr/",
        "teaser": null
      },{
        "title": "stub_model and mock_model",
        "excerpt":"I decided to stop using stub_model and mock_model. I do not use them enough to get fluent with them. Every time I have to deal with them, something breaks in an unexpected way. I just decided to stand with good old stubs.   I am also thinking of switching to RR (double ruby) and to use proxy objects to get simplify mocking even further. Did you try it ?  ","categories": ["mocking","rspec","ruby","testing"],
        "tags": [],
        "url": "/stubmodel-and-mockmodel/",
        "teaser": null
      },{
        "title": "Ruby thirdparties best practices",
        "excerpt":"I am just finishing migrating www.mes-courses.fr from rails 2 to rails 3, and I can assure that I learnt these lessons the hard way …      use Bundler http://gembundler.com   keep your Gemfile simple : just the top level dependencies, and no version constraints   try to stick to popular gems : you’ll find a lot more answers to your questions from the web. It’s often simpler to write and maintain a little extra code than to depend on a crappy thirdparty that is supposed to meet your need   stick with the most recent versions of your dependencies. With a good test suite and with Bundler, it should be rather easy to upgrade, so do it often to avoid both the upgrade tunnel effect and struggling with bugs and incompatibility of unmaintained old versions of gems.   Edit   I discovered the ruby toolbox that categorizes and sorts gems based on their popularity !   Edit 2   I discovered it is sometimes necessary to add constraints to get the latest versions of gems that would otherwise conflict (directly or not) with others.  ","categories": ["gem","ruby","Best Practices"],
        "tags": [],
        "url": "/ruby-thirdparties-best-practices/",
        "teaser": null
      },{
        "title": "Setting up Postgre 9.1 for rails development",
        "excerpt":"I am using rails to build www.mes-courses.fr. I use ubuntu for my development os, and heroku for deployment. As heroku enforces the usage of Postgre, I chose to use Postgre on my development setup also. When I upgraded from ubuntu 10.04, I had to review my database configuration.   I have a script in script/setup that installs dependencies and databases to setup a new dev environment :   #!/bin/sh  ## packages dependencies  sudo apt-get install libxml2-dev libxslt1-dev postgresql libpq-dev sqlite3 libsqlite3-dev  ## installing gems  bundle install   rbenv rehash  ## creating dbs  sudo -u postgres createuser --superuser mes_courses  sudo -u postgres psql --command=\"alter user mes_courses with encrypted password 'secret'\"  sudo sed -i 's/\\(local *all *all *\\)peer/\\1md5/' /etc/postgresql/9.1/main/pg_hba.conf  bundle exec rake db:create:all  ## initializing dbs  bundle exec rake db:migrate  RAILS_ENV=test bundle exec rake db:migrate  RAILS_ENV=production bundle exec rake db:migrate   Here is the corresponding config/database.yml   development:  adapter: postgresql  database: mes_courses_development  encoding: utf8  pool: 5  timeout: 5000  username: mes_courses  password: secret  host: localhost  port: 5433  test:  adapter: postgresql  database: mes_courses_test  encoding: utf8  pool: 5  timeout: 5000  username: mes_courses  password: secret  host: localhost  port: 5433  production:  adapter: postgresql  database: mes_courses_production  encoding: utf8  pool: 5  timeout: 5000  username: mes_courses  password: secret  host: localhost  port: 5433    Hope this helps !  ","categories": ["postgre","rails","ruby"],
        "tags": [],
        "url": "/setting-up-postgre-91-for-rails-development/",
        "teaser": null
      },{
        "title": "Hackers et peintres",
        "excerpt":"This is a french translation of the famous Hackers and painters essay written by Paul Graham.   Hackers et peintres   Mai 2003   (Cet article est dérivé d’une conférence à Harvard, qui intégrait un premier discours à Northeastern.)   Lorsque j’ai terminé mes études supérieures en informatique, je suis allé étudier la peinture dans une école d’art. Beaucoup de personnes semblaient surprises que quelqu’un qui s’intèresse aux ordinateurs puisse également s’intéresser à la peinture. Ils semblaient penser que le hacking et la peinture étaient des travaux très différents – que le hacking était froid, précis et méthodique, et que la peinture était l’expression frénétique d’un besoin primaire.   Ces deux images sont fausses. Le hacking et la peinture ont beaucoup de point communs. En fait, de tous les différents types de personnes que j’ai connu, les hackers et les peintres sont parmis les plus semblables. Les hackers et les peintres ont en commun qu’ils sont tous deux des créateurs. Tout comme les compositeurs, les architectes, et les écrivains, les hackers et les peintres essayent de créer des belles choses. Ils ne font pas de la recherche en soi, bien que si jamais ils découvrent une nouvelle technique pendant qu’ils essayent de créer de belles choses, c’est encore mieux.   Je n’ai jamais aimé le terme de “science informatique”. Principalement parce qu’un tel concept n’existe pas. L’informatique est un ensemble de domaines vaguement reliés ensemble qui ont été jetés ensemble par un accident de l’histoire, comme la Yougoslavie. A une extremité vous avez des gens qui sont de vrais mathématiciens, mais qui appellent ce qu’ils font de l’informatique pour obtenir des subventions de la DARPA. Au milieu vous avez des gens qui travaillent sur ce qui pourrait être l’histoire naturelle des ordinateurs – étudier le comportements des algorithmes de routage de données à travers un réseau, par example. Et ensuite, à l’autres extrémité, vous avez les hackers, qui essayent d’écrire des programmes intéressants, et pour qui les ordinateurs ne sont qu’un moyen d’expression, come le béton l’est pour les architectes ou la peinture l’est pour les peintres. C’est comme si les mathématiciens, physiciens, et architectes devaient tous être dans le même département.   Parfois, ce que les hackers font s’appelle “ingénieurie logicielle” mais ce terme est tout aussi trompeur. Les bons concepteurs de logiciel ne sont pas plus des ingénieurs que les architectes. La frontière entre l’architecture et l’ingénieurie n’est pas précisement définie, mais elle existe. Cela se situe entre le quoi et le comment : les architectes décident ce qu’il faut faire, et les ingénieurs trouvent comment le faire.   Le quoi et le comment ne devraient pas restés trop séparés. Vous cherchez les ennuis si vous décidez ce qu’il faut faire sans comprendre comment le faire. Mais le hacking peut en fait être bien plus que juste décider comment implémenter une spécification. A son meilleur, c’est créer la spécification – et il s’avère que la meilleure façon de faire cela est de l’implémenter.   Peut être qu’un jour la “science informatique” sera, comme la Yougoslavie, découpée en ses différentes sous parties. Ca pourrait être bien. Surtout si cela signifiait l’indépendance pour mon pays natal, le hacking. Regrouper ensemble toutes ces différentes sortes de travaux est peut être pratique administrativement, mais c’est intellectuellement trompeur. C’est l’autre raison pour laquelle je n’aime pas le terme “science informatique”. On peut dire que les gens au milieu font quelque chose qui ressemble à une science experimentale. Mais les gens aux extrémités, les hackers et les mathématiciens, ne font en fait pas de science.   Les mathématiciens ne semblent pas embêtés par cela. Ils se mettent joyeusement à la tâche de prouver des théorèmes, comme les autres mathématiciens du département de math, et ils occultent probablement rapidement le fait que le batiment dans lequel ils travaillent est intitulé “science informatique” à l’extérieur. Mais pour les hackers, l’étiquette est un problème. Si ce qu’il font est appelé science, cela les fait s’imaginer qu’ils devraient agir scientifiquement. Donc au lieu de faire ce qu’ils veulent vraiment faire, c’est à dire créer des magnifique logiciels, les hackers dans les universités et les centre de recherche pense qu’ils devraient écrire des publications de recherche.   Dans le meilleur des cas, les publications ne sont qu’une formalité. Les hackers écrivent des programmes cools, et ensuite écrivent une publication à ce sujet, et la publication devient un portail vers la réalisation qu’est le logiciel. Mais souvent ce décalage engendre des problèmes. En commençant par vouloir créer de belles choses, il est facile de finir par construire des choses moches mais qui sont de meilleurs sujets pour une publication.   Malheureusement, les belles choses ne font pas toujours les meilleurs sujets de publication. Primo, les sujets de recherches doivent être originaux – et comme quiconque a écrit une thèse le sait, le moyen d’être certain d’explorer un territoire vierge est de choisir un territoire dont personne ne veut. Deuxio, la recherche doit être substantielle – et les systèmes tordus engendrent des publications plus fournies, parce que vous pouvez écrire à propos des obstacles que vous avez dû surmonter pour faire quelque chose. Rien n’engendre des publications plus fournies que de partir avec les mauvaises hypothèses. La majorité de l’IA en est un exemple; si vous supposez que la connaissance peut être représentée commune une liste d’expressions de prédicats logiques dont les arguments représente des concepts abstraits, vous allez devoir écrire beaucoup de publications pour expliquer comment faire fonctionner cela. Comme Ricky Ricardo avait l’habitude de dire “Lucy, vous avez beaucoup d’explications à faire.”   La manière de créer quelque chose de génial est souvent de faire quelques subtiles modifications à quelque chose qui existe déjà, ou de combiner des idées existantes d’une manière légérement différente. Il est difficile de transmettre ce genre de travaux dans une publication.   Alors pourquoi les universités et les laboratoires de recherche continuent de juger les hackers par leurs publications ? Pour les mêmes raisons que les “aptitudes scolaires” sont mesurées par des tests simplistes et standards, ou que la productivité des programmeurs est mesurée en nombre de lignes de code. Ces tests sont faciles à appliquer, et il n’y a rien de plus tentant qu’un test facile qui d’une certaine manière, fonctionne.   Mesurer ce que les hackers essayent effectivement de faire, créer de magnifiques logiciels, serait beaucoup plus difficile. Vous avez besoin de bon gout pour juger une bonne conception. Et il n’y a pas de corrélation, sauf peut être une négative, entre la capacité des gens à repérer une bonne conception et leur confiance qu’il le peuvent.   Le seul test externe est le temps. Avec le temps, les belles choses tendent à s’épanouir, et les moches tendent à être rejetées. Malheureusement, le temps nécessaire pour cela peut être plus long qu’une vie. Samuel Johnson a dit qu’il fallait cent ans pour que la réputation d’un écrivain converge. Vous devez attendre que les amis influents de l’écrivain meurent, et ensuite que tous leurs suiveurs meurent.   Je pense que les hackers doivent simplement se résigner à avoir une grande part d’aléatoire dans leurs réputations. Dans ce sens ils ne sont pas différents des autres créateurs. En fait, ils sont chanceux en comparaison. L’influence de la mode n’est pas tout à fait aussi grande dans le hacking que dans la peinture.   Il y a pire que les gens qui comprennent mal votre travail. C’est plus dangereux si vous même comprenez mal votre travail. C’est dans les domaines connexes que vous pouvez trouver des idées. Si vous vous trouvez dans le département d’informatique, il sera naturel de croire, par exemple, que le hacking est la version appliquée de ce que l’informatique théorique théorise. Pendant toutes mes études supérieures, j’éprouvais un malaise latent à me dire que je devrais connaitre plus de théorie, et que j’avais été très négligent d’avoir oublié toutes ces choses à trois semaines de l’examen final.   Maintenant j’ai réalisé que je me trompais. Les hackers ont à peut prêt autant besoin de comprendre la théorie du calcul que les peintres ont besoin de comprendre la chimie de la peinture. Vous avez besoin de savoir comment calculer la complexité temporelle et spatiale et ce qu’est la complétude de Turing.Vous avez peut être également besoin de vous souvenir au moins du concept de machine à état, au cas où vous auriez à écrire un parser ou une librairie d’expression régulières. En fait les peintres doivent se souvenir de bien plus concernant la chimie de la peinture.   Je me suis rendu compte que les meilleurs sources d’idées ne sont pas les autres disciplines qui ont le mot “ordinateur” dans leur nom, mais les autres disciplines occupées par des créateurs. La peinture est une source bien plus riche en idées que la thérie du calcul.   Par exemple, on m’a appris à l’université qu’il fallait complétement mettre au point un programme sur le papier avant de s’approcher d’un ordinateur. Je me suis rendu compte que je ne programmais pas comme cela. Je me suis rendu compte que j’aimais m’assoire devant mon ordinateur, pas devant une feuille de papier. Pire, à la place d’écrire patiemment un programme complet et de m’assurer qu’il était correct, j’avais tendance à simplement balancer du code qui était désespérement buggé, et de le façonner petit à petit. Le debugging, comme on me l’a appris, n’était qu’une sorte de relécture finale où vous pouviez corriger les erreurs d’écritures et d’étourderies. Avec ma manière de travailler, on aurait dit que la programmation était du debugging.   Pendant longtemps, je me suis senti coupable à cause de ça, tout comme je me suis senti mal parce que je ne tenais pas mon crayon comme on me l’apprenais à l’école élémentaire. Si j’avais seulement regardé les créateurs d’autres disciplines, les peintres et les architectes, je me serais rendu compte qu’il y avait un nom pour ce que j’étais en train de faire : des ébauches. Aussi loin que je sache, la manière avec laquelle il m’ont appris à programmer à l’université était complétement fausse. Vous devriez concevoir vos programmes en même temps que vous les écrivez, tout comme les écrivains et les peintres et les architecte le font.   Ce rendre compte de cela a de réelles conséquences sur le conception de logiciel. Cela veut dire qu’un langage de programmation doit, avant tout, être maléable. Un langage de programmation est là pour réfléchir à des programmes, pas pour exprimer des programmes auxquels vous avez déjà réfléchi. Ca devrait être un crayon, pas un stylo. Le typage statique serait une superbe idée si les gens écrivait effectivement des programmes à la manière de ce qu’on m’a appris à l’université. Nous avons besoin d’un langage qui nous permette de griffoner et maculer et étaler, pas un langage où vous devez vous assoir avec une tasse à thé remplie de types en équilibre sur vos genoux et tenir une conversation polie à votre grand-tante stricte qu’est le compilateur.   Tant qu’on est sur le sujet du typage statique, s’identifier aux créateurs nous évitera un autre problème qui touche les sciences : l’envie de math. Tout le monde dans les sciences croit en secret que les mathématiciens sont plus intelligents qu’ils ne le sont. Je pense que les mathématiciens le croient aussi. Dans tous les cas, le résultat est que les scientifiques tendent à en sorte que leur travail ait l’air le plus mathématique possible. Ca ne fait surement pas beaucoup de mal dans un domaine comme la physique, mais plus vous vous éloignez des sciences naturelles, et plus le problème devient sérieux.   Une page de formules donne un air tellement sérieux. (Astuce : pour impressionner encore plus, utilisez des variables grecques.) Et donc il y a une grande tentation de travailler sur les problèmes qu’on peut traiter de manière formelle, plutôt que les problèmes qui sont, par exemple, importants.   Si les hackers s’identifient avec les autres créateurs, comme les écrivains et les peintres, ils ne seront pas tentés de faire cela. Les écrivains et les peintres ne souffrent pas de l’envie de math. Ils prensent qu’ils font quelque chose de complétement différent. Comme, je crois, le pensent les hackers.   Si les universités et les labos de recherche ne permettent pas aux hackers de faire le genre de travail qu’ils veulent faire, peut être que leur place est dans les entreprises. Malheureusement, la plupart des entreprises ne laisseront pas les hackers faire ce qu’ils veulent non plus. Les universités et les labos de recherches obligent les hackers à être des scientifiques, les entreprises les obligent à être des ingénieurs.   J’ai personnellement découvert cela seulement récemment. Lorsque Yahoo a acheté Viaweb, ils nous ont demandé ce que je voulais faire. Je n’ai jamais beaucoup aimé la partie business, et j’ai dit que je voulais juste hacker. Lorsque je suis arrivé à Yahoo, j’ai compris que pour eux hacking voulait dire implémenter des logiciels, pas les concevoir. Les programmeurs étaient vus comme des techniciens qui traduisaient les visions (si tel est le mot) des product managers en code.   Cela semble être le système par défaut dans les grandes entreprises. Ils le font parce que ça diminue l’écart type du résultat. Seulement un petit pourcentage des hackers peuvent effectivement concevoir des logiciels, et il est difficile de les trouver pour les personnes dirigeant une entreprise. Donc à la place de confier le future des logiciels à un seul hacker brillant, la plupart des entreprises font en sorte qu’il soit conçu en groupe, et que le hackers se contente d’implémenter la conception.   Si vous voulez gagner de l’argent à un certain moment, rappelez vous de cela, parce que c’est la raison pour laquelle les startups gagnent. Les grandes entreprises veulent réduire l’écart type des résultats de conception parce qu’elle veulent éviter les désastres. Mais lorsque vous attenuez les oscillations, vous perdez les points hauts aussi bien que les bas. Ca n’est pas un problème pour les grandes entreprises, parce qu’elle ne gagnent pas en faisant des supers produits. Les grandes entreprises gagnent en étant moins nulles que les autres grandes entreprises.   Donc si vous vous trouvez un moyen de faire une guerre de conception avec une entreprise assez grande pour que son software soit conçu par des directeurs produits, ils ne seront jamais capables de vous suivre. Ces opportunités ne sont pas facile à trouver cependant. Il est difficile d’engager une guerre de conception avec une grande entreprise, tout comme il est difficile d’engager un enemis dans son chateau en combat au corps à corps. Il serait très facile d’écrire un meilleur traitement de texte que Microsoft Word, par exemple, mais Microsoft, à l’intérieur de leur chateau du monopole du système d’exploitation, ne se rendrait surement même pas compte que vous l’auriez fait.   Les champs de bataille des guerres de conception sont les nouveaux marchés, où personne n’a encore réussi à établir des fortifications. C’est là que vous pouvez gagner beaucoup en prenant l’approche courageuse de la conception, et en ayant les même personnes qui à la fois conceoivent et implémentent le prduit. Microsoft eux même ont fait cela au début. Comme Apple. Et Hewlett-Packard. Je soupçconne que quasiment toute les startups à succés l’ont fait.   Donc une manière de construire des logiciels géniaux est de démarrer votre propre startup. Il y a deux problèmes avec cela, cependant. L’un est que dans une startup il y a beaucoup à faire en plus d’écrire des logiciels. A Viaweb je me considérais chançeux si j’arrivais à hacker un quart de mon temps. Et les choses que j’avais à faire les trois autres quarts du temps allait du pénible au terrifiant. J’ai un benchmark pour cela, parce qu’à une occasion j’ai dû quitter une réunion du conseil pour me faire soigner des caries. Je me souviens être assis sur le fauteuil du dentiste, attendant la fraise, et me sentir comme si j’étais en vacances.   L’autre problème avec les startup est que l’intersection entre le genre de logiciels qui rapportent de l’argent et le genre qui sont intéressant à écrire est petite. Les languages de programmation sont intéressants à écrire, et le premier produit de Microsoft en était un en fait, mais personne ne voudra payer pour un langage de programmation maintenant. Si vous voulez faire de l’argent, vous aurez tendance à devoir travailler sur des problèmes qui sont trop vilains pour que quiconque veuille les résoudre gratuitement.   Tous les créateurs font face à ce problème. Les prix sont déterminés par l’offre et la demande, et il n’y a tout simplement pas autant de demande pour les choses sur lesquelles il est amusant de travailler que sur les choses qui résolvent les problèmes terre à terre des différents clients. Jouer dans une pièce de théatre hors Broadway ne paye tout simplement pas autant que de porter un déguisement de Gorille dans une cabine pour quelqu’un à une foire commerciale. Ecrire des livres ne paye pas autant que d’écrire des copies de publicités qui finiront aux ordures. Et hacker des langages de programmation ne paye pas aussi bien que de trouver comment connecter la base de donnée historique d’une entreprise à leur serveur web.   Je pense que la réponse à ce problème, dans le cas du logiciel, est un concept connu de presque tous les créateurs : le job de jour. Cette expression à commencée avec les musiciens, qui se produisent la nuit. Plus généralement, ça veut dire que vous avez une sorte de travail que vous faites pour l’argent, et une autre par passion.   Presque tous les créateurs ont un job de jour au début de leur carrière. Il est célébre que les peintres et les écrivains en ont. Si vous êtes chanceux vous pouvez trouver un job de jour qui est très proche de votre vrai travail. Les musiciens semble souvent travailler dans des magasins de disques. Un hacker qui travaille sur un langage de programmation ou un système d’exploitation pourrait également être en mesure de trouver un job de jour qui l’utilise. [1]   Lorsque je dis que la réponse pour les hackers est d’avoir un job de jour, et de travailler sur de beaux softwares à côté, je ne propose pas cela comme une nouvelle idée. C’est la raison d’être du hacking open-source. Ce que je dis c’est l’open-source est probablement le bon modèle, parce qu’il a été indépendament confirmé par tous les autres créateurs.   Ca me semble très surprenant qu’un employeur soit récalcitrant à laisser les hackers travailler sur des projest open-source. A Viaweb, nous aurions été récalcitrant à embaucher quiconque ne le faisait pas. Lorsque nous faisions passer des entretiens à des programmeurs, la chose à laquelle nous faisions le plus attention était de savoir quel était le genre de logiciels qu’ils écrivaient dans leur temps libre. Vous ne pouvez pas faire quelque chose vraiment bien sans aimer le faire, et si vous aimer le hacking, inévitablement vous travaillerez sur vos projets personnels. [2]   Compte tenu que les hackers sont des créateurs plutôt que des scientifiques, le bon endroit pour trouver des métaphores n’est pas dans les sciences, mais auprès des autres sortes de créateurs. Qu’est ce que peut encore nous apprendre la peinture à propos du hacking ?   Une chose qu’on peut apprendre, ou en tout cas confirmer, de l’exemple de la peinture est comment apprendre à hacker. On apprend à peindre principalement en le faisant. Idem pour le hacking. La plupart des hackers n’apprenne pas à hacker en suivant des cours de programmations à l’université. Ils apprennent à hacker en écrivant leurs propres programmes à 13 ans. Même à l’université, vous apprenez principalement à hacker en hackant. [3]   Comme les peintres laissent une trace de leur travail derrière eux, on peut les voir apprendre en faisant. Si vous observez le travail d’un peintre par ordre chronologique, vous verrez que chaque peinture est construite sur des choses qui ont été apprises lors des précédentes. Lorsqu’il y a quelque chose dans une peinture qui fonctionne vraiment bien, vous pouvez souvent en trouver la version 1 en plus petit dans une peinture plus ancienne.   Je pense que la plupart des créateurs travaillent de cette manière. Les écrivains et les architectes le semblent aussi. Peut être qu’il serait bon pour les hackers d’agir plutôt comme les peintres, et de régulièrement redémarrer de zéro, plutôt que de continuer à travailler pendant des années sur un projet, et d’essayer d’incorporer toutes leurs dernières idées comme évolutions.   Le fait que les hackers apprennent à hacker en faisant est un autre indice d’à quel point le hacking est différent des sciences. Les scientifiques n’apprennent pas les sciences en les pratiquant, mais en faisant des expériences et des hypothéses. Les scientifiques commencent par faire du travail parfait, dans le sens où ils essayent juste de reproduire le travail que quelqu’un d’autre a déjà faire pour eux. Tout à la fin, ils arrivent au point où ils peuvent faire du travail original. A la différence des hackers qui, depuis le début, font du travail original; qui est juste très mauvais. Donc les hackers commencent originaux, et deviennent bons, et les scientifiques commencent bons, et deviennent originaux.   L’autre moyen d’apprendre pour les créateurs est par l’exemple. Pour un peintre, un musée est une librairie de référence de techniques. Pendant des siècles l’apprentissage traditionel des peintres à consister à copier des oeuvres de grands maîtres, parce que la copie vous oblige à regarder dans le détail la manière dont la peinture est faite.   Les écrivains font cela aussi. Benjamin Franklin a appris à écrire en résumant les points dans les essais de Addison et Steel et ensuite en essayant de les reproduires. Raymond Chandler a fait la même chose avec les histoires de détectives.   Les hackers, de même, peuvent apprendre à programmer en regarder des bons programmes– pas seulement ce qu’ils font, mais le code source aussi. Un des avantages les moins médiatisés du mouvement open-source est qu’il a simplifier l’apprentissage de la programmation. Lorsque j’ai appris à programmer, nous devions principalement nous appuyer sur des exemples dans des livres. Le gros morceaux de code disponible à l’époque était Unix, mais même cela n’était pas open-source. La plupart des gens qui en lisait les sources lisaient des photocopies pirates du livre de John Lion, lequel, bien qu’écrit en 1977 n’a pas été autorisé à la publication avant 1996.   Un autre exemple que nous pouvons prendre de la peinture est la manière avec laquelle les peintures sont construites par améliorations successives. Les peintures commencent d’habitude avec un croquis. Progressivement les détails sont ajoutés. Mais ça n’est pas qu’une technique d’ajout. Parfois le plan de départ se révèle faux. D’innombrables peintures, lorsque vous les observée aux rayons X, se révèlent contenir des membres qui ont été déplacés ou traits de visage qui ont été ajustés.   Voici un cas où nous pouvons apprendre de la peinture. Je pense que le hacking devrais fonctionner de cette manière aussi. Il est irréaliste d’espèrer que les spécifications d’un programme soient parfaites. Vous serez plus à l’aise si vous admettez ceci dès le début, et que vous écriviez des programmes de manière à permettre aux spécifications de changer à la volée.   (L’organisation des grandes entreprises rend cela très difficile à faire, voici donc une autre point où les startups ont un avantage.)   Aujourd’hui, sans doute tout le monde connait les danger de l’optimisation prématurée. Je pense que nous devrions être tout aussi inquiet de la conception prématurée– décider trop tôt ce que notre programme devrait faire.   De bons outils peuvent nous aider à éviter ce danger. Un bon langage de programmation devrait, comme la peinture à l’huile, nous permettre de changer d’avis plus simplement. Le typage dynamique est un gain ici parce que vous n’avez pas à vous engager sur des représentations spécifiques des données dès le début. Mais la clef de la flexibilité, je pense, est de rendre le langage très abstrait. Le programme le plus facile à modifier est celui qui est très court.   Ceci résonne comme un paradoxe, mais une peinture extraordinaire doit être encore meilleure que ce qu’elle doit être. Par exemple, lorsque Léonard a peint le portrait de Ginevra de’Benci à la Gallerie Nationale, il mit un genévrier derrière sa tête. Il peint avec minutie chacune de ses feuilles. Beaucoup de peintre auraient pu penser que cela était juste quelque chose à mettre en fond pour encadrer sa tête. Que personne ne regarderait cela d’aussi prêt.   Pas Léonard. Le travail qu’il fournissait pour une partie d’une peinture ne dépendait pas de la distance à laquelle il imaginait qu’on la regarderait. Il était comme Michael Jordan. Implacable.   L’acharnement est toujours un succes parce que, dans l’ensemble, les détails invisibles deviennent visibles. Lorsque les gens passent devant le portrait de Ginevra de’ Benci, il capte souvent immédiatement leur attention, avant même qu’il regarde l’écritau et l’explication qui indique Léonard de Vinci. Tous ces détails invisibles se combinent pour produire quelque chose de tout simplement ahurissant, comme un milier de voix à peine audibles qui chantent à l’unisson.   Les excellents logiciels, eux aussi, exigent une dévotion fanatique à la beauté. Si vous regardez à l’intérieur de bons logiciels, vous trouverez des parties que personnes n’est supposé voir et qui sont magnifiques. Je ne prétends pas écrire des excellents logiciels, mais je sais que lorsqu’il s’agit de code je me comporte d’une manière qui me vaudrait une prescription de médicament si je l’utilisais dans la vie de tous les jours. Ca me rend fou de voir du code qui est mal indenté, ou qui utilise des noms de variables affreux.   Si un hacker n’était qu’un simple implémenteur, transformant une specification en code, alors il pourrait juste travailler d’un bout à l’autre comme quelqu’un qui creuse un fossé. Mais si un hacker est un createur, nous devons tenir compte de l’inspiration.   En hacking, comme en peinture, le travail arrive par cycle. Parfois vous êtes excité par un nouveau projet et vous voulez travailler seize heures par jours dessus. D’autres fois rien ne semble intéressant.   Pour faire du bon travail vous devez prendre tenir compte de ces cycles, parce vous êtes sensible à la manière dont vous leurs réagissez. Lorsque vous conduisez une voiture avec transmission manuelle sur une coline, vous devez parfois rétrograder pour éviter de caller. De la même manière, rétrograder évite parfois à l’ambition de caller. En peinture tout comme en hacking il y a des tâches tellement ambitieuses qu’elles en sont sont terrifiantes, et d’autres qui sont confortablement routinières. C’est une bonne idée de conserver des tâches faciles pour les moment où vous auriez caller autrement.   En hacking, cela peut litéralement signifier garder des bugs. J’aime le debugging : c’est le moment unique ou le hacking est aussi simple que les gens le pensent. Vous avez un problème complétement conscrit, et tout ce que vous avez à faire est de le résoudre. Votre programme est supposé faire x. A la place il fait y. Où va-t’il de travers ? Vous savez que vous allez gagner à la fin. C’est aussi relaxant que de peindre un mur.   L’exemple de la peinture peut nous apprendre à non seulement à gérer notre propre travail, mais aussi à travailler ensemble. Beaucoup d’oeuvres du passé sont le travail de beaucoup de mains, même si il se peut qu’il n’y ait qu’un nom sur le mur qui la soutient au musé. Léonard était un apprenti  à l’atelier de Verrocchio et a peint l’un des anges dans son Baptême du christ. Ce genre de chose était la règle, pas l’exception. Michelange a été considéré particulièrement dévoué pour avoir insister à peindre lui même tous les personnages du plafond de la chapelle Sistine.   Aussi loin que je sache, lorsque les peintres travaillent ensemble sur une peinture, ils ne travaillent jamais sur les même parties. Il était commun pour un maître de peindre les personnages principaux et pour les assistants de peindre le fond et les autres. Mais vous n’aviez jamais un type qui paignait sur le travail des autres.   Je pense que cela est également le bon modèle de collaboration pour le logiciel. Ne le poussez pas trop loin. Lorsqu’un morceau de code est hacké par trois ou quatres personnes, n’appartenant à aucun de ceux-ci, il finira comme une pièce commune. Il aura tendance à être morne et abandonné, et à accumuler la saleté. La bonne manière de collaborer est, je pense, de diviser les projets en modules précisement définis, chacun avec un propriétaire connu, et avec des interfaces entre eux qui soient aussi bien conçues et, si possible, aussi flexibles qu’un langage de programmation.   Comme la peinture, la plupart des logiciels sont destinés à des humains. Et donc les hackers, comme les peintres, doivent avoir de l’empathie pour des choses vraiment géniales. Vous devez être capable de voir les choses du point de vue de l’utilisateur.   Lorsque j’étais un enfant on me disait tout le temps de regarder les choses du point de vue de quelqu’un d’autre. Ce que cela voulait dire en pratique était de faire ce que quelqu’un d’autre voulait plutôt que ce que je voulais. Cela biensûr donna une mauvaise réputation à l’empathie, et je me fis un principe de ne pas la cultiver.   Et bien, qu’est ce que j’avais tord. Il s’est révélé que de regarder les choses depuis le point de vue des autres est pratiquement le secret du succès. Ca ne veut pas nécessairement dire se sacrifier. Loin de là. Comprendre ce que pense quelqu’un autre n’implique pas que vous agissiez dans son interêt; dans certains situation– pendant la guerre, par exemple– Vous voulez faire exactement l’opposé. [4]   La plupart des créateurs font des choses pour un public humain. Et pour intéresser un public vous devez comprendre ce dont il a besoin. Presque toutes les plus grandes peintures sont des peinture de personnes, parce que, par exemple, ce sont les gens qui intéressent les gens.   L’empathie est probablement la différence la plus importante entre un bon hacker et un hacker génial. Certains hackers sont très intelligents, mais en ce qui concerne l’empathie sont pratiquement solipsistes. C’est difficile pour de telles personnnes de concevoir des logiciels géniaux [5], parce qu’ils ne peuvent pas voir les choses du point de vue de l’utilisateur.   Une bonne manière de dire à quel point des gens sont empathiques et de les regarder expliquer un problème technique à quelqu’un sans formation technique. Nous connaissons tous des gens qui, bien qu’intelligents, sont juste comiques lorsqu’ils font cela. Si à une soirée repas quelqu’un leur demande ce qu’est un langage de programmation, ils diront quelque chose comme “Ah, un langage de haut niveau est celui utilisé par le compilateur pour générer le code machine.” Langage de haut niveau ? Compilateur ? Code machine ? Quelqu’un qui ne sait pas ce qu’est un langage de programmation ne sait évidement pas ce que ces choses sont non plus.   Une chose que le logiciel doit faire est de s’expliquer soit-même. Donc pour écrire de bon logiciels vous devez comprendre à quel point les utilisateurs en comprennent peu. Il vont essayer le logiciel sans préparation, et il vaut mieux qu’il fasse ce qu’ils ont deviné qu’il faisait, parce qu’ils ne vont pas lire le manuel. Le meilleur système que j’ai vu selon ce critère est le premier Macintosh, en 1985. Il faisait ce que les logiciels ne font quasiment jamais : il fonctionnait tout de suite. [6]   Le code source, lui aussi, devrait s’expliquer lui-même. Si je pouvais amener les gens à se souvenir d’une seule citation à propos de la programmation, ça serait celle au début de Structure and Interpretation of Computer Programs.      Les programmes devraient être écrit pour que les gens puissent les lire, et seulement accessoirement que les machines les exécutent.    Vous devez avoir de l’empathie non seulement pour vos utilisateurs, mais également pour vos lecteurs. C’est dans votre interêt, parce que vous serez l’un d’eux. Beaucoup de hackers ont écrit un programme et ont dû y revenir six mois plus tard pour se rendre compte qu’ils n’avaient aucune idée de comment est ce qu’il fonctionnait. Je connais plusieurs personnes qui ont ont juré de ne plus utiliser Perl après de telles expériences. [7]   Le manque d’empathie est associé à l’intelligence, au point que ça en soit une sorte de mode à certains endroits. Mais je ne pense pas qu’il ait une corrélation. Vous pouvez être bon en math et en sciences naturelles sans avoir à apprendre l’empathie, et les gens dans ces matières sont plutôt intelligents, donc les deux qualités sont devenues associées. Mais il y a également beaucoup d’imbéciles qui ne sont pas empathiques. Ecoutez seulement aux personnes qui appellent pour poser des questions dans les talk shows. Ils posent leur question d’une manière tellement tordue que le présentateur est souvent obligé de reformuler la question pour eux.   Donc, si le hacking fonctionne comme la peinture et l’écriture, est-ce aussi cool ? Après tout, vous n’avez qu’une seule vie. Autant la passer à travailler sur quelque chose de génial.   Malheureusement, la question est difficile à répondre. Il y a toujours un grand délais avant le prestige. C’est comme la lumière d’une étoile lointaine. La peinture a du prestige maintenant grâce au travail génial que des gens ont fait il y a cinq siècles. A cette époque, personne ne pensait que ces peintures étaient aussi importantes que nous le pensons aujourd’hui. Ca aurait semblé très étrange aux gens de cette époque que Federico da Montefeltro, le Duke of Urbino, serait un jour principalement connu comme le type avec le drôle de nez dans une peinture de Piero della Francesca.   Donc bien que j’admette que le hacking n’ait pas l’air aussi cool que la peinture maintenant, nous devrions nous souvenir que la peinture elle même n’avait pas l’air aussi cool pendant son age d’or que maintenant.   Ce qu’on peut dire avec une certaine certitude est que c’est l’âge d’or du hacking. Dans la plupart des domaines les grandes oeuvres sont faites tôt. Les peintures faites entre 1430 et 1500 sont toujours insurpassées. Shakespeare apparu en même temps que le théatre professionel naissait, et a poussé le milieu si loin que depuis, tout dramaturge a du vivre dans son ombre. Albrecht Durer fit même chose avec la sculpture, et Jane Austen avec les romans.   Encore et encore nous assistons au même schéma. Une nouvelle matière apparait, les gens en sont tellement enthousisathes qu’ils explorent la plupart de ses possibilités pendant les premières générations. Le hacking semble être dans cette phase maintenant.   La peinture n’était pas, à l’époque de Léonard, aussi cool que son travail l’a rendue. Le hacking deviendra cool ou pas suivant ce que nous arrivons à faire avec cette nouvelle matière.   Notes   [1] Le plus grand tord qu’a fait la photographie à la peinture est peut être qu’elle a tué les meilleurs jobs de jour. La plupart des grands peintres de l’histoire sont subvenus à leurs besoins en peignant des portraits.   [2] On m’a dit que Microsoft déconseille à ses employés de contribuer à des projest open-source, même dans leur temps libre. Mais maintenant une telle proportions des meilleurs hackers travaillent sur ldes projets open-source que l’effet principal de cette politique pourrait être de s’assurer de ne pas embaucher les programmeurs de première classe.   [3] Ce que vous apprenez à propos de la programmation à l’université est très semblable à ce que vous apprenez à propos des livres ou des vêtements ou des rencontres : qu’est ce que vous aviez mauvais gout au lycée.   [4] Voici un exemple d’empathie appliquée. A Viaweb, si nous ne parvenions pas à choisir entre deux alternatives, nous nous demandions, qu’est ce que nos concurrent détesteraient le plus ? A un moment, un concurrent a ajouté une fonctionalité à leur logiciel qui était globalement inutile, mais comme c’était une des rares qu’ils avaient et pas nous, ils en firent beaucoup de bruit dans la presse. Nous aurions pu essayer d’expliquer que cette fonctionalité était inutile, mais nous avons décidé que ça énerverait plus notre concurrent si nous l’implémentions simplement, nous avons donc hacké notre propre version en une après midi ce jour là.   [5] Sauf les éditeurs de texte et les compilateurs. Les hackers n’ont pas besoin d’empathie pour concevoir ceux-ci, parce qu’ils en sont eux même des utilisateurs typiques.   [6] Enfin, presque. Ils ont quelque peut dépassé la quantité de RAM disponible, entrainant beaucoup de swap disque pénible, mais cela pouvait être corrigé après quelques mois en achetant un lecteur de disque supplémentaire.   [7] La manière de rendre les programmes facile à lire n’est pas de les remplir de commentaires. Je pousserais la citation d’Abelson et Sussman un pas plus loin. Les langages de programmation devraient être conçus pour exprimer des algorithmes, et seulement accessoirement pour que les machines les exécutent. Un bon langage de programmation devrait être plus efficace pour expliquer un programme que l’Anglais. Les commentaires ne devraient être nécessaires que lorsqu’il y a une sorte de bricolage dont vous devez mettre en garde le lecteur, tout comme il n’y a des flèches sur la route qu’aux virages serrés et innattendus.   Merci   à Trevor Blackwell, Robert Morris, Dan Giffin et Lisa Randall pour avoir lu les ébauches de ceci, et à Henry Leitnet et Larry Finkelstein pour m’avoir invité à parler.&lt;/p&gt;   Fixes and improvements are welcome.  ","categories": ["Hacking"],
        "tags": [],
        "url": "/hackers-et-peintres/",
        "teaser": null
      },{
        "title": "Rails url_helpers mixup when using rails engines",
        "excerpt":"I just tried to integrate the blogit rails engine directly inside www.mes-courses.fr. Up till now, I was using a separated radiant app to deal with the blog part. It was working fine, but I had a few problems:      I had to use special (javascript heavy) google analytics code through all cross app links, and that sometimes failed (somewhere in the combinaison of heroku, rails, and analytics)   It was difficult to share the theme between the apps : on one side, templates, partials and css, on the other, rows in a database   It was difficult to share session state between the two apps (login for example)   As my blog is very basic, and as I am the only one editing the styles, I decided to embed a blog engine inside my app. I selected and tried a few engines : Monologue, Blogit, RefineryCMS and to roll my own. Refinery looks way too complex to integrate regarding my basic needs. Monologue is not design to integrate with devise users (which I already use) and I would prefer to contribute to an existing gem rather than to write the nth rails blog engine …   After a few experiments, I settled on blogit. Unfortunately, I had really strange issues with links within blogit pages : all links were prefixed with /blog and blogit links where like /blog/blogit?controller=posts&amp;amp;action=new. Hacking around in the rails console showed me that doing consecultively :   include Blogit::Engine.routes.url_helpers include Rails.application.routes.url_helpers   It resulted in the same behavior. After a long debugging session, I eventually discovered that I had a helper in my application which explicitly required Rails.application.routes.url_helpers. This was causing the conflict. I changed my code to avoid this inclusion, and that fixed the links …   … nearly. Now rendering the application template for a blog page failed with a “message not understood” for the main app’s xxx_path methods. Hopefully, I had found a fix for this error earlier on the web. Now everything was working like a charm.   To conclude, I was convinced by blogit, and I am going to use it to migrate my blog. I already have a few ideas for improvements I will have to do (disqus comments and archive helper for example).  ","categories": ["ruby","rails","gem"],
        "tags": [],
        "url": "/rails-urlhelpers-mixup-when-using-rails-engin/",
        "teaser": null
      },{
        "title": "Rails autoload good practices",
        "excerpt":"I started using rails autoload to load files in my lib folder of http://www.mes-courses.fr. Before that, I had been using hand written require statements, and later hand written autoload statements. Rails autoload are by far the best approach for this. It has a few pitfalls though. Here are the best practices I discovered so far.      Use autoload to load files in your lib folder. In config/application.rb   config.autoload_paths += %W(#{config.root}/lib)      Inside the lib folder, organize your files with directories and use corresponding modules to create logic namespaces   Don’t declare the namespaces in a single line like this :   module MyApp::Utils  class FileHelper  ...  end end   If ever this file is autoloaded first, you will get an error like undefined constant MyApp, it gets more likely with deeper namespace structure. Prefer the following nested declaration :   module MyApp  module Utils    class FileHelper      ...    end   end end      Doing include MyApp::Utils to include the have access to Utils members (ie “using namespace” in C++) will not work as well as with explicit requires. So if it does not work well, prefer to use FileHelper = MyApp::Utils::FileHelper   Whenever you are using a base class, I found out that autoload does not always manage to load the base class correctly, in this case, explicitly requiring the base class fixes the issue.   If ever you try to monkey patch one of your class directly (in a test for example), the real class might not get autoloaded since it is already declared in the monkey path :   class MyApp::Engine   ... end describe MyApp::Engine do   before :each do     @engine = MyApp::Engine.new(\"name\")     ...    end    ... end   This might trigger an error like wrong number of arguments for MyApp::Engine.new. Knowing MyApp::Engine from the spec file, rails does not try to autoload the other part ! Here is how I fixed this   module MyApp::EngineExtras   ... end MyApp::Engine.send(:include, MyApp::EngineExtras) ...   This works as expected.   At the moment, I still have an issue I did not manage to fix neatly : how can we include namespaces in spec and cucumber step files without polluting the global namespace ?  ","categories": ["ruby","rails"],
        "tags": [],
        "url": "/rails-autoload-good-practices/",
        "teaser": null
      },{
        "title": "Motivation game",
        "excerpt":"At MegaCorp where I am currently working, there is a small team that is dedicated to find solutions to software bugs that regularly happen in production. The software system is very old and buggy, so it can get quite stressful at times …   One thing users want is to get a daily email summarizing all the issues currently being looked after by this team, so that they can be sure that someone is looking into their problems. The trick is it is not always easy to take the time to write a nice and polished email when one feels completely overwhelmed by urgent stuff to do.   After trying many ways such as :      good will   every day checklist   ploting KPIs   physical threat (just kidding)   Nothing had worked … until entered the “Motivation game”. Here is the idea :      On A3 sheet of paper, print a funny comic (Dilbert and xkcd are my favorites)   Hide it behind post-its   Stick it on the wall, next to your whiteboard   Every day, during your stand up meeting, remove a post-it if someone sent the email the previous day   If no one sent the email, then cover up the current frame with post-its again   Here is what it looks like :      PS : We also set up a Kanban board with WIP limits in order to avoid the issues beforehand.  ","categories": ["Lean","gamification","continuous improvement"],
        "tags": [],
        "url": "/motivation-game-53790/",
        "teaser": null
      },{
        "title": "Windows licker like with C#",
        "excerpt":"In Growing object-oriented software guided by tests they use a java library called Window Licker to pilot a gui running in test thread. Next time I started a new project, I wanted to apply the techniques I had learned in the book. As the project was in C#, I searched for a C# equivalent of Window Licker. I did not find any, but it turned out to be really simple to create an equivalent.   Here is the user test code I wanted to be able to write (inside a SpecFlow step) :   [Given(@\"TheUserEntersHisEmail\"\"(.*)\"\"\")] public void GivenTheUserEntersHisEmail(string email) {   User.Enters(ScenarioContext.Current.UserForm().EmailTextBox, email); }   Here UserForm is an extension method I added to the ScenarioContext that is responsible for returning the current user form.   As you have noticed, the various gui components must be exposed through some kind of public api. What I did was to expose the main gui window as a property on the application top level object, and then have properties to access all the sub windows and the controls that the tests needed to access. It kind of breaks encapsulation, but it allows to keep the tests working during refactoring !   Eventually, I had to write the User class, and it turned out to be really simple :   static class User {   internal static void Enters(TextBox textBox, string text)   {     textBox.Invoke(new MethodInvoker(() =&gt; textBox.Text = text));   } }   That’s it, all that was needed was to ask the thread of the control to update the text !   ","categories": ["c-sharp","testing"],
        "tags": [],
        "url": "/windows-licker-like-with-c/",
        "teaser": null
      },{
        "title": "Cloning an rbenv version",
        "excerpt":"There is no doc about this, but it turns out to be very simple, just copy the version directory :   cd ~/.rbenv/versionscp -R original-version new-version   That’s it !  ","categories": ["ruby"],
        "tags": [],
        "url": "/cloning-an-rbenv-version/",
        "teaser": null
      },{
        "title": "How I set up a trustico rapid ssl certificate on heroku ssl endpoint",
        "excerpt":"This can be quite time consuming if it is the first time you set up an ssl endpoint. Here is how I did it :      Purchase a RapidSSL certificate for my domain on www.trustico.com. Make sure to use the insurance option so that you can download the private key later. I had to setup an email account at admin@mes-courses.fr so that I could receive their confirmation link email.&lt;/li&gt; &lt;li&gt;Once you have bought your certificate, login in to you Trustico account and download your certificate, the intermediate certificate, and the private key.&lt;/li&gt;&lt;li&gt;Concatenate the final and intermediate certificates to a single file (let’s call it server.crt).   Add the ssl endpoint add on to your Heroku application   heroku addons:add ssl:endpoint      Upload your certificate   heroku certs:add server.crt private.key      Run the following and verify that you have an ssl endpoint with an explicit trusted ‘True’ value   heroku certs      Note the ssl-endpoint full domain from the previous command line, and add a DNS CNAME record from your domain to this endpoint   Check that there are no A DNS records pointing to Heroku IPs on your dns configuration (It used to be the case with older versions of Heroku)   Wait until dns servers are updated to check that it is working.   You can check how your dns settings are spreading with www.whatsmydns.net and http://www.reverse-dns.fr/. The Heroku ssl-endpoint help page was a real brain saver.   EDIT 10/10/2013   Any certificate will eventually expire, and need to be renewed. The process for this (at Trustico at least) is to generate a completely new certificate. To install this new certificate on Heroku, start by combining the new certificate files as you did the first time, and then use   heroku certs:update server.crt private.key   instead of Heroku certs:add. This should be enough, there is no need to update any dns entry.   To make sure the new certificate is used, visit the Heroku ssl endpoint directly (get it by running Heroku certs) and then visit your ssl site to check the certificate infos from your browser.  ","categories": ["DNS","heroku","SSL"],
        "tags": [],
        "url": "/how-i-set-up-a-trustico-rapid-ssl-certificate/",
        "teaser": null
      },{
        "title": "How to install a patched ruby interpreter with rbenv and ruby-build",
        "excerpt":"A background scheduled task I am trying to run on heroku is failing because it gets out of memory. I needed to use a ruby memory profiler to understand exactly what the issue was. ruby-prof seemed great, but it needs a patched ruby interpreter to collect memory information.   After a bit of searching and trying, the simplest thing I managed to do was to      download the ruby sources   patch them   create a ruby-build package definition with them   install with rbenv   Here is a script that does this for ruby 1.9.2-p125 and gcdata patch.     ","categories": ["ruby"],
        "tags": [],
        "url": "/how-to-install-a-patched-ruby-interpreter-wit/",
        "teaser": null
      },{
        "title": "How to copy a database from an heroku app to another",
        "excerpt":"For mes-courses.fr, I am using another heroku app as “integration” app, where I can do late verifications before deploying to my production app. If you don’t already do this, I recommend you to start now !   One thing that I needed was to copy data from my production app to my integration app. It turns out that heroku makes this very easy :   heroku pgbackups:restore DATABASE `heroku pgbackups:url --app my-app-prod` --app my-app-integ --confirm my-app-integ  ","categories": ["heroku","postgre"],
        "tags": [],
        "url": "/how-to-copy-a-database-from-an-heroku-app-to/",
        "teaser": null
      },{
        "title": "TOO_MANY_REDIRECTS when changing ssl requirements",
        "excerpt":"While I was trying to enforce no ssl on a page of www.mes-courses.fr with Rack::SslEnforcer, I lost a few hours trying to fix a TOO_MANY_REDIRECTS error … The server was redirecting from http to https in loop !   It turned out it was my browser (chromium) that had cached a previous redirection.   I fixed it by right-clicking the faulty redirection in the chrome network debugger and asking a cache clear.  ","categories": ["ruby","SSL","web"],
        "tags": [],
        "url": "/toomanyredirects-when-changing-ssl-requiremen/",
        "teaser": null
      },{
        "title": "The poor man's memory profiling",
        "excerpt":"While working on www.mes-courses.fr, a background scheduled task that was running fine on heroku started to fail with out of memory errors. After searching a bit, I discovered that the inputs had changed, and that the memory consumption of my task was linearly correlated to the size of the inputs.   So I tried to setup an automatic test to verify that the memory consumption of my task would remain small enough for it to run on heroku. This is what I wanted to do :      write a unit test for this   run the task once to warm up the memory   run the task once for some small sample input and note the peak memory usage   run the task once for some large sample input and note the peak memory usage   check that the memory usages are very close, whatever the size of the inputs   Everything there is quite straightforward, appart from “note the peak memory usage”. Here is what I came up with      note the initial memory usage   start a thread that garbage collects and notes the memory usage every 10 ms   process the data   tell the thread to stop   memory usage is the difference between the maximum and initial memory usages   Here is the code in ruby, but it can be easily translated to any language (I did it for C# once)     Unfortunately in ruby, memory usage is not directly available without patching and rebuilding the interpreter, but allocated objects count is available, and it’s actually enough for our purpose.  ","categories": ["memory","ruby","testing"],
        "tags": [],
        "url": "/the-poor-mans-memory-profiling/",
        "teaser": null
      },{
        "title": "#1 rule for monitoring emails",
        "excerpt":"I just spent a few hours debugging my rails app on Heroku to understand why the hell I did not systematically receive the monitoring emails that my app was sending …   My app was actually rendering the email template.   Wether the mail came through actually depended on the mail content !   It seems there was a spam filter somewhere between the sender and the receiver that was blocking some emails, without warning me in any way. I was sending the email to a custom domain email from ‘OVH’ and then forwarding emails to my gmail account, it was handy to apply automatic labels.   Here is the #1 rule : always send your monitoring emails directly to the final recipient ! There will be less risk that some messages get lost.   I changed the recipient to my gmail address, updated my label filter, and everything is back to normal (a lot of time lost for nothing).  ","categories": ["web","exploitation"],
        "tags": [],
        "url": "/1-rule-for-monitoring-emails/",
        "teaser": null
      },{
        "title": "How to stub around a call to the original method with rspec ?",
        "excerpt":"Update 05/23/2014: I created a gem for this, read an introduction here   Rspec mocks now features a ‘and_call_original’ method to create simple proxy mocks. But how could we build more complex proxies ?   For the sake of the subject, let’s take an example that is not a testing best practice, but that everybody will understand. Suppose you want to simulate a faulty network in your tests. You’d like to stub Net::HTTP.get so that it raises errors from time to time. Here is what you could do   i = 0 original_get = Net::HTTP.method(:get) Net::HTTP.stub(:get) do |*args, &amp;block|   i = i+1   raise RuntimeError.new(\"network down\") if i%3 == 0   original_get.call(*args, &amp;block) end   Note the block is taken into account, in this example, it does not matter so much, but forgetting it can bring up really strange issues.  ","categories": ["ruby","rspec","testing"],
        "tags": [],
        "url": "/how-to-stub-around-a-call-to-the-original-method-with-rspec/",
        "teaser": null
      },{
        "title": "How to evaluate an xpath in chrome ?",
        "excerpt":"According to stack overflow, simply use something like the following from the chrome console :  $x(\"//img\")   It works great.  ","categories": ["web","chrome","html"],
        "tags": [],
        "url": "/how-to-evaluate-an-xpath-in-chrome/",
        "teaser": null
      },{
        "title": "Rubular: a really usefull ruby regex tool",
        "excerpt":"Working with regular expressions is always a try and fail and retry … experience. It really helps to have an interactive tool with which to tune up your expression. This is exactly what Rubular is made for. Give it a try next time you need to work with ruby regular expressions.  ","categories": ["ruby","regex"],
        "tags": [],
        "url": "/rubular-a-really-usefull-ruby-regex-tool/",
        "teaser": null
      },{
        "title": "If new cucumber transform breaks everyhing ...",
        "excerpt":"After reading The cucumber book I decided to add clever cucumber transforms but steps started to fail all over the place … Even completly unrelated scenarios were failing …   I should have read the Cucumber transforms doc page and particularly the “Transforms wisdom” section before anything, it would have been a real time saver. To summarise, when a step is executed, all transforms regexps are tried on the step captures, and the first matching transform is applied ! Inlining the transform global inside the step regex removes duplication, but in no way does it imply which transform will be applied !   For example   CAPTURE_NUMBER = Transform /^.*$/ do |digits|   Float(digits) end  When /^I withdraw (#{CAPTURE_NUMBER}) from \"([^\"]+)\"$/ do |amount, bank_name|   bank = Bank.find_by_name(bank_name)   bank.withdraw(amount) end   will match the bank name with CAPTURE_NUMBER, and you’ll get an “invalid value for Float” error.  ","categories": ["ruby","cucumber","testing","bdd"],
        "tags": [],
        "url": "/if-new-cucumber-transform-breaks-everyhing-dot-dot-dot/",
        "teaser": null
      },{
        "title": "Don't repeat names in cucumber scenarios",
        "excerpt":"Update 06/12/2014: I created a gem for this and other things   When the same name is repeated all over the place in a cucumber scenario, it can be difficult to read.  Scenario: Withdrawing some cash   Given a deposit account with 1000€   When I withdraw 100€ from the deposit account   Then there should be 900€ on the deposit account  It would be better if we could write it like that  Scenario: Withdrawing some cash   Given a deposit account with 1000€   When I withdraw 100€ from the account   Then there should be 900€ on the account  Sometimes we actually want to repeat the names though, either for clarity, or if we are dealing with many accounts within the same scenario.   To make both my scenarios more readable and my steps more versatile, I created special main_account_name accessors and a custom transform.  def main_account_name   @main_account_name ||= \"credit\" end def main_account_name=(account_name)   @main_account_name ||= account_name end  CAPTURE_ACCOUNT_NAME = Tranform(/^(a|an|the) *(.*) account$/) do |_prefix, account_name|   if account_name == \"\"     main_account_name   else     account_name   end end  When creating the account, I added some code to set the main_account_name  Given(/^(#{CAPTURE_ACCOUNT_NAME}) with (\\d+)€$/) do |account_name, amount|   ...   self.main_account_name= account_name end  It is then possible to write steps like  When(/^I withdraw (\\d+)€ from (#{CAPTURE_ACCOUNT_NAME})$/) do |amount, account_name|   ... end  that will match both “the deposit account” and “the account” depending on context.   It would be really nice to be able to write things like    Then there should be 900€ on it  but because of the way cucumber handles transforms, the only way I know to do that is to write a new step definition.  ","categories": ["ruby","testing","cucumber","bdd"],
        "tags": [],
        "url": "/dont-repeat-names-in-cucumber-scenarios/",
        "teaser": null
      },{
        "title": "Display full backtraces in rspec",
        "excerpt":"I use rspec a lot. I thinks it’s a great testing framework.   In order to get shorter error output, rspec cleans backtraces from outside code. Here are all the patterns that are removed from the full backtrace :   DEFAULT_BACKTRACE_PATTERNS = [   /\\/lib\\d*\\/ruby\\//,   /org\\/jruby\\//,   /bin\\//,   %r|/gems/|,   /spec\\/spec_helper\\.rb/,   /lib\\/rspec\\/(core|expectations|matchers|mocks)/ ]   Most of the time, that’s great. Sometimes though, we get messages that seem completly unrelated to the code. Even worse, we might get misleading messages, such as “method called with unexpected number of arguments” that does not refer to the code in the backtrace, but to some other library code …   I thought it would be nice if we could switch this cleaning off sometimes. Here is how I did this :   RSpec.configure do |config|    # RSpec automatically cleans stuff out of backtraces;   # sometimes this is annoying when trying to debug something e.g. a gem   if ENV['FULLBACKTRACES'] == 'true'     config.backtrace_clean_patterns = []   end    # some other configuration here  end   This way, it is still possible to call rspec as before to get the standard behaviour, but it is now possible to specify the FULLBACKTRACES variable to get full backtraces.   FULLBACKTRACES=true bundle exec rspec spec  ","categories": ["ruby","rspec","testing"],
        "tags": [],
        "url": "/display-full-backtraces-in-rspec/",
        "teaser": null
      },{
        "title": "Hitting the middle ground between classicist and mockist TDD",
        "excerpt":"From Martin Fowler’s point of view, I must have been a mockist. With using mocks extensively comes quite a few advantages :      test failures often pinpoint the falsy code   easier test organisation mimicking that of the code   faster tests   simpler test initialization   Mocks also have their own problems, but mostly :      especialy with dynamicaly typed languages, a mock for class A might not implement the same methods than the real class A, but the test might be passing though ! This ampers refactoring with a longer feedback loop and mock setup rewriting   That’s what always bothered me. Eventually I tried a combinaison of techniques that seem to work well together and provides most of the best of both worlds.      extensive use of factories (with FactoryGirl) to simplify setup   use of an in memory sqlite database to get a fast full working db   implement fully functional fakes for some parts of the system   carefull use of mocks, inspired from Gregory Brown’s thoughts on mocks            when a test is too slow       to cut off a dependency to a subsystem that is not available in a unit test       to simplify overly long test data setup           use of test proxies (as in rr) to inject specific behaviour or to perform specific checks without modifying the rest of the program.   Here is how I implemented this with rspec :    With all this in place, it is most of the time possible to write straightforward tests. For example here, only real objects are used. @order gets its value when Order.create! is called.   it \"should create an order with the cart\" do   capture_result_from(Order, :create!, into: :order)    check_in_cart    @order.should_not be_nil   @order.cart.should == @cart end   Maybe I should swith to rr …   EDIT 20/08/2014:   I eventually moved this code into its own gem, it’s on github.  ","categories": ["ruby","rspec","testing","mocking"],
        "tags": [],
        "url": "/hitting-the-middle-ground-between-classicist-and-mockist-tdd/",
        "teaser": null
      },{
        "title": "Matching meta tags with Capybara 2",
        "excerpt":"As I updated my bundle, some capybara have_selector(…) matches started to fail. Here was the message :   expected to find xpath \"//meta[@http-equiv='refresh']\" but there were no matches. Also found \"\", which matched the selector but not all filters. (Capybara::ExpectationNotMet)   After some searching, I eventually understood that it was a modification in the behaviour of Capybara 2 that only matches elements in the html body, and not in the head anymore. If trying to match the title, stackoverflow suggests to use :  expect(response).to have_title('My page')   To match meta tags, I had to resort to the following :  meta_refresh_tags = Nokogiri::HTML(page.source).xpath(\"//meta[@http-equiv='refresh']\") expect(meta_refresh_tags).not_to(be_empty, 'could not find a meta refresh tag')   I could do with something nicer, but it’s ok for the moment.  ","categories": ["testing","ruby","web"],
        "tags": [],
        "url": "/matching-meta-tags-with-capybara-2/",
        "teaser": null
      },{
        "title": "Be careful not to bundle FakeWeb in production",
        "excerpt":"For testing purpose, I added FakeWeb to my app. Later, I deployed it to a staging env on heroku to find out that my scrapper started to fail with some strange Net::HTTPForbidden were occuring after about 90 minutes of scrapping, with no clear reason. It turned that I had mistakenly added FakeWeb to all environments, and that just removing it fixed the issue !   As it is working now, I didn’t take the time to dig deeper into it …  ","categories": ["testing","ruby"],
        "tags": [],
        "url": "/be-careful-not-to-bundle-fakeweb-in-production/",
        "teaser": null
      },{
        "title": "Ditching autotest for guard",
        "excerpt":"I have been using autotest for 2 years, and it’s been great ! The first time I ran it I thought “This rocks !” and I have always been using it since …   A little later I read the book Continuous Testing: with Ruby, Rails and JavaScript and it suggested to use watchr instead. I never switched, partly because I did not take the time, and partly because watchr seemed to need quite a bit of manual configuration. Then, while contributing to other gems, I stumbled upon guard and this one seemed great.   Switching to guard was in fact very simple, It took about 15 minutes, a lot less than the time I had spent configuring or tweaking autotest. I simply added these to my Gemfile  gem 'guard' gem 'guard-rspec' gem 'guard-cucumber'  I ran the install steps, and everything was working ! It’s a shame I did not do the change earlier.   ","categories": ["testing","ruby"],
        "tags": [],
        "url": "/ditching-autotest-for-guard/",
        "teaser": null
      },{
        "title": "Jasmine and coffeescript setup for rails",
        "excerpt":"As I started to write more javascript code in my rails app, it became obvious that I should be testing it ! I wanted to use jasmine and coffeescript. I searched a long time to find out what was the most common and supported setup for a rails app, so now I recommand   gem 'jasmine' gem 'jasminerice' gem 'guard-jasmine'      The jasmine gem is by far the most common jasmine gem, it is supported by Pivotal Labs so there is no fear of it being droped soon.   The guard-jasmin gem is supported by MKSoft, and is not only handling continuous phantomjs headless jasmine testing through guard, but it also provides command line and rake tools to run your jasmine specs during continuous integration.   Eventually, the jasminerice gem makes it easy to write your jasmine specs with coffeescript. It also wraps a version of jasmine-jquery to assist client javascript testing. Although there is a “looking for maintainer” message on the README page, there are some recent commits, so it seems that pull requests are still being merged even if the project is not actively developpped anymore … I belive volunteers are welcome.   All 3 have detailed and up to date setup and usage instructions.  ","categories": ["ruby","testing","rails","javascript","coffeescript","jasmine"],
        "tags": [],
        "url": "/jasmine-and-coffeescript-setup-for-rails/",
        "teaser": null
      },{
        "title": "www.agileavatars.com : order customized avatar magnets",
        "excerpt":"As agile and lean methodologies are gaining some place inside the workplace, especially in large corporations, I heard and saw quite a few teams struggling with their whiteboard because of :      falling post-its   too few magnets   looking alike customized magnets   The best practice seemed to order magnetic paper, and loose half a day to create individual avatars, print them and stick them to small pieces of magnetic paper.   This is the main idea behind AgileAvatars.com. The main feature would be :      create the avatars for your whole team   pass a group order, and receive the whole package directly at work   For the moment, I am just testing the idea, so please enter your email in the contact form to push the product. Tweet about it and it will be even better !   If it turns out people are interested, I’ll start defining the MVP with the interested users :      How much should I charge to burn the magnets ?   Should it be a web or phone app ?   How to create the avatars ?  ","categories": ["agile","side-project","startup","agileavatars.com"],
        "tags": [],
        "url": "/www-dot-agileavatars-dot-com-order-customized-avatar-magnets/",
        "teaser": null
      },{
        "title": "My own side project best practices after reading the 'Side Project Book'",
        "excerpt":"First, I recommend this book to any side project infected person : it’s a great motivation boost !   After reading it, it seems that there are no absolute rules to side projects success. Some did no marketting and just did what they loved, others did à carefull market study before building anything, some did the whole things, others outsourced the code or the design, or even both … That said, as I read somewhere, it seems that one is more likely to achieve commercial success if he first focusses on the market, then marketting, then the design, and eventually on the features.   Common advices from the interviewees are :      ship early   iterate   don’t give up   Why not apply these principle to “building a side project” instead of “building XXX in my free time” ?   From now on, I am going to set up project target duration and revenue. For example, if I don’t make at least 100€ / month after one year, I’ll stop working on it and start a new one.   I hope that by iterating more quickly through projects I’ll learn more and increase my chances of successes as the time goes. Keeping the same target duration and revenue for all projects should make me better at filtering ideas that are a good fit to my resources. As I tend to get bored quickly, it should also allow me to keep my motivation high.   Let’s see what happens !  ","categories": ["boostrapping","side project","startup","book"],
        "tags": [],
        "url": "/my-own-side-project-best-practices-after-reading-the-side-project-book/",
        "teaser": null
      },{
        "title": "Simplest way to speed up rspec with in memory sqlite db",
        "excerpt":"There are already a lot of articles explaining how to setup an in memory SQLite database to speed up Rails specs or unit tests. Most of them explain how to change your database.yml and to run setup your schema before running the tests. It works fine.   There’s a catch though : suppose you are using cucumber, it’s likely you’d rather run cucumber on a real database (PostgreSQL, MySQL or whatever). Most gems expect cucumber and rspec to both run in the test environment … Every time I updated my bundle or that I wanted to use a new test gem, I would hit an issue about cucumber being run in its own ‘cucumber’ environment : unexpected warnings and things not working out of the box.   Eventually, I ditched the cucumber env, setup a PostgreSQL db on the test env, and injected the in memory sqlite database right inside spec_helper.rb :   In database.yml :   test:   adapter: postgresql   database: mes_courses_test   encoding: utf8   pool: 5   timeout: 5000   username: mes_courses   password: secret   host: localhost   port: 5433   At the bottom of spec_helper.rb   setup_sqlite_db = lambda do   ActiveRecord::Base.establish_connection(adapter: 'sqlite3', database: ':memory:')    load \"#{Rails.root.to_s}/db/schema.rb\" # use db agnostic schema by default   # ActiveRecord::Migrator.up('db/migrate') # use migrations end silence_stream(STDOUT, &amp;setup_sqlite_db)  ","categories": ["ruby","rails","rspec","testing","cucumber"],
        "tags": [],
        "url": "/simplest-way-to-speed-up-rspec-with-in-memory-sqlite-db/",
        "teaser": null
      },{
        "title": "Spork alternative compatible with cucumber",
        "excerpt":"Version 1.3.0 of Cucumber droped spork support, so I had to find something else. I am using Guard setup with rspec and Cucumber for my Rails app.   I first tried Zeus with guard-zeus, it kind of worked, but it ruined my Guard console with the Zeus server status, and it left zombie processes on guard exit …   Eventually, I tried Spring, and once I made sure rspec and cucumber both ran on the test environment, it worked fine with minimal configuration. Here is what I had to do to get it working      remove spork from your Gemfile   remove require ‘spork’, Spork.prefork and Spork.each_run calls from spec/spec_helper.rb and features/support/env.rb   install spring   gem install spring      add spring to your Gemfile ! Also they say it is not required I later had an error complaining it wasn’t.   group :test, :development do   ...   gem 'spring'   ... group      update your Guardfile example :   guard :rspec, cli: \"--tag ~@slow\", all_after_pass: true, all_on_start: false, keep_failed: true, spring: true, bundler: false do   ... end   guard 'cucumber', all_on_start: false, cli: '--format progress --no-profile', command_prefix: 'spring', bundler: false do   ... end      add a .spring.rb file to automaticaly restart Spring on main file changes. I think I’ll have to update this file when I discover that a test failed because spring needs to be restarted   Spring.watch \".spring.rb\" Spring.watch \"spec/factories\" Spring.watch \"features/env.rb\"  Spring.watch_method = :listen  ","categories": ["ruby","rspec","cucumber","testing"],
        "tags": [],
        "url": "/spork-alternative-compatible-with-cucumber/",
        "teaser": null
      },{
        "title": "Ruby regex captures oneliner",
        "excerpt":"Surely this post is nothing new for experienced ruby developpers, but I found it so handy, that I thought it deserved a post of its own though. The problem is :      How do I match and assign captures from a regex in a single line of code ?    Here is the idea :   &gt; brand, item = /([^,]*), (.*)/.match(\"APPLE, ipad\").captures =&gt; [\"APPLE\", \"ipad\"] &gt; brand =&gt; \"APPLE\" &gt; item =&gt; \"ipad\"   There are a few variations around this, like grouping some catpures in an array :   &gt; brand, *details = /([^,]*), ([^:]*): (.*)/.match(\"APPLE, ipad: iOs\").captures =&gt; [\"APPLE\", \"ipad\", \"iOs\"] &gt; brand =&gt; \"APPLE\" &gt; details =&gt; [\"ipad\", \"iOs\"]   Or to ignore some capture, we could shorten skip some captures :   &gt; item, details = /([^,]*), ([^:]*): (.*)/.match(\"APPLE, ipad: iOs\")[2..-1] =&gt; [\"ipad\", \"iOs\"] &gt; item =&gt; \"ipad\" &gt; details =&gt; \"iOs\"   But this only works for first or last captures, when this is not the case, one can also use the functional programming ‘_’ sink convention :   &gt; brand, _, details = /([^,]*), ([^:]*): (.*)/.match(\"APPLE, ipad: iOs\").captures =&gt; [\"APPLE\", \"ipad\", \"iOs\"] &gt; brand =&gt; \"APPLE\" &gt; details =&gt; \"iOs\"   One can even use ‘_’ multiple times on the same match   &gt; _, _, details = /([^,]*), ([^:]*): (.*)/.match(\"APPLE, ipad: iOs\").captures =&gt; [\"APPLE\", \"ipad\", \"iOs\"] &gt; details =&gt; \"iOs\"   I hope this helps.  ","categories": ["ruby","regex"],
        "tags": [],
        "url": "/ruby-regex-captures-oneliner/",
        "teaser": null
      },{
        "title": "RSpec matchers combinators",
        "excerpt":"Rspec matchers are a lot like predicates. Predicates that can talk … The good thing about predicates, as anybody who has done a bit of functional programming will tell you, is that they are easy to combine together into bigger predicates. I was really suprised to see that rspec does not come with such simple combinators as ‘and’ or ‘or’.   Here is a gist where I define simple combinators :     With this and the email_spec matchers, it is possible to write something like this to find if an email was sent by rails :   expect(all_emails).to have_any_that(and_(deliver_to(receiver),                                          have_subject(subject),                                          have_body_text(body)))   It enables expressive test code and high reusability.   Using the ‘all’ combinator it is possible to write things like that :   expect(sample_items_attributes).to all_ have_key(:price)   I had to prepend ‘all’ and ‘and’ with an underscore because I would otherwise get conflicts… I’m not overly satisfied with this, but it will do for the moment.   I was happily supprised by the readability of error messages when the mach fails. Rspec does a very good job about this. I think it would be possible to get even better error messages by explicity using the sub matchers messages though. If anyone is interested, help yourself !  ","categories": ["rspec","ruby","testing"],
        "tags": [],
        "url": "/rspec-matchers-combinators/",
        "teaser": null
      },{
        "title": "RSpec Matchers Combinators 2 : spec_combos gem",
        "excerpt":"I already wrote about my RSpec Matchers Combinators, I decided to extract them into the spec_combos gem.   To use it, just add the following to your Gemfile :   gem 'spec_combos', :group =&gt; 'test'   Compared to the first version of the matchers, I added better error messages. For example, consider the following complex nested assertion :   expect([1,2,3]).to all_{and_(be_odd, be_instance_of(Fixnum))}   It outputs the following error :   Failure/Error: expect([1,2,3]).to all_{and_(be_odd, be_instance_of(Fixnum))}   expected [1, 2, 3] to all be odd and be an instance of Fixnum, but the following were not:     [1] (2): expected 2 to be odd and be an instance of Fixnum, but:       expected odd? to return true, got false   I hope this might help !  ","categories": ["rspec","ruby","testing","gem","open source"],
        "tags": [],
        "url": "/rspec-matchers-combinators-2-spec-combos-gem/",
        "teaser": null
      },{
        "title": "I just replaced Feedburner with MailChimp",
        "excerpt":"Until now, I had been using Feedburner to manage the email subscriptions to my blog on mes-courses.fr. It had been working fine I thought it had been working fine until a friend of mine asked me if I was still working on it, since it had been a time since she did not receive any email !   It turned out that feedburner had not been forwarding emails for 6 months and that it had gone unnoticed … I had already had this issue before, and it was because my rss was not well formed according to feedburner (something related to unicode encoding I think). So to summarise :      I lost trust in feedburner to forward my posts correctly   Feedburner is not maintained by Google anymore   They have been persistant rumors about Google shutting down Feedburner completly especially since the Reader shut down   I started looking for alternatives. I tried Nourish and MailChimp I did not manage to make Nourish work like I wanted, but I found MailChimp an intuitive and well documented product.   The setup with MailChimp was a lot longer than with Feedburner, but the result is great. I feed a lot more confident about these emails being delivered now.   There are a few points I still miss though :      The social forward does not seem to work the same easy way, it looks like if MailChimp pushes the email content to your networks, but not a direct link to your blog   MailChimp does not track non-email subscribers to your rss. I’ll have to check my analytics to see how to do it.  ","categories": ["rss","web"],
        "tags": [],
        "url": "/i-just-replaced-feedburner-with-mailchimp/",
        "teaser": null
      },{
        "title": "AgileAvatars.com magnets for real",
        "excerpt":"A software team is now using Scrum and AgileAvatars.com magnets in their daily work ! A few days ago, I sold my first lot of agile magnets. These customers were ready to experiment and iterate, and after trying some things that did not work so well, we came to a great result :      As the surface of the magnets is rather small (4 by 4 centimeters), it required enough magnetic force to hold post-its and bristol cards to the board. I now use some 0.75 mm thick magnetic board, the same that is used to stick ads on driving cars   The magnets will be moved from hand to hand every day, I chose high quality glossy paper to make sure they’ll stand the test of time.      I did not yet build any automatic order site or app, but I can now ship magnets for real though ! Don’t hesitate to contact me through my email or the contact form on AgileAvatars.com if ever you want more !  ","categories": [],
        "tags": [],
        "url": "/agileavatars-dot-com-magnets-for-real/",
        "teaser": null
      },{
        "title": "How I organized my javascript assets in Rails",
        "excerpt":"Lately, I have been introducing javascript and coffeescript in www.mes-courses.fr (with structural architecture issues as a side effect, but that’s another story …).   Here are the general goals I had when using the rails asset pipeline :      most of js compiled in a single file   no js code in your html files : I want to use unintrusive javascript techniques, and have some outside js hook code that will wire js and html together   sometimes, I want some js code to be only explicitly included by some particular pages (example: hooking a timer on the document ready event of a particular html page). This means that I cannot simply compile all js code in a single minified file   In unit tests, I want to be able to manually set up and tear down the js hooks   So, here is how I eventually organized my js code :      I create a specific dependencies.js file that will require all outside dependencies   I split my js code under 3 subdirectories :            explicit : hooks that will be explicitly included       hooks : all the other hooks       application : all the real code           In my application.js, I include dependencies and all files under application and hooks. Here is my app/assets/javascript/application.js   //= require dependencies.js //= require_tree ./application //= require_tree ./hooks      In my tests, I only load dependencies and files from the application directory. Here is my spec/javascript/spec.js.coffee   #= require ../../app/assets/javascripts/dependencies.js #= require_tree ../../app/assets/javascripts/application #= require_tree ./      Hook code only consists of calling a setup function from the ‘real’ code. For example, here is my app/assets/javascript/hooks/cart_lines/index.coffee file   $(document).ready(window.mesCourses.cartLines.index.setUp)   this setUp function is defined in the app/assets/javascript/application/cart_lines/index.coffee file      This way, from the unit tests, I can explicitly call this setUp function, here is the beginning of my spec/javascript/application/cart_lines/index_spec.coffee file   describe 'mesCourses.cartLines.index', -&gt;    beforeEach -&gt;     window.mesCourses.cartLines.index.setUp()    ...   And you, how are you organizing your javacript assets ?  ","categories": ["rails","assets","testing","javascript"],
        "tags": [],
        "url": "/how-i-organized-my-javascript-assets-in-rails/",
        "teaser": null
      },{
        "title": "Trying to explain monads in java",
        "excerpt":"A few days ago, a colleague currently taking the coursera course about reactive programming in scala, asked me to explain him what monads are. It’s always a tough question, and I rarely manage to give un understandable answer simply. This time though, I kind of managed to pass him some understanding of monads :      When modelising a statefull data structure with immutable constructs, one has to pass in and return the data structure state of every function   This results in a lot repeated code to pass this state around   With a monad, you can factorize this glue code and only write the ‘real’ code   I thought it might be a good subject for a java kata ! This is what I tried to do in java-monads-kata. Here is some sample monadic code from the kata itself :   @Test public void pops_objects_in_reverse_push_order() {   monad = monad.     bind(push(A)).     bind(push(B)).      bind(pop());    assertEquals(B, monad.value);    monad = monad.bind(pop());   assertEquals(A, monad.value);    assertEquals(empty(), monad.stack); }   You can have a look at all the final code, or go through the whole history to get the ‘kata’ feeling. It’s a shame Github does not offer a nice chronological repo history slideshow, for better experience, I recommend that you use Chrome with Github improved : this allows to view diffs right from the Github history.   The resulting code is still quite far from a Haskell monad :      Functions are not first class objects in Java. It is written in Java 7, without lambda, which does not help neither.   Java does not have Haskell type class polymorphism, but only supports OO polymorphism and very little covariance   The whole monad thing, designed at simulating/isolating side effects has a WTF feel in Java where side effects are just everywhere   I’d love to hear some feedback about it.  ","categories": ["functional programming","monads","kata"],
        "tags": [],
        "url": "/trying-to-explain-monads-in-java/",
        "teaser": null
      },{
        "title": "Sprints are not sprints",
        "excerpt":"I really don’t know why Scrum Sprints are called sprints ! From my experience, the number one mistake made by team starting with Scrum is to work as quickly and dirty as possible to complete the sprint, forgetting the sustainable pace.      Finding another word is difficult though, I thought of ‘stage’ or ‘milestone’ that both convey the long run idea, but both feel more content than time bounded. A more exotic word could be a ‘Scrum push’, it conveys slow and intense action action rather than quick results.      Overall, the traditional agile ‘iteration’ is not bad at all, at least a lot better than Sprint.   EDIT 01/08/2014:   The ‘Quick and Dirty’ Sprint strategy, is like trying to win a marathon with a greedy algorithm:   while not finished   sprint(100m) end   Not likely to work … Marathoners know that they’ve got to stick to a constant speed during the whole race in order to finish it. The way to get faster is to :      increase this cruise speed just a bit   get at ease with it during a few races   repeat   Is there something to learn from this to improve software development speed ?  ","categories": ["agile","scrum"],
        "tags": [],
        "url": "/sprints-are-not-sprints/",
        "teaser": null
      },{
        "title": "Online store scrapping DSL gem",
        "excerpt":"Since I decided to stop Mes Courses to focus on AgileAvatars, I have been extracting open source gems from the code base. The last one is Storexplore : a declarative scrapping DSL that lets one define directory like apis to an online store.   As explained in the Readme, it allows one to declare a store this way :   Storexplore::define_api 'dummy-store.com' do    categories 'a.category' do     attributes do       { :name =&gt; page.get_one(\"h1\").content }     end      categories 'a.category' do       attributes do         { :name =&gt; page.get_one(\"h1\").content }       end        items 'a.item' do         attributes do           {             :name =&gt; page.get_one('h1').content,             :brand =&gt; page.get_one('#brand').content,             :price =&gt; page.get_one('#price').content.to_f,             :image =&gt; page.get_one('#image').content,             :remote_id =&gt; page.get_one('#remote_id').content           }         end       end     end   end end   And to use it like that :   Api.browse('http://www.dummy-store.com').categories.each do |category|    puts \"category: #{category.title}\"   puts \"attributes: #{category.attributes}\"    category.categories.each do |sub_category|      puts \"  category: #{sub_category.title}\"     puts \"  attributes: #{sub_category.attributes}\"      sub_category.items.each do |item|        puts \"    item: #{item.title}\"       puts \"    attributes: #{item.attributes}\"      end   end end   I tried my best to make this library high quality :      The code evolved from a simple procedural script to a dsl through constant refactorings   Real world features like constant memory usage have been added to fix production bugs   Documented with samples and rdoc   Extensive automated testing   Let’s hope it will be usefull for some.  ","categories": ["ruby","scrapping","dsl","gem","open source"],
        "tags": [],
        "url": "/online-store-scrapping-dsl-gem/",
        "teaser": null
      },{
        "title": "My humble advices about how to write maintainable tests",
        "excerpt":"I love writing automated tests … or rather, I hate having to work in untested code. I find it makes my life unnecessarily stressful. On the other hand, the cost of maintaining badly written tests can sometimes outweigh their benefits. This is usually the moment where the team resorts to manual testing, and gets back to the ways of ‘the good old days’. Personally, I don’t like the good old days when we had to stay up all night to add even more mess to fix something for an important deadline.   Here is how I try to make my tests as maintainable as possible :      Write the tests before the code, it gets a (short) time to get used to, but after that, it’s just a lot more fun. Just try it for a while   Write tests with no side effects, otherwise, it will not be possible to run your tests alone, or in a different order ! So don’t use globals   Write readable tests : did you ever had to fix a test that you cannot figure out the intent ? A lot of the other points just help writing more readable tests   Write small tests : they are usually faster to run, allow you to test more edge cases, and make a better job at pinpointing the actual faulty code. The receipe for writing short tests is to follow the given-when-then pattern :            start your test by setting the context (given)       do the thing you actually want to test (when)       verify that it did what you wanted (then)           Remove code duplication from your tests, in the same way as you would in production code. This will help you when you’ll want to modify that constructor that is used in 764 test files …   Use test data builders. This will avoid duplicated and long context setup at the beginning of every test. Don’t use factory methods or the object mother pattern, it just does not scale. In java, this usually means rollying your own, in ruby, just use Factory Girl   Use custom assertion objects. This will avoid duplicated and complicated verification code at the end of every test. It will also help to improve assertion messages. In ruby, this comes built in rspec and its matcher dsl. Lately, in Java I have been using Fest Assert   Use the extended red -&gt; red with explicit error message -&gt; green -&gt; refactor in place of the shorter red -&gt; green -&gt; refactor. By spending some time to improve your assertion messages, you’ll eventually save time to understand what broke when the test fails   As I already wrote about, only use mocks to            speed up a test that is too slow       cut off a dependency to an unavailable subsystem       simplify an overly complex test setup           Use constructor based dependency injection. It’s straightforward, low tech, and simplifies test setup   As there is no need to mock immutable data structures, I found that using immutable classes for values simplifies tests   Usually use hand coded mocks. Hand coded mocks become difficult to maintain when the code becomes too tangled, they can help me to know that I am doing something wrong (not mocking at the correct place, testing implementation, not doing enough refactoring …). On the contrary, mock framework make this so easy that I usually miss the issue completely until it is too late   Except when your mocking framework provides object proxing and automatic unmocking. Ruby’s RR provides this. Both features can be really useful when needed.   Use existing mocks when possible. For example, Sqlite in memory database : it speeds up the tests, removes the need for any environment setup, and is usually very simple to setup   Last of all, listen to your tests : if they get difficult to write, they might be a design improvement opportunity lying somewhere   All in all there is nothing new here. A lot of things come from GOOS others from Clean Code, the mocking ‘requirements’ come from an article from Gregory Brown, I found others from my own experience and from a lot of other sources I cannot remember now …   Happy testing !  ","categories": ["testing","agile","tdd"],
        "tags": [],
        "url": "/my-humble-advices-about-how-to-write-maintainable-tests/",
        "teaser": null
      },{
        "title": "Retroactively add keywords for your existing Octopress posts",
        "excerpt":"At the moment, I am exploring the world of SEO, and so I thought I could start with my blog. I found SEO for Octopress websites that I followed to add keywords and descriptions to this blog.   To fill actual keywords for all my existing posts, I had 2 options :      edit around 60 posts by hand   write a script to parse Yaml post descriptions and extract and inject keywords   Sorry, I chose the geeky solution …        Just add this code to your toplevel Rakefile, and run bundle exec rake add_keywords and keywords will be added to your existing posts.  ","categories": ["blogging","octopress","seo"],
        "tags": [],
        "url": "/retroactively-add-keywords-for-your-existing-octopress-posts/",
        "teaser": null
      },{
        "title": "Programming as an exponential problem",
        "excerpt":"As said Tom Cargill      The first 90% of the code accounts for the first 90% of the development time. The remaining 10% of the code accounts for the other 90% of the development time.    By extrapolation, this would mean that every time we increase the requirements by 10%, we need to double the total development time ! That would mean that solution complexity is an exponential function of the complexity of the problem.   That could explain why techniques that work well for small problems don’t work well at all for large problems, and vice et versa. For example   In the small (think one page script) | In the large (think multi millions lines system) :————————————:|:————————————————:  Dynamic typing                       | Static typing  Mutable globals                      | Immutability  Imperative style                     | Declarative style  Manual memory management             | Garbage collection  Shared memory                        | Message passing     Just for fun, let’s suppose that we could deduce a unique constant C for every language such that      Here is a plot of this formula with different values of C (0.5, 1 and 2)      We can see that small values of C are best for small problems, whereas greater values are evolve better with larger problems. For a given problem, there is quite a difference in the solution complexity, if the formula was true, and that we knew in which zone of complexity our problem will always be, we could choose the appropriate technology ! Experienced engineers already have the gut knowledge about how to chose the right tool for the job !   That’s not all, let’s have a bird’s eye view of the same formulas      I increased the maximum problem complexity by a factor of 3, I had to multiply the solution complexity by 100 ! In the end, these exponential curves all seem frighteningly vertical. This could explain why the divide and conquer approach works so well in software : 2e^x &lt; e^(2x). Abstract and powerful APIs might be our best weapon against complexity.   People behaviour does not match this exponential hypothesis though :      At work, I’ve seen quite a few projects started from scratch, and everybody expecting it to maintain it’s initial speed during its whole lifetime   Some recent hiring or investing trend seem to rely on hackathons, startup week ends, or coding games, all ‘in the small’ exercises   I’ve observed in quick and dirty overtime work to meet a deadline … If productivity is proportional to the solution complexity, that crunch mode would be completely unproductive   This leads to more interesting questions :      Is my exponential model complete garbage ?   Or are humans particularly bad at forecasting an exponential behaviour ?   If so, what practices could we adopt to stop relying on this misleading gut feeling ?  ","categories": ["programming","reflexion","complexity"],
        "tags": [],
        "url": "/programming-as-an-exponential-problem/",
        "teaser": null
      },{
        "title": "Harley Davidson programming",
        "excerpt":"   Maybe we should add motors to these desk chairs ;-)  ","categories": ["comic","joke","programming"],
        "tags": [],
        "url": "/harley-davidson-programming/",
        "teaser": null
      },{
        "title": "Auchandirect-ScrAPI : an unofficial API Ruby Gem",
        "excerpt":"Every brands should provide an API for developpers … unfortunately, it far from the truth right now. A few years ago, when I started my mes-courses.fr side project, I would have loved to find a french online grocery providing an open API. I had to resort to scrapping (that’s how I learnt that heavily relying on scrapping for a 15hr/week side project is not a good fit … but that’s another story).   As I am taking mes-courses.fr down, I have extracted the whole unofficial API I had built around http://www.auchandirect.fr (I’m talking to you french hackers !) into an open source Ruby Gem. Briefly :      It walks the whole store, from categories to items   Given valid credentials, it can fill and save a cart   It’s LGPL : anyone can use it as long as they give back any improvement to the community   It’s using Storexplore, another of my mes-courses.fr rip-off open source Ruby Gem   It’s tested on Travis and I’m currently trying to make it daily tested with Traviscron   There’s mainly one thing it cannot do :      It cannot procede to any payment or ordering   It’s available on Github   Happy scrapping !  ","categories": ["ruby","gem","scrapping","API"],
        "tags": [],
        "url": "/auchandirect-scrapi-an-unofficial-api-ruby-gem/",
        "teaser": null
      },{
        "title": "Automatic Travis daily build with Heroku scheduler",
        "excerpt":"As I just released auchandirect-scrAPI, and that it relies on scrapping, I needed a daily build.   The Travis team is already working on this, and I found a small utility app called TravisCron where anyone can register his repo for an automatic build.   Unfortunately, the feature is not yet ready in Travis, and the TravisCron guys did not yet activate my repo. After having a look at the TravisCron source code and the Travis API, I found out that it is really simple to do the same thing on my own.   That’s how I created daily-travis. It’s a tiny Rake task, ready to be pushed and automaticaly scheduled on heroku that will restart the latest build when run.   Details are in the README   @Travis : Thanks again for your service.     ","categories": ["travis","continuous-integration","open-source","heroku","rake"],
        "tags": [],
        "url": "/automatic-travis-daily-build-with-heroku-scheduler/",
        "teaser": null
      },{
        "title": "How we introduced efficient agile retrospectives",
        "excerpt":"6 months ago, our team started to run systematic iteration retrospectives. Within these 6 months, our team became more agile than ever. Running efficient retrospectives is what makes good teams great, it is what truly makes a team agile. Here is our story.   At the begining, we started with a standard retrospective format inspired from The Art Of Agile Development (A truly great book by the way). As I was used to running retrospectives, I did the first one. This is how it goes :      Repeat Norm’s Kerth’s prime directive to everyone (5 minutes) :      Regardless of what we discover today, we understand and truly believe that everyone did the best job they could, given what they knew at the time, their skills and abilities, the resources available, and the situation at hand.    If the message is awkward, the animator can explain that the point is to avoid counter productive blaming (even if it is unlikely to happen in your setting …)      Do some kind of brainstorming (10 minutes) :   One his own, everyone writes what went well on green post-its, and what did not went so well on blue post-its. Everybody sticks them up on the flipchart.      Group insights togethers (15 minutes) :   The animator reads every post-it aloud, asking team members for details, and tries to group notes together. When everyone is ok with the groups, dot vote : every team member gets 3 points, that he can assign as he wishes on any group. Here is what the board should look like at this point.         Find actions to improve the process (30 minutes) :   Pick up the 3 most voted issues, give everyone 5 minutes to think of useful actions to fix these. Then take another 5 minutes so that people can discuss their solutions in pairs, and another 5 minutes to do the same in groups of 4. Eventually, as the animator reports on the flipchart, do the same altogether. Make sure that the solutions are doable within the next sprint, if not discuss and split them until they are. Again, dot vote for the prefered actions.         Do it :   As soon as the retrospective is finished, the animator must enter the actions in the coming sprint backlog. These actions are not user stories, but they must be done. If there are not, the team cannot be agile as they will be no continuous improvement. Don’t assign any story points to these items, but let the velocity auto adjust for a given amount of improvements during every sprint.   After the first one, every team member animated such a retrospective, one after the other. When everyone was at ease with this, we changed the format. We bought Agile Retrospectives: Making Good Teams Great and everyone was responsible for designing his own retrospective session when his turn came. This allows different and various insights.   To conclude, here are a few retrospectives hints and guidelines      Everyone’s voice should be equal, everyone should feel free to talk   Don’t worry if good ideas are not selected the first time, they’ll come back   At the begining of every retrospective, review what happened about what was decided during the previous one   Use sticky flipcharts and stick them on the wall as you go through the different activities   Prepare flipcharts in advances to make the retrospective run more smoothly   Use colored post its to highlight different aspects (good things, bad things or whatever)   Use markers to write on post its (how can one read it from the back of the room otherwise ?)   If you haven’t yet, you have no excuses not to start now !  ","categories": ["agile","retrospectives","scrum","continuous improvement"],
        "tags": [],
        "url": "/how-we-introduced-efficient-agile-retrospectives/",
        "teaser": null
      },{
        "title": "Refactoring trick to insert a wrapper",
        "excerpt":"Last week at work, we decided that we needed an Anticorruption Layer between our code and another team’s. They have been using our internal data structures as they needed to, in an ad hoc way. This turned out to be an issue when we want to refactor our code. The goals of this layer are :      to provide an explicit API layer, controlling what is accessible from the outside   to allow us to improve our implementation independently of this API   The first step all the team agreed on is to provide direct wrappers around our classes. Unfortunately, some of these classes had more than a thousand references to it and our IDE does not provide any automated refactoring for this (introduce a wrapper class, and only use it in some part of the code). We found a trick ! Here it is :      Make sure you have a clean SCC state   Rename the class to be wrapped (let’s call it Foo) into FooWrapper   From SCC, revert the part of the code where you want to continue using Foo directly   In SCC, revert Foo and FooWrapper   Manually (re)create the FooWrapper class   Create FooWrapper.wrap(x) and FooWrapper.unwrap(x) methods   Fix all the compilation issues (mostly by calling wrap() and unwrap())   Run your tests and fix any remaining points   That saved us a whole lot of time. If your layer contains several classes with references between them, they is an optimal order through which to introduce the wrappers. Any order will work, but some will require more temporary calls to wrap and unwrap (step 7.). At the end, wrap() and unwrap() methods should only be called from within the layer.   Often you’ll find out that to complete the wrapping of a class, you’ll first need to wrap another class, you can :      Follow the mikado method strictly : upgrade your mikado graph, revert all your changes, and try to wrap this other class. It can seem slow, but it is completely incremental   Wrap the 2 classes at the same time : this is the best way when wrapping this other class is rather straightforward   Insert temporary calls to wrap() and unwrap() : they’ll be removed when you’ll later wrap the other class. This might be the only way if the classes have cyclic dependencies.   ","categories": ["refactoring","anticorruption layer","mikado-method"],
        "tags": [],
        "url": "/refactoring-trick-to-insert-a-wrapper/",
        "teaser": null
      },{
        "title": "Coding as a living can be dangerous to your health",
        "excerpt":"In 1991, John F. Woods wrote :      Always code as if the guy who ends up maintaining your code will be a violent psychopath who knows where you live.  Code for readability.    Here is how it might go :     ","categories": ["comic","joke","programming","clean code"],
        "tags": [],
        "url": "/coding-as-a-living-can-be-dangerous-to-your-health/",
        "teaser": null
      },{
        "title": "Better error messages when testing html views",
        "excerpt":"When testing html views, either from RSpec or from Cucumber, XPath can be really helpful to quickly find expected elements.   Unfortunately, a bit like regular expressions, when you start to use xpath to solve a problem, you often end up with 2 problems … Part of the reason is that xpaths tend to be cryptic. In the case of testing, error messages coming from unmatched xpath are even more crytic !   That’s why I had the idea for xpath-specs : a small gem that allows to associate a description with an xpath, to nest xpaths together, all this to simplify tests and assertion failure reporting.   For example, with an assertion like this :   expect(html).to contain_a(dish_with_name(\"Grilled Lobster\")   Here is the kind of failure message one can get :   expected the page to contain a dish that is named Grilled Lobster (//table[@id='dish-panel']//tr[td[contains(.,'#{name}')]])        it found a dish (//table[@id='dish-panel']//tr) :           &lt;tr&gt;&lt;td&gt;Pizza&lt;/td&gt;...&lt;/tr&gt;        but not a dish that is named Grilled Lobster (//table[@id='dish-panel']//tr[td[contains(.,'#{name}')]])   And here is the required setup :   # spec/support/knows_page_parts.rb  module KnowsPageParts   def dish     Xpath::Specs::PagePart.new(\"a dish\", \"//table[@id='dish-panel']//tr\")   end    def dish_with_name(name)     dish.that(\"is named #{name}\", \"[td[contains(.,'#{name}')]]\")   end end    Have a look at the readme for more details.  ","categories": ["ruby","rspec","html","matcher"],
        "tags": [],
        "url": "/better-error-messages-when-testing-html-views/",
        "teaser": null
      },{
        "title": "My new gem for creating rspec proxies",
        "excerpt":"I already wrote a lot about test proxies (here, here and here).   I just took the time to transform my previous gist in a full fledged ruby gem. It’s called “rspecproxies” and it can be found on github. It’s fully tested, documented and there’s a usage section in the readme to help anyone get started.   Here are the pain points proxies try to fix :      Without mocks, it is sometimes just awfully painfull to write the test (do you really want to start a background task just to get a completion ratio ?)   With classic stubs, you sometimes have to stub things you are not interested in in your test, you end up with unmaintainable extra long stub setup   Let’s have a look at a few examples of tests with proxies :      Verify actual load count without interfering in any behaviour   it 'caches users' do   users = User.capture_results_from(:load)    controller.login('joe', 'secret')   controller.login('joe', 'secret')    expect(users).to have_exactly(1).items end      Use proxies to stub an object that does not yet exist   it 'rounds the completion ratio' do    RenderingTask.proxy_chain(:load, :completion_ratio) {|s| s.and_return(0.2523) }     renderingController.show     expect(response).to include('25%') end   I’d really love to see more code tested with proxies, it makes the whole testing so much more natural. As with any testing techniques, we get more thorough testing from the ease of writing the test.  ","categories": ["ruby","rspec","testing","mocking","agile","tdd"],
        "tags": [],
        "url": "/my-new-gem-for-creating-rspec-proxies/",
        "teaser": null
      },{
        "title": "Cucumber_tricks gem : my favorite Gherkin and Cucumber tricks",
        "excerpt":"I just compiled my Gherkin and Cucumber goodies into a gem. It’s called cucumber_tricks and the source code can be found on github. It’s also tested on travis and documented in details on relish.   The goal of all these tricks is to be able to write more natural english scenarios. Here is an extract from the readme of the gem, which explains what it can do :   Use pronouns to reference previously introduced items   foo.feature   Given the tool 'screwdriver' When this tool is used   steps.rb   A_TOOL = NameOrPronounTransform('tool', 'hammer')  Given /^(#{A_TOOL})$/ do |tool|   ... end   Use the same step implementation to handle an inline arg as a 1-cell table   steps.rb   GivenEither /^the dog named \"(.*)\"$)$/,             /^the following dogs$/ do |dogs_table|   ... end   foo.feature   Given the dog \"Rolphy\" ... Given the following dogs   | Rex  |   | King |   | Volt |   Add default values to the hashes of a table   foo.feature   Given the following dogs   | names | color |   | Rex   | white |   | King  | Sand  |   steps.rb   Given /^the following dogs$$/ do |dogs|   hashes = dogs.hashes_with_defaults('names', 'tail' =&gt; 'wagging', 'smell' =&gt; 'not nice')  #  hashes.each do |hash| #    expect(hash['smell']).to eq('not nice') #  end    ... end   Define named lists from a table   foo.feature   Given the following dishes   | Spaghetti Bolognaise | =&gt; | Spaghetti | Bolognaise sauce |       |         |   | Burger               | =&gt; | Bread     | Meat             | Salad | Ketchup |   steps.rb   Given /^the following dishes$$/ do |dishes|   name_2_dishes = dishes.hash_2_lists  #  expect(name_2_dishes['Burger']).to eq(['Bread','Meat','Salad','Ketchup'])    ... end   Visit relish for more detailed documentation.  ","categories": ["ruby","cucumber","testing","gem"],
        "tags": [],
        "url": "/cucumber-tricks-gem-my-favorite-gherkin-and-cucumber-tricks/",
        "teaser": null
      },{
        "title": "Enabling agile practices and elephant taming",
        "excerpt":"Everybody knows about the agile software development promise “Regularly and continuously deliver value”. This is how it is supposed to work :      Iterative   Focusing on what is needed now   Release as soon as possible   Planning small stories according to the team’s velocity      It all seems common sense and simple. Especialy for people who don’t code. That’s not the whole story though, let’s have a look at a few variations :   Suppose a team uses Scrum but does not do any automated testing. As soon as the software will be used, bugs will create havoc in the planning. The velocity will quickly fall, within a few monthes, the team won’t be able to add any value. Surely, things could be improved with some rewrite and upfront design … this does not sound like Scrum anymore.   Now let’s suppose that another team is also using Scrum, uses automated tests, but missunderstood Sprint and KISS for quick-and-dirty-coding. Hopefully, this team won’t get too many bugs in production ! Unfortunately, any change to the source code will trigger hundreds of test failures : again, the velocity will decrease. I’ve been in such projects, in about 2 years, the team got really slow, and might eventually drop their test suit …   These two examples show that automated testing improves the situation, but also that it is not enough ! There are quite a few agile practices that are in fact enabling practices. These are the practices that are required for the process to accomplish the agile promise described at the begining of this article. Most come from eXtreme Programming and have been reincarnated through Software Craftsmanship. That’s what Kent Beck meant when he said that XP practices reinforce each other. Here are a few examples :   For example let’s take coding standards and pair programming which really seem to be a programmer choice. It turns out that they help to achieve collective code ownership. Which in turn helps to get ‘switchable’ team members. Which helps to make good team estimates. Which is required to have have a reliable velocity. Which is a must have to regularly deliver value on commitment !   It turns out that all of the other original XP practices help to achieve the agile promise.      After a lot of time spent writing software, I now tend to think of the code as the elephant in the room. It directly or indirectly constrains every decision that is make. Recognize and tame your elephant or you’ll get carted away …   … or dragged away …   … or trampled …  ","categories": ["agile","testing"],
        "tags": [],
        "url": "/enabling-agile-practices-and-elephant-taming/",
        "teaser": null
      },{
        "title": "RIP mes-courses.fr",
        "excerpt":"Rest In Peace mes-courses.fr. Here is what it looked like :      I wanted to create a really fast on-line grocery front-end, where people could shop for the week in only 5 minutes. It supported shopping recipes instead of individual items but I also envisioned to allow automatic menus recommendations, and automatic item preference selection. I started 4 years ago, and this is my last doing on the subject :). If you’re thinking about starting your own side project, this post is for you.   Here are the lessons I learned      As a professional programmer, I largely underestimated the non programming time required for a serious side project. It represents more than half the time you’ll spend on your project (marketing, discussing with people, mock ups and prototypes)   When I started, I kind of estimated the time it would take me to build a first prototype. Again I ridiculously underestimated this :            because of the previous point       because on a side project, you’ll be on your own to handle any infra issue       because you don’t have 8 hours per day to spend to your project (As a professional developer, dad of 2, I only managed to get 10 to 15 hours per week)           A small project does not require as much infrastructure as a big one. I lost some time doing things as I do when working on projects with more than 100K lines of code. So next time :            I’ll stick to end to end cucumber tests for as long as possible       I’ll use an economical framework like described in Donald G. Reinertsen’s Flow book to prioritize improvements vs features           Eventually, what killed me was that I could not go around the “experiment -&gt; adapt” loop fast enough. The project was just too big for my time            I’ll try to select a subject a project that suits my constraints of time and money       This will be one of the first hypotheses that I’m willing to verify       Web marketing and HTML design are more important than coding skills to run experiments : I’m learning both           Scrapping is a time hog. I won’t start any side project involving scrapping anymore.   Using on-line services always saved me a lot of time. They are a lot more reliable than anything I could setup. Mainly, this was :            Mailing services       Cloud deployment           Go the standard way. Again, anytime I did something a bit weird, it turned out to cost me some time            Use standard open source software, stick to the latest version       Use standard and wide spread technology           Automated testing and deployment saved me some time from the start. Especially with the small amount of time that I could spend on my project, it was really easy to forget details and to make mistakes.            Here is the Heroku deployment script I used to test and deploy in a single shell call       And here is a Heroku workaround to run some cron tasks weekly, this allowed me to run some scrapping tests every week on Heroku           It took all my time ! Next time I start a side project, I’ll be prepared to            Spend all my free time on it (my time was divided between day-job, family, side project)       Spend all my learning time (books, on-line trainings …) for it       Choose something that I am passionate about !       Choose a different kind of project to fit my constraints                    Joining an existing open source project would let me focus on technical work at my own pace           Volunteer for a not for profit project might be less time intensive while allowing some fulfilment                           I did my project alone, and it was hard to keep my motivation high on the long run. Next time :            I’ll join someone else       I’ll time box my project before a pivot or starting something completely different           I did not manage to get anything done before I settled a regular daily rhythm. I used to work from 5:30am to 7:30am, I first tried in the evening, but after a day’s work, I was too tired to be really productive.   When I could afford it, paying for things or services really saved me some time. I’m thinking of            A fast computer       Some paying on-line services           It is sure that doing a side project seriously is a heavy time investment, but there’s also a lot of benefits !      Here is what I gained      Real experience with new technologies. For me, this included            Ruby on Rails       Coffeescript       HTML scrapping       Dev-ops practices with Heroku       Web design with HTML and CSS           I also learned a lot of non technical skills in which I was completely inexperienced            Web marketing       Blogging       Mailing           Trying to bootstrap a for profit side project is like running a micro company, it’s a good opportunity to understand how a company is ran. This can help you to become a better professional during your day-job.   Having control on everything is a good situation to use Lean techniques.            Lean Start-up       Lean product development (Donald G. Reinertsen)           Failing allowed me to actually understand Lean Start up ! The ideas are easy to understand in theory, the practice is a very different thing. It should help me for my next project.   Resolving real problems on my own was a very good source for valuable blog articles.   I collaborated with very clever people on open source libraries            By fixing some bugs in some libraries I was using       By releasing some parts of my code as open source libraries           Next time, I hope I’ll get more euros as well !   You’ve got nothing to loose from trying ! Just do it. Give yourself 1 year to get some small success, and then continue or repeat with something else !  ","categories": ["side project","mes-courses","lessons-learned"],
        "tags": [],
        "url": "/rip-mes-courses-dot-fr/",
        "teaser": null
      },{
        "title": "The Holy Code Antipattern",
        "excerpt":"As I’ve encountered this situation in different disguise in different companies, I now assume it’s a widely applied antipattern.   Context   A team of programmers inherits a piece of code from one of their bosses. They find it really difficult to maintain : it is difficult to understand, fix, and change.   The Antipattern      As this piece of code seems too complex to be maintained by a team of simple programmers, as the boss, just forbid them :      to refactor any part of it   to rewrite it from scratch   to use something else instead   Consequences      This often limits the number of bugs that appear in this library, but …   It slows down development, because of the micro management required to enforce this pattern   It frustrates programmers, and it is likely that the best ones will leave   It prevents better design   Even worse, in the long run, it prevents great domain driven design to emerge from merciless refactoring   In the end, it makes the whole organization less performant   Examples           Your boss wrote something a few years ago, if the domain is more or less complex, the resulting code is complicated. The subject eventually got the reputation of being ‘touchy’. Your boss is the only person who effectively manages to change anything in there. He’s a bit afraid that by trying to improve it, the whole thing might just break down and become a bug nest. So, now that he has some authority, he forbids anyone to touch it. If a change is finally required, he’ll micro manage it !            Your big boss spent some over time on writing an uber-meta-generic-engine to solve the universe and everything. After seeing many developpers fixing the same kind of bugs over and over, he decides that it’s time to remove the dust from his compiler and that he starts building something that should solve the root cause of all these. In the spirit of the second system effect, he adds all bells and whistle to his beloved project, trying to incorporate a solution to every different issue he has seen during the last decade. This code grows and grows in total isolation of any real working software. When he eventually thinks it is ready, he justs drops the whole thing to your team, that is now responsible to integrate and use this thing in the running system. He’s micro managing the whole thing, and you don’t have any choice but to comply and succeed. This usually generates gazillions of bugs, makes projects really late and ruins the developpers’ lives.       Alternatives      Use collective code ownership so that knowledge about the code is shared by design   Trust programmers to design and architecture the system   Use constant refactoring to let tailor made domain driven designs emerge from the system  ","categories": ["code","antipattern","management"],
        "tags": [],
        "url": "/the-holy-code-antipattern/",
        "teaser": null
      },{
        "title": "The Flow book summary (lean software development part 1)",
        "excerpt":"A few weeks ago, I read The principles of product development flow from Donald G. Reinertsen.      I read it both for work and for my side projects, and I think it will be useful for both. The book is about lean product development, and is in fact a collection of 175 lean principles that one can study and understand in order to make better decisions when developing new products. The principles are divided into the following 8 categories      Economics   Queues   Variability   Batch sise   WIP constraints   Cadence, synchronization and flow control   Fast feedback   Decentralized control   I really loved the book. I have not been thrilled like that by a book since Kent Beck’s 1st edition of Extreme Programming Explained. Where Kent Beck described some values, principles and practices that work. D.G. Reinertsen has the ambition to help us to quantify these practices in order not move from belief based to fact based decisions. For this, he gives us the keys to creating an economical framework with which we should be able to convert any change option to its economical cost      Lately, I’ve been thinking of an economical framework of my own, that I could use on the projects I am currently involved in. This post is the first of a series about this :      The Flow book summary   Why eXtreme Programming works ?   How to measure your speed with your business value ?   Measure the business value of your spikes and take high payoff risks   What optimization should we work on ?   You don’t have to ask your boss for a fast build   A Plan for Technical Debt   How I’ll Measure the Lean Startup Value of Information in My Next Side Project   My Dream Lean Software Development Tool   Next part will feature an explanation of the XP practices with the lean principles. Stay tuned.  ","categories": ["agile","lean","book"],
        "tags": [],
        "url": "/the-flow-book-summary-lean-software-development_part_1/",
        "teaser": null
      },{
        "title": "Why eXtreme Programming works ? (Lean Software Development part 2)",
        "excerpt":"I’ve been programming for quite some time now, in different teams, using various methodologies. I also had the luck to do XP for at least 3 different projects. To me the conclusion is obvious, XP delivers more. Even better, programmers working with XP seem to be happier. The only thing I’ve seen that works better than XP, is fine tunning it once the team has mastered the main principles.      XP was first put in place at the Chrysler C3 project for SmallTalk performance issues. After being called for performance issues, Kent Beck discovered that these were only the tip of the iceberg, everything was going astray. As the expert in the room, people started to ask him how to organize. I remember reading some time ago that without having thought about it before, he gathered all the most efficient programming techniques he knew together into a new process. XP was born.   So the question is : what did Kent Beck put in XP so that it works so well ? Let’s go through the Flow book and its 175 lean product development principles, to see if we get some explanations.      Going through the 12 core XP practices, the main Scrum ceremonies and a few common additions, I’ll try to explain why they work through the Flow book’s principles.   Whole Team   This is the same thing as Pizza Team, Feature team, Cross Functional Team. It just means put everyone involved in the creation of the product in the same room (On site customer, sales, product people, programmers, quality controllers, operation people …).                  Ref       The principle of …       Summary       page                       B17       Proximity       Proximity enables small batch sizes       129                 W12       T-Shaped resources       Develop people who are deep in one area and broad in many       155                 W13       Skill overlap       Cross-train resources at adjacent processes       156                 F24       Alternate routes       Develop and maintain alternate routes around points of congestion       201                 F25       Flexible resources       Use flexible resources to absord variation       202                 FF14       Locality of feedback       Whenever possible, make the feedback local       226                 FF19       Colocation       Colocation improves almost all aspects of communication       230                 FF23       Overlapping measurement       To align behaviors, reward people for the work of others       233                 D7       Alignment       There is more value created with overall alignment than local execellence       252                 D13       Peer-level coordination       Tactical coordination should be local       257                 D18       Response frequency       We can’t respond faster than our (internal) response frequency       261                 D22       Face-to-face communication       Exploit the speed and bandwidth of face-to-face communications       263           Planning Game   The work is split into user stories. The customer then estimates the business value of each story, before the programmers poker estimate the required work for them. The game is then to maximize the scheduled business value creation for the coming iteration (1 to 3 weeks).                  Ref       The principle of …       Summary       page                       E4       Economic value-added       The value added by an activity is the change in the economic value of the work product       32                 E7       Imperfection       Even imperfect answers improve decision making       36                 E9       Continuous economic tradeoffs       Economic choices must be made continuously       37                 E10       Perishability I       Many economic choices are more valuable when made quickly       38                 E14       Market I       Ensure decision makers feel both cost and benefit       42                 E15       Optimium decision timing       Every decision has its optimum economic timing       44                 Q10       Queueing discipline       Queue cost is affected by the sequence in which we handle the jobs in the queue       69                 Q13       Queue size control I       Don’t control capacity utilization, control queue size       75                 Q14       Queue size control II       Don’t control the cycle time, control queue size       76                 V5       Variability pooling       Overall variation decreases when uncorrelated random tasks are combined       95                 V6       Short-term forcasting       Forecasting becomes exponentially easier at short time-horizons       96                 B18       Run length       Short run lengths reduce queues       130                 B20       Batch content       Sequence first that which adds value most cheaply       131                 W1       WIP constraints       Constrain WIP to control cycle time and flow       145                 W3       global constraints       Use global constraints for predictable and permanent bottlenecks       147                 W6       Demand blocking       Block all demand when WIP reaches its upper limit       151                 W8       Flexible requirements       Control WIP by shedding requirements       152                 W19       Adaptive WIP constraints       Adjust WIP constraints as capacity changes       162                 F5       Periodic resynchronization       Use a regular cadence to limit the accumulation of variance       177                 F7       The cadence reliability       Use cadence to make waiting times predictable       179                 F9       Cadenced meetings       Schedule frequent meetings using a predictable cadence       180                 F18       The local priority       Priorities are inherently local       196                 FF10       Agility I       We don’t need long planning horizons when we have a short turning radius       222                 FF21       Hurry-up-and-wait       Large queues make it hard to create urgency       232                 D4       Opportunity       Adjust the plan for unplanned obstacles and opportunities       249                 D14       Flexible plans       Use simple modular plans       258           Small Releases   Make a lot of small releases.                  Ref       The principle of …       Summary       page                       Q2       Queueing waste       Queues are the root cause of the majority of economic waste in product development       56                 V8       Repetition       Repetition reduces variation       99                 B1-8       Batch size       Reducing batch size reduces cycle time, variability in flow, risk, overhead and accelerates feedback, while large batches reduces efficiency, lower motivation and urgency and cause exponential cost and schedule growth       112-117                 F8       Cadenced batch size enabling       Use a regular cadence to enable small batch size       179                 FF7       Queue reduction by feedback       Fast feedback enables smaller queues       220                 FF8       Fast-learning       Use fast feedback to make learning faster and more efficient       220                 FF11       Batch size feedback       Small batches yield fast feedback       223                 FF20       Empowerment by feedback       Fast feedback gives a sense of control       231                 FF21       Hurry-up-and-wait       Large queues make it hard to create urgency       232                 D23       Trust       Trust is built through experience       264           Customer Tests   The customer assists the programmers into writing automated use case tests.                  Ref       The principle of …       Summary       page                       V16       Variability displacements       Move variability to the process stage where its cost is lowest       107                 B17       Proximity       Proximity enables small batch sizes       129                 F30       Flow conditioning       Reduce variability before a bottleneck       208                 FF7       Queue reduction by feedback       Fast feedback enables smaller queues       220                 FF8       Fast-learning       Use fast feedback to make learning faster and more efficient       220                 FF11       Batch size feedback       Small batches yield fast feedback       223                 FF14       Locality of feedback       Whenever possible, make the feedback local       226                 FF19       Colocation       Colocation improves almost all aspects of communication       230                 FF20       Empowerment by feedback       Fast feedback gives a sense of control       231                 D8       Mission       Specify the end state, its purpose and the minimum possible constraints       252                 D16       Early contact       Make early and meaningful contact with the problem       259           Collective Code Ownership   Every programmer is responsible to evolve and maintain all the source code, and not just his part.                  Ref       The principle of …       Summary       page                       Q7       Queuing structure       Serve pooled demand with reliable high-capacity servers       64                 W12       T-Shaped resources       Develop people who are deep in one area and broad in many       155                 F25       Flexible resources       Use flexible resources to absord variation       202                 FF23       Overlapping measurement       To align behaviors, reward people for the work of others       233                 D1       Perishablility II       Decentralize control for problems and opportunities that age poorly       246                 D4       Virtual centralization       Be able to quickly reorganize decentralized resources to create centralized power       250                 D5       Inefficiency       The inefficiency of decentralization (as opposed to silos) can cost less than the value of faster reponse time       251           Coding Standards   All programmers agree on coding conventions for all the source code they write.                  Ref       The principle of …       Summary       page                       W12       T-Shaped resources       Develop people who are deep in one area and broad in many       155                 F25       Flexible resources       Use flexible resources to absord variation       202           Sustainable Pace   As the value created by a knowledge work does not increase linearly with the time spent, it’s wiser to work a number of hours that both maximizes the work done while allowing the team to keep on going forever if needed.                  Ref       The principle of …       Summary       page                       E5       Inactivity       Watch the work product, not the worker       33                 Q3       Queueing capacity utilization       Capacity utilization increases queues exponentially       59                 B9       Batch size death spiral       Large batches lead to even large batches       118           Metaphor   Whether an actual metaphor or an ubiquitous language, the idea is to build a shared customer oriented architecture and design of the system.                  Ref       The principle of …       Summary       page                       W12       T-Shaped resources       Develop people who are deep in one area and broad in many       155                 F25       Flexible resources       Use flexible resources to absord variation       202           Continuous Integration   All the code of all the team is merged, tested, packaged and deployed very frequently (many times per day)                  Ref       The principle of …       Summary       page                       Q2       Queueing waste       Queues are the root cause of the majority fo economic waste in product development       56                 V8       Repetition       Repetition reduces variation       99                 B1-8       Batch size       Reducing batch size reduces cycle time, variability in flow, risk, overhead and accelerates feedback, while large batches reduces efficiency, lower motivation and urgency and cause exponential cost and schedule growth       112-117                 B12       Low transaction cost       Reducing the transaction cost per batch lowers overall costs       123                 B16       Transport batches       The most important batch is the transport batch       128                 B19       Infrastructure       Good infrastructure enables small batches       130                 F29       Resource centralization       Correctly managed, centralized resources can reduce queues       206                 FF7       Queue reduction by feedback       Fast feedback enables smaller queues       220                 FF11       Batch size feedback       Small batches yield fast feedback       223                 FF16       Multiple control loops       Embed fast control loops inside slow loops       228                 FF21       Hurry-up-and-wait       Large queues make it hard to create urgency       232           Test Driven Development   Programmers write failing tests (both customer and unit tests) before actual real code                  Ref       The principle of …       Summary       page                       V15       Iteration speed       it is usually better to improve iteration speed than defect rate       106                 V16       Variability displacements       move variability to the process stage where its cost is lowest       107                 B1-8       Batch size       Reducing batch size reduces cycle time, variability in flow, risk, overhead and accelerates feedback, while large batches reduces efficiency, lower motivation and urgency and cause exponential cost and schedule growth       112-117                 F30       Flow conditioning       Reduce variability before a bottleneck       208                 FF7       Queue reduction by feedback       Fast feedback enables smaller queues       220                 FF8       The fast-learning principle       Use fast feedback to make learning faster and more efficient       220                 FF11       The batch size principle of feedback       Small batches yield fast feedback       223                 FF14       The locality principle of feedback       Whenever possible, make the feedback local       226                 FF16       The principle of multiple control loops       Embed fast control loops inside slow loops       228                 FF20       The empowerment principle of feedback       Fast feedback gives a sense of control       231           Refactoring   Programmers improve the design of the system continuously, meaning in very frequent baby steps. This removes the need for a big design up front.                  Ref       The principle of …       Summary       page                       E13       The first decision rule principle       Use decision rules to decentralize economic control       41                 V9       The reuse principle       Reuse reduces variability       100                 B9       The batch size death spiral principle       Large batches lead to even large batches       118                 B19       The infrastructure principle       Good infrastructure enables small batches       130                 F28       The principle of preplanned flexibility       For fast responses, preplan and invest in flexibility       205                 D12       The second agility principle       Develop the ability to quickly shift focus       255           Simple Design   Do the simplest thing that could possibly work. No need to write things that don’t add business value yet. (Note that simple does not mean easy)                  Ref       The principle of …       Summary       page                       E19       Insurance       Don’t pay more for insurance than the expected loss       49                 V12       Variability consequences       Reducing consequences is usually the best way to reduce the cost of variability       103                 B9       Batch size death spiral       Large batches lead to even large batches       118                 B15       Fluidity       Loose coupling between product subsystems enables small batches       126                 W12       T-Shaped resources       Develop people who are deep in one area and broad in many       155                 F25       Flexible resources       Use flexible resources to absorb variation       202                 D12       Agility II       Develop the ability to quickly shift focus       255           Pair Programming   Programmers sit at the same computer in pairs to write code. One write the code, and the other comments. The keyboard changes hands very frequently.                  Ref       The principle of …       Summary       page                       B13       Batch size diseconomies       Batch size reduction saves much more than you think       124                 B21       Batch size I       Reduce the batch size before you attack bottlenecks       133                 W12       T-Shaped resources       Develop people who are deep in one area and broad in many       155                 W13       Skill overlap       Cross-train resources at adjacent processes       156                 F25       Flexible resources       Use flexible resources to absord variation       202                 F30       Flow conditioning       Reduce variability before a bottleneck       208                 FF14       Locality of feedback       Whenever possible, make the feedback local       226                 FF16       Multiple control loops       Embed fast control loops inside slow loops       228                 FF19       Colocation       Colocation improves almost all aspects of communication       230                 FF20       Empowerment by feedback       Fast feedback gives a sense of control       231                 D13       Peer-level coordination       Tactical coordination should be local       257                 D22       Face-to-face communication       Exploit the speed and bandwidth of face-to-face communications       263           Spikes   Programmers conduct time boxed experiment to gain insights                  Ref       The principle of …       Summary       page                       V2       Asymmetric payoffs       Payoff asymmetries enable variability to create economic value       88                 V7       Small experiments       Many small experiments produce less variation than one big one       98           Slack Time   Keep some buffer time at the end of the iteration where team members can either close the remaining stories or work on improvements.                  Ref       The principle of …       Summary       page                       V11       Buffer       Buffers trade money for variability reduction       101                 B9       Batch size death spiral       Large batches lead to even large batches       118                 B19       Infrastructure       Good infrastructure enables small batches       130                 F6       Ccadence capacity margin       Provide sufficient capacity margin to enable cadence       178                 D12       Agility II       Develop the ability to quickly shift focus       255                 D15       Tactical reserves       Decentralize a portion of reserves       258           Daily Stand Up Meeting   The whole team starts every working day by a quick synchronization meeting                  Ref       The principle of …       Summary       page                       B3       Batch size feedback       Reducing batch size accelerate feedback       113                 W12       T-Shaped resources       Develop people who are deep in one area and broad in many       155                 W20       Expansion control       Prevent uncontrolled expansion of work       163                 F5       Periodic resynchronization       Use a regular cadence to limit the accumulation of variance       177                 F9       Cadenced meetings       Schedule frequent meetings using a predictable cadence       180           Retrospective meeting   At the end of every iteration, the team meets for a retrospective, discussing what they did in order to improve                  Ref       The principle of …       Summary       page                       B9       Batch size death spiral       Large batches lead to even large batches       118                 B19       Infrastructure       Good infrastructure enables small batches       130                 F9       Cadenced meetings       Schedule frequent meetings using a predictable cadence       180                 FF8       Fast-learning       Use fast feedback to make learning faster and more efficient       220                 FF20       Empowerment by feedback       Fast feedback gives a sense of control       231                 D21       Regenerative initiative       Cultivating initiative enables us to use initiative       263           Demos   At the end of every iteration, the team demonstrates what it did to the customer                  Ref       The principle of …       Summary       page                       E14       Market I       Ensure decision makers feel both cost and benefit       42                 B3       Batch size feedback       Reducing batch size accelerate feedback       113                 B9       Batch size death spiral       Large batches lead to even large batches       118                 F9       Cadenced meetings       Schedule frequent meetings using a predictable cadence       180                 FF7       Queue reduction by feedback       Fast feedback enables smaller queues       220                 FF8       Fast-learning       Use fast feedback to make learning faster and more efficient       220                 FF20       Empowerment by feedback       Fast feedback gives a sense of control       231                 FF21       Hurry-up-and-wait       Large queues make it hard to create urgency       232                 FF23       Overlapping measurement       To align behaviors, reward people for the work of others       233                 D23       Trust       Trust is built through experience       264           Visual Whiteboard   Display the stories of the current sprint on the wall in a 3 columns whiteboard (TODO, DOING, DONE)                  Ref       The principle of …       Summary       page                       W23       Visual WIP       Make WIP continuously visible       166                 F27       Local transparency       Make tasks and resources reciprocally visible at adjacent processes       204                 D17       Decentralized information       For decentralized decisions, disseminate key information widely       260           Conclusion   Whaoo that’s a lot ! I did not expect to find so many principles underlying XP (I even removed principles that were not self explanatory). For the XP practitioner that I am, writing this blog post helped me to deepen understanding of it. As XPers know, XP is quite opiniated, it’s both a strength and a weakness if you try to apply it outside of its zone of comfort. This explains why some lean subjects are simply not addressed by XP.   To summarize, here is where XP hits the ground :      In spite of its image of ‘a process for nerdy programmers’ XP turns out to be a quite evolved lean method !   XP anihilates batch size and feedback time   Pair programming is well explained   And here is where to look at when you’ll need to upgrade XP      Better tradeoffs might be found with a real quantitative economical framework   Synchronization principles might help working with other teams   Kent Beck could not have read the Flow book when he invented XP, but it seems he just had a bit of advance on the rest of us …   This was part 2 of my suite of article about Lean Software Development, Part 1 was The Flow book summary, Part 3 will be How to measure your speed with your business value ?  ","categories": ["lean","agile","extreme programming"],
        "tags": [],
        "url": "/why-extreme-programming-works-lean-software-development-part-2/",
        "teaser": null
      },{
        "title": "How to measure your speed with your business value ? (Lean Software Development Part 3)",
        "excerpt":"There is a french idiom that basicaly is      No use to run, all that is needed is to start on time …    an agile engineer would add      … and to go in the good direction    Indeed, velocity or mean cycle time as speed measures have their shortcomings :      Can be falsified by story point inflation !   Does not tell the team or its product owner whether they are working on the right thing.   Wouldn’t it be great if we could track the business value we are creating instead ? Wouldn’t it be more motivating for developpers to know how they are contributing to the bottom line ? Wouldn’t it help various people to align inside the organization ?   How to track and prioritize based on your business value   From Reinertsen’s Flow book, we learned that the cost of delay is the main driver of the value creation : the faster you deliver a feature, less you spend the cost of delay of that feature, the more value you are creating for your company. This article suggests that the cost of delay can be computed with the following formula :      cost of delay = f(user business value) + g(time criticality) + h(risk reduction opportunity)    This other article suggests that they are different types of tasks that corresponds to the different terms of the formula above.      Here is how we could link the 2 articles :      Stories with deadlines : either through legal or market constraints, not doing this will put you out of business (‘time criticality’ in the formula)   Stories that influence the bottom line : by increasing the sales figures when delivered, or decreasing them when not delivered, which is kind of the same (‘user business value’ in the formula)   Risk reduction tasks : by mitigating risk or streamlining the process, these tasks actually improve the bottom line of other stories (‘risk reduction opportunity’ in the formula)   The later type of task will be detailed in other articles. Let’s focus on the two others.   The case of the deadlined feature ?   First, I’d say its business value is 0, until it’s too late. You should not be working on it too soon, but you should not be working on it too late neither !   In his book The Art of Agile Development James Shore details in great details how an agile team can commit to deliverables (I really recommend this part, I might even write a post about it). He explains that in order to commit, teams should multiply their estimates by 4, or by 1.8 if they are very rigourous in their application of all the XP practices.   So a rule to handle such a task could be to      estimate it   multiply that estimate by 4   substract this time from the deadline   prioritize it so that it can be started at that date, but not earlier   don’t expect to be creating any value by completing these stories   What’s the business value of other typical user stories   This article suggests that in this case the cost of delay is equal to the business value of the feature for the user. But how can we have an idea of its actual user business value ?   Before actually selling and getting the money, it’s just an estimation. With the good information, some people will make better estimates than others, never the less, it’s still an estimate. Let’s try a “Business Value Poker” ! Here are a few ideas about how to conduct this:      Estimate business value at the same time as you estimate the complexity of a story   Create some business value $ poker estimate cards, write an app for this, or bring in some Poker Chips to estimate the value   Invite some business people (sales, marketting …) to the meeting to get real knowledge (being organized as a feature team will help)      At the end, when you have the estimated cost of delay and duration of every task, you can directly prioritize using the WSJF (Weighted Shortest Job First) :      WSJF = Cost of Delay / Duration    Just do the tasks by decreasing order of WSJF.   At the end of the sprint, just as we track the story points we completed with the velocity, we could track the business value we created, that would be our business value speed. If you have access to real sales numbers, it might be interesting to see if it’s possible to correlate the figures.   Afterthoughts   The more I learn about Lean principles, the more I find our current Issues Tracking Systems (I’m used to Jira) limited. They seem to be databases with a nice UI, whereas what we need are tools to help us to make better decisions out of the multitude of items … How come they do not provide something as simple as the WSJF ?   Edit 12/09/2014   I got some pretty positive feedback from practicing these business value pokers. Inviting the product owner forced him to explain in details why he believed some features were more valuable than others. On the opposite, it allowed the developpers to hightlight how some seemingly unimportant stories were critical to a long term goal. In the end, everyone, including the product owner, is asking for more. It’s a good practice that helps introducing the business value / cost of delay concept.   This was part 3 of my suite of article about Lean Software Development, Part 2 was Why eXtreme Programming works ?, Part 4 will be Measure the business value of your spikes and take high payoff risks.  ","categories": ["agile","lean","business value"],
        "tags": [],
        "url": "/how-to-measure-your-speed-with-your-business-value-lean-software-development-part-3/",
        "teaser": null
      },{
        "title": "How to setup a weekly fruit basket in no time",
        "excerpt":"   If you’re interested in agile software development, just read Kent Beck’s Extreme Programming Explained : Embrace Change 1st edition. It’s only 200 pages, it was written in 1999, but it’s still tremendously relevent, and it’s got the highest ratio of information per page on the subject.   If you actually read it, you’ll notice that Kent emphasizes about having food at the office. He claims that it improves the moral and builds team cohesion.   As my company already provides free drinks, my first attempt was to asked for weekly fresh fruits baskets. They are currently experimenting regular self service fruit baskets deliveries in some offices. Unfortunately not in ours yet. Let’s hope it changes soon. Meanwhile, we decided to handle the thing ourselves.   Here comes the fruit basket lean startup !   First, let’s setup the simplest way it could possibly work      Invest 10€   Buy some fruits from the closest shop   Put them in a basket next to my desk   Let people buy them for 50c each   Leave a plastic cup next to the basket to receive the money   Hold the accounting public and visible in your wiki for example   Repeat every monday      Then, verify that it is sustainable   It turns out it works fine !   … Until someone started to steal some money !   If we forgot to hide the money cup before we left in the evening, obvious isn’t it ? We tried the following, in that order :      Setup an automatic reminder to hide the money before leaving … FAIL   Setup a fake webcam and a warning notice … FAIL   Only keep 1€ worth of change in the money cup, and repeatedly lock up the rest in a safe place … SUCCESS !   With just a bit of time, anyone, anywhere can setup a fresh fruit basket at work. It does improve the moral and build the team.  ","categories": ["team building","workplace"],
        "tags": [],
        "url": "/how-to-setup-a-weekly-fruit-basket-in-no-time/",
        "teaser": null
      },{
        "title": "Performance is a feature",
        "excerpt":"Now that is a widespread title for blog articles ! Just search Google, and you’ll find “Performance is a feature” in Coding Horror and others.      What’s in it for us ?   If performance is indead a feature, then it can be managed like any feature :      It should result from use cases            During use case X, the user should not wait more than Y seconds for Z            It can be split into user stories           Story 1: During use case X, the user should not wait more than 2*Y seconds for Z     Story 2: During use case X, the user should not wait more than Y seconds for Z         They can be prioritized against other stories           Let’s forget about performance for now and deliver functionality A as soon as ready, we’ll speed things up later.     Let’s fix basic performance constraints for use case X for now, every story will have to comply with these constraints later.         The performance on these use cases should be automatically tested and non regressed           If we slow things too much and these tests breaks, we’ll have to optimize the code.     But as long as we don’t break the tests, it’s ok to unoptimize the code !      Maybe that’s a chance to stop performance related gut feeling quarrels !  ","categories": ["agile","testing","performance"],
        "tags": [],
        "url": "/performance-is-a-feature/",
        "teaser": null
      },{
        "title": "Can agile teams commit?",
        "excerpt":"Making commitments to deliver software is always difficult. Whatever the margin you might take, it invariably seems wrong afterward …   Most Scrum, XP or Kanban litterature does not adress the issue, simply saying that commitment is not required, as long as you continuously deliver value (faster than your competition). That’s kind of true, but sometimes you need commitments, for example if your customer is not yet ready to deploy your new software every friday worldwide …   So how can you do it while remaining agile ?   Grossly speaking, you have 2 options :   Do it as usual   Discuss with your experts, take some margin, do whatever voodoo you are used to. This will not be worse than it used to be. It might turn out better, thanks to your agile process, you should be able to deploy with a reduced the scope if needed.      Use your agile process metrics   This technique is explained in The Art of Agile Development, in section “Risk Management”, page 227.      Let’s estimate the time you’ll need before the release      First, list all the stories you want it your release   Then estimate them with story points.   Now that you have the total number of story points you’ll have to deliver before the release, apply a generic risk multiplier :                  Chances of making it       Using XP practices       Otherwise       Description                       10%       x1       x1       Almost impossible (ignore)                 50%       x1.4       x2       50-50 chance (stretch goal)                 90%       x1.8       x4       Virtually certain (commit)           As explained in The Art of Agile Development page 227, these numbers com from DeMarco’s Riskology system. Using XP practices means fixing all bugs at all iteration, sticking rigorously to DONE-DONE, and having a stable velocity over iterations.   This factor will account for unavoidable scope creep and wrong estimations.     Use you iteration velocity to know how many sprints you’ll need to finish.   For example :   Suppose you have 45 stories that account for a total of 152 story points, and that your velocity is 23 story points per iteration. You need to do a commitment, but hopefully, you are applying all the XP practices. So you can compute :      Number of sprints = 152*1.8/23 = 12 sprints, (24 weeks, or about 5.5 months)    What about unknown risks ?   Unfortunately, using just the previous, you might miss some unavoidable important tasks you’ll need to do before you can release. Think about monitoring tools and stress testing, when did your product owner prioritize these ? These are risk management activities that need to be added to your backlog in the first place. Here is how to list most of them.      Do a full team brainstorming about anything that could possibly go bad for your project   For every item discovered, estimate            It’s probability of occurrence (low, medium, high)       It’s consequences (delay, cost, cancellation of the project)           For every item, decide whether to            avoid it : you have to find a way to make sure this will not happen       contain it : you’ll deal with the risk when it occurs       mitigate it : you have to find a way to reduce it’s impact       ignore it : don’t bother with unlikely risks of no importance           Finally, create stories to match your risk management decisions. These might be :            Monitoring systems helps to contain a risk       Logging helps to mitigate a risk       An automated scaling in script for situations of high demand helps both mitigate and contain the risk           Simply add these stories to your backlog, and repeat the previous section. You can now make your commitment   Afterthoughts   Contrary to the widespread belief, agile practices and metrics actually helps to make commitments.   It would be better if we had project specific statistics instead of generic risk multipliers. It’s a shame that task tracking tools (JIRA &amp; friends) still don’t help us with this.   We should keep in mind though, that estimating all your backlog in advance takes some time and is actually some kind of waste. If possible, just sticking to build (and sell) the thing that is the most useful now is more simple (this guy calls it drunken stumble).    Tech Mesh 2012 - Building an Application Platform: Lessons from CloudBees - Garrett Smith from Erlang Solutions on Vimeo.  ","categories": ["agile","planning","risks"],
        "tags": [],
        "url": "/can-agile-teams-commit/",
        "teaser": null
      },{
        "title": "From Zero to Pair Programming Hero",
        "excerpt":"In my team at Murex, we’ve been doing Pair programming 75% of our time for the past 9 months now.      Before I explain how we got there, let’s summarize our observations :      No immediate productivity drop   Pair programming is really tiring   Quality expectations throughout the team soared up   As a result, the quality actually increased a lot   But existing technical debt suddenly became incompatible with the team’s own quality criterion. We went on to pay it back, which slowed us down for a while   Productivity is regularly going up as the technical debt is reduced   It helped us to define shared coding conventions   Pair programming is not for everyone. It has likely precipitated the departure of one team member   It certainly helped the team to jell   Newcomers can submit code on day 1   The skills of everyone increase a lot quicker than before   Bonus : it improves personal skills of all the team members   If you are interested in how we got there, read on, here is our story :   Best Effort Code Reviews   At the beginning, only experienced team members were reviewing the submitted code, and making remarks for improvement over our default review system : Code Collaborator.   This practice revealed tedious, especially with large change lists. As it was not systematic, reviewers constantly had to remind to the reviewees to act on the remarks, which hindered empowerment.   Systematic Code Reviews   Observing this during a retrospective, we decided to do add code review to our Definition Of Done. Inspired by best practices in the Open Source world, we created a ruby review script that automatically creates Code Collaborator reviews based on the Perforce submits. Everyone was made observer of any code change, and everyone was to participate in the reviews.   At first, to make this practice stick, a few benevolent review champions had to comment all the submitted code; once the habit was taken, everyone participated in the reviews.   Code Collaborator spamming was certainly an issue, but using Code Collaborator System Tray App helped each of us to keep up to date with the remaining reviews to do.   Bonus : As everyone was doing reviews, and that reviews of small changes are easier, submits became smaller.      This was certainly an improvement, but it remained time consuming. We believed we could do better.   Pair Programming   After 1 or 2 months of systematic code reviews, during a retrospective (again) nearly all the team members decided to give pair programming a try.   We felt the difference after the very first day : pair programming is intense, but the results are great. We never turned back.   With pair programming in place, we had to settle a pair switching frequency. We started with the full story, tried a one day rotation, and eventually settled on “MIN(1 week, the story)”.   This is not set in stone and is team dependent. It may vary depending on the learning curve required to work on a story. We might bring it down later maybe.   Remote Pair Programming   Ahmad, a Murex guy from Beirut joined the team a few months ago. We did not want to change our way of working, and decided to try remote pair programming.   Initial Setup   At the beginning, we were using Lync (Microsoft’s chat system) with webcams, headphones and screen sharing. It works, but Lync’s screen sharing does not allow seamless remote control between Paris and Beirut. Here is how we coped with this :      Only exceptionally use Lync’s “Give Control” feature : it lags too much   Do small submits, and switch control at submits   When you cannot submit soon, shelve the code on perforce (you would just pull your buddy’s repo with git), and switch control   As a result, Ahmad became productive a lot more quickly. We are not 2 sub teams focusing on their own area of expertise, but 1 single distributed team sharing everything.      Improved Setup   Remote pair programming as such is workable, but does not feel as easy as being collocated. Here are a few best practices we are now using to improve the experience :      Keep your pair’s video constantly visible : either on your laptop of in a corner of you main screen, but it’s important to see his facial expression all the time   In order to allow eye contact, place your cam next to the window containing the video of your pair.   Using 2 cameras, ManyCams and a small whiteboard allows to share drawings !         We started using the Pomodoro technique for control switching. MarinaraTimer provides an online and shareable timer.   We also started maintaining a todo list with an online shareable editor such as CollabEdit or Google Docs.   Future Setup   We are currently welcoming a new engineer in Beirut, and as we will be doing more remote pair programming, we’ll need to make this more seamless. Control sharing and lag through Lync remain the main issues. We don’t have a solution for that yet, but here are the fixes we are looking into      Saros is an Eclipse plugin for remote concurrent and real time editing of files. Many people can edit the files at the same time. We are waiting for the Intellij version that is still under development         Floobits is a commercial equivalent of saros. We tried it and it seems great. It’s not cheap though, especially with in-house servers.   Screenhero is a commercial low-lag, multi cursor screen sharing tool. Unfortunately, it currently does not work behind a proxy, and so we were not able to evaluate it yet.   Final thoughts   I believe that collocated, and remote, pair programming are becoming key skills for a modern software engineer.   I hope this will help teams envisioning pair programming. We’d love to read about your best practices as well !  ","categories": ["agile","remote","pair programming","code reviews"],
        "tags": [],
        "url": "/from-zero-to-pair-programming-hero/",
        "teaser": null
      },{
        "title": "Measure The Business Value of Your Spikes and Take High Payoff Risks (Lean Software Development Part 4)",
        "excerpt":"Lately at work, we’ve unexpectedly been asked by other teams if they could use our product for something that we had not forseen. As we are not sure whether we’ll be able to tune our product to their needs, we are thinking about doing a short study to know the answer. This looks like a great opportunity to try out Cost of Delay analysis about uncertain tasks.   Unfortunately, I cannot write the details of what we are creating at work in this blog, so let’s assume that we are building a Todo List Software.   We have been targeting the enterprise market. Lately, we’ve seen some interest from individuals planning to use our todo list system for themselves at home.   For individuals, the system would need to be highly available and live 24/7 over the internet, latency will also be critical to retain customers, but the product could get a market share with a basic feature set.   On the other side, enterprise customers need advanced features, absolute data safety, but they can cope with nightly restarts of the server.   In order to know if we can make our todo list system available and fast enough for the individuals market, we are planning to conduct a pre-study, so as not to waste time on an unreachable goal. In XP terms, this is a spike, and it’s a bunch of experiments rather than a theoretical study.      When should we prioritize this spike ?   If we are using the Weighted Shortest Job First metric to prioritize our work, we need to estimate the cost of delay of a task to determine its priority. Hereafter I will explain how we could determine the value of this spike.   Computing the cost of delay   The strategy to compute the cost of delay for such a risk mitigation task is to compute the difference in cost of delays with or without doing it.   1. The products, the features, the MVP and the estimates   As I explained in a previous post, for usual features, cost of delay is equivalent to it’s value. Along with our gross estimates, here are the relative values that our product owner gave us for the different products we are envisioning.                  Feature       $ Enterprise       $ Individuals       Estimated work                       Robustness       20*       20*       2                 Availability       0       40*       2                 Latency       0       40*       1                 Durability       40*       13       2                 Multi user lists       20*       8       2                 Labels       20       13       2                 Custom report tool       13       0       3                 TOTAL Cost Of Delay of v1       80       100                   Stared (*) features are required for the first version of the product. Features with a value of 0 are not required for the product. Eventually, unstared features with a non null business value would be great for a second release.   It seems that the individuals market is a greater opportunity, so it’s worth thinking about it. Unfortunately for the moment, we really don’t know if we’ll manage to get the high availability that is required for such a product.   The availability spike we are envisioning would take 1 unit of time.   2. Computing the cost of delay of this spike   The cost of delay of a task involving some uncertainty is the probabilistic expected value of its cost of delay. We estimate that we have 50% of chances of matching the availability required by individuals. It means that CoD of the spike = 50% * CoD if we match the latency + 50% CoD if we don’t match the availability.   2.a. The Cost of Delay if we get the availability   Let’s consider the future in which we’ll manage to reduce the latency. The cost of delay of a spike task is the difference in Cost with and without doing the spike, per relevent months.   2.a.i. The cost if we don’t do the spike   Unfortunately, at this point in this future, we don’t yet know that we’ll manage to get to the availability.                  Feature       $ Enterprise       $ Individuals       $ Expected       Estimated work       WSJF                       Latency       0       40*       20       1       20                 Durability       40*       13       26       2       13                 Robustness       20*       20*       20       2       10                 Availability       0       40*       20       2       10                 Labels       20       13       17       2       9                 Multi user lists       20*       8       14       2       7                 Custom report tool       13       0       8       3       3           We’ll resort to WSJF to prioritize our work. Here is what we’ll be able to ship :                  Product       Delay       CoD       Cost                       Individuals       7       100       700                 Individuals Durability       7       13       91                 Individuals Labels       9       13       117                 Enterprise       11       80       880                 Enterprise labels       11       20       220                 Individuals Multi user lists       13       8       104                 Enterprise Custom reports       16       13       208                                         2320           2.a.ii. The cost if we do the spike   In this case, we would start by the spike, and it would tell us that we can reach the individuals availability and so that we should go for this feature first. Here will be our planning                  Feature       $ Enterprise       $ Individuals       Estimated work       Enterprise WSJF       Individuals WSJF                       Feasibility spike                       1                                 Latency       0       40*       1               40                 Availability       0       40*       2               20                 Robustness       20*       20*       2       10       10                 Durability       40*       13       2       20       7                 Multi user lists       20*       8       2       10       4                 Labels       20       13       2       10       7                 Custom report tool       13       0       3       4                   Here is how we will be able to ship :                  Product       Delay       CoD       Cost                       Individuals       6       100       600                 Individuals Durability       8       13       104                 Individuals Multi user lists       10       8       80                 Enterprise       10       80       800                 Individuals Labels       12       13       156                 Enterprise Labels       12       20       240                 Enterprise Custom reports       15       13       195                                         2175           2.a.iii. Cost of delay of the spike if we reach the availability   By making the spike, we would save 2320 - 2175 = 145$   Without doing the spike, we would discover whether we would reach the availability when we try it, around time 7 (see 2.a.i).   So the cost of delay for the spike would be around 145/7 = 21 $/m   2.b. The Cost of Delay if we don’t get the availability   Let’s now consider the future in which we don’t manage to increase the availability.   Using the same logic as before, let’s now see what happens   2.b.i. The cost if we don’t do the spike   Unfortunately, at this point in this future, we don’t yet know that we’ll not manage to get to the availability.                  Feature       $ Enterprise       $ Individuals       $ Expected       Estimated work       WSJF                       Latency       0       40*       20       1       20                 Durability       40*       13       26       2       13                 Robustness       20*       20*       20       2       10                 Availability       0       40*       20       2       10                 Multi user lists       20*       8       14       2       7                 Labels       20       13       17       2       9                 Custom report tool       13       0       8       3       3           When we’ll fail at the availability, we’ll switch multi user lists and labels to be able to ship to enterprises as quickly as possible. Here is what we’ll ship.                  Product       Delay       CoD       Cost                       Enterprise       9       80       720                 Enterprise Labels       11       20       220                 Enterprise Custom reports       14       13       182                                         1122           2.b.ii. The cost if we do the spike   In this case, we would start by the spike, and it would tell us that we won’t match the availability required for individuals and so that that there’s no need to run after this now.                  Feature       $ Enterprise       Estimated work       WSJF                       Feasibility spike               1                         Durability       40*       2       13                 Robustness       20*       2       10                 Multi user lists       20*       2       7                 Labels       20       2       9                 Custom report tool       13       3       3           Here is how we will be able to ship :                  Product       Delay       CoD       Cost                       Enterprise       7       80       560                 Enterprise Labels       9       20       180                 Enterprise Custom reports       12       13       156                                         896           2.b.iii. Cost of delay of the spike if we reach the availability   By making the spike, we would save 1122 - 896 = 226$   As before, without doing the spike, we would discover whether we would get the availability when we try it, around time 7.   So the cost of delay for the spike is around 226/7 = 32 $/m   2.c. Compute overall Cost of Delay of the Spike      Given that we estimate that there is a 50% chances of making the latency, the overall expected cost of delay is   50% * 21 + 50% * 32 = 26.5 $/m   Inject the spike in the backlog   With the Cost of Delay of the spike, we can compute it’s WSJF and prioritize it against other features.                  Feature       $ Enterprise       $ Individuals       Expected $       Estimated work       WSJF                       Feasibility Spike                       26.5       1       26.5                 Latency       0       40*       20       1       20                 Durability       40*       13       26       2       13                 Robustness       20*       20*       20       2       10                 Availability       0       40*       20       2       10                 Multi user lists       20*       8       14       2       7                 Labels       20       13       17       2       9                 Custom report tool       13       0       8       3       3           The spike comes at the top of our backlog. Which confirms our gut feeling.   Conclusion   Doing this long study confirmed classic rule of thumbs      Don’t develop many products at the same time   Do some Proof Of Concepts early before starting to work on uncertain features   Tackle the most risky features first   By improving the inputs, we could get more quality results :      If we had access to real sales or finance figures for the costs   If we did some sort of poker risk estimation instead of just guessing at 50% chances   Obviously, the analysis itself is not perfect, but it hints to the good choices. And as Don Reinertsen puts it, using an economical framework, the spread between people estimations goes down from 50:1 to 2:1 ! This seems a good alternative to the experience and gut feeling approach which :      can trigger heated unfounded discussions   often means high dependence on the intuition of a single individual   As everything is quantitative though, one could imagine that with other figures, we could have got to another conclusion, such as :      The spike is not worth doing (it costs more than it might save)   The spike can wait a bit      This was part 4 of my Lean Software Development Series. Part 3 was How to measure your speed with your business value, continue on Part 5 : What optimization should we work on ?.  ","categories": ["lean","agile","extreme programming"],
        "tags": [],
        "url": "/measure-the-business-value-of-your-spikes-and-take-high-payoff-risks-lean-software-development-part-4/",
        "teaser": null
      },{
        "title": "Setting up Octopress with Vagrant and rbenv",
        "excerpt":"I recently got hands on an abandonned laptop that was better than the one I was currently using for my personnal hackings, so I decided to switch to this one. I felt this was the time to learn Vagrant and save me some time later on. I settled on creating a Vagrant environment for this Octopress blogging. That proved a lot longer than I thought it would.      If you want to jump to the solution, just have a look at this git change. Here is the slightly longer version.           Add a Vagrantfile and setup a VM. There are explainations about how to do this all over the web, that was easy.            Provision your VM. That proved a lot more complex. There are a lot of examples using variants of Chef, but the steep learning curve for Chef seemed unneccessarily complex compared to what I wanted to do. Eventually, I figured it out using simple shell provisioning.         config.vm.provision \"shell\", inline: &lt;&lt;-SHELL     echo \"Updating package definitions\"     sudo apt-get update      echo \"Installing git and build tools\"     sudo apt-get -y install git autoconf bison build-essential libssl-dev libyaml-dev libreadline6-dev zlib1g-dev libncurses5-dev libffi-dev libgdbm3 libgdbm-dev   SHELL    config.vm.provision \"shell\", privileged: false, inline: &lt;&lt;-SHELL     git config --global user.name \"john.doe\"     git config --global user.email \"john.doe@mail.com\"      if [ ! -d \"$HOME/.rbenv\" ]; then       echo \"Installing rbenv and ruby-build\"        git clone https://github.com/sstephenson/rbenv.git ~/.rbenv       git clone https://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build        echo 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' &gt;&gt; ~/.bashrc       echo 'eval \"$(rbenv init -)\"' &gt;&gt; ~/.bashrc      else       echo \"Updating rbenv and ruby-build\"        cd ~/.rbenv       git pull        cd ~/.rbenv/plugins/ruby-build       git pull     fi      export PATH=\"$HOME/.rbenv/bin:$PATH\"     eval \"$(rbenv init -)\"      if [ ! -d \"$HOME/.rbenv/versions/2.2.0\" ]; then       echo \"Installing ruby\"        rbenv install 2.2.0       rbenv global 2.2.0        gem update --system       gem update        gem install bundler       bundle config path vendor/bundle        rbenv rehash     fi      cd /vagrant     bundle install      if [ ! -d \"/vagrant/_deploy\" ]; then       bundle exec rake setup_github_pages[\"git@github.com:philou/philou.github.com\"]       git checkout . # Revert github deploy url to my domain       cd _deploy       git pull origin master # pull to avoid non fast forward push       cd ..     fi   SHELL           Setup port forwarding. That should have been simple … after forwarding port 4000 to 4000, I could still not manage to access my blog preview from the host machine. After searching throughout the web for a long time, I eventually fixed it with by adding --host 0.0.0.0 to the rackup command line in Octopress Rackfile            Setup ssh forwarding. In order to be able to deploy to github pages with my local ssh keys, I added the following to my Vagrantfile.         # The path to the private key to use to SSH into the guest machine. By   # default this is the insecure private key that ships with Vagrant, since   # that is what public boxes use. If you make your own custom box with a   # custom SSH key, this should point to that private key.   # You can also specify multiple private keys by setting this to be an array.   # This is useful, for example, if you use the default private key to   # bootstrap the machine, but replace it with perhaps a more secure key later.   config.ssh.private_key_path = \"~/.ssh/id_rsa\"    #  If true, agent forwarding over SSH connections is enabled. Defaults to false.   config.ssh.forward_agent = true      Fix virtual box synced folder. When I tried to pimp my favicon up, changing the png in the host machine did not update it on the guest ! I lost almost 3 hours figuring this out … searching google, I eventually found that Virtual Box synced folders can have issues, and that installing the guest additions is recommended. For this, just enter the following in the command line from your project’s working dir :   vagrant plugin install vagrant-vbguest vagrant reload   I’ll tell you if this does not do the trick.   I admit it was a lot longer than I expected it to be, but at least now it’s repeatable !      Next steps will be to use Docker providers and Dockerfile to factorize provisioning and speedup up VM startup.  ","categories": ["octopress","ruby","vagrant"],
        "tags": [],
        "url": "/setting-up-octopress-with-vagrant-and-rbenv/",
        "teaser": null
      },{
        "title": "What Optimization Should We Work On (Lean Software Development Part 5)",
        "excerpt":"At work, we are building a risk aggregation system. As it’s dealing with a large bunch of numbers, it’s a huge heap of optimizations. Once that its most standard features set is supported, our job mostly consists of making it faster.   That’s were we are now doing.      How do we choose which optimization to work on ?   The system still being young, we have a wide range of options to optimize it. To name just a few : caches, better algorithms, better low level hardware usage …   It turns out that we can use the speedup factor as a substitute for business value and use known techniques to help us to make the best decisions.   Let’s walk through an example   I. List the optimizations you are thinking of   Let’s suppose we are thinking of the following 3 optimizations for our engine      Create better data structures to speed up the reconciliation algorithm   Optimize the reconciliation algorithm itself to reduce CPU cache misses   Minimize boxing and unboxing   II. Poker estimate the story points and speedup   Armed with these stories, we can poker estimate them, by story points and by expected speedup. As a substitute for WSJF, we will then be able to compute the speedup rate per story point. We will then just have to work on the stories with the highest speedup rate first.                  Title       Story Points       /10       /2       -10%       ~       +10%       x2       x10       Expected Speedup ratio*       Speedup rate / story point**                       Data Structures       13                                       4 votes       5 votes               x 1.533       x 1.033                 Algorithm       13               1 vote       1 vote       2 votes       1 vote       2 votes       2 votes       x 1.799       x 1.046                 Boxing       8                                       9 votes                       x 1.1       x 1.012           * Expected speedup ratio is the logarithmic average of the voted speedups  ** Speedup rate is “speedup(1/ story points)“   So based on speedup rate, here is the order in which we should perform the stories :      Algorithm   Data Structures   Boxing   III. And what about the risks ?      This poker estimation tells us something else …      We don’t have a clue about the speedup we will get by trying to optimize the algorithm !    The votes range from /2 to x10 ! This is the perfect situation for an XP spike.                  Title       Story points       Expected Speedup rate                       Algorithm spike : measure out of context CPU cache optimization speedup       2       ?           In order to compute the expected speedup rate, let’s suppose that they are 2 futures, one where we get a high speedup and another where we get a low one.   They are computed by splitting the votes in 2 :      low_speedup = 0.846   high_speedup = 3.827   If the spike succeeds   We’ll first work on the spike, and then on the algorithm story. In the end, we would get the speedup of the algorithm optimization.      spike_high_speedup = high_speedup = 3.827   If the spike fails   We’ll also start by working on the spike. Afterwards, instead of the algorithm story, we’ll tackle another optimization stories, yielding our average speedup rate for the duration of the algorithm story. The average speedup rate can be obtained from historical benchmark data, or by averaging the speedup rate of the other stories.      average_speedup_rate = (1.033 * 1.011)1/2 = 1.022   spike_low_speedup = average_speedup_ratestory_points = 1.02213 = 1.326   Spike speedup rate   We can now compute the average expected speedup rate for the full period ‘spike &amp; algorithm’ stories. From this we will be able to get the speedup rate and finally, to prioritize this spike against the other stories in our backlog.      spike_speedup = (spike_low_speedup * spike_high_speedup)1/2 = 2.253   spike_speedup_rate = spike_speedup1/(spike_story_points + algorithm_story_points) = 2.2531/(2 + 13) = 1.056   IV. Putting it all together   Here are all the speedup rate for the different stories.                  Title       Speedup rate / story point                       Data Structure       x 1.033                 Algorithm       x 1.046                 Boxing       x 1.012                 Algorithm spike       x 1.056           Finally, here is the optimal order through which we should perform the stories :      Algorithm spike   Algorithm (only if the spike proved it would work)   Data Structures   Boxing   Summary   The math are not that complex, and a simple formula can be written to compute the spike speedup rate :      I think most experienced engineers would have come to the same conclusion by gut feeling …   Nevertheless I believe that systematically applying the such method when prioritizing optimizations can lead to a greater speedup rate than the competition in the long run. This is a perfect example where taking measured risks can payoff !   This was part 5 of my Lean Software Development Series. Part 4 was Measure the business value of your spikes and take high payoff risks, Part 5 will be You don’t have to ask your boss for a fast build.  ","categories": ["agile","lean","extreme programming","performance"],
        "tags": [],
        "url": "/what-optimization-should-we-work-on-lean-software-development-part-5/",
        "teaser": null
      },{
        "title": "Trellospectives : Remote Retrospectives with Trello",
        "excerpt":"As a distributed team working from Paris and Beirut, after pair programming, it was time for our retrospectives to get remote !      Initial setup   At first we were using the visio conference system. The retrospective facilitator would connect with the remote participants through instant chat and forward theirs post-its. We also used an extra webcam connected to the laptop in order to show the whiteboard in the other room.   Pros      Anyone can do it now   Kind of works   Cons      We often used to loose 5 minutes setting all the infrastructure up   The remote room cannot see the board clearly through the webcam   The animator has to spend his time forwarding the other room’s points   There is a ‘master’ and a ‘slave’ room   Sensei Tool   When Mohamad joined the team in Beirut, we thought that this was not going to scale … We decided to try something else. With the availability of the new conferencing system, we had the idea to use a web tool to run the retro. We found and tried senseitool.com. After creating accounts for every member of the team and scheduling a retrospective through the tool, we could all equally participate using our smartphones. The retrospective follows a typical workflow that is fine for teams new to the practice.   Pros      Even easier to setup   Works fine   Cons      The website was a bit slow   The retrospective was too guided for an experienced team, we did not get as good outputs as we used to   Everyone could participate as easily   Trello      Asking Google, we discovered that some teams were having success using Trello for their remote retrospectives. We decided to give it a try. Ahmad from Beirut got to work on our first retrospective with it. He had to prepare it beforehand (as we always have been doing). In practice :      Ahmad created an organization for our team   We all registered to Trello and joined the organization (we like joining (smile))   Ahmad created a custom board for each activity   During the meeting, we used the video conferencing system and the instant chat to have both visio and screen sharing   The animator used a laptop to manage the Trello boards   Everyone of us could add post-its through his smartphone app   Pros      The setup is easy   The retrospective worked well and delivered interesting output   We actually all see the board   The smartphone app works well   It is possible to vote directly through Trello   Everyone could participate as easily   We can classify post-its with labels   We can insert pictures and photos   There are a lot of chrome extensions to Trello (Vertical Lists for Trello), Card Color Titles for Trello   Cons      There is nothing to ‘group’ post its together   We need to prepare custom boards for every activity   We would need to pay for the gold version with custom backgrounds and stickers   Conclusion      While missing a few features that would make it awesome, Trello is the best tool we found for remote retrospective, and is better than our initial physical setup. We’re continuing to use it, and we now have to figure out      If we could find a way to speed up the meeting preparation   How to handle ‘graph oriented’ activities such as the ‘5 whys’     ","categories": ["agile","remote","retrospectives","continuous improvement"],
        "tags": [],
        "url": "/trellospectives-remote-retrospectives-with-trello/",
        "teaser": null
      },{
        "title": "Real Programmers Have Todo Lists",
        "excerpt":"EDIT 2017-03-03 : This article is old, I am now using an improved Jira Personnal Kanban.   EDIT 2015-08-18 : This article is old, I am now using Personnal Kanban instead of TODO lists.   Productive programmers maintain a todo list. No Exception.      Why is it so important   As programmers, here is the typical internal discussion we have all day long :      - Why the hell am I doing this again ?  … hard thinking …  - Yes ! I remember now :  - Encapsulate this field  - In order to move it to another class  - In order to move this other function there too  - In order to be able to remove that other static variable  - In order to refactor the login module  - In order to remove the dependency between the custom search query generator and the login module  - In order to refactor the query generator  - In order to be able to optimize it  - In order to speed up the whole website !    Phew, now that’s a list ! A 9 frame stack, all in our heads, and that’s only a simple example. Knowing that us humans usually have around 7 ‘registers’ in our brains, this makes a lot of clutter to remember.   Maintaining all this in a todo list frees us some brainpower !   What happens when you use a todo list   Quite a lot in fact :         It’s satisfying to check something as done !   Our programming gets better, because we can fully concentrate on it   We have a clear idea about what’s done, what’s still to be done, and why we are doing it   We avoid getting lost in things that don’t really need to be done   We can make better choices about what to do, what to postpone, and what not to do   We can make more accurate estimates about the time it will take to finish the job   In the end, all this makes you feel less stressed and more productive !   How to do it   There are many ways to maintain a todo list. Which to choose is not as important as having one. Here are my 2 design principles for a todo list system :      It goes in finer details than a typical bug tracking software   It should help you to concentrate on the few items you can do in the coming hours   For example, I am now using a simple TODAY … TOMORROW … LATER … scheme. I tend to avoid deep hierarchies as it gets in the way of my second principle. I like to keep DONE items visible to keep track of what I did for up to 1 day.      Here is a list of tools you can use to set up a todo list :      Any text editor using a simple format convention will do   Dropbox or any other synchronization tool can be helpful to access it from different places   Org Mode of Emacs has built-in support for todo lists. It’s a simple text file, but with color highlighting and shortcuts   Google Keep might do just fine for you   Google Docs can also be useful, especially if you need to share your todo list with others (when pair programming for example)   Trello is also a good one, it can even be used as a personal kanban board   Any other todo list tool that suits you !   If you are not already using a todo list, start now and become more productive ! No excuse !  ","categories": ["programming","personal-productivity"],
        "tags": [],
        "url": "/real-programmers-have-todo-lists/",
        "teaser": null
      },{
        "title": "The Agile Change Management Viral Hack",
        "excerpt":"We just discovered a hack to spread agile (BDD by the way) through an organization. It works provided :      There is a BDD testing expert in your team   Your team is using the work of another software team from your company   If this team does not use agile practices, they are likely to regularly create regressions or to be late at providing new versions.   Use your client role in the relation, and propose your help ! Spend some time with them to help them put automated customer tests in place. Be genuinely nice with them, show example, be available and, obviously, bring improvement. With some chance, they might soon be asking for more.   Given there are too many bugs When you can help Then DO IT !  ","categories": ["agile","testing","coaching"],
        "tags": [],
        "url": "/the-agile-change-management-viral-hack/",
        "teaser": null
      },{
        "title": "You don't have to ask your boss for a fast build (Lean Software Development part 6)",
        "excerpt":"A slow build costs money. I mean it costs a whole lot of money all the time !   Spending some time to speed up the build is like an investment, you’ll pay some money now, but then it’s only a matter of time until you get a return on investment. Here is the trick, if you manage to get it quickly, no one will even notice that you spent some time making the build faster !   With a bit of maths, you can even get what Reinertsen calls a “Decentralized Decision Rule”, making it possible for anyone in the organization to figure out if he should spend some time on the build or not; without the need to ask the permission to anyone.      Our example   Our team is constituted of 5 pairs, each running the build at least 10 times per day. Let’s figure out the value of 1 minute build time speed-up      The whole team would save : 1m x 5 pairs x 10 builds = 50 minutes per day   In a 2 weeks sprint, this would amount to around 1 day of work   This means that if a pair spends half a day to get a 1 minute build speed-up, it would not change the output of the sprint, and it would in fact increase the throughput of the team for further sprints.   Anyone in our team that spots a potential 1 minute build time speed-up that would take less that 1 man.day to implement should do it right away, without asking the permission to anyone   Other Benefits   A direct benefit is that the issue will not have to be re-discussed every time someone spots a build time improvement. This will save some management time, and more build speed-up actions will eventually be undertaken.   The astute lean reader will have noticed that I completely ignored the second effect of fast feedback :      if the build is faster   we will run it more often   we’ll spot errors earlier   less errors will be submitted   the overall throughput will be increased even more   Another hidden benefit concerns the Cost of Delay (the cost of not selling the product NOW). As Cost of Delay typically trumps the work costs, this means that any improvement to the build time will bring even greater ROI in the long term.   Variations      If your sponsor agrees, you can negotiate a longer return on investment period for your decision rule. For example, if he agreed to increase the horizon to 2 sprints, we could undertake more build time speed-up tasks. You might also prefer only to discuss really long ROI investments with him.   While designing the 777 Boeing used a similar decision rule to meet the required weight of the plan : any engineer could increase the production cost of 300$ provided it saved a pound of weight on the plane. This fixed issues they previously had with department weight budgets and escalation.   Finally, it would be great if we had the same rule for technical debt ! Imagine that you knew both the costs of fixing and not fixing your technical debt, you could then decided whether it makes sense to work on the debt right now or not. But that’s for a later experiment.   This was part 6 of my Lean Software Development Series. Part 5 was What optimization should we work on ?, Part 7 will be A Plan for Technical Debt.  ","categories": ["agile","lean"],
        "tags": [],
        "url": "/you-dont-have-to-ask-your-boss-for-a-fast-build-lean-software-development-part-6/",
        "teaser": null
      },{
        "title": "Actors and Green Threads in Java Demystified",
        "excerpt":"After finishing my concurrency-kata, one of the things that most surprised me, is how simple it was to prototype the Actor Model in Java using Green Threads.   The Code   First, here is the base class for all actors.   public class Actor implements Runnable {     private final ExecutorService threadPool;     private final ConcurrentLinkedQueue&lt;Runnable&gt; mailbox = new ConcurrentLinkedQueue&lt;&gt;();     private boolean stoped;      public Actor(ExecutorService threadPool) {         this.threadPool = threadPool;     }      public void run() {         if (stoped) {             return;         }          Runnable nextMessage = mailbox.poll();         if (nextMessage != null) {             nextMessage.run();         }         submitContinuation();     }      public void start() {         submitContinuation();     }      protected void stop() {         stoped = true;     }      protected void send(Runnable runnable) {         mailbox.add(runnable);     }      private void submitContinuation() {         threadPool.submit(this);     } }    As you can see, I simply used Runnable as the type of the messages.   The Actor itself is Runnable, meaning that it can be submitted to the thread pool. When executed :      it tries to handle a message from the mailbox if there is one.   It then re-submits the actor itself   This ensures that only one thread is executing messages on an actor at a given time, and it also avoids spawning new thread for every new actor.   As an example, here is how I used this to make an actor of an existing InProcessChatRoom class.   public interface ChatRoom {     void broadcast(Output client, String message);     ... }   public class InProcessChatRoom implements ChatRoom { \t... }   public class ChatRoomActor extends Actor implements ChatRoom {      private final ChatRoom realChatroom;      public ChatRoomActor(ChatRoom realChatroom, ExecutorService threadPool) {         super(threadPool);         this.realChatroom = realChatroom;         start();     }      @Override     public void broadcast(final Output client, final String message) {         send(new Runnable() {             @Override             public void run() {                 realChatroom.broadcast(client, message);             }         });     }     ... }  ChatRoomActor is in fact some kind of proxy to use from other actors to send messages to the chat room.   As with any implementation of the Actors Model, the neet thing is the separation of threading and logic. That makes it so much simpler ! (You can get more detail about the complexity I am talking about by taking a look at the concurrency-kata)   Performances   Here is a performance summary of this implementation compared to others on a “throughput vs clients” benchmark of the style “Enter while others are talking”      Results can be disappointing compared to other implementations but the example itself is a bit misleading. The chatroom does exclusively message passing, there is not much computation to parallelize, in a different setting, the results would have been completely different.   Limitations   As you can see, this implementation is just a quick prototype, nothing production ready. Here are the main limitations I can think of right now :      It uses Busy Waiting for the next message, meaning that it consumes unnecessary resources, and that more important messages to other Actors might be delayed   Usually, actor messages are selected on their type rather than on their order of arrival, this is not the case here   The usage of the Runnable interface as base message type, though handy, opens the door to inter-thread calls that might violate the model altogether   There is absolutely no support of out of process actors until the messages are Serializable   Going Further   I started this concurrency-kata as a training material about concurrency for colleagues at work. In the spirit of the coding kata, it’s a git repo you can walk, explore and experiment with.   So if you want to learn more about different models of concurrency, you are welcome to have a look at the How-To section in the README file.   On my part, although it was a lot more work than I would have guessed at the beginning, I barely scratch the surface of the subject ! I could now :      extract the CSP or Actor implementation and make them more robust   practice and present the whole kata as a 2 hours live coding session   prepare a hands-on training about concurrency   So, if you are willing to do any of the above you are welcome to contribute !  ","categories": ["programming","java","concurrency"],
        "tags": [],
        "url": "/actors-and-green-threads-in-java-demystified/",
        "teaser": null
      },{
        "title": "Bye Bye Programmer's TODO List, Hello Personnal Kanban on Jira",
        "excerpt":"EDIT 2017-03-03 : This article is old, I am now using an improved Jira Personnal Kanban.   Not long ago, I wrote that Real Programmers have TODO lists … I was wrong, I now work without a TODO list ! So either I’m not a real programmer anymore, or I’m actually using TODO List v.2.0. Read on !   Motivations   My work has become quite varied lately. On top of programming and pairing, I am also doing quite some coaching within the team. For the whole Murex programmers community, I’m organizing Coding Dojos, Brown Bag Lunches and Technical Talk Video Sessions. Finaly, like all of us, I have to cope with my share of organization overhead.   Multitasking was starting to kill me. I was feeling exhausted at the end of the day, with the certainty that I was not getting much done …   Personnal Kanban To The Rescue   Kanban is a method to organize your work relying on Work In Progress limits : it minimizes multitasking and encourages prioritization.   As its name suggests it, Personal Kanban is simply applying Kanban to your own tasks. It turns out that :      My team tasks are already in JIRA   Some guys have already tried to use JIRA as a todo list   JIRA supports Kanban boards with WIP limits and all   The Kanban Board   In our team, TAYARA Bilal had already experimented the approach and asked to JIRA admins to create us a custom project for our todo lists. I piggybacked on it and created my own kanban board. Here is what it looks like.      Mixing Project Stories And Personnal Tasks   JIRA allows creating a kanban board that spans many projects ! You can simply choose multiple projects when you setup the board for the first time, or you can edit your board filter like this :   project in (POPIMDB, POPABTODO) ...   This makes it possible to see all of my work at a glance on the same board.   Work In Progress Limit      JIRA allows WIP limits, warning me with a red background when I am multitasking or when I am getting late on my tasks   Color Conventions      JIRA makes it possible to assign different colors to cards, for example      Red for tasks that are due soon   orange for cards that are due some time   light brown for project stories   green for other programming tasks   blue for other tasks   Swimlanes      JIRA has swimlanes, separating project from personal tasks   Reports   An extra bonus with JIRA Kanban board is that they have reports ! Here is my cumulative flow diagram for my first week of usage :      Configuration   Here is the JQL query I used to configure it this way.   -- board filter project in (POPIMDB, POPABTODO) AND (Assignee = pbourgau OR Co-Assignees in (pbourgau) OR mentors in (pbourgau)) AND (status != CLOSED OR updated &gt;= -1d) ORDER BY Rank ASC  -- Swimlanes priority = \"1-Very High\" -- Expedite project in (\"POP IMDB\") and (labels not in (SLACK) OR labels is EMPTY) -- IMDB Stories -- and a blank filter for Other Tasks  -- Card Colours duedate &lt;= 7d or priority = \"1-Very High\" -- red duedate is not EMPTY -- orange labels in (SLACK) -- green type = Task -- blue -- and an empty filter for light brown   The End Result   By setting a WIP limit of 3 on the “In Progress” column, the following naturally happened :      Once I have started a programming task, I now defer any other activity in the TODO column until I am finished. (HINT: If you get invited to meetings all the time, lock your agenda with ‘Unbookable’ days when you start programming)   It actually pushed me into finishing the concurrency-kata training I had started long ago.   I also set a high WIP limit (around 10) on the TODO column, this way, I get a kind of warning that next time I finish a programming task, I should take some time off to prune the column.   The overall result is that I do lot less multitasking. I get the feeling of doing steadier, more efficient work.   If you are suffering from multitasking and decide to give it a try, I’d love to read about your experience !  ","categories": ["programming","kanban","jira","personal-productivity"],
        "tags": [],
        "url": "/bye-bye-programmers-todo-list-hello-personnal-kanban-on-jira/",
        "teaser": null
      },{
        "title": "Mining GitHub For New Hires",
        "excerpt":"In search of an experienced software engineer   We have been trying to hire such a profile for the last year … The position is hopefully filled now. During that year, we have tried to mine github for candidates. Here is how we did it.      Software engineers, especially experienced, are known to be hard to find. Over the past months, we had steadily been improving our hiring process :      By regularly rewriting and optimizing our job post   By posting it on Twitter   By defining a precise interview template   We went from very few applications to :      More applications   More experienced candidates   Regular interviews   Effective interviews   Unfortunately, we were still not interviewing candidates as skilled as we would have liked to. We were convinced that we were offering a great job : the project is very interesting, and the team is a dream to work in.   How could we reach more great devs ?   Someday, I played with github’s Rest Api and I managed to write a short ruby script that finds the contributors to a given project that are living near Paris (France).   require 'rubygems' require 'rest_client' require 'json'  RestClient.proxy = \"http://proxy:3128\"  def github_get(resource)   JSON.parse(RestClient.get(\"https://api.github.com#{resource}\", params: {                               access_token: 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX',                               per_page: 200})) end  repo = ARGV[0]   puts \"searching for #{repo} contributors in France\"   contributors = github_get(\"/repos/#{repo}/contributors\") logins_locations = contributors.map do |user|   begin     login = user['login']     location = github_get(\"/users/#{login}\")['location']     {login: login, location: location}   rescue Exception =&gt; e     puts \"could not see details of #{login} #{e}\"     {login: login }   end end   puts \"Here are all the contributors\" puts logins_locations french_contributors = logins_locations.select do |login_location|   location = login_location[:location]   location != nil and     (location.downcase.include?('france') or      location.downcase.include?('paris')) end  puts \"----------------------------\" puts \"Here are all the frenchcontributors\" puts french_contributors   What’s next ?   We eventually filled the position before following our github experiment. We might continue some day though ! Here is a list of improvements I thought of :      Gather a list of projects that overlap with what we are building   Search for repositories using some keyword or regular expression   List github users that contributed to such projects, that live near us and that know java   Use more complex algorithms to find best candidates (Algorithmic Recruitment With GitHub)   Spend more time socializing on GitHub (The Complete Guide To Recruiting And Sourcing Candidates On GitHub)   It really looks like if software is eating recruitment …  ","categories": ["programming","ruby","hiring","github"],
        "tags": [],
        "url": "/mining-github-for-new-hires/",
        "teaser": null
      },{
        "title": "A Plan For Technical Debt (Lean Software Development Part 7)",
        "excerpt":"The sad truth :      The technical debt metaphor does not help me to fix it.       Here is my modest 2€ plan about how to try to get out of this.   Why does the metaphor fall short ?   The debt comparison effectively helps non programming people to understand that bad code costs money. Unfortunately, it does not tell you how much. As a consequence, deciding whether it’s best to fix the technical debt or to live with it remains a gut feeling decision (aka programmers want to stop the world and fix all of it while the product owner wants to live with it).   They are very good reason why we cannot measure how much the technical debt costs :      It is purely subjective : bad code for someone might be good code for another. Even worse, as you become a better programmer, yesterday’s master piece might become today’s crap. More often, as a team gains insight on the domain, old code might suddenly appear completely wrong …   Tools such as Sonar only spot the a small part of the debt. The larger part (design, architecture and domain) remains invisible   Finally, non-remediation cost (the time wasted working on the bad code) is often overlooked and very difficult to measure : it depends on what you are going to work in the future !   No surprise it’s difficult to convince anyone else why fixing your debt is a good investment.      The Plan   In the team, we usually try not to create debt in the first place. We have strong code conventions and working agreements. We are doing a lot of refactoring in order to keep our code base clean. But even with all this, debt creeps in :      a pair worked on something and did not know that there is another part of the system that does roughly the same thing   we understand something new about the domain and some previously fine code becomes debt !   like all programmers, we are constantly in a hurry, and sometimes, we just let debt through   …   If the required refactoring is small enough, we just slip it inside a user story and do it on the fly. The real problem comes larger refactorings.   The strategy to deal with those is to get estimations of both the remediation and non-remediation costs. This way, the technical debt becomes an investment ! Invest X$ now and receive Y$ every month up to the end of the life of product. Provided you have the Cost Of Delay of the product, you can estimate the cost of delay of this individual technical debt fix. For example :      Let’s define the product horizon as its expected remaining life span at any moment   Suppose the product has a 5 years (60 months) horizon   Suppose the Cost Of Delay of the full product is 150K€/month   Suppose that the technical debt costs 10 days (0.5 month) to fix   Suppose that that once fixed, you’ll save 2 days (0.1 month) of work per month   By doing the fix now, at the end of the 5 years, you would have saved : (60 - 0.5) * 0.1 - 0.5 = 5.45 months   Using CoD, this ammounts to : 5.45 * 159K = 817.5K €   Dividing by the number of months, we finaly get the CoD for this technical debt fix : 817.5K / 60 = 13 625 €/month   This can be compared to the CoD of other backlog items, allowing us to prioritize large refactorings as we would of any feature or story.   One nice thing about this is that it not only helps to know if a refactoring is cost effective, but also when is the best moment to do it. As the CoD of the refactoring is proportional to inverse of the product horizon, a premature refactoring for a startup product might become a real bargain after the product has settled as a market leader. Here are examples of possible product horizons :                  Context       Horizon                       Startup       6 months                 3 years old company       3 years                 Market leading product       10 years                 Aging System       5 years                 Legacy System       2 years               Oh, and just one more thing … prioritizing technical debt fixes in your backlog will create some real time to focus on and only on refactoring, reducing task switching and saving even more time.   All this sounds great ! There’s just one last little thing : how do we get estimations of both costs of the technical debt ?   Idea 1 : Collective Estimations   When I attended Donald Reinertsen’s training, I asked him the question and he answered :      I’d gather the top programmers in a room and I’d make them do an estimation of both costs.    So I asked my team if they wanted to do the following :      whenever you spot a large piece of debt, create a JIRA issue for it   at the end of your next sprint planning session, go through all your technical debt issues, and for each            estimate the remediation cost in story points       estimate the non-remediation cost on the coming sprint, taking the prioritized stories into account           using the ROI horizon for every issues, collectively decide which one to tackle and add them to the sprint backlog   To keep the story short, it did not stick. I bet it was just too boring.   Idea 2 : Technical Debt Code Annotations   During a retrospective, we discussed marking technical debt directly in the code to decide when to fix it. I created 2 code annotations so that this can be done. Here is an example of some identified technical debt :   public final class Transformers {     private Transformers() {    }     @TechnicalDebt(storyPoints = 8, description =      \"We need to find a way to do all the ast rewriting before staring the analysis\", wastes = {      @Waste(date = \"2015/05/14\", hours = 16, summary =        \"For union, we lost quite some time identifying which transformers were not copying the full tree\")})    public static AstNode analyzeAst(AstNode ast) {      ...   The @TechnicalDebt annotation identifies areas of the code that could be improved. The @Waste annotation is a way to log time wasted because of this bad code.   By comparing the time to fix the technical debt and the flow of extra work it incurs, we should be able to more easily justify and prioritize these in our backlog.   We are thinking of writing a sonar plugin to keep track of this technical debt right in our Sonar dashboard. It would :      create a technical debt item in sonar for every @TechnicalDebt annotation found in the code   link it with a mirror technical debt issue in JIRA   use the story points we entered in the annotation as remediation cost   extrapolate the non remediation cost by the sum of wasted hours registered during the last month   We just started using those, and I cannot give enough feedback for the moment. I bet not enough @Waste items will be entered though … again, it might just be too boring      Idea 3 : Sonar and IDE Plugins   If it’s too boring to add @Waste annotations in the code, it might be easier to have an IDE plugin with 1 big button to register some time wasted on the local @TechnicalDebt zone.   Pushing things a bit further, it might even be possible to estimate non remediation cost by having a look at what files are read the more, what files are triggering the more test failures when changed, etc.   Unfortunately, that’s a long shot, we’re definitely not there yet !   Possible Improvements   The Mikado Method   Whether you’ve got these estimations or not, it’s always a good practice to learn how to  use the mikado method. It’s great to split a refactoring into smaller part and spread them over many sprints.   The pill is easier to swallow for everyone, and it keeps the code releasable at any given time.   Decision Rule   Provided you have :      Product CoD   Top Features CoD   Product horizon   You could easily come up with a decision rule to help us prioritizing technical debt more quickly, without the need for a formal planning.   References      Identifying and Managing Technical Debt   Managing Technical Debt   This was part 7 of my suite of article about Lean Software Development, Part 6 was You don’t have to ask your boss for a fast build, Part 8 will be How I’ll Measure the Lean Startup Value of Information in My Next Side Project  ","categories": ["agile","lean","software","technical debt","planning","mikado-method"],
        "tags": [],
        "url": "/a-plan-for-technical-debt-lean-software-development-part-7/",
        "teaser": null
      },{
        "title": "Feedback On 360° Feedback Session",
        "excerpt":"   If you remove your job, you are promoted. (a classic lean quote)    In Management 3.0, Jurgen Appelo suggests doing full team 360° feedback sessions instead of more traditional manager-collaborator meetings.      He argues in favor of this practice in order to :      obviously, get some feedback and improve   also give feedback to the manager so he too can improve   help the team to further self organize   practice everyone’s people skills   have more quality objective feedback than subjective manager feedback   free some manager time   Our Experience   We just gave it a try. As a fist experiment, we did it at just 3, all willing to try. Here is the ROTI (Return On Time Invested)                  Grade(/5)       Comment                       5       Useful and healthy. It’s a way to stop grumbling. It’s also the occasion to say things that we often don’t.                 5       I’m leaving the room with real improvement topics. It calms my emotions, it’s like “balm for the heart”                 5       I’m getting out with great advises. I think it’s great for team spirit. It took 1h for just the 3 of us, I’m wondering how we’ll manage this if we are more ?           If you want to try it   A few last minute advises  :      Don’t force it onto people, start with volunteers   There must be a safe and positive atmosphere in the team   This is an improvement exercices, and it should not be used as any kind of evaluation   Learn how to give feedback            Our company provides trainings on non violent communication and positive feedback, maybe yours does too !       Appelo explains how to give written feedback in his other book #Workout. Though better suited for email feedback, I found it a great way to prepare for the session.              I’d like to ear about your experiences with such collaborative feedbacks.  ","categories": ["agile","management30","feedback"],
        "tags": [],
        "url": "/feedback-on-360-degrees-feedback-session/",
        "teaser": null
      },{
        "title": "Silosis",
        "excerpt":"I just invented the word, I found it funny :      Silosis: an internal organization of an enterprise in which people are grouped by job titles. Although not obvious at first sight, it usually involves excessively high communication costs. Worst cases can create vicious circles where more ‘communication people’ are added which in turn increase the overall communication costs …       Here are other reasons I don’t like silos :      Silos create local optimums, which lean theory (and mathematics) taught us is bad   Silos create specialists, which bring the whole system to a halt when they leave   Silos create work queue, which increase cycle time   Silos create some form of black market in the organization, through which people can bypass the official communication channel and actually get the job done   Silos hides the big picture to anyone, thus removing autonomy from and demotivating people   I could go one for hours … I just hate silos.     ","categories": ["lean","joke"],
        "tags": [],
        "url": "/silosis/",
        "teaser": null
      },{
        "title": "Binary Hand Planning Poker",
        "excerpt":"At work, we are using as we all have Android phones, we are using the Scrum Poker Cards app to do our planning poker.   This usually works fine, but newcomers usually have one bad remark about it :      It’s not geeky enough    Obviously, it also does not work if one lakes a phone, but who would come in a meeting without it nowadays?   Enters the binary hand poker!                  Score       Sign                       1                        2                        3                        5                        8                        13                        20                  Now that’s geeky! I still have to find a symbol for the coffee cup and the question mark though …  ","categories": ["agile","joke"],
        "tags": [],
        "url": "/binary-hand-planning-poker/",
        "teaser": null
      },{
        "title": "The technical debt ponzi scheme",
        "excerpt":"Madoff would have been better off managing software projects than investing on Wall Street !      Whereas in finance, a Ponzi scheme is a sure road to jail, it seems to be the de-facto standard in software development.   A few months ago, I read Managing Software Debt, hoping to find methods about how to manage technical debt with some hard numbers (instead I found some good practices to avoid it, but that’s another story). At no place did I read that taking more debt to pay the interests of existing debt was a good practice though …   Thinking of it, I can remember of such Ponzi Schemes in nearly every organization I worked in. Here are a few typical manifestations I saw :      writing bogus code to compensate for some other bogus code   creating tools to workaround existing technical debt. Ex:            exotic build tools to build some code riddled with cyclic dependencies that no sane build tool can build       in house tools that do 10% of what standard (open source) tools can do on code following main standards           If this goes on for too long, you can end up in a technical debt death spiral : you know debt is out of control, so taking debt becomes the only way of actually getting anything done. “Let’s win this client now, because we won’t be able to later …”. It’s like running to one’s own ruin.      If your organization is in this stage, you might think at the ‘time horizon’ of your product, and discover that fixing the technical debt sometimes brings more value than getting this new client !  ","categories": ["agile","joke","software","technical debt"],
        "tags": [],
        "url": "/the-technical-debt-ponzi-scheme/",
        "teaser": null
      },{
        "title": "Fixing the 'TLS-enabled daemon' docker error on Ubuntu",
        "excerpt":"   I am using Ubuntu, Vagrant and Docker together to handle my various development environments. It usually works just fine : it’s fast, low weight, it keeps my machine clean and it’s quite simple once you’ve setup your first VM. Until last time … read on !   When I tried to start my docker session through Vagrant as usuall, here is the error I suddenly got :   philou@philou-UX31E:~/code/concurrency-kata$ vagrant up /home/philou/.ssh/known_hosts updated. Original contents retained as /home/philou/.ssh/known_hosts.old A Docker command executed by Vagrant didn't complete successfully! The command run along with the output from the command is shown below.  Command: [\"docker\", \"ps\", \"-a\", \"-q\", \"--no-trunc\", {:notify=&gt;[:stdout, :stderr]}]  Stderr: time=\"2015-10-19T06:16:27+02:00\" level=\"fatal\" msg=\"Get http:///var/run/docker.sock/v1.17/containers/json?all=1: dial unix /var/run/docker.sock: no such file or directory. Are you trying to connect to a TLS-enabled daemon without TLS?\"   Stdout: ssh: connect to host localhost port 2222: Connection refused   After a bit of Google searching, I found that the docker service needed to be started. service docker start did the trick … until next reboot !   With still some more Googling, I found a page explaining how to manage Ubuntu services. In order to make the docker service automaticaly start at every reboot, I just had to enter the following :   systemctl enable docker.service   That did the trick ! I thought that might be useful for others.  ","categories": ["ubuntu","unix","vagrant","docker","workaround"],
        "tags": [],
        "url": "/fixing-the-tls-enabled-daemon-docker-error-on-ubuntu/",
        "teaser": null
      },{
        "title": "How I'll measure the lean startup value of information in my next side project (Lean Software Development Part 8)",
        "excerpt":"There must be a way to know the real money value of Lean Startup ‘knowledge’.      Lean Startup is about gathering ‘knowledge’ through experiments instead of building things. Flow book suggests that the value of an information is its expected impact on the bottom line. So by combining both, there should be a way to compute a quantitative value for the knowledge.   The overall plan would be :      Start with a rough business plan, but assume that all that’s in it has got 1 chance out of 10 of being right   Design experiments to precise the likelyhood of what’s in the business plan   Compute the value of each experiment as the difference of the expected bottom line after and before running it (This should be possible if relying on the business plan for other parameters)   Work on the task with the highest expected value, be it an experiment, a development, a sales task, or whatever   Adapt you business plan with any new information   Repeat   If it becomes obvious that the expected final earnings are too low, pivot   Flow explains that all the principles are pretty robust to bad inputs and errors. In short, the final estimations of earnings are still a lot better than without using any specific method. In the case of Lean Startup, it means that a false business plan is a good enough starting point.   How to assess the likelyhood of a figure ?   By default let’s assume the value of a feature is somewhere between 0 and your highest estimate. As we’ll go through experiments we’ll be able to narrow this value range and have a more accurate estimate for the real value.   To keep things simple we could assume that the probability of value is uniformaly distributed within this range. From what I read in Waltzing With Bears, here would be a more realistic probability distribution.      How to estimate the value of an experiment ?   I’m not going to go into the details here, because the logic is exactly the same as with technical experiments (which I already wrote about)   Since then though, I read Waltzing With Bears which presents a way of doing Monte Carlo simulations in order to estimate risks on your project. I guess the same thing could be used for value, which would save us from a lot of complex math.      When to stop doing experiments ?   Isn’t all this overkill ? One could argue that the point of Lean Startup should be to build just enough knowledge ! In fact, this value of information concept naturaly helps to find out when running the experiment is not worth it compared to actually doing the real thing.   BTW, a tool doing all this for your automaticaly would be great, wouldn’t it ?   What I’ll do for my next bootstrap side project      Start with a target hourly earning rate.   Collect a list of projects I’d be interested in.   Pick a project and write a business goal for it.   Assign likelyhoods and ranges on every figure in the business plan.   Design experiments to improve the figures.   Create tasks to actually build and sell the product.   Compute values for each of these tasks.   Work the most valueable first.   If the figures prove that I will not make my target wage with this project, I’ll pivot, and restart from 2.   Otherwise, I’ll repeat from 4.   This was part 8 of my suite of article about Lean Software Development, Part 7 was A Plan for Technical Debt, Part 9 will be My Dream Lean Software Development Tool.  ","categories": ["lean","software","planning","business value"],
        "tags": [],
        "url": "/how-ill-measure-the-lean-startup-value-of-information-in-my-next-side-project-lean-software-development-part-8/",
        "teaser": null
      },{
        "title": "My Dream Lean Software Development Tool (Lean Software Development part 9)",
        "excerpt":"Current software project tracking tools suck !      By writing this series of posts, I realized that they should help us to prioritize rather than just serving as a nice GUIs on top of a ticket database.   Let’s take the story of developer Joe as an example. Joe is working for Megacorp, where he makes a good living, but feels like a small cog in the machine. He has lost the thrill of programming … He’s still staying informed about latest trends in software world, but his job involves aging technologies. He knows that if he wants to remain in the course, he has to change gears. One day, he decides to do something now, and starts a side project in the hope of someday making a living out of it.   He does not have a large amount of money to invest in his project, so he decides to bootstrap his product using Lean Startup. He reads and understands that there is a lot of different kind of activities to do if he wants to realize his dream : experiments, marketing, sales, design, coding …   Unfortunately, he’s so tired of programming boring stuff that he contracted the developer bias : he starves to code cool things again ! He knows the rest is important, he tries to do some of it, but he just doesn’t realize how much needs to be done. So, without realising it, he wastes a lot of time coding something that people eventually don’t buy. After 1 or 2 years of hard work, a working product but no users, he eventually quits, quite depressed …   I guess you don’t want to be Joe ! I know you’ll think it won’t happen to you … believe me, it can, I learned it the hard way !   Now suppose you had a tool that estimates the expected values of all different tasks :      By applying Monte Carlo simulations to probabilistic value and size ranges, it could compute which features we should work on first. As a side effect, this could give us some release date estimates for a feature, depending on its position in the backlog.   By having explicit experiment tasks with target value or size ranges, it could compute the value of information and prioritize experiments versus other tasks.   Crossing informations from IDEs, VCS, Quality Trackers and CIs, it could detect technical debt items (aka. productivity opportunities), compute their values in the long run, and prioritize them with other tasks.   If we found a way to integrate marketing, sales and design tasks, any kind of activity could be stored in the backlog, valued and prioritized.   Here is what the backlog could look like :      All different kind of activities (developments, marketing, experiments, refactorings) would fit in the backlog. The system would estimate the value of every task for Joe, advising him what to work on first. It would make it obvious just how much market experiments are important when starting ! With such a tool, he is a lot more likely to build his new life.   The good thing with such a tool is that it could be built by feeding itself. Eating its own dog’s food has always been the nice thing about writing software tools.   I’d like to know, what do you think of the idea ? Would you use such a tool ? Is it worth me spending my nights building it ?   This was 9th and last part of my suite of article about Lean Software Development, Part 8 was How I’ll Measure the Lean Startup Value of Information in My Next Side Project.  ","categories": ["lean","software","planning","business value"],
        "tags": [],
        "url": "/my-dream-lean-software-development-tool-lean-software-development-part-9/",
        "teaser": null
      },{
        "title": "How to do screencasts on Ubuntu",
        "excerpt":"It is easy to record and share great screencasts on Ubuntu (and I guess on Linux in general).   Recently, I had to create a screencast to demonstrate how to use Storexplore, a gem I wrote that transforms online stores into APIs. Here is the result     Don’t lose your time searching for anything else than SimpleScreenRecorder (I did, and actually did lose quite some time). Follow the installation instructions :   sudo add-apt-repository ppa:maarten-baert/simplescreenrecorder sudo apt-get update sudo apt-get install simplescreenrecorder # if you want to record 32-bit OpenGL applications on a 64-bit system: sudo apt-get install simplescreenrecorder-lib:i386   It’s dead simple to use, just try to record your first video, and you should know it all !   At the difference of other tools I tried (I’m looking at you RecordMyDesktop) I could easily encode my video to efficient formats (mp4, h.264 &amp; mp3) that are well known on the internet, and in particular, by YouTube.  ","categories": ["linux","ubuntu","screencast"],
        "tags": [],
        "url": "/how-to-do-screencasts-on-ubuntu/",
        "teaser": null
      },{
        "title": "Great Developers Are Free",
        "excerpt":"For many reasons. But mostly because they are key to efficiently growing a software organization.      A tale of 2 teams   Let’s have a look at 2 software teams in the world of corporate finance.   The junior in-house team      One is an in-house software development team in a large bank. The project already has a bad reputation among developers.The bank has difficulties to hire so the team is constituted of 10 rather junior developers. Unfortunately, without guidance and long term vision, the quality of the code suffers. It looks like an ad-hoc composition of various technologies, glue code, and reinvented frameworks. The overall result is a barely good enough product that costs a small fortune in maintenance and support. The user experience is awful, which make it difficult to the team leaders and the users to collaborate effectively. As a result of all this, the project is suffering from high turnover. Managing this project is really challenging … and kind of depressing.   The experienced software team      Now let’s imagine the same product developed at a software house. It’s been on the market for a while now, but new features are regularly added to it at a good pace. The product is solid and the users are happy using it. The structure of the team is completely different : 5 developers, mainly experienced, coming from various backgrounds. As a result, the team builds on all their different expertises to build real competitive advantages. They tend to get into healthy debates about a lot of things, such as :      Should we re-use or re-write ?   Is this gold plating or plain technical debt ?   Which technology should we use to build this ?   In the end, that makes the product even better. The human side of the team is also completely different. These experienced developers have all been exposed to the big picture during their careers, and they know things like :      ‘Business talk’ and so they can discuss product topics with the product managers   The best way to go through chores at work is to do them right now   Enough management, testing, ops to make the team self organized   The single junior developer in the team tends to mimic this model, and has ease finding mentors, to teach him their trade and to give him career counsels.   As a result, the product and the team remain healthy, and keep providing benefits to all stakeholders.   Benefits of experienced teams   First, from a purely short term financial point of view, it’s a no brainer ! Even if you pay experienced developers twice as much as juniors, the costs will still be on par. But the difference in created value is huge !   Let’s then have a look at the longer term, organizational aspect of things. Obviously, managing such teams requires a lot less work ! Both because of their size and because experienced developers tends to manage themselves very well … Promoting self-organized &amp; cross-functionnal teams is a great opportunity to reduce the management cost and friction, making the organization more reactive and cost efficient in the long run.   What is an experienced developer ?   Obviously, I’ve worked with people that were experienced on paper, but not in practice. I read that repeating 10 times the same year of work only amounts to 1 year of experience.   If I was asked to give a definition, I’d say that experienced developers have worked on different kind of systems (embedded, web, client, server …) using different technologies (Java, Ruby, C, Spring, Rails, Javascript …). Their experiences need not be professional, I’ve seen a physics teacher that hacked himself into a great hacker through side projects and open source. Speaking of the subject, good developers always spent a lot of time learning, reading, doing side projects and contributing to open source. Some can be found in local user groups and online programming communities.   So How do you get them ?   I can only think of 3 obvious ways :      train them   keep them   hire them   I’m pretty sure training them will not be an issue if you already have enough of them. So that brings us to the 2 other points.      Lot’s of articles have been written about attracting and keeping the best developers. Some companies even made it their differentiating point. Here are a few links :      In his post “The Management Team” Joel Splosky details the inverted pyramid model   Joel Spolsky’s (again) Development Abstraction Layer explains all the physical environment that makes programmers happy at work   After disclosing employee salaries, Buffer was inundated with resumes when buffer.com publicly disclosed their salary formula, the number of applications the company received doubled   Spotify organizes Hack Weeks where  the company stops for a while to invent new things   Google summarizes it “Do Cool Things That Matters” in their long advertises Life At Google   These are just a few examples and are by no way the only things that motivate developers. To summarize, provide the best working conditions (careful, that not money), and communicate a lot about it.   Hiring is too important to be left to HR   Jurgen Appelo said      Management is too important to be left to managers.    I guess the same thing can be said about hiring.   Engineers are absolutely required to take the main role when hiring other engineers. Companies that seriously want to have the best developers on board are spending substantial engineering time on hiring. Here are some well known examples :      Google is known to conduct around 9 interviews before hiring someone   In its Handbook for New Employee Valve explicitly states that hiring is your most important role   Endword   As Tom DeMarco said about quality, in Peopleware :      Great developers are free, but only for those who are willing to pay for them   ","categories": ["software","selforganizing","team building","workplace","hiring","enterprise"],
        "tags": [],
        "url": "/great-developers-are-free/",
        "teaser": null
      },{
        "title": "How To Write Good Performance Stories",
        "excerpt":"If you’re having difficulties writing good performance related stories for your project, that’s no surprise ! We’ve been through the same troubles and we found a way that works a lot better.      Solution 1 : Performance is a feature   Right ? In typical agile way, a story about performance would be written like      As marketing, I want the page load to perform under 1 second, In order for the customers to stay on the site    If the performance bottleneck and the fix are obvious, that might work very well. If that’s your case, then go on, that’s the simplest way!   Unfortunately for us, we are not in that case. At work we are building a risk engine and we need to perform extremely well on a wide set of different scenarios. Imagine a story such as      As a risk manager, I want the VAR scenario to compute in less than 1 second, in order to have real time data       we’re pretty sure that there won’t be 1 but many bottlenecks to fix before reaching the expected performance   speeding up the VAR scenario might slow down other scenarios   That makes this kind of story too fuzzy. We tried them, and we had difficulties to estimate and close them.   Solution 2 : Back to the traditional way   Without an agile backlog, developers would have worked on tasks such as      Add caching to the computation engine    That’s estimable, but we’ve got no clue of the why ! When it’s done, we won’t be able to know whether it’s worth to keep it or not. In fact, it’s just not a story !   Solution 3 : Mixing both   Here is how we we are now writing performance stories at work :      As a risk manager, I want to have caching in the computation engine, in order to the VAR scenario compute under 1 second (in order to have real time data)    It’s now estimable, we know what it’s there for. It’s obviously not enough though : we know that we will very likely need to do other performance stories after this one.   Embrace uncertainty      The fact is that in our context, getting better performances is hard :      it requires a lot of work   we don’t know how much at the beginning   we often try things that don’t work   This makes the whole project more uncertain, so we’re better off embracing this uncertainty in the way we write and prioritize our performance stories.  ","categories": ["agile","performance"],
        "tags": [],
        "url": "/how-to-write-good-performance-stories/",
        "teaser": null
      },{
        "title": "How We Started Exploratory Testing",
        "excerpt":"Manual testing is important. Here is how we got to love exploratory testing.      Initial situation   At work we are building a risk computation engine for the financial markets. It uses a DSL to describe the exact computations to estimate the risk on the data it knows. This in itself is already complex enough to justify the heavy investment in automated testing we did.   With 90% of automated test coverage, Cucumber scenarios to verify quality, everything should just work … shouldn’t it ?   First try at exploratory testing      In The art of agile development, James Shore details the practice of exploratory testing as a great way of both :      improving the quality of the product by finding bugs   improving the process   That’s why we gave it a try. Here is the recipe for an exploratory session :      Book 1 hour for the full team to do exploratory testing   Prepare a downloadable zip with all the material required to run and test your software   Ask everyone to pick a particular aspect of the system to test during this session   Record bugs when you find one   Spend 30 minutes just after the session to filter duplicate bugs and make sure they are well described   Obviously, we also added our special sauce      We were to do exploratory testing in pairs, as we do programming, to find more bugs   We gamified it by granting a price to the pair that finds the most bugs. Do you remember how I brought fruits for sale in the office ? It turns out we are slowly earning money with the fruits … enough for the price to be 5 fruits each for the wining pair !   This transformed developers into ferocious testers ! I guarantee that with such incentives you’ll find bugs … as we did.      Doing it systematically   We took some time to fix all these bugs during a few sprints. And did another exploratory testing session a few months after …   As we were still finding bugs in the second session, we decided to make them part of the every sprint. As we got better at testing, a lot of bugs started to get uncovered ! So much that we had to change something.   Improving our process   We meet in retrospective and here is what we decided :      Fix all bugs before working on any other story (obviously, automated tests are added in the process)   Classify what exactly is a bug. For example, for us, a bug is something that used to work or silently returns bad results or corrupts the data   Add exploratory testing by another pair to our definition of done   The number of bugs we find during exploratory testing sessions is starting to decrease. We hope that we’ll soon be able to do exploratory testing on the fly, as part of our daily work and to completely remove the specific sessions.   Benefits   Exploratory testing brought us a lot.      Obviously, the product is a lot more solid   We are surely saving time that would have been lost if the bugs had been uncovered by the users   Fixing the bugs forced us to fix some technical debt : bugs often came from areas of the code that we were not so proud or confident of   Exploratory testing is a nobrainer. All teams should do it.   So, in the end, as in the old days, we are back to regular manual testing … but only for exploration.  ","categories": ["workplace","testing","agile"],
        "tags": [],
        "url": "/how-we-started-exploratory-testing/",
        "teaser": null
      },{
        "title": "Most Common Ways To Speed up an algorithm",
        "excerpt":"Algorithms are hard, and making them fast is even harder … But there are shortcuts that work quite often !      The Challenge   Imagine you just arrived to your new job, and you are asked to make a part of the system faster. After a bit of investigation, you discover that most of the time is spent in some weird in-house algorithm that seems to take forever. How can you optimize this without deep knowledge neither in algorithm science nor in the code itself ?   Here are 4 tricks to reduce the complexity of algorithms (I’m using fairly basic examples for the sake of understandably. Most of these exact examples could be done better using standard libraries, but I hope it will be easy to adapt to other situations) :   Replace a nested loop by first building a hash and then looping   # before orders.each do |order|   client = list_of_clients.find {|client| client.id = order.client_id }   handle_order(order, client) end  # after clients_by_id = {} list_of_clients.each do |client|   clients_by_id[client.id] = client end orders.each do |order|   handle_order(order, clients_by_id[order.client_id]) end  This reduces the complexity from O(2) to O(1). This is tremendous. On large lists O(2) algorithms are terrible.   Remove unnecessary accumulations   The most classic example is the use of a string buffer :   // Before report = \"\" line_items.each do |line_item|   report += line_item.to_s + \"\\n\" end  // After report = [] line_items.each do |line_item|   report &lt;&lt; line_item.to_s   report &lt;&lt; \"\\n\" end report.join  Again, this reduces the complexity from O(2) to O(1). Every language has variants of Java’s StringBuilders. This does not only apply for strings, it works any time you are repeatedly accumulating results inside a loop but where you could do it only once at the end.   Cache intermediate or previous results   This is called memoization. Some algorithms (especially recursive algorithms) repeatedly compute the same thing again and again. Spotting this pattern is an opportunity to move an algorithm out of exponential complexity. For example, Dijsktra’s algorithm for finding the shortest path in a graph uses this technique to go from O(en) to O(n2) complexity. If you suspect this could be helpful, your best friend is logging to trace actual parameters and results.      A word of caution : using memoization with mutable inputs or outputs will harm your mental health.   Zip merge   There are 2 ways to merge sorted lists into a unique sorted list : the fast, and the slow …   # The slow (list_1 + list_2).sort  # The fast i_1 = 0 i_2 = 0 result = []  while i_1 &lt; list_1.size and i_2 &lt; list_2.size   if list_1[i_1] &lt;= list_2[i_2]     result &lt;&lt; list_1[i_1]     i_1 += 1   elsif list_2[i_2] &lt;= list_1[i_1]     result &lt;&lt; list_2[i_2]     i_2 += 1   end end  while i_1 &lt; list_1.size   result &lt;&lt; list_1[i_1]   i_1 += 1 end  while i_2 &lt; list_2.size   result &lt;&lt; list_2[i_2]   i_2 += 1 end   Obviously, the slow version is a lot easier to read than the fast one. And the fast one could benefit from a bit of refactoring also … Nevertheless, the slow version is at best in O(n.ln(n)) whereas the fast on is in O(n). On large data, that can make a big difference.   Is that all ?   Obviously not, there can be a lot of other things going on slowly in algorithms, but from my experience, a software engineer can have a good career without knowing more about algorithms theory than that.   In the end, you manage to optimize this in-house algorithm, you become the company’s hero, you need your job and get a pay raise !   End word   The fact is, in 15 years of writing software, I did not write a lot of algorithmic code. I can categories my working with algorithms in 3 :      Write a simple algorithm for a non performance critical feature   Optimize an existing somewhat algorithmic part of code   Write a complex algorithm for a performance critical part of the system   Case 1. is not really an issue since however the code will be written, it will run fast enough. If you’re in case 3, there’s no shortcut, you’ll have to dig deep into algorithms and optimization, this happens rather rarely though. This leaves us with case 2, which I just wrote about.   Interestingly, my current job is deep into case 3 ! We’re building a risk engine for corporate markets and are borrowing a lot of techniques from database science … which is, you can guess, rather algorithmic !  ","categories": ["performance","algorithms","ruby"],
        "tags": [],
        "url": "/most-common-ways-to-speed-up-an-algorithm/",
        "teaser": null
      },{
        "title": "How I fixed 'Warning: Authentication failure. Retrying' with Vagrant",
        "excerpt":"Lately, I went into an upgrade cycle : Ubuntu, which forced me to upgrade Docker, and then Vagrant … you know the story. Unfortunately, my vagrant config did not want to start anymore with the following error :   Warning: Authentication failure. Retrying Warning: Authentication failure. Retrying Warning: Authentication failure. Retrying Warning: Authentication failure. Retrying Warning: Authentication failure. Retrying Warning: Authentication failure. Retrying Warning: Authentication failure. Retrying ...      I tried to destroy and re-create my box with the same result : it eventually timedout, but the provisions were not executed.   After searching the internet and a lot of experimentation, I managed to make it work by commenting out the following line in my Vagrantfile   # config.ssh.private_key_path = \"~/.ssh/id_rsa\"   I think I added this a long time ago to be able to push to github from my Vagrantbox, but after trying it, it work with the difference of my having to validate the remote ssh key, that’s not of a big deal.  ","categories": ["vagrant"],
        "tags": [],
        "url": "/how-i-fixed-warning-authentication-failure-retrying-with-vagrant/",
        "teaser": null
      },{
        "title": "How To Keep Programming When Assigned A Management Job",
        "excerpt":"Remaining a competent developer is a career long effort, if you stop programming, you’ll loose it ! As time goes, we are regularly offered pushed into management positions, sometimes by cluelessness, other times by cheer necessity ! Be it temporary or long term, here are some techniques to remain a relevant developer.   How Joe became a manager   def work   if manager?     go_to_meetings   else     program   end end   Imagine you’re Joe, an expert developer in a small software company. Everything is going fine, he’s working on interesting subjects with 4 other, rather junior, team mates. Management is OK, at least good enough not to cause too many troubles.   Suddenly, the team manager leaves for a better position in another company. Unprepared for this, the small organization has difficulties to find a suitable replacement, and, you’ll guess it, Joe is asked to take on management responsibilities until someone gets hired. How is Joe going to continue to do a good job at both programming and management ?   His first reaction   Two weeks into the job, Joe takes a step back and summarizes :      he feels depressed when he does not program for a long time   he has the feeling he’s been jumping from A to B to C to Z to A, and so on for two weeks without actually getting a lot done   he feels everyone is waiting for him to do things before they move   his mailbox is starting to make him nervous   it’s difficult to program anything if you are interrupted by meetings throughout the day   Joe figures out that he needs to reduce his management work if he wants to get back to programming.  After setting up an efficient TODO list. He has already read some personnal effectiveness books, so he knows how one can reduce his amount of work :      say no : he’ll have to keep this in mind all the time, especially when accepting meetings or when asked for some new work   automate : some tasks can be automated, others can be partly automated by a good process. He’ll have to regularly stop and think to improve how he works   delegate : he’d like to push more work to the team, but it will take some time. He decides to read things on self-organized team to know how to do   Joe’s measure of programming time   In order to get some continuous time for programming, Joe books ‘programming days’ in his calendar. This is also a way to say no to other meetings during this period. He starts with all Mondays, he hopes he’ll be able to book others days as he manages to do more programming. His ideal would be to book all week for programming (while still having the management job get done !)   This works well, and gives Joe a motivation boost since he has the opportunity to program again :-). There are still a few glitches though :      Joe often doesn’t program ‘on his own’ on Mondays because he is spending is time with junior programmers who are really appreciating his help   Joe once made the mistake of committing to a critical programming task that he did not manage to finish in 1 day and had to hand it over to another developer, losing more time   Joe is realizing that he is more of a programming coach than a direct developer now, and that he should not commit on critical tasks but rather help others to do so. Pair programming is becoming the norm for him.   Communication time   While doing his full programming days, Joe realized that emails can wait. If people want an immediate answer, let them use the phone ! He now reads and answers his emails in the morning when he arrives at work, at noon just after lunch and on evenings, just before leaving. That’s good enough for a day !   Joe also had to interrupt his programming day once for an urgent meeting … he now reserves 1 hour at the end of his programming days to handle such urgent meetings without interrupting his programming time.   Self organized teams   Meanwhile, Joe read some books about self organized teams, such as Management 3.0 Workout by Jurgen Appelo.      Self organization is some kind of aggressive delegation. Joe sees management as described in these books as a way to increase purpose and motivation throughout the team, while getting him more time for programming.   This kind of management perfectly suits agile teams. His team is already using some agile practices, such as automated testing, some kind of continuous integration, and quite a few others, to varying degrees. Joe decides to embark his team on a road to self organization, and to start by applying all the standard Scrum and eXtreme Programming practices. In parallel, he introduces the rolling Scrum Master role, where sprint after sprint, a different team member is responsible for :      organizing the recurring team meetings   representing the team in outside meetings   grooming the backlog before the demo and planning   This alone frees Joe 1 or 2 days of programming every week. He now helps his team mates to master all the technical agile practices they are not used to.   More self organization   A few months later, the team is doing well, Joe has some time for programming, but he still has some pure management job to do. From what he read from self organized teams, even these subjects can be delegated ! He decides to start workshops with the team :      How can we handle vacations in a self organized way ?   How can we handle the hiring in a self organized way ?   How can we handle performance feedback and pay raises in a self organized way ?   The journey is long and rough : one team member left as they were going seriously into agile. Nevertheless the team is already more motivated than it ever was, and delivers more value than it ever did.   The end result for Joe   Joe is now programming nearly as much as his team mates ! Most of his programming time is coaching time though. He does not sit and hack his way into a feature as he used to do. He’s missing that a bit, he’s also missing learning new technologies.   That’s why he started working on side projects at home, using the latest hype JavaScript framework. He’s reading generalist programming books also, such as the 7 XXX in 7 weeks series which allow him to learn without loosing time on technical quirks he’s not interested in at the moment …      Good luck Joe !  ","categories": ["agile","management","management30","selforganizing","personal-productivity"],
        "tags": [],
        "url": "/how-to-keep-programming-when-assigned-a-management-job/",
        "teaser": null
      },{
        "title": "How I fixed 'invalid byte sequence in US-ASCII' exception with octopress (and vagrant)",
        "excerpt":"I don’t know if you noticed, but Octopress 2 preview is slow as hell on Virtual Box. I wanted to try using Docker instead. So a few weeks ago, I started to upgrade my Vagrant, Virtual Box and Docker stack …   Hell loose !      I know all open source software needs to be kept up to date at all time, but it’s not that easy. Especialy when the versions in the Ubuntu repostiories are really old and you had to install them manually. Anyway, after a bit of tedious work, I finally got to the point where my Octopress blog was running on a brand new stack. And I got this error :   vagrant@239c4077ae16:/vagrant$bundle exec rake generate ## Generating Site with Jekyll unchanged sass/screen.scss /vagrant/vendor/bundle/gems/liquid-2.3.0/lib/liquid/htmltags.rb:44: warning: duplicated key at line 47 ignored: \"index0\" Configuration from /vagrant/_config.yml Building site: source -&gt; public YAML Exception reading index.markdown: invalid byte sequence in US-ASCII /vagrant/plugins/backtick_code_block.rb:13:in `gsub': invalid byte sequence in US-ASCII (ArgumentError) \tfrom /vagrant/plugins/backtick_code_block.rb:13:in `render_code_block' \tfrom /vagrant/plugins/octopress_filters.rb:12:in `pre_filter' \tfrom /vagrant/plugins/octopress_filters.rb:28:in `pre_render' \tfrom /vagrant/plugins/post_filters.rb:112:in `block in pre_render' \tfrom /vagrant/plugins/post_filters.rb:111:in `each' \tfrom /vagrant/plugins/post_filters.rb:111:in `pre_render' \tfrom /vagrant/plugins/post_filters.rb:166:in `do_layout' \tfrom /vagrant/vendor/bundle/gems/jekyll-0.12.1/lib/jekyll/page.rb:100:in `render' \tfrom /vagrant/vendor/bundle/gems/jekyll-0.12.1/lib/jekyll/site.rb:204:in `block in render' \tfrom /vagrant/vendor/bundle/gems/jekyll-0.12.1/lib/jekyll/site.rb:203:in `each' \tfrom /vagrant/vendor/bundle/gems/jekyll-0.12.1/lib/jekyll/site.rb:203:in `render' \tfrom /vagrant/vendor/bundle/gems/jekyll-0.12.1/lib/jekyll/site.rb:41:in `process' \tfrom /vagrant/vendor/bundle/gems/jekyll-0.12.1/bin/jekyll:264:in `&lt;top (required)&gt;' \tfrom /vagrant/vendor/bundle/bin/jekyll:23:in `load' \tfrom /vagrant/vendor/bundle/bin/jekyll:23:in `&lt;main&gt;'   After some searching on the internet, I found this post by Alex Zeitler that made me figure out that locales were not set correctly by default on my vm. I just needed to set this once and for all in my Vagrantfile   config.vm.provision \"shell\", privileged: false, inline: &lt;&lt;-SHELL   if [ -z \"$LANG\" ]; then     echo \"Setting LANG to us UTF8\"     echo 'export LANG=en_US.UTF-8' &gt;&gt; ~/.bashrc   fi   if [ -z \"$LC_ALL\" ]; then     echo \"Setting LC_ALL to us UTF8\"     echo 'export LC_ALL=en_US.UTF-8' &gt;&gt; ~/.bashrc   fi SHELL   After reprovisioning my box, generate and preview were working as expected !   :-( Now I’ve got a pygement-ruby issue to deal with, I guess I’ll be back on the subject soon …  ","categories": ["octopress","vagrant"],
        "tags": [],
        "url": "/how-i-fixed-invalid-byte-sequence-in-us-ascii-exception-with-octopress-and-vagrant/",
        "teaser": null
      },{
        "title": "Recipes For An Agile Workspace",
        "excerpt":"Nowadays, start-ups and tech companies seem to be competing for the most beautiful offices in order to attract top talents.      Unfortunately, sometimes, it’s just the good old office in disguise. At the opposite, some teams or companies actually think and decide how to organize their offices. For example, here is what Kent Beck says about office setup in XP embrace change      If you don’t have a reasonable place to work, your project won’t be successful. The difference between a good space for the team and a bad space for the team is immediate and dramatic.    Joel Spolsky has been blogging a lot about the office setups at FogBugz      While defining its agile culture, Spotify too designed custom offices      How does it work ?   Let’s review a few office design practices to understand how they work :   The team room or space   Having a delimited room helps to build a real team. First, by isolating from noises, it helps the team to focus on its work and current topics. Second, by providing a visual barrier to cross before anyone can disturb the team. Finally, it allows the team to organize its own visual management.      Private team gathering area   An area where the team (and only the team) can meet at any time is great to do on the spot meetings (without the hassle of finding a room). This area should be comfortable and provide a relaxed and creative atmosphere (which is very important to conduct efficient retrospectives). It can also be used to take breaks and lunches, fostering team spirit again.      Writable walls   It’s pretty easy to cover the walls with whiteboard stickers so that there is plenty of room for discussing design and otherwise keeping important things visible for everyone.   Laptops, Wifi and movable desks   Lean manufacturing favors simple, versatile and configurable machines over powerful specialised ones, which are often pretty long to setup. In software, we don’t rely on machines as much as factories do, but there might be something to take here. Using laptops, a good Wifi and movable desks, it is pretty easy to reconfigure your office as it best fits the team at a particular moment. This way a team can easily adapt to personal preferences and turnover.   Wall screens   Agile and Lean thinkings both heavily use visual management. Having a lot of wall screens dedicated to displaying team relevant information in the team space brings a lot of value. To get the maximum benefit, these wall screens should be fully configurable and movable by the team.      Team budget   Eventually, in order to make sure the team can always work as efficiently as possible, it should have some money to spend for its workspace. It could be used to buy any of the upper, but also anything else that might help the team at a particular point in time. Without such a budget, teams have to make a request to the office department to get anything, often to no avail, or with extra long delays.   Distributed teams   Distributed teams bring their own challenges … and workplaces needs.   Remote team space   If your team is split between 2 places (as our team is at Murex) then obviously, each site should have its own delimited space, with its own gathering area.   Constant visio connection   Connecting gathering areas through visio is a great way to build the distributed team. It prevents loosing time when a meeting is required. It also builds the team by showing what’s happening on the other side and by making it possible to take distributed lunches and breaks !      Distributed whiteboards   I never had the chance to work with one of these, but I often missed a whiteboard when working remotely … Any input on this subject would be welcome !   Lego offices   Here is another extract from Kent Beck’s “XP Embrace Change”      The courage value finds its expression in the XP attitude toward facilities. If the corporate attitude toward facilities is at odds with the team’s attitude, the team wins. If the computers are in the wrong place, they are moved. If the partitions are in the way, they are taken down. If the lights are too bright, they are taken out. If the phones are too loud, one day, mysteriously, they are all found to have cotton stuffed in the bells.    Improving the office is a cheap way to make a software organization more efficient. For another reference, here is what Tom DeMarco &amp; Tim Lister wrote in Peopleware      Police-mentality planners design workplaces the way they would design prisons: optimized for containment at minimal cost. We have unthinkingly yielded to them on the subject of workplace design, yet for most organizations with productivity problems, there is no more fruitful area for improvement than the workplace. As long as workers are crowded into noisy, sterile, disruptive space, it’s not worth improving anything but the workplace.    Instead of designing offices, furniture departments in companies could provide a constant stock of building blocks for office building. Teams could use a workplace budget to build the workspace that suits them most. A good Wifi, laptops, enough floor power plugs, extra screens, movable tables, movable walls and easy to fix wall monitors are all things that would make this possible.     ","categories": ["agile","lean","workplace","selforganizing","team building"],
        "tags": [],
        "url": "/recipes-for-an-agile-workspace/",
        "teaser": null
      },{
        "title": "How Long Can Your Inner Drive Last ?",
        "excerpt":"Any software project (job, startup or side project) will require some time before one can get real feedback from real users.      The hard truth   Your inner drive will only last up to some point. Without feedback, your motivation will die, and this will kill your project.   How to deal with it ?   Know it before you start   From my own past experiences, I could find that my inner drive has usually disappeared after 2 years (I don’t want a word about the time I actually wasted to discover this …). You too can try to estimate how long you can keep on without much feedback, go through your previous experiences to get an idea.   Once you have an idea of how long you can keep on without much feedback, you’re in a much better place to decide to embark on a new project.   Don’t drown in the code   I once started a side project partly because I was fed up with the poor technologies I was asked to use at work. My project was some kind of salvation. The drawback of this situation is that I tended to dive into code way too early ! Building a real product takes time, and that’s a sure way to get late feedback.   Use Lean Startup techniques      Lean Startup is all about getting constant user feedback, even before having any user. I especially liked the book UX for Lean Startups that explains all the ways to get feedback from the cheapest (interviews) to the most expensive (HTML mockups) without actually coding anything   Watch out for the Duke Nukem Forever syndrome   Be very careful of endeavors that promise an Eldorado after long hard work that should last months or years. Products need to ship early with as fewest features, not late with many features. If you embark on such project, you’re pretty likely to :      burn out before the end   deliver something that is already outdated the day it goes live   never deliver anything   If you’re already on such a project, I strongly suggest quitting.   End word   Maybe getting real feedback from real users takes time, but getting very early feedback from future users is almost always possible.   Keep going, get some feedback !  ","categories": ["software","side-project","lean-startup"],
        "tags": [],
        "url": "/how-long-can-your-inner-drive-last/",
        "teaser": null
      },{
        "title": "Are Most Agile Teams Doing Continuous Improvement The Silly Way ?",
        "excerpt":"A few weeks ago, as I was looking the internet for Lean principles to improve our way of working, I fell upon this book Petit guide de management lean a l’usage des équipes agiles (NB: the book is in French, the title means ‘Little lean management guide at the usage of agile teams’). It made me think and I thought it deserved a blog post.      It explains that agility can be though of as a set of practices and principles, shared through a huge community. This makes it a great production system, where improvement are mostly brought by gut feeling retrospectives and trial of other teams practices. At the contrary, Lean is very light framework for continuous improvement, relying on a more systematic waste elimination.   After this introduction, the bulk of the book is composed of a set of 9 detailed real life stories demonstrating the lean way of bringing improvements. Are is a summary of 3 of these :   Unknown Category at Project Condor      The team maintains a virtual call center of poor quality, resulting in lots of incidents in production. Here is how they deal with the situation :      They start by showing the issues, by categorizing them from the logs, they discover that they are mainly related to training, network, but surprisingly, the majority (30%) cannot be categorized and fall in the ‘unknown’ category.   By improving logging, the unknown category falls to 5% ! Fixing network timeout issues then makes the client a lot happier.        Eventually, they go to client’s site while monitoring the logs at the same time. They discover that remaining issues are explained by              some users are using a double click to hack the system and jump ahead of the queue       the hang up and hang off buttons being too close, which results in operator misleadingly ending their communication       calls to wrong numbers being logged as incidents           The authors conclude that while uncomfortable, going to the clients increased motivation for everyone, fixed issues and made the client happier.   All Guilty !   An author is called to help a team which is working on unifying reimbursement systems after a merge. The project is late and the product is unreliable.   The first step he takes is to visualize a target (next batch in 3 months) and the flow      The flow whiteboard shows that tasks get stuck when in need of clarifications from business analysts. Tension between people is already increasing. They Team decides to visualize this with ‘blocker’ post-its.   After an inquire with the BAs, it turns out that they don’t see the waiting tasks the issue management software.   The final step is to agree all together (developers and BAs) on an uniform way of defining and following blocking issues      Tickets are discussed at the stand-up, and unblocked issues are also visualized. As a result in only 2 weeks, the process fluidifies, and the tensions decrease.   The authors conclude that cross functional teams work better   PDCA   This story starts in a rather typical way : the client would like the team to go faster.   The team engages into a series of Plan Do Check Act cycles.   Hypothesis 1 : There must be some obvious waste   The team decides to log any waste occurring for 2 weeks. Even with discipline, only 2 hours are spotted during the 60 man.days of the sprint   Hypothesis 2 : Too much refactoring or too much test writing   For a few weeks, the authors keeps a daily log of the team activities during after every stand-up meeting.   It turns out that writing tests accounts for 5,5% of the time, refactoring for only 2% but programming for 40% !   Hypothesis 3 : if there is something to improve, it must be in programming.   For 20 half days, the author embarks on the tedious task of keeping a very detailed log of activities while taking the role of navigator in pair programming sessions.       They clearly understand that most time is taken not in writing tests, doing refactoring or writing complicated code, but in understanding existing code, third-parties and APIs   Aftermath   First, they avoided loosing time on improving the wrong thing The team also agreed on the practice of asking for help at the good person when starting stories. Doing that, they got a nearly 100% speed boost !   The rest of the book   The book highlights a lot of other continuous improvement practices. For example :      The ‘problem box’ where team members can log any waste they are going through during their work. This made me think of my Plan For Technical Debt   Individual improvement follow up : a single team member is responsible to drive an improvement to its conclusion, in order to make sure that it is not forgotten   I personally found this book to be just great ! It’s short and focused, pragmatic, and a pretty easy read. If you liked Scrum and XP from the trenches I think you should read it. More generally, I think it’s very useful for anyone involved in the development process who would like to push agility a little further.   There’s a catch though, it’s in french ! I guess I could take the time to translate it, tell me if you are interested.  ","categories": ["lean","agile","book","continuous improvement"],
        "tags": [],
        "url": "/are-most-agile-teams-doing-continuous-improvement-the-silly-way/",
        "teaser": null
      },{
        "title": "Stop feeling like a kid every time you ask a day off",
        "excerpt":"Most companies don’t have unlimited vacations. Most likely, if you are an employee, your days off need to be validated by your boss. If you are like me, this feels like pulling teeth … Every time I have to ask for a day off, I feel like a kid asking for an extra candy   That’s not the only issue with how we traditionally deal with vacations :      The boss has to make sure that there are always enough people at the office to handle emergencies   If the boss is himself away, getting a day off at the last moment becomes difficult   Our Delegation Board   Our team at work is already rather agile : we’re using a blend of XP and Scrumban, we follow solid engineering practices and we are always trying to improve to deliver more value to our users. After reading Management 3.0 and Management Workouts I was convinced that I needed to move the team towards even more self organization.   We started by doing delegation poker, we created a delegation board, and set out to move all cards we could to the right side of the board.      We spotted the following topics that could be more self organized      vacations   hiring   business objectives and end of year evaluations   various recurring scrum related activities   self development objectives   Vacations   We decided to tackle vacations first, because it seemed easy. We did a quick brainstorming, and here is what we came up with :      We cannot change the company policy, so I’ll still have to officially validate everyone’s vacations in the system but …   I’ll validate anything asked !   Our internal system allows to check the other team member’s vacations, so everyone in the team is responsible to check that there will always be at least 2 people at work before taking a day off      It’s been 6 months, and it’s been working like a charm. I think it made everything simpler once we had a clear rule about when vacations were OK or not.  ","categories": ["management","agile","selforganizing","management30"],
        "tags": [],
        "url": "/stop-feeling-like-a-kid-everytime-you-ask-a-day-off/",
        "teaser": null
      },{
        "title": "Scrum teams do not need A Scrum Master",
        "excerpt":"We don’t have an official scrum master in our team anymore. We now have 7 ! A different team member assumes the full scrum master role at every sprint.      Why did we switch to Scrum Master round robin ?   It all started with our delegation poker (see previous post). We noticed that there were still quite some activities that I (the manager) was the only one to do. We hoped that by sharing these activities throughout the team, we would :      increase everyone’s motivation by showing a bigger picture   leave me more time for programming   learn new skills to everyone   How did we do it ?   Before our delegation board meeting, we had prepared all the activities we are regularly doing. We found quite a few recurrent non programming tasks that traditionally eschewed to the team lead (me in that case). The initial plan was to find ways to delegate these activities to the team during the meeting, but it would have took way too much time.   Having identified all these activities, I individually suggested at everyone in the team to do a round robin scrum master role, which would handle all these activities for a sprint. I was happily surprise by the quasi complete buy-in from all the team. Before I announced it to the team, I did 2 things :   List all the activities of the scrum master.   Here is a subset :      attend other teams meetings   make sure there are enough workable stories for the next planning session with the product owner   prepare and animate a retrospective   animate the demo   Make this more fun !   Having #fun #geeky #work #environment Our scrummaster is #DarthVader or #Yuda #starwars at @Work_at_Murex @pbourgau pic.twitter.com/CbZXCYwBge &mdash; Ahmad Atwi (@ahmadatwi) 1 février 2016   I bought totems for the scrum master. The team chose to keep Yoda in Paris and send Darth Vador in Ahmad suitcase back in Beirut.   What did we get ?   After a few sprints of this, the feedback I got is that people like the increased involvement in the big picture (Daniel Pink told us that already). On my part, it allowed to increase my programming time to nearly 7 uninterrupted days per sprint (10 days). I’m quite happy with this ! As for the third goal, learning new skills, I guess we’ll need a bit more time to figure this out.  ","categories": ["management","agile","selforganizing","management30"],
        "tags": [],
        "url": "/scrum-teams-do-not-need-a-scrum-master/",
        "teaser": null
      },{
        "title": "Make hiring everyone's business",
        "excerpt":"Let me tell you a typical hiring story. A bit more than 10 years ago, I was contractor at a bank on a C++ front office application. The system had initially been well engineer, but it had been stuffed together merged with another system that had a completely different architecture. Ha, and there were no automated tests. As you’d guess, we had quite our dose of bugs, and maintaining this application was not easy.      The manager wanted to hire a C++ programmer to beef up the team. Along with the division’s architect he was conducting interviews to find just that guy. A few weeks later, he found a known C++ &amp; Windows local expert. A few weeks later, the team unanimously declared that we could not work with him because he was both pretentious and incompetent.   I’m pretty sure every developer already witnessed a similar story.   Managers do the hiring   This is the usual practice. The motivation is that managers have more experience so they should be better at it, and also that it alleviates developers from non-programming work. As we saw in the introduction story, there are issues with this approach too :      it’s pretty bad for the team ‘fit’ : there is no team building here. You often end up with a collection of individual working at the same place rather than with a real team.   during these interviews, the candidate only interacts with a particular type of profiles, who can tell how he’ll be doing with more junior team members for example ?   as usual, this administrative work removes some programming from the manager’s time. That’s often not a good bargain   it decreases the team sense of control, as it is being ‘forced’ to work with someone they did not choose. This makes it harder for them to take responsibility of what they are trying to achieve.   How we self organized hiring   As I already explained in a previous post we engaged in various self delegation initiative by doing a delegation board. During this meeting, we identified hiring as something that could be self delegated.   In order to fix that, we started a workshop for 1 hour per day with a sub group of the team. We started with a brainstorming, then we voted, and finally presented our proposition to the rest of the team, which actually validated it. The whole thing lasted for one week (5 hours).      What we decided for hiring   Here is the process we have decided to follow to handle hiring team members      First, everyone in the team is responsible to speak up when he thinks we need to hire someone, during our retrospectives for example   To handle new applications, we have prepared an online interview. It contains open questions and online coding exercises, on cyber-dojo, which allows us to track all the candidates iterations   We do a round robin to find out who will follow the candidate through the hiring process. This means taking notes, organizing interviews, keeping the rest of the team up to date.   When a candidate passes the online interview, we will receive him for an technical and general interview with 2 people from the team.   All the team will then meet the candidate for a coffee break, in order to know if he will fit in the team   We’ll then play the Google hiring committee until we reach a consensus   When in doubt, we’ll do extra interviews, and repeat   That’s just what we came up with, if you start this road, you might well come up with a very different process.   One more thing   It’s great to have a selective hiring, but you’ll also need to improve the input of the hiring pipeline, but that’s another story …  ","categories": ["selforganizing","management30","hiring"],
        "tags": [],
        "url": "/make-hiring-everyones-business/",
        "teaser": null
      },{
        "title": "How to deal with the incentive system in an agile team ?",
        "excerpt":"For an organization, there’s something schizophrenic about valuing team work, wanting agility but sticking to individual objectives and assessments.      As highlighted in Build To Last genuine company values must permeate all aspects of the organization. Being agile also means putting teamwork first and that is quite incompatible with typical individual management by objectives and bonuses.   This is nothing new, many have already written about the subject      Joel Spolsky : Incentive Pay Considered Harmful   Daniel Pink : Drive : The Surprising Truth About What Motivates Us   Alfie Kohn : Punished by Rewards: The Trouble with Gold Stars, Incentive Plans, A’s, Praise, and Other Bribes   Jurgen Appelo : The Bonus System   The problem is you usually don’t have the choice ! Whether your team wants to or not, the chances are that your company already has a mandatory individual bonus scheme in place.   Our delegation initiative   As you might remember if you read my previous articles, our team decided to move to more self organization. After identifying the end of year evaluation as something that could be further delegated, we embarked on brainstorming sessions about how to do it.   We discussed quite some options during our workshops. Things like :      What is the goal of the annual evaluation ?   Should some people have a central role in the process ?   Should we encourage efforts or results ?   Should evaluations be public or private ?   What’s at stack at the annual evaluation ?   Should individual learning plans be part of the evaluation ?   What we came up with   As a guiding principle, we agreed to value efforts rather than results. Sometimes, results are independent of efforts, and through efforts, it is always possible to get the best of any situation, even if the best is not a good result in itself.   Throughout the year   We are all sharing the same team objectives :      “the team finishes the prioritized epics” : we do regular epics prioritization sessions with our product owner and add the main epics to everyone’s objectives   “work on the same subject as the whole team” : we have an overspecialization matrix that uses correlation to make sure that everyone touches the same parts of the system as the rest of the team   Every 2 months, we are doing 360° feedback sessions, for everyone to give continuous feedback to others.   Optionally, we setup Kudo boxes to give and receive thanks from others.      At the end of the year   When the time of the official end of year evaluation comes, we do 360°, 1 to 1 rating on the following topics.      commitment &amp; courage (doing not so fun work, find difficult compromises, say the truth, strive for quality work)   helping others (giving a training, providing coaching, help others, do some evangelization)   self improvement (taking feedback into account, improve technical skills, work on soft skills)   team player (build friendly environment, contribute to team improvement, positive attitude, use a non violent communication)   Grades are signed, and anyone can ask someone else for a discussion about his grades.   Overall, everyone gets an average grade, which we use to suggest a company grade (below, achieved, exceeded) to the HR system. It then gets homogenized with the rest of the company, but this is then out of our control.   One thing that made this possible is also that after taking a close look at the company grade system, we saw that not so much was at stake … Provided we make sure everyone has what it takes to do the job, anyone can get an achieved evaluation by doing enough efforts. Getting an exceeded is like a bonus for a particular good year.   Last word   I’m pretty sure we could make this more simple in an environment with more end to end feedback (start-up or pizza teams) but we feel that our current alternative is a lot better than having the manager deciding everything on his own.  ","categories": ["selforganizing","management30","company-bonus"],
        "tags": [],
        "url": "/how-to-deal-with-the-incentive-system-in-an-agile-team/",
        "teaser": null
      },{
        "title": "Trello Templates to Boost Your Remote Retrospectives",
        "excerpt":"I already wrote how we started to use Trello to do our remote retrospectives. A while ago, my team mate Bilal Tayara started to collect our activities into trello templates.      New activities   I just imported new activities we have been using in the past months :      SMART Goals   5 Whys   Circle of Questions   Communication   Engagement   ESVP   Fishbone   Focus On/Focus Off   Force Field Analysis   Gifts   Glad Sad Mad   Health Radar   HHH   Hot Air Balloon   How are you feeling today ?   Improvements Brainstorming   Learning Matrix   Note To Self   Plus / Delta   Problem -&gt; Action   Punctual Paulo   Retrospective Planning Game   Risk Brainstorming   ROTI   Sail Boat   Satisfaction Histogram   Take Away   Team   Team Radar   Warmup Question   What happened since last retro ?   How to use these templates   Clone the template to your organization every time you want to use it. There are instructions in the template to explain how to use it. For more detailed instructions, have a look at the original :      Agile Retrospectives, Making Good Teams Great by Esther Derby and Diana Larsen, Foreword by Ken Schwaber         Getting Value out of Agile Retrospectives, Second Edition by Luis Gonçalves and Ben Linders         Fun Retrospectives by Taina Caetano and Paulo Caroli      One last thing : in order to improve this collection, we’d like to have your feedback on existing activities, or your own new templates for other activities.  ","categories": ["retrospectives","remote","trello","continuous improvement"],
        "tags": [],
        "url": "/trello-templates-to-boost-your-remote-retrospectives/",
        "teaser": null
      },{
        "title": "How We Decentralized Our Company's Training Program",
        "excerpt":"Maybe your company too has a development program you can use to track and organize your training path. That’s already great ! It is often not perfect though. Sometimes, the initiative in itself can feel like an afterthought. Some other times, the process is completely left to the employee and his manager, with varying results, from great to forced upon everyone.      How did we rebuild this program in our team   As you might remember if you read my previous posts, we started a self organization initiative using a delegation board. When we tried to decentralize the end of year evaluation, we started discussing the subjects of personal development. We decided to have a dedicated workshop about this subject   Guiding principles   In our company, the development program allows a manager to assign specific learning goals to his collaborators, and anyone can also create his own goals to justify going to specific trainings. Starting from that, we agreed that the development program should be used to help anyone in the team to improve skills that are interesting for both the company and the employee. If the employee is interested in something completely unrelated to work, then he should obviously tackle that in his own time. If the company needs its employee to acquire some immediate mandatory skills, then that should be part of the daily job and subject to the end of year evaluation.   The idea is to try to find a win-win combinations, were employees are motivated to work their best at a company that they know helps them to full-fill their long term goals.   What did we came us with ?      This workshop was quite effective :      First of all, everyone is responsible for his own development, it’s a chance offered by the company. It cannot be forced unto people.   It is based on volunteer mentorship : everyone should have regular meetings with his mentor to asses his progress on his long term goal, to try to get feedback and ideas about how to move forward. We started by saying that the team leader is the default mentor, but anyone can find another mentor at any moment.   The process starts with the search for a long term goal. Examples are “I want to be an agile coach”, or “I want to be a performance expert”   With the help of the mentor if needed, the long term goal should be split in yearly objectives and tracked using OKRs   Everyone is free to track his progress has he wishes. Visual tracking makes the discussion with the mentor a lot easier. We have been looking at tools to track OKRs but it was surprisingly difficult to find one that suited our use. Most are enterprisy tools with manager-managee relationships. We just wanted simple personal tracking tools. For my part, I adapted a Trello board I found on the internet.   Conclusion   After doing a few months of this, I can say that this has a positive aspect on motivation, people told me that they felt more in control of their destiny. Something else I noted, is that junior developers need help and guidance. They often don’t really know what they want to do in a few years, so they really need a mentor to help them find out the what and the how. The other side of this is that a team obviously needs senior developers to act as mentor … among other things.  ","categories": ["management","agile","selforganizing","management30","career"],
        "tags": [],
        "url": "/how-we-decentralized-our-companys-training-program/",
        "teaser": null
      },{
        "title": "A Lego Office Experiment",
        "excerpt":"Rome wasn’t built in a day, neither will be your Lego Office … This week is Devoxx in Paris, for the occasion, we decided to stream a Lego office experiment that my team did.   What are Lego Offices   I wrote about Lego office in a previous post, as a concept and a set of material that would allow teams to easily configure their workspaces as they wish. At that time, I thought of :      laptops   extra monitors   tables and chairs on wheels   movable walls   movable white boards   movable wall screens   a good Wifi   Our experiment       The last floor of our building in Paris has a nice (really) open space area with rolling tables that we could use. We moved all the team there for one week. As we don’t have laptops with Wifi, we simply asked for enough power and network plugs.        The place is large enough to have dedicated rest and meeting space. We managed to create a visio setup using our build monitor. We even had a remote lunch once, which we should repeat more often !   What’s missing   Obviously, this was a quick experiment, nothing like a fully architectured Lego workspace, still it helped us to discover issues and blocking points. It turns out that cables are an issue !      After getting some feedback throughout the company, it turns out that some developers would never trade their high end connected desktop workstation for a Wifi laptop. Our C++ build can be pretty CPU and network intensive … I guess that means that a real Lego office might require one of the following :      A technical floor with all the place required to bring any kind of cables to your machine   Powerful servers in a data center to do the grunt work for you   Single cable machines (Apple’s USB-C is the only one I can think of right now)   Last minute question   Thinking of it … could different remote offices such as your home, or a co-working space be considered as Lego blocks ?  ","categories": ["selforganizing","workplace"],
        "tags": [],
        "url": "/a-lego-office-experiment/",
        "teaser": null
      },{
        "title": "Pair Programming, From Pain Zone To Skill Zone",
        "excerpt":"We’re doing pair programming almost all the the time in our team. A few weeks ago, we went to Devoxx Paris, and 2 team mates used pair programming at a hands on session : they finished way before the others and had the time to take a 30 minute break. Pair programming delivers more … when it works.   Flavors of Pair Programming   Throughout our experience, we have identified some situations where it does or does not work :                  Pain Zone       Skill Zone                       Wandering buddies       Mates                 Expert / Novice       Mentor &amp; Apprentice           The Pain Zone   Pair programming can be painful. It can feel like a waste of time or become extremely frustrating. Here are the typical situations and how to fix them.   The Wandering Buddies      That’s what you’d get if you asked to newcomers straight out of school to pair program on existing source code on day 1. They’d be dabbling here and there, without going anywhere. Each one would try to get the keyboard to try his way, but in the end, no general direction would be followed.   The cure in this case (as often) is to stop and think. At least one in the pair needs to learn how to pair program. It’s not difficult, that’s just :      Discuss and agree on a general plan first   Keep a todo list   Track your advancement on your todo list while you don’t have the keyboard   Stop and discuss from time to time   By starting that, the pair will quickly move into the ‘mates’ state.   The Expert / The Novice      That’s when a ‘lone-wolf’ expert pairs with a junior. The expert picks the keyboard, dashes through the tasks, and does not wait for his pair to follow (which usually means he does not).   Regular pair programming means a change in job responsibilities for such expert programmers. If they were expert doers before, they should now become expert mentors or coaches. That’s quite a difference to them: it means that their goal becomes to make their pair finish the task! It can be very frustrating at first. Nevertheless, the overall skill level increases so fast that’s it’s very fulfilling for the team as a whole.   The Skill Zone   This is where we want to be. When it happens, pair programming feels productive, interesting and fulfilling. Contrary to what is often thought, this can happen whatever the skill levels of the 2 programmers.   The Mates      The programmers have equivalent skills on the subject at hand. They organized in a way that they are both learning from each other:      By having to verbalize what and why they are doing things   By challenging each others assumptions   By regularly trying a different way than what they would have done alone   The Mentor &amp; The Apprentice      Other times, there can be a real difference in skills about the topic at hand. In this situation, the goal of the most knowledgeable is to train his pair, who’s goal is to learn. The task becomes some kind of exercise on which to practice.   During a typical pairing session, a pair might switch many times between mentor and mates states. Each one in the pair might assume the mentor’s role for different aspect of the work. At the end of the day, everyone should feel as if they had learned and accomplished good work.   Last Word of Caution   Pair programming is exhausting. Don’t expect to be able to do 40 hours of pair programming per week. After 6 hours in a day, one usually starts to get a lot less efficient.   Both juniors and experienced programmers also need some time by their own, to experiment things by themselves or to complete a simple task without having worry about explaining everything.   These are the reasons why XP stresses having a sustainable pace and leaving space for Slack time  ","categories": ["agile","pair programming","coaching"],
        "tags": [],
        "url": "/pair-programming-from-pain-zone-to-skill-zone/",
        "teaser": null
      },{
        "title": "Real Developers Ship",
        "excerpt":"If you are a developer and you are not shipping, you’re in the danger zone. I believe it’s mostly your fault, and it’s time to act.      Nothing new   It’s been told before. Famous blogs have said the same thing :      In Hacker News Real Programmers ship without any explanation, as if it meant it all.   In Coding Horror Version 1 Sucks, But Ship It Anyway Jeff Atwood stresses the benefits of shipping early.   In Coding Horror again Shipping Is Not Enough Jeff Atwood goes one step further and explains that we need to create software that is going to be used.   In his fatherly advice to new programmers Chuck Jazdzewski’s says “You don’t get paid to program, you get paid to ship.”   What does it really mean ?   If you are not shipping, are you really developing something ? Maybe it’s more some kind of research project, or maybe you are just there for learning ? Real Developers know what they are in for.   If you are not shipping because the quality is too bad, that’s because you’ve been developing poor quality software … Real developers write quality software. (BTW, shipping bug fixes is not shipping)   If you are not shipping because you don’t have yet enough features, that’s because your MVP is not small enough (despite the current Skateboard metaphor, this is not an MVP). So Real developers follow lean (startup) principles.   If you are not shipping because it’s much work, that’s because you did not automate delivery. Real developers use continuous delivery.   If you are not shipping because someone else said so, maybe it’s time to convince her. Real developers know how to deal with people.   If you are not shipping because you are in a shitty environment … Real developers work on their skills to be able to quit when needed.   Shipping above all else   My point is that every developer or organization containing developers should put shipping value as their absolute priority, and things should get a lot simpler.  ","categories": ["agile","software"],
        "tags": [],
        "url": "/real-developers-ship/",
        "teaser": null
      },{
        "title": "How I fixed the unknown language Pygments error in Octopress",
        "excerpt":"Last time I tried to insert a code snippet in my Octopress blog, I was hurt by the following error :   vagrant@ae4a04cebb73:/vagrant$ bundle exec rake generate ## Generating Site with Jekyll unchanged sass/screen.scss Configuration from /vagrant/_config.yml Building site: source -&gt; public /vagrant/plugins/pygments_code.rb:27:in `rescue in pygments': Pygments can't parse unknown language: ruby. (RuntimeError) \tfrom /vagrant/plugins/pygments_code.rb:24:in `pygments' \tfrom /vagrant/plugins/pygments_code.rb:14:in `highlight' \tfrom /vagrant/plugins/backtick_code_block.rb:37:in `block in render_code_block' \tfrom /vagrant/plugins/backtick_code_block.rb:13:in `gsub' \tfrom /vagrant/plugins/backtick_code_block.rb:13:in `render_code_block' \tfrom /vagrant/plugins/octopress_filters.rb:12:in `pre_filter' \tfrom /vagrant/plugins/octopress_filters.rb:28:in `pre_render' \tfrom /vagrant/plugins/post_filters.rb:112:in `block in pre_render' \tfrom /vagrant/plugins/post_filters.rb:111:in `each' \tfrom /vagrant/plugins/post_filters.rb:111:in `pre_render' \tfrom /vagrant/plugins/post_filters.rb:166:in `do_layout' \tfrom /vagrant/vendor/bundle/gems/jekyll-0.12.1/lib/jekyll/post.rb:195:in `render' \tfrom /vagrant/vendor/bundle/gems/jekyll-0.12.1/lib/jekyll/site.rb:200:in `block in render' \tfrom /vagrant/vendor/bundle/gems/jekyll-0.12.1/lib/jekyll/site.rb:199:in `each' \tfrom /vagrant/vendor/bundle/gems/jekyll-0.12.1/lib/jekyll/site.rb:199:in `render' \tfrom /vagrant/vendor/bundle/gems/jekyll-0.12.1/lib/jekyll/site.rb:41:in `process' \tfrom /vagrant/vendor/bundle/gems/jekyll-0.12.1/bin/jekyll:264:in `&lt;top (required)&gt;' \tfrom /vagrant/vendor/bundle/bin/jekyll:22:in `load' \tfrom /vagrant/vendor/bundle/bin/jekyll:22:in `&lt;main&gt;'    I had been switching to Docker in Vagrant to host my Octopress installation, but that did not seem related …   Plan A : Upgrade Octopress   It turned out it was related to my migration to Vagrant and Docker.xs This kind of nasty error often comes from obsolete environment. Internet confirmed confirmed it :      Ubuntu now uses Python3 instead of Python2   Octopress2 uses an old version of Pygments   That version of Pygments requires Python2   I thought that I did not want to play this game against open source libs, so I set out to update. Octopress 3 has been released. It’s not yet documented yet, and there is no official upgrade guide, but some guys have done it :      http://samwize.com/2015/09/30/migrating-octopress-2-to-octopress-3/   http://dgmstuart.github.io/blog/2016/01/22/migrating-from-octopress-2-to-3/   https://www.justinrummel.com/migrating-from-octopress-2-to-octopress-3/   I tried it … but I stopped before the end. It’s kind of working, but fundamental features like category archives required too much Jekylling for me at the time. And themes are still an issue … I mean, stock theme uses a clean and simple gem based update workflow, but most other themes rely on a tricky git clone-merge update workflow.   Plan B : Find a hosted blog engine for hackers   I found quite a few options :      Medium Looks cool, but there is no simple migration tool, and I’ve been burned by postero.us shutting down, I want to keep control on my content   Posthaven Having loved postero.us, this one seems cool, It’s just I’m not sure the guys are keeping the motivation to increase their feature list with their current business model   Ghost Looked great, but too expensive for me (19$ per month)   Codrspace This is a special blogging platform for coders, but it seems it has not been updated for a long time   LogDown This also is a special blogging platform for hackers. It seemed great and had everything I was looking for … I was ready to spend the 50$ per year for it as it could save me a lot of hassle. Unfortunately, I could not find a way to forward the categories of a post as Twitter hashtags as I’ve been doing with Octopress and Feedburner   In the end, I was rather disappointed by the alternatives, even though I was ready to pay for them.   Plan C : Fix the error in Octopress 2   Eventually, the fix was rather easy. People had fixed it by patching mentos.py file in the pygments.rb gem to run Python 2 instead of the stock Python. That’s not great because it means that I’d have to redo the patch every time I updated my gems, or rebuilt my workspace. Here is what I added to my Vagrantfile instead :   sudo apt-get -y install python2.7 sudo ln -sf /usr/bin/python2.7 /usr/bin/python  Here is the commit on my Github repo.   Conclusion   I’m sticking with Octopress2 for the moment. Maybe I’ll re-try to migrate to Octopress3 later on, when an official migration tool or guide is available. Meanwhile, I’m still dreaming of my perfect blogging platform :      Open source   Static HTML generation   Self or SAAS hosted   Great integration with social media and the web   Easy to maintain   Plugable themes   I’m looking for my next side project at the moment, that could be a useful one ! Anyone interested ?  ","categories": ["blogging","software","octopress"],
        "tags": [],
        "url": "/how-i-fixed-the-unknown-language-pygments-error-in-octopress/",
        "teaser": null
      },{
        "title": "Software is like writing and revising a giant book",
        "excerpt":"With time, I discovered a way of explaining the subtleties of my developer job to my uninformed relatives and friends.   Sharing what your developer job is about with others can be very frustrating. Some people think you are “Just playing around with computers” as if it was not serious work. Others think that it is an extremely Cartesian and solitary activity with no place for communication or creativity. Trying to explain the diversity and richness of a developer’s work is often a disappointing experience.   The book metaphor      I explain to people that writing software is not unlike writing a huge book. A book long many millions of lines. I explain that the challenge is to maintain the whole story of the book coherent. I stress how difficult this is given that no one can take the time to read the full book, that the authors come and go, and that the book is under constant heavy revision.   Details of our work   The metaphor is quite valid and even holds about more subtle aspects of our work. Here is a summary of the last discussion I had about the book metaphor :           (Him) Do you have some ‘gatekeepers’ for different sections of the story to make sure that these parts remain coherent ?     (Me) That’s what we would call strong code ownership, this was the norm 20 years ago, but that the industry is moving to more collective code ownership, in order to decrease the bus factor.     (Him) What is the bus factor ?     (Me) The number of person who need to be hit by a bus to block your development progress     (Him) I see, that’s kind of funny. But then, does that mean that any developer can change any part of the code ? Isn’t that dangerous ?     (Me) Sure that would be difficult, that’s not really how we do it. We make a lot of efforts to split the big story in many smaller independent ones as much as possible. This way, teams specialize on different ‘chapters’ and work collectively within it.     (Him) I see     (Me) And if you need to change something in another ‘chapter’ you should ask the guys who know it better to help you.     (Him) That makes sense. And what happens if two developers want to modify the same part of the story at the same time     (Me) That can happen. People might even want to modify the story in conflicting ways ! We’ve got tools, processes and best practices to minimize this. If nothing is done, we’ll get what we call a ‘merge conflict’.     (Him) Ho … I wouldn’t have guessed, but it seems collaboration is really important in your work, isn’t it ?     (Me) Sure it is !      We are software writers   All this reminds me of DHH’s talk about writing software :    ","categories": ["software","complexity"],
        "tags": [],
        "url": "/software-is-like-writing-and-revising-a-giant-book/",
        "teaser": null
      },{
        "title": "How To Keep Up With Software Technologies",
        "excerpt":"Since I started to program professionally 15 years ago, a lot of things have changes. A lot of technologies appeared and became mainstream while others fell out of fashion.   As software developers, it is really challenging to stay fit and productive in new technologies. I cannot obviously say that I am an expert in all new technologies, but I can say that I can get up to speed in a pretty short time in nearly all.   If there is a single reason for that I strongly believe it is because “I studied my classics” !      How does it work   At the same time I started to program all day long for my living, I also started to read a lot of programming books on my spare time. This allowed me to learn at night and practice at day, setting everything in my brain.   This might come as a surprise, but I never read a lot of books about the technologies I was using. I’d rather study fundamental programming books. These contain a lot of transportable knowledge, unlike in most books about a particular technology.   I believe this is a pretty good use of time since it made me a better programmer and also greatly reduced the time I need to master most new technologies. I can now easily relate them to some more general concept I learned before.   For example, I never read a book about UI. By the time I had to do some UI programming, I had absolutely no issue getting up to speed on the topic. I had already seen quite some UI code examples in other books, I knew that most UI frameworks are built from a set of Object Oriented patterns. Coming from a different perspective, I was even able to suggest improvements to the code that UI experts had not thought of.   That’s not to say that I never read books about a particular technology, sometimes, that’s just what you need to start something new. But when I do, I usually find them quite easy to read and digest. And I tend to skip things I intuitively understand and use them as cookbooks containing recipes for particular problems rather than end to end walk-through.   My books   The Pragmatic Programmer: From Journeyman to Master      As a young programmer this book made me understand that my job was not only about writing code, but about building maintainable systems. It provided me with tools to do so, 10 years later, I can still remember “The power of plain text”. This is also the book that made me have a first look into Ruby :).   Refactoring: Improving the Design of Existing Code      Here is the theory behind all automated IDE refactorings. Reading this book had a profound influence on my coding style. The examples made me understand the difference between readable and unreadable code … It’s also the foundation for any kind of incremental design and architecture. That’s the book that got me started with TDD.   Domain-Driven Design: Tackling Complexity in the Heart of Software      This book teaches good object oriented programming. Some say it is tough to read, but it’s definitely worth doing so. Among other things, it explains how to use functional programming concept in your object oriented project, by separating Value Objects and Entities for example.   The C Programming Language      With around 250 pages, it’s pretty difficult to find a programming book with a greater value/page ratio. This one will teach you all there is to learn about C, and help you understand everything that was built on top of C (and that’s quite a lot …)   Structure and Interpretation of Computer Programs      Compared to C, Lisp is at the other side of the language spectrum. Learning C and Lisp gives the ability to put nearly anything in contrast to these 2 languages. Lisp is dynamic, functional and meta. It can be morphed into nearly anything and SICP teaches how to do so. There’s a Lisp practice that is invaluable to any programmer : use your language to express your problem as simply as possible. SICP teaches that.   Programming Erlang: Software for a Concurrent World      Neither the C book nor SICP deals with distribution and concurrency. That’s what this book does. I had never programmed distributed systems before reading this book. After I read it, I learned how to code for distribution in a maintainable way. The lessons taught by Erlang are applicable in many languages.   Not the best books   I did not and will never read all programming books available. There might be newer books that treat the same subjects better, but these books are the ones that taught me the most about our craft. My point is that learning fundamentals and things far away from our daily technologies will teach us more.  ","categories": ["book","career","programming"],
        "tags": [],
        "url": "/how-to-keep-up-with-software-technologies/",
        "teaser": null
      },{
        "title": "Anti Ugly-Code Glasses",
        "excerpt":"I think I found a way to fix the dirty code problem once and for all …   In The Hitchhiker’s Guide to the Galaxy Zaphod Beeblebrox has some anti panic glasses. They feature some special danger detection mechanism that turns them opaque black to save their bearer from panicking.      (It turns out that Zaphod has two heads …)   In 2016, it’s shocking that some many hours are lost by poor developers reading ugly code.   Let’s build anti ugly-code glasses ! We’d just need an IDE or editor plugin, connect it to Sonar in order to get the quality of the current file, and if too bad, shut the glasses black !   Finally the killer feature for augmented reality glasses !  ","categories": ["joke","programming","technical debt"],
        "tags": [],
        "url": "/anti-ugly-code-glasses/",
        "teaser": null
      },{
        "title": "Legacy Code Coverall Day",
        "excerpt":"   Some days, coding feels like speleology or mining … Dealing with weird and undocumented old logic can even some time be damaging to the mind ! I think we should go to work wearing the adequate protection !   OK, maybe we cannot start to go to work wearing a coverall everyday, but let’s say very 21st of June is now legacy code day, the day when every developer maintaining legacy code comes to work dressed like that !   Obviously, that’s a joke, but it could attract the attention of other non-coding people on the state of their codebase and amount of their technical debt.  ","categories": ["joke","programming","technical debt"],
        "tags": [],
        "url": "/legacy-code-coverall-day/",
        "teaser": null
      },{
        "title": "The Size Of Code",
        "excerpt":"The CFO’s debt is visible in his balance sheet. The CTO’s technical debt is invisible. What about making it visible ?   Developers have an intuitive sense of the technical debt in some parts of the system. But few have an accurate estimation of its full extent. Even the size of a code base is difficult to grasp. In the same way, the size of the code is just a number. But the fact are there : between 10 000 and 10 000 000 lines of code, the rules aren’t the same, but it’s only invisible data on hard drives …   Showing It   If we had a device or a trick to show to non-developers the size of the source code, people might start to feel the embarrassment of working in a bloated code base. Unfortunately, for the moment, the only ideas I had are somehow unrealistic, albeit funny !   First Idea : Printouts   Suppose we printed all the source code every Monday, and then keep it around for everyone to feel its size. We could leave it in the middle of the place, or in the CTO’s office, so that he’d actually be hindered by the space loss. The larger the code, the bigger the troubles.   It’s possible to print 50 lines on a sheet of paper, that’s 100 on both sides. That’s 50 000 in a pack of 500 pages. And eventually, 200 000 in this kind of standard case :      Keeping these printouts in sync with the real cost would make the thing even more painful realistic. Imagine all the printings costs, and moving around cases of paper every day … ;)   Second Idea : Inflatable Device   What about an inflatable device linked to SonarQube (or any other code metrics tracking system) ? It could grow as new code is written. We could make it as large as we want : 1m3 for every 10K lines of Code, making the whole office a difficult place to walk around. Try to figure out how to work with this thing in the office :              By Jimmy Kuehnle        Third Idea : Sand   For maximum pain, let’s use real sand instead of an inflatable device ! Imagine the mess with some sand lying around in the office. If the only way to clean up the mess was to clean up the code, surely everyone would take the issue seriously !      Final Word   Obviously, these are jokes, but I guess there’s a real need there. If we managed to make non developers feel the size and cost of the code base, it would be easier to agree on priorities.  ","categories": ["joke","programming","technical debt"],
        "tags": [],
        "url": "/the-size-of-code/",
        "teaser": null
      },{
        "title": "5 Years of Blogging About Software",
        "excerpt":"5 years ago, I started blogging. I started really casually, my posts were personal reminders and notes rather than real well thought of articles. Nevertheless, it did me great good :      I’ve been invited to talk at Meetups   I’ve had the joy of seeing some articles being tweeted many times   I received interesting job offers from all over the world   6 months ago, after reading Soft Skills: The software developer’s life manual, I set up the practice of writing at least one article per week, and here is my (very encouraging) graph of sessions since then:      Excuses Why Not To Blog   Here is a collection of the (bad) excuses you’ll often hear people say for not blogging :      I don’t know how to write …    Blogging regularly is actually a pretty good way to improve your writing skills. As usual, the key is to fake it until you make it.      I’m not into this social media stuff …    You don’t need to share anything personal on your software blog. In the end, your blog is a professional tool.      I don’t have anything interesting to say …    They are others in the same situation as you who would like to see more posts about the kind of uninteresting things you just discovered. Wouldn’t you have liked someone to have written the newby article about « how to do XXX » you just spent 3 days to crack ?      I don’t have the time …    Make it ! Time is never found, it is made. In the end, it’s just a matter of prioritization.   Obviously, there are other totally valid reasons why not to blog, but I’ll assume you’re able to recognize those.   Why Would You Blog ?   On the other side, if you jump into blogging, you can expect a lot of returns :      First thing is that you’ll obviously gain more visibility. I’ve got readers from all over the world, and my articles are sometimes re-tweeted many times.   You’ll improve your writing skills. Writing skills turn out to be unexpectedly important for software writers !   In order to lay down your ideas about something, you’ll need to dig a bit more into. It is said to be the last step to learning.   It can act as a personal documentation. I used to write mine as a how-to notepad on which I could refer later on.   If you have a day job, you can re-post your articles there. You should gain extra visibility and expose the company to new ideas.   How to start   Once you’ve decided that you want to blog, starting should not be an issue.   Pick a platform   There are a lot of blogging platforms out there. For programmers, I would recommend a few though.                  Logo       Platform       Pros       Cons                              Jekyll       Free, Open Source, Github hosting, static HTML generation, markdown &amp; Git based, made for programmers       There is no server, which means no automatic things like auto-publishing on a specified day                        Medium       Free, no setup, good looking, simple to use       It’s a private company, and it could close some day ! It happened to postero.us (I remember, I was there…)                        Wordpress       Cheap (hosting from $3 / month), Open Source, many hosting providers, huge community and plugin ecosystem, SEO plugin       Migrations can be rocky, too many plugins can create security and performance issues                        Ghost       Open Source, supports a great business model, you keep your data, good looking, simple to use, no plugin required, built-in SEO       Limited to pure blogging, hosting from $5 / month but ghost.org hosting starts at $30 / month           Then, it’s up to you !   Start with how-to articles   When I started my blog, it was mostly has a personal how-to reference. It allowed me to come back to it and find out how I did something last time. I thought that if it was important to me, it must be important to others as-well !   Blog regularly   Blogging every week made a huge difference to me. My traffic went from erratic to steadily increasing. I am currently observing a 11% traffic increase per month. This means that it nearly quadruples every year : I’m not going to stop now !   Integrate with the web   This boils down to social networks and analytics. Obviously, you’ll want to use Google Analytics to see how people are reading your content. I’m using the venerable Feedburner to automatically post my new articles on twitter. There’s an option to use your post categories as hashtags, be sure to make it works, it brings a lot of traffic.   It’s all up to you now !  ","categories": ["blogging","software","octopress","career"],
        "tags": [],
        "url": "/5-years-of-blogging-about-software/",
        "teaser": null
      },{
        "title": "Docker Compose trick : How to have an overridable environment variable in development mode ?",
        "excerpt":"I have recently been playing with Docker and Docker Compose while starting my new side project. I’ve fallen into a situation where my production container uses a value for an environment variable, but while developing, I’ll need both a different default and the ability to override this value.   I’m using Rails and found various references about how to deploy Rails app using Docker, but in the end, I decided to use Heroku which handles a lot of ops for me. Rails uses the RAILS_ENV environment variable to know if it’s going to run in development, test or production mode. The heroku/ruby image sets RAILS_ENV=production, but we usually want to use RAILS_ENV=development locally. I could have overridden RAILS_ENV in a docker-compose.override.yml file, but that would prevent me from running my app in production locally.   The trick   I eventually fixed my issue with combination of 2 files.   docker-compose.override.yml   web:   ...   environment:     RAILS_ENV: \"${RAILS_ENV}\" ...   .env   RAILS_ENV=development   The logs   My app starts in development mode by default :   philou@philou-UX31E:~/code/planning-poker$ docker-compose up web Starting planningpoker_herokuPostgresql_1 Recreating planningpoker_web_1 Attaching to planningpoker_web_1 web_1               | Puma starting in single mode... web_1               | * Version 3.4.0 (ruby 2.2.3-p173), codename: Owl Bowl Brawl web_1               | * Min threads: 5, max threads: 5 web_1               | * Environment: development web_1               | * Listening on tcp://0.0.0.0:8080 web_1               | Use Ctrl-C to stop   But I can still override RAILS_ENV to test for example :   philou@philou-UX31E:~/code/planning-poker$ RAILS_ENV=test docker-compose up web planningpoker_herokuPostgresql_1 is up-to-date Recreating planningpoker_web_1 Attaching to planningpoker_web_1 web_1               | Puma starting in single mode... web_1               | * Version 3.4.0 (ruby 2.2.3-p173), codename: Owl Bowl Brawl web_1               | * Min threads: 5, max threads: 5 web_1               | * Environment: test web_1               | * Listening on tcp://0.0.0.0:8080 web_1               | Use Ctrl-C to stop  ","categories": ["docker","heroku"],
        "tags": [],
        "url": "/docker-compose-trick-how-to-have-an-overridable-environment-variable-in-development-mode/",
        "teaser": null
      },{
        "title": "How to boot a new Rails project with Docker and Heroku",
        "excerpt":"A few years ago, I used Heroku to deploy my side-project. It provides great service, but I remember that updates to the Heroku Stack was a nightmare … Versions of the OS (and nearly everything) changed. The migration was a matter of days, and while doing a side-project, this was difficult. At the time, I remember thinking that using branches and VMs would have been the solution.   Now that I started to use Heroku again, I decided to use Docker from the beginning. More specifically, I am expecting :      to have a minimal setup on my host machine   to use the same infrastructure in dev than in production   to simplify switching to a new machine   to simplify the migration to the next Heroku stack   As an added benefit, if ever someone else joins me in my side-project, it will be a matter of minutes before we can all work on the same infrastructure !   Heroku provides a tutorial about how to deploy an existing Rails app to heroku using containers. Unfortunately, I did yet have an existing rails app … So the first challenge I faced, was how to create a Rails app without actually installing Rails on my machine. The trick is to bootstrap rails in docker itself before packaging all this for Heroku.      1. Install the required software   I installed only 4 things on my host machine     Docker instructions   Docker Compose instructions   Heroku Toolbelt instructions   Heroku container plugin heroku plugins:install heroku-container-tools   That’s all I changed to my host machine.   2. Setup docker   First, let’s create a new dir and step into it. Run :  mkdir docker-rails-heroku cd docker-rails-heroku  To prepare the Heroku setup, create a Procfile  web: bundle exec puma -C config/puma.rb  and app.json  {   \"name\": \"Docker Rails Heroku\",   \"description\": \"An example app.json for container-deploy\",   \"image\": \"heroku/ruby\",   \"addons\": [     \"heroku-postgresql\"   ] }  To generate docker files for Heroku, run :  heroku container:init  You want to run Rails in dev mode locally, so we need to override Heroku’s default env (Check my previous post for details)   Create an .env file  RAILS_ENV=development  and docker-compose.override.yml  web:   volumes:     - '.:/app/user'   environment:     RAILS_ENV: \"${RAILS_ENV}\"  shell:   environment:     RAILS_ENV: \"${RAILS_ENV}\"   3. Create the Rails app   It’s now time to follow the official docker-compose rails tutorial to bootstrap the rails app and directories :   Change Dockerfile to  # FROM heroku/ruby  FROM ruby:2.2.0 RUN apt-get update -qq &amp;&amp; apt-get install -y build-essential libpq-dev nodejs RUN mkdir /myapp WORKDIR /myapp ADD Gemfile /myapp/Gemfile ADD Gemfile.lock /myapp/Gemfile.lock RUN bundle install ADD . /myapp   Create a bootstrap Gemfile with the content  source 'https://rubygems.org' gem 'rails', '4.2.0'  Bundle install within the container requires a existing Gemfile.lock  # Create an empty Gemfile.lock touch Gemfile.lock   It’s now time to build your docker container to be able to run rails and generate your source files. Run the following :  # Build your containers docker-compose build  # Run rails within the shell container and generate rails files docker-compose run shell bundle exec rails new . --force --database=postgresql --skip-bundle   Unfortunately, rails is ran as root inside the container. We can change ownership and rights with this command :  # Change ownership sudo chown -R $USER:$USER .  # Change rights sudo chmod -R ug+rw .   4. Make it Heroku ready   Now that the rails files are generated, It’s time to replace the bootstrap settings with real Heroku Dockerfile   Revert Dockerfile to simply :  FROM heroku/ruby   Heroku uses Puma so we need to add it to our Gemfile  # Use Puma as the app server gem 'puma', '~&gt; 3.0'  We also need to add a config file for Puma. Create config/puma.rb with this content (you can check heroku doc for details)  workers Integer(ENV['WEB_CONCURRENCY'] || 2) threads_count = Integer(ENV['RAILS_MAX_THREADS'] || 5) threads threads_count, threads_count  preload_app!  rackup      DefaultRackup port        ENV['PORT']     || 3000 environment ENV['RACK_ENV'] || 'development'  on_worker_boot do   # Worker specific setup for Rails 4.1+   # See: https://devcenter.heroku.com/articles/deploying-rails-applications-with-the-puma-web-server#on-worker-boot   ActiveRecord::Base.establish_connection end   It should now be possible to rebuild the container, and run the app :  # Rebuild the containers docker-compose build  # Start the rails app using the web container docker-compose up web  The app should be accessible at http://0.0.0.0:8080      5. Deploying to heroku   We’re almost ready to deploy to heroku.   First, we need to exclude development files from our image. For this, we need to create a .dockerignore file with the content  .git* db/*.sqlite3 db/*.sqlite3-journal log/* tmp/* Dockerfile .env docker-compose.yml docker-compose.override.yml README.rdoc  It’s then classic Heroku deploy commands :   # create an Heroku app heroku apps:create &lt;your-app-name&gt;  # And deploy to it heroku container:release --app &lt;your-app-name&gt;   Your app should be accessible on line at https://.herokuapp.com/      Rails does not provide a default homepage in production. But you can check the logs with   heroku logs --app &lt;your-app-name&gt;   6. Running commands   When in development mode, you might want to run rails or other commands on your source code again. The shell container exists just for that, run docker-compose run shell ....  # For example, to update your bundle docker-compose run shell bundle update   EDIT 2016-07-20   For the moment, there’s a catch with bundle install or update commands, as the gems are installed outside the shared volume, only Gemfile.lock will be updated, which required to run docker-compose build again … I’ll have a look into this later and see if I can fix that.   docker-compose run shell bundle update docker-compose build  ","categories": ["rails","docker","heroku"],
        "tags": [],
        "url": "/how-to-boot-a-new-rails-project-with-docker-and-heroku/",
        "teaser": null
      },{
        "title": "How to setup Rails, Docker, PostgreSQL (and Heroku) for local development ?",
        "excerpt":"My current side project is an online tool to do remote planning pokers. I followed my previous tutorial to setup Rails, Docker and Heroku.   Naturally, as a BDD proponent, I tried to install cucumber to write my first scenario.   Here is the result of my first cucumber run :   $ docker-compose run shell bundle exec cucumber rails aborted! PG::ConnectionBad: could not translate host name \"postgres://postgres:@herokuPostgresql:5432/postgres\" to address: Name or service not known ...   It turned out that I had taken instructions from a blog article on codeship that mistakenly used host: instead of url: in their config/database.yml   After fixing that in my database.yml file, things where only slightly working better :   $ docker-compose run shell bundle exec cucumber rails aborted! ActiveRecord::StatementInvalid: PG::ObjectInUse: ERROR:  cannot drop the currently open database : DROP DATABASE IF EXISTS \"postgres\"   The thing is the config was still using the same database for all environments. That’s not exactly what I wanted. I updated my config/database.yml :   default: &amp;default   adapter: postgresql   encoding: unicode   pool: 5   timeout: 5000   username: postgres   port: 5432   host: herokuPostgresql  development:   &lt;&lt;: *default   database: planning_poker_development  test: &amp;test   &lt;&lt;: *default   database: planning_poker_test  production:   &lt;&lt;: *default   url: &lt;%= ENV['DATABASE_URL'] %&gt;   Victory ! Cucumber is running   $ docker-compose run shell bundle exec cucumber Using the default profile... 0 scenarios 0 steps 0m0.000s Run options: --seed 45959  # Running:    Finished in 0.002395s, 0.0000 runs/s, 0.0000 assertions/s.  0 runs, 0 assertions, 0 failures, 0 errors, 0 skips   Fixing rake db:create   By searching through the web, I found that people were having similar issues with rake db:create. I tried to run it and here is what I got :   $ docker-compose run shell bundle exec rake db:create Database 'postgres' already exists Database 'planning_poker_test' already exists   Why is it trying to create the postgres database ? It turns out that the DATABASE_URL takes precedence over what is defined in my config/database.yml. I need to unset this variable locally. I already have the docker-compose.override.yml for that :   web:   environment:     DATABASE_URL:   ...  shell:   environment:     DATABASE_URL:   ...   Rake db:create works just fine now :   $ docker-compose run shell bundle exec rake db:create Database 'planning_poker_development' already exists Database 'planning_poker_test' already exists   Starting a psql session   During all my trouble-shootings, I tried to connect to the Postgresql server to make sure that the databases where created and ready. Here is how I managed to do that :   1. Install psql client   On my Ubuntu machine, that was a simple sudo apt-get install postgresql-client-9.4.   2. Finding the server port   The port can be found through config/database.yml or through docker ps. Let’s use the later, as we’ll need it to find the server IP as well.   $ docker ps CONTAINER ID        IMAGE            COMMAND                  CREATED             STATUS              PORTS           NAMES b58ce42d2b2b        postgres         \"/docker-entrypoint.s\"   46 hours ago        Up 46 hours         5432/tcp        planningpoker_herokuPostgresql_1   Here the port is clearly 5432.   3. Finding the server IP   Using the container id we got on previous docker ps command, we can use docker inspect to get further details :   $ docker inspect b58ce42d2b2b | grep IPAddress             \"SecondaryIPAddresses\": null,             \"IPAddress\": \"172.17.0.2\",                     \"IPAddress\": \"172.17.0.2\",   4. Connecting to the database   Connecting is now just a matter of filling the command line.   $ psql -U postgres -p 5432 -d planning_poker_development -h 172.17.0.2 planning_poker_development=# select * from schema_migrations;  version --------- (0 rows)   5. Installing psql client directly in the shell   It should be possible to install the psql client in the shell container automatically, but I must admit I did not try this yet. It should just a matter of adding this to the Dockerfile   RUN apt-get install postgresql-client-&lt;version&gt;  ","categories": ["rails","docker","heroku","postgre"],
        "tags": [],
        "url": "/how-to-setup-rails-docker-postgresql-and-heroku-for-local-development/",
        "teaser": null
      },{
        "title": "Is There Any Room For The Not-Passionate Developer ?",
        "excerpt":"   In Rework, Basecamp guys David Heinemeier Hansson and Jason Fried advise to “Fire the workaholics”, while in Zero to One Peter Thiel argues that great working conditions (as described within Google for example) result from 10x technological advantages, not the other way round.   Back in 1983, Bill Gates said :     You have to think it’s a fun industry. You’ve got to go home at night and open your mail and find computer magazines or else you’re not going to be on the same wavelength as the people [at Microsoft].    Where do we stand now ? Do you need to live and breath programming to remain a good developer ?   What about the 40h per week rule ?      Studies have repeatedly demonstrated that 40h per week is the most productive work load, but in Outliers, the Story of Success Malcolm Gladwell explains that getting fast to the 10000 hours of practice is a required road to success. As my Aïkido professor says, the more you practice, the better you get …   In Soft Skills: The software developer’s life manual John Somnez also makes the point for hard work, that while he long believed that smart work would be enough, it’s only when he put more in that he managed to drastically change his career.   During an argument, DHH argued in favor of work life balance whereas Jason Calacanis said that working in a startup had to be an all-in activity. In the end, they agreed that what matters is passion.   From my own experience, whenever I work on something I am passionate about :      I am more productive   I feel energized rather than dulled by the work   When I look around me, all the great developers I know are passionate and putting in more than 40 hours per week in programming. I also noticed that passion and efforts have always been pretty good indicators of future skills.   But then, how do passionate people manage to remain productive when working more than 40 hours per week ?   What about the under the shower idea ?   In Pragmatic Thinking and Learning: Refactor Your Wetware (Pragmatic Programmers) (which is a great book BTW), Andy Hunt explains that our R-mode works in the background, and needs time away from the task at hand to come up with “out of the box” creative solutions.   XP argues for a sustainable pace, but at the same time, Uncle Bob says that we should put in 60 hours (40 for employer, and 20 for yourself) of work per week to become and remain ‘professionals’ (I guess that’s from The Clean Coder if I remember correctly).   On my side, 6 to 8 solid hours of pair-programming on the same subject is the most I can do before becoming a Net Negative Producing Programmer. But I can do more programming per day if I work on a side project at the same time though !   I guess that’s how passionate people do it, they have different topics outside of their main work :      they read books about programming   they have their own side projects   they read articles about programming   they might maintain a programming blog   they might attend, organize or speak at meetup   Most of the time, this does not make for more work, but rather for more learning. If I’ve noticed that all the great programmers around me are passionate and strive to improve at their craft, I’ve also noticed that overworked workaholics usually aren’t very productive.   Special challenges for mums and dads   I think that Bill Gates 1983 statement still holds. If you are not passionate about programming, you’ll have a hard time remaining and succeeding as a programmer in the long run.   The great thing about all this passion is that we can experience an energized work environment, always bubbling with change and novelty. On the flip side, keeping up with all is not always easy.   As we developers gain more experience, we tend to loose patience with everything that just feels as a pain in the ass, and will want :      Powerful languages and technologies   An efficient working environment   Smart colleagues   Unfortunately, that might also be the moment in your life when you become a parent, and you’ll want  a stable income to sustain your family and some time to spend with your kids.   That is when things get tricky. Neither can you jump ship for the next cool and risky startup where you’ll do great things, nor can you find enough time moonlighting to improve your skills … To add pain to injury, even with 10 years of experience in various languages and technologies, most companies won’t look at your resume unless it contains good keywords … It looks like the developer’s version of  The Innovator’s Dilemna !   Lack of passion and parenthood might partially explain why people stop being developers after a while. I can quickly think of 2 bad consequences of this :      We tend to reinvent the wheel quite a lot (I’m looking at you, .js frameworks …)   We might be meta ignoring (ignoring that we ignore) people skills that could make us all more efficient   EDIT 2017/01/11 Thanks for all your valuable comments, here is a follow up   Chinese translation  ","categories": ["software","programming","career","side project"],
        "tags": [],
        "url": "/is-there-any-room-for-the-not-passionate-developer/",
        "teaser": null
      },{
        "title": "When the Boy Scout Rule Fails",
        "excerpt":"            Original Tweet by Marteen van Leeuwen        Here goes the boy scout rule :      Always check a module in cleaner than when you checked it out.    Unfortunately, this alone does not guarantee to keep the technical debt under control. What can we do then ?   Why the boy scout rule is not enough   I can easily think of a few issues that are not covered by the boy scout rule.   It only deals with local problems   In it’s statement, the boy scout rule is local and does not address large scale design or architecture issues. Applying the boy scout rule keeps files well written, using with clear and understandable code. From a larger perspective though, it does very little or slow improvement to the overall design.   These large scale refactorings are very difficult to deal with using the boy scout rule alone. It could be done but would require to share the refactoring goal with all the team, and then track its progress, while at the same time dealing with all the other subjects of the project. That’s starting to sound like multitasking to me.   It’s skill dependent   Another point about the boy scout rule (and to be fair, about any refactoring technique) is that programmers will be able to clean the code only as much as their skills allow them to !   Imagine what would happen when a new master developer arrives in a team of juniors, he’d spot a lot of technical debt and would suggest improvements and ways to clean the code. Code that was thought of as very clean would suddenly be downgraded to junk !   The point here is that the boy scout rule cannot guarantee that you have no technical debt, because you don’t know how much you have !   That’s where the debt metaphor reaches its limits and flips to some productivity investment. By investing time to perform some newly discovered refactoring, you could get a productivity boost !      Domain-Driven Design: Tackling Complexity in the Heart of Software, Eric Evans calls this knowledge distillation. He means that little by little, the team gains better understanding of the domain, sometimes going through what he calls a ‘breakthrough’. These breakthroughs often promote existing code to technical debt …   It’s context dependent   Developers alone are not the only one responsible for creating technical debt. Changes to the environment also do.   For example, if the market conditions change, and that new expectations for the product are slowly becoming the norm, your old perfectly working system becomes legacy and technical debt. As an example, let’s examine what happened to the capital markets software industry in response to the 2008 crisis.      The sector became a lot more regulated   Risk control is moving from nightly batches to real time   The demand for complex (and risky) contracts decreased   As a consequence, trading on simpler contracts exploded   All these elements combined invalidated existing architectures !   New technologies also create technical debt. Think the switch from mainframe to the web.   What do we need then ?   Should we stop using the boy scout rule ? Surely not, it would be a total non-sense. Submitting clean and readable code is a must.   But it is not enough. If you have spotted some large scale refactoring that could bring some improvement, we should do what a fund manager would do :      Estimate the return on investment   If it is good enough, do it now   Obviously, large refactorings should also be split into smaller value adding cost reducing items. But then what ?      In The Nature of Software Development Ron Jefferies says that we need a unique value-based prioritization strategy for everything, including technical improvements. Once you’ve got that, there’s no sense in splitting and embedding your refactoring in other tasks, this will just increase your work in progress, reducing your throughput and cycle time.   Frankly, I think that’s easier said than done. I can think of two ways :      As Ron Jefferies tends to say, have a jelled-cross-functional team discuss and prioritize collectively   As Don Reintersen advocates, use an economical framework to estimate the return on investment   At least that’s a starting point !  ","categories": ["technical debt","software","refactoring"],
        "tags": [],
        "url": "/when-the-boy-scout-rule-fails/",
        "teaser": null
      },{
        "title": "How to Grow a Culture Book",
        "excerpt":"Have you read valve’s Handbook for new employees ?      In Management 3.0 terms, that’s a culture book. It’s a great way to build and crystallize a culture, and it serves as a guide for newcomers, and can later serve as an hiring ad for your team or company.   The good thing about a culture book, is that you don’t have to write it in one go. It’s a living artifact anyway, so you’d better not ! Our current culture book has emerged from a collection of pages in our wiki.   It started as working agreements   The first real contributions to our culture book (though we did not know it at the time) was spending some time in retrospectives to define and review our working and coding conventions.   When we started doing retrospectives, we had to discuss, agree and formalize the decisions we made about our way of working. We usually did a ‘review how we work’ activity at the beginning the retros, spending 10 minutes to make sure we all understood and agreed on our current working conventions. If there was any disagreement or update required, we would discuss them during the retro, and at the end, add, remove or modify items from our agreement page.   It continued as self-organization workshops   After a while, we had built up a pretty extensive set of working and coding conventions. The team had already become quite productive, but to keep the momentum in the long run, we needed to increase self-organization. By reading Management 3.0 books and Management Workout (which has been re-edited as Managing for Happiness) in particular, I found description about how to use a delegation board and delegation pokers to measure and formalize the current delegation level of a team.   We did this, and started a lot of self-organization workshops :      Stop feeling like a kid every time you ask a day off   Scrum Teams Do Not Need a Scrum Master   Make hiring everyone’s business   How to deal with the incentive system in an agile team ?   How We Decentralized Our Company’s Training Program   After each of these workshops, we created a wiki page, explaining how we planned to handle the subject in the team.   The book   At that point, we had fairly extensive and formal descriptions of our working practices and conventions. By reading this set of pages, someone would get a pretty accurate grasp of our principles and values.   Wondering how we could write our own culture book, I had an “Aha !” moment and realized that all I had to do was to create a wiki page pointing to all our different agreement pages. This only took 5 minutes.      At the moment, our culture book serves 3 purposes :      documentation for the team members   guide for newcomers   description about how we work for people in the company who might want to move to our team   Next step would be to add a dash of design, a few war stories, export it as a PDF, and use it outside to advertise the team and the company.  ","categories": ["management30","management","agile","selforganizing"],
        "tags": [],
        "url": "/how-to-grow-a-culture-book/",
        "teaser": null
      },{
        "title": "How to prepare a new Ruby env in 3 minutes using Docker",
        "excerpt":"One or two weeks ago, I registered to the Paris Ruby Workshop Meetup and needed a Ruby env. I have been using Vagrant quite a lot to isolate my different dev envs from each other and from my main machine. As I’ve been digging more into Docker lately, I thought I’d simply use Docker and Docker Compose instead.   I turned out to be dead simple. All that is needed is a docker-compose.yml file to define the container, record the shared volume and set a bundle path inside it :   rubybox:   image: ruby:2.3   command: bash   working_dir: /usr/src/app   environment:     BUNDLE_PATH: 'vendor/bundle'   volumes:     - '.:/usr/src/app'   Without the custom bundle path, bundled gems would be installed elsewhere in the container, and lost at every restart.   To use the Rubybox, just type docker-compose run rubybox and you’ll get a shell from within your ruby machine, where you can do everything you want.   In fact, I found the thing so useful, that I created the Rubybox git repo to simplify cloning and reusing. I’ve already cloned it at least 3 times since then !   git clone git@github.com:philou/rubybox.git cd rubybox docker-compose run rubybox  ","categories": ["ruby","docker","software"],
        "tags": [],
        "url": "/how-to-prepare-a-new-ruby-env-in-3-minutes-using-docker/",
        "teaser": null
      },{
        "title": "RSpecProxies now supports .to receive(xxx)... syntax",
        "excerpt":"   Pure mocks are dangerous. They let defect go through, give a false sense of security and are difficult to maintain.   I’ve already talked about it before but since then, DHH announced that he was quitting TDD, the Is TDD Dead ? debate took place, and the conclusion is that mockist are dead.   They are still times when mocks feel much simpler than any other things. For example, imagine your process leaks and crashes after 10 hours, the fix is to pass an option to a thirdparty, how would you test this in a fast test ? That’s exactly the kind of situation where using test proxies saves you from mocks. A test proxy defers everything to the real object but also features unintrusive hooks and probes that you can use in your test. If you want a code example, check this commit, where I refactored a rails controller test from mocks to a RSpecProxies (v0.1).   I created RSpecProxies a while ago, a while ago, and it’s syntax made it alien to the RSpec work, it needed an update. RSpec now supports basic proxying with partial stubs, spies, the and_call_original and the and_wrap_original methods. RSpecProxies 1.0 is a collection of hooks built on top of these to make proxying easier, with a syntax that will be familiar to RSpec users.   Before original hook   This hook is triggered before a call a method. Suppose you want to simulate a bad connection :   it 'can simulate unreliable connection' do   i = 0   allow(Resource).to receive(:get).and_before_calling_original { |*args|     i += 1     raise RuntimeError.new if i % 3 == 0   }    resources = Resource.get_at_least(10)    expect(resources.size).to eq(10) end   After original hooks   RSpecProxies provides the same kind of hook after the call :   it 'can check that the correct data is used (using and_after_calling_original' do   user = nil   allow(User).to receive(:load).and_after_calling_original { |result| user = result }    controller.login('joe', 'secret')    expect(response).to include(user.created_at.to_s) end   Here we are capturing the return value to use it later in the test. For this special purpose, RSpecProxies also provides 2 other helpers :   # Store the latest result in @user of self allow(User).to receive(:load).and_capture_result_into(self, :user)  # Collect all results in the users array users = [] allow(User).to receive(:load).and_collect_results_into(users)   Proxy chains   RSpec mocks provides the message_chain feature to do build chains of stubs. RSpecProxy provides a very similar proxy chain concept. The main difference is that it creates proxies along the way, and not pure stubs. Pure stubs assume that you are mocking everything, but as our goal is to mock as little as possible, using proxies makes more sense.   When using a mockist approach, the message chain is a bad smell because it makes your tests very brittle by depending on a lot of implementation. In contrast, proxy chains are meant to be used where they are the simplest way to inject what you need, without creating havoc.   For example, suppose you want to display the progress of a very slow background task. You could mock a lot of your objects to have a fast test, of if you wanted to avoid all the bad side effects of mocking, you could run the background task in your test, and have a slow test … Or, you could use a chain of proxies :   it 'can override a deep getter' do   allow(RenderingTask).to proxy_message_chain(\"load.completion_ratio\") { |e| e.and_return(0.2523) }    controller.show    expect(response).to include('25%') end   Here the simplest thing to do is just to override a small getter, because from a functionnal point of view, that’s exactly what we want to test.   Last word   The code is on github, v1.0.0 is on rubygems, it requires Ruby v2.2.5 and RSpec v3.5, the license is MIT, help in any form are welcome !  ","categories": ["ruby","rspec","testing","agile","tdd","mocking"],
        "tags": [],
        "url": "/rspecproxies-now-supports-to-receive-xxx-dot-dot-dot-syntax/",
        "teaser": null
      },{
        "title": "Kudo Boxes for Kids",
        "excerpt":"How do you get your kids to participate with housekeeping ? I guess that’s the dream of all parents. As so, we’ve tried quite a lot of tactics throughout the years. Carrots and stick never really worked, so we tried positive reinforcement, gratitude … Unfortunately, nothing really made any noticeable improvement.   Until now !   At work, we’ve been using Kudo Boxes for a while now. A kudo box is a small mailbox where teammates can drop a word of thank or some praise (No blame allowed here !)   Why not try the same thing at home ? During the summer holidays, we’ve build kudo boxes for everyone in the family.      It’s a nice and easy way to express gratitude for any good stuff our kids do. The great thing is that it’s cheap, it’s easy to carry cards around and to hand one out at any moment.   What happened ?   First, we now have very joyful kudo reading sessions : our kids rush to the boxes to check for new cards. The second most noticeable change we observed is that they are both participating more in the house chores ! For example, as soon as we start cooking, they might spontaneously dress the table up. Or they might bring tools to help us as best as they can when we are tending to the garden.   To summarize, it seems it brought a lot of joy and love in the house.   How we started ?      They are many ways to build a kudo box. The simplest way might be to get an old shoe box, and to cut a hole in the cover. We bought a wooden box with four drawers, and spent some time all together to decorate it. This in itself was already fun.   We started using simple pieces of paper as kudo cards, but I later downloaded and printed a bunch of official kudo cards from the Management 3.0 website. It turns out there is a version in french.   Bonus   An unexpected, but great, side effect is that my spouse and I started to get kudos as well ! It’s really nice to receive a word from your kids. For example, here is a drawing I got from my daughter.     ","categories": ["management30","agile"],
        "tags": [],
        "url": "/kudo-boxes-for-kids/",
        "teaser": null
      },{
        "title": "How to use Hackernews and Reddit for blogging",
        "excerpt":"A few weeks ago, I posted my latest article Is There Any Room for the Not-Passionate Developer ? on Hackernews and Reddit Programming. The post stayed on the fist page for a while, and I got a lot of traffic. If you are yourself blogging, you might be interested to know how it occurred, and what I learned in the process.   How it started      In Soft Skills, the software developer’s life manual John Somnez explains that posting your blog articles on HN or Reddit might bring you a ton of traffic, but that comments can be hard to swallow at time. Within a few hours of writing my blog post it had generated some positive activity on twitter (favorites and retweets) from my regular followers. That’s a good sign that the post is good enough. As I had promised myself in such case, I submitted the post to both HN and Reddit.   What happened ?      I don’t know for sure on Reddit, but I know my post stayed on the first page of HN for a few hours, it even went up to the third place for a while. In the process I got a lot of traffic, a lot more than I am used to. I also got a ton of comments, on HN, Reddit and directly on my post. John Somnez had warned that comments on HN and Reddit can be very harsh, so I went through these quickly, took notes about the points that seemed interesting, but I only responded to comments on my website.   Overall, the comments were pretty interesting though, and brought a lot of valid points. I’m planning to write a ‘response’ article to take all these into perspective.   Most of the traffic was made in the day I submitted my post, but I had more traffic than usual for 2 or 3 days. Since then, the traffic has settled down, but I now get between 2 and 5 times more traffic than I typically had on a daily basis ! An online Taiwanese tech magazine also asked me the permission to translate my post in Chinese !   I’m not sure about the performance of my website during the traffic spike. I’m using Octopress to statically generated html on Github Pages, so that should be fine. I am also using a custom domain, and I need to make sure my DNS is correctly configured for this to perform well.   Advice for bloggers   So here is what I am going to do regarding HN and Reddit in the future :      It can bring so much traffic and backlinks that I’ll definitely continue to submit blog posts from time to time   For the moment, I’ll stick to only submitting the articles from which I already received good feedbacks, I don’t want to get a bad karma or reputation on these websites   I might submit old articles that gathered good reviews at the time I wrote them   Concerning comments, I’ll try to grow an even thicker skin. Maybe at some point I’ll try to answer on HN or Reddit   Of course, depending how this works, I will adapt !  ","categories": ["software","blogging"],
        "tags": [],
        "url": "/how-to-use-hackernews-and-reddit-for-blogging/",
        "teaser": null
      },{
        "title": "How NOT to use mocks, my talk at Paris.rb",
        "excerpt":"As I already wrote about, mocks can be trecherous … I gave a talk about how to avoid them last tuesday at Paris.rb meetup. Here are the slides.   It talks about testing, mocking, but also Domain Driven Design and test proxies à la RR (but for rspec).       Check the speaker’s comments for all the details.  ","categories": ["mocking","testing","ruby"],
        "tags": [],
        "url": "/how-not-to-use-mocks-my-talk-at-paris-rb/",
        "teaser": null
      },{
        "title": "Flavors of TDD",
        "excerpt":"During the years doing some coding dojos with the same circle of people, I came up with my own style of practicing TDD. Lately, I had the chance to do a pair programming session with someone I did not know. That made me realize that they are in fact even more ways to practice TDD than I thought.   Mockist vs Classisist   A lot has already been written (and discussed) about these two approaches. I myself have already blogged about the subject, I even gave a talk about it. From my own point of view, I believe that the inconvenients of making mocking the default far outweights the benefits. I’m not saying that mocks aren’t useful from time to time, but rather that they should remain the exception.   Top-Down vs Bottom-Up   That’s the reason why I wrote this post. This is the main difference I found between my style and my pair’s. Let me explain.      Top-Down   Doing TDD top-down means starting with high level end to end tests, implementing the minimum to make it work, refactor and repeat. A bit like BDD, the point is to focus on the expected behavior, and avoid writing useless things. The bad point is that the refactoring part can get pretty difficult. On real life code, strictly following top-down would mean writing a feature test first, passing it with a quick and dirty implementation, to then spend hours trying to refactor all that mess … good luck !   Here is another example, from coding dojos this time. Having had success with the top-down approach during previous dojos, we once intentionally tried to code Conway’s Game of Life using a top-down approach. We did so by writing high level tests that were checking special patterns (gliders …). That was a nightmare ! It felt like trying to reverse engineer the rules of the game from real use cases. It did not bring us anywhere.      Bottom-Up   At the other side of the spectrum, you can do bottom-up TDD. This means unit testing and implementing all the small bricks you think you’ll need to provide the expected overall feature. The idea is to avoid tunnels and to get fast feedback on what you are coding. The bad side is that you might be coding something that will end up being unnecessary. Be careful, if you find yourself spending a lot of time building up utility classes, you might be doing too much bottom-up implementation.   The Numerals to Romans Kata is a good exercise to fail at bottom-up. Every time I did this exercise during a coding dojo, people new to it would start to come up with complicated ways to do it (often involving complex array manipulation). Compared to that, applying disciplined bottom-up TDD brings a brutally effective solution for Numerals to Romans.   Mixed approach   Both approaches have their pros and cons. I really believe developers who are serious about TDD should master both, and learn when to apply each. In fact, as often, the best approach lies somewhere in the middle. Here’s my recipe :      Start with a high level feature test   try to make it pass …   … (usually) fail   rollback or shelve your test and draft implementation   build a brick   unshelve   try to make it pass …   … and so one until the high level test finally passes.   In fact, it’s a lot like the Mikado Method for building features instead of refactoring.   Practice in dojos   It’s possible to intentionally practice this in coding dojos as well. Most kata should be OK, as long as the group agrees to fix it using this particular approach up front.   If during the dojo, you’ve just written a test, suddenly realize that it won’t be easy to get it passing, and that you’ve got the elements spread out in your code, this is the time ! Comment the test, get the green bar, refactor, uncomment the test, try to make it pass, repeat … Eventually, you’ll have all the bricks to make it easy to pass your test.      Some might say this is not ‘pure’ TDD, but that sounds like cargo cult to me ! As long as you make sure you are not building useless stuff, and that you keep the feedback loop as short as possible, you’re on the right track.  ","categories": ["tdd","testing","programming"],
        "tags": [],
        "url": "/flavours-of-tdd/",
        "teaser": null
      },{
        "title": "Top 5 talks I attended at JavaOne 2016 (Part 1)",
        "excerpt":"With a few other colleagues, I had the chance to be sent to San Francisco last week to attend the JavaOne 2016 conferences by my company.      Here is super short list of the conferences I attended which I found really interesting   Preventing errors before they happen   Werner Dietl &amp; Michael Ernst   Since Java 6, it is possible to pass custom annotation processors to javac. Since Java 8, it is possible to add annotations to types. The guys behind the Checker Framework used this to create custom pluggable type systems for your Java programs. These type systems enforce properties on your program, and will emit warnings or errors at compile time otherwise.   Here are a few example :      declare @Immutable MyObject myObject to make sure that myObject won’t be muted   declare @NonNull MyObject myObject to make sure that myObject is never null      Under the hood, the compiler behaves as if @Immutable MyObject and MyObject where completely separate types, and it knows and tracks specific ways of converting between the two. The framework provides a simple API to define your own type systems. They did a live demo showing how to quickly define things like @Regex String, @Encrypted String or @Untainted String (which forbids user input strings to avoid SQL injections).   The talk was really interesting, the framework seems lightweight and to integrate well with the typical tool stack. I definitely will give it a try the next time I have a bit of slack time.   Here are the slides and a previous session of the presentation   Keeping Your CI/CD Pipeline as Fast as It Needs to Be   Abraham Marin-Perez   Continuous Delivery and Microservices are what you need to do, aren’t they ? Well, when actually trying to setup a CI / CD pipeline for all your code, things quickly get complicated pretty fast ! The speaker presented how to deal with this complexity by using metrics from your VCS and build servers to draw an annotated graph of your build pipeline.         He used the build time to set the size of every node : the longer, the larger   The color for the change rate : the more often it was built the warmer the color   It was then possible to determine other metrics such as :      the impact time of every node : build time + build time of all the dependencies   the weighted impact time : impact time * change rate   the overall average impact time : sum of all the weighted impact times   the overall max impact time : max of all the impact times   Using this and your SLAs it is possible to define policies for your build times such as “the max build time should not be more than X”. If you want to speed up your build, you can set a target build time and analyzing the graph should help you to understand what architecture changes you need to make to your system in order to meet this build time (this sounds a lot like Toyota’s Improvement Kata …)   I loved this talk ! I found the speaker captivating, he presented novel ideas which is not always the case.   Here are the slides, and the same presentation at Devoxx UK.   To Be Continued   I promised 5, and that’s only 2 talks ! Stay tuned, I’ll write about the 3 others in the coming weeks. Here they are.  ","categories": ["java","programming"],
        "tags": [],
        "url": "/top-5-talks-i-attented-at-java-one-2016-part-1/",
        "teaser": null
      },{
        "title": "Top 5 talks I attended at JavaOne 2016 (Part 2)",
        "excerpt":"This is my second post relating the talks I attended at JavaOne 2016. Here is the beginning of the story. Here we go.   Euphoria Despite the Despair   Holly Cummins   Our jobs aren’t always fun … and that’s in fact an issue ! Studies show that people who have fun at work are 31% more productive ! The talk was organized in 3 parts :      What is fun ?   How to remove the parts that are not fun ?   How to add even more fun ?   She defined what she called the funtinuum, which is that fun is a function of engagement and interaction. Basically, you won’t have fun if you are doing nothing, or if no one cares about your work. That aligns well with Daniel Pink’s drivers of motivation : Autonomy, Mastery and Purpose.   If something is not fun, it’s because it does not require engagement or interaction. It’s either boring or no one cares, or both. If that’s the case, it’s probably some kind of waste in some sense … Removing un-fun activities would mean removing waste. It’s interesting to note how this sounds like lean Muda ! She gave examples such as :      automate stuff   pair programming transforms criticism into collaboration (bonus: it gives excuse to skip meetings)   go #NoEstimates because estimating is painful and useless   YAGNI defers useless things until they really add value   Organize to skip meetings and other boring stuff   …   Last step is to add fun to the workplace. She warned that adding fun before removing the un-fun stuff would feel fake and would make things worse …      To add fun, she suggested using things like :      gamification (there was actually another great talk about gamification)   build a hacking contest instead of a security training   Install a Siren of Shame for whoever breaks the build   …   Here are the slides   Java 9: The Quest for Very Large Heaps   Bernard Traversat, Antoine Chambille      This talk might not be of interest for all, but it is for us at work. It went through the improvement to come to Java 9’s G1 garbage collector. To summarize, to scale to very large heaps, it will split the memory into regions. Objects should be allocated on different regions depending on their specificities, which might help to build NUMA aware applications. Having the heap split up in smaller chunks enables the GC to run in parallel, which can speed up the old generation GC by up to 50 times !   Java 9 is scheduled for march 2017   Agility and Robustness: Clojure + spec   Stuart Halloway   I haven’t been touching Clojure for a while but I gave the language a try a few years ago. I had heard about Clojure spec but hadn’t taken the time to look at it in details. As I understood it all, Spec is like some sort of Design by Contract on steroïds ! Clojure is not statically typed, but you can now assign spec metadata to values. A spec is roughly a predicate. By defining specs for the inputs and outputs of functions, it is possible to verify at runtime that the function is behaving correctly.   As did Bertrand Meyer in the classic OOSC2, who advised to use contracts during development only, Stuart explained that we should care about developer vs production time instead of compile vs runtime. From this point of view, it is not of great importance whether the compiler or the continuously running test suite finds an issue.   But specs are a lot more than predicates ! They can be used to :      enable assertions at runtime   generate documentation   generate test cases   generate precise call logs   get precise error messages   explore a function and see how it can be called   He went on to compare the virtues of Clojure spec with static typing (à la Java) and example based testing :      Although I don’t believe that generative testing can ever replace example based testing altogether, it certainly can help.   All in all, the presentation was insanely great and engaging. It made me seriously think of going into Clojure programming again !   Here are the slides and the the same talk at Strangeloop   Conclusion   Overall, JavaOne was great ! If I had the opportunity, I’d go back every year ! There was a lot of other great talks I did not write about in these 2 posts, for example :      Development Horror Stories was a lot of fun, especially the winning story !   Hacking Hiring was full of good advises   Managing Open Source Contributions in Large Organizations was full of good ideas   Increasing Code Quality with Gamification was very inspiring   Edit 17 October 2016   I summarized 3 others JavaOne talks here.  ","categories": ["java","programming","clojure"],
        "tags": [],
        "url": "/top-5-talks-i-attended-at-javaone-2016-part-2/",
        "teaser": null
      },{
        "title": "3 More Great Talks From JavaOne 2016",
        "excerpt":"After the top 5 talks I attended at JavaOne here are more !   Managing Open Source Contributions in Large Organizations   James Ward   This talk was very interesting for companies or organizations that want to use Open Source in some way without ignoring the risks.   After an introduction listing the benefits of contributing to open source, James explained the associated risks :      Security (evil contributions or information leaks)   Quality (bad contributions, increased maintenance or showing a bad image)   Legal (responsibility in case of patent infringing contribution, ownership of a contribution, licenses)   He then explained that there are 3 ways to deal with the issue :                  Strategy       Description       Pros       Cons       Popularity       Examples                       Do nothing       Devs just contribute without saying it       Easy, Gets it done       Need to stay under the radar, Risks for all parties are ignores       +++++       Most open source code on Github is shared in this manner                 Join a foundation       Joining an existing open source foundation, with a framework       Everything out of the box (infra, governance), builds trust       Rules can be heavy, Ownership is given to the foundation       +++       Linkedin put Kafka in the Apache Foundation                 Build tools       Use your own tools to mitigate the main risks associated with the ‘Do nothing’ strategy       Built on top of Github, Keep control, Keeps things easy       Need to develop, test and operate the tools       +       Demo of a tool plugged into Github to enforce a contributor license agreement for anyone pushing a pull request              The ‘build tools’ strategy looks promising, even if it is not yet widely used !   Here are the talk and the slides on the authors website.   Java Performance Analysis in Linux with Flame Graphs   Brendan Gregg   This is what a flame graph looks like :      Technically, it’s just an SVG with some Javascript. It shows the performance big picture. It aggregates data from Linux and JVM profilers. Vertically, you can see the call stacks in your system. The larger a block, the more time is taken inside a function (or in a sub call). The top border is where the CPU time is actually taken. If you want to speed up your system, speed up the wider zones at the top of the graph.   At Netflix, the speaker is a performance engineer, and his job is to build tools to help other teams discover performance issues. This is how they use Flame Graphs :      Compare 2 flame graphs at different times to see what changed   Do a canary release and compare the new flame graph before finishing the deployment   Taking continuous flame graphs on running services helps identify JVM behavior like JIT or GC   They use different color themes to highlight different things   They also use them to identify CPU cache misses   By the way, I also thought this was a great example of using an innovative visualization to manage tons of data.   I could find neither the video nor the slides of the talk, but I managed to find a lot of others talks about Flame Graphs, as well as extra material on the speaker’s homepage.   Increasing Code Quality with Gamification   Alexander Chatzizacharias   You might be wondering why we should care about gamification ?      Worldwide 11.2 billion hours are spent playing every week !   People love to play because it makes them feel awesome   Games are good teachers   At work we are the ones who need to make others successful   But only 32% of workers are engaged in their work !   Games rely on 4 main dynamics :      Competition (be very careful of closed economics which can be very bad for teams)   Peer pressure (Public stats push teams and individual to conform to the norm)   Progression (regular recognition of new skills is motivating)   Rewards (Badges, Level ups, Monkey Money, real money …)   He went on to demonstrate two games that are based on Jenkins and Sonar that aim at better code quality :      One mobile app developed during a 24h Hackathon at CGI which might be open sourced at some point   Another one called ‘Dev Cube’ created at an university, where you get to decorate you virtual cubicle      At the end of the talk, he gave the following recommendations :      Understand the needs of all to respond to everyone’s personal goals   Don’t assign things to do, that’s not fun, give rewards instead   Keep managers out of the picture   To keep it going, you need regular improvements, special events and new rules   KISS !   Playing at work might not be unproductive in the end !   The same talk given at NLJug, unfortunately, it’s in Dutch. English slides are here though.  ","categories": ["java","programming","performance","open-source"],
        "tags": [],
        "url": "/3-more-great-talks-from-javaone-2016/",
        "teaser": null
      },{
        "title": "ReXP : Remote eXtreme Programming",
        "excerpt":"My colleague Ahmad from Beirut gave a talk at Agile Tour Beirut on Saturday about how we adopted XP to a distributed team at work. I gave him a hand and played the remote guy during the talk.   If Marvel wants to create coding superheroes I propose @ahmadatwi and @pbourgau from @Work_at_Murex. &quot;Ballsy&quot; pres at #Agiletourbeirut pic.twitter.com/SkVMPPGcrq &mdash; Xavier René-Corail (@XCorail) 15 octobre 2016   With me through Skype, we did a first demo of remote pair programming on FizzBuzz using IDEA and Floobits     We then did a demo of remote retrospectives using Trello     When should I use ReXP   The conclusion is that :      If people are spread over 2 or a few cities, and that they are enough at every place to build a teams, just build different teams at every location   If people are spread over a lot of places, maybe involving many time zones, then the open source, pull request based work-flow seems the best   Otherwise, if there are not enough people to build 2 teams, that they are spread over only a few locations, that the time difference is not too big, then stretching XP to Remote will work best   As it is said that “nothing beats XP for small collocated teams”, I guess “nothing beats ReXP for small almost collocated teams”.   Tools to make it better   As Ahmad said in his talk, tools already exist. We could add that more would be welcome :      Floobits or Saros help tremendously for remote pairing, but maybe cloud based IDEs like Eclipse Che or Cloud 9 will make all these tools useless !   Trello works well for remote retrospectives, but some great activities like the 5 whys are still difficult to do with Trello. I’m sure new tools could do better.   I’m currently building a remote planning poker app   My other colleague Morgan wants to build a virtual stand up token to make it flow   Finally, here are the slides :       Extreme Practices - AgileTourBeirut - 2016  from Ahmad Atwi   EDIT 2016/11/23 : the full video is now on YouTube  ","categories": ["agile","extreme programming","pair programming","retrospectives","remote","rexp"],
        "tags": [],
        "url": "/rexp-remote-extreme-programming/",
        "teaser": null
      },{
        "title": "Continuously Deliver a Rails App to your DigitalOcean Box using Docker",
        "excerpt":"I decided to use my latest side project as an occasion to learn Docker. I first used Heroku as a platform for deployment (see previous post). It works fine but I discovered the following shortcomings :      Heroku does not deploy with Docker, which means that I’d get quite different configurations between dev and prod, which is one of the promises of Docker :(   The dockerfile provided by docker runs bundle install in a directory outside of the docker main shared volume, this forces to do bundle update twice (once to update Gemfile.lock and a second time to update the actual gems …)   None of these issues could be fixed without moving away from Heroku.   A great Tutorial / Guide   I followed Chris Stump’s great tutorials to setup Docker for my app, to continuously integrate on CircleCI and to continuously deploy on a private virtual server on DigitalOcean.   The first 2 steps (Docker &amp; CI) worked really out of the box after following the tutorial. Dealing with step 3 (CD) was a bit more complicated, because of :      the specificities of DigitalOcean   the fact that I’m a no deployment expert …      What did I need to do to make it work   Setup SSH on the DigitalOcean box   I started by creating a one-click DigitalOcean box with Docker pre-installed. That’s the moment where I had to setup SSH in order to make CircleCI deploy to my box. DigitalOcean has a guide for this, but here is how I did it :      Create a special user on my dev machine adduser digitaloceanssh   Log as this user su digitaloceanssh, and generated ssh keys for it ssh-keygen   Print the public key cat ~/.ssh/id_rsa.pub and copy paste it in your DigitalOcean box setup   Print the private key cat ~/.ssh/id_rsa and copy past it in your circle-ci job ssh keys   The benefit of this is that you should now be able to ssh in your DigitalOcean box from your digitaloceanssh user ssh root@&lt;ip.to.digital.ocean&gt;   Optional : update the box   The first time I logged into my box, I noted that packages were out of date. If you need it, updating the packages is a simple matter of apt-get update &amp;&amp; apt-get upgrade   Fix deployment directory   By default, the home dir of the root user on the DigitalOcean box is /root/. Unfortunately, Chris Stump’s tutorial assumes it to be /home/root/. In order to fix that, I ssh-ed in the box and created a symbolic link : ln -s /root /home/root.   Install docker-compose on the box   Chris Stump’s tutorial expects docker-compose on the deployment box, but DigitalOcean only installs Docker on its boxes … Install instructions for docker-compose can be found here. Don’t use the container option, it does not inherit environment variables, and will fail the deployment, just use the first curl based alternative.   Warning : replace ALL dockerexample   This comes as an evidence, but be sure to replace all the references to ‘dockerexample’ to your own app name in all of Chris Stump’s templates (I forgot some and lost a few rebuilds for that)   Create the production DB   Chris Stump’s deployment script works with an existing production DB. The first migration will fail. To fix this, just do the following :      ssh into the DigitalOcean server   run DEPLOY_TAG=&lt;latest_deploy_tag&gt; RAILS_ENV=production docker-compose -f docker-compose.production.yml run app bundle exec rake db:create   You can find the latest DEPLOY_TAG from the CircleCi step bundle exec rake docker:deploy   How to access the logs   It might come handy to check the logs of your production server ! Here is how to do this :      ssh in your production server   run the following to tail on the logs DEPLOY_TAG=`cat deploy.tag` RAILS_ENV=production docker-compose -f docker-compose.production.yml run app tail -f log/production.log   Obviously, tail is just an example, use anything else at your convenience.   Generate a secret token   Eventually, the build and deployment job succeeded … I had still one last error when I tried to access the web site : An unhandled lowlevel error occurred. The application logs may have details.. After some googling, I understood that this error occurs when you did not set a secret key base for your rails app (details). There is a rails task to generate a token, all that was needed was to create a .env file on the server with the following :   SECRET_KEY_BASE=&lt;GENERATED_SECRET...&gt;   What’s next ?   Obviously, I learned quite a lot with this Docker exploration. I am still in the discovery phase, but my planning poker side project is now continuously built on circleci, and deployed to a DigitalOcean box.   The next steps (first, find a better subdomain, second, speed up the build job) will tell me if this kind of deployment is what I need for my pet projects. If it turns out too complicated or too difficult to maintain, Dokku is on my radar.  ","categories": ["docker","rails","continuous-integration"],
        "tags": [],
        "url": "/continuously-deliver-a-rails-app-to-your-digital-ocean-box-using-docker/",
        "teaser": null
      },{
        "title": "How to kill Scrum Zombies ?",
        "excerpt":"First of all, what is that ? Usually, Scrum zombies go in groups, and quite often, you’ll find a full team of them :   A typical team of Scrum Zombies follows Scrum pretty well, does all the ceremonies, adopted good engineering practices, and might even be delivering OK. But all the fun is gone, everyone is on autopilot, no continuous improvement is happening anymore, retrospectives are dull and repetitive … There’s a gut feeling in the air that sooner or later, the project will miss a turn.      Sounds familiar ?   What’s going on exactly ?   When dev teams want to get more (agile, lean, reactive, or whatever) they often resort to hiring a full time coach. At first, a coach can have a great impact on the team. He will unblock change, show different ways of getting things done and train the team to new practices. Once all this is done, the coach becomes like any team member, or sometimes just leaves. That’s the point when the team, as a whole, has to take on responsibility for continuous improvement.   What’s needed then ?   The team needs to be able to conduct their own experiments and improvements. For this, they need divergent thinking, or creativity, or thinking out of the box; name it as you prefer. In a complex world, no single individual can bring all the answers to all the upcoming issues any team will face. Once the coach has put in place the practices necessary to continuous improvement, it’s up to the team.   Unlike what the common idea says, creativity does not come out of thin air, it is cultivated !   Diversity in the team   Diversity does not mean minority quotas in your team. Diversity means diversity of interest, of way of thinking, of mentality, of way of working … The more diverse your team members, the more likely they’ll find innovative ways to work out of their current problems.   Slack   Removing any slack from the planning is the surest way to kill creativity and innovation. Great ideas often come at unexpected moments (see Pragmatic Thinking and Learning: Refactor Your Wetware) because the mind works in the background to find them. You want to leave some time for that.   Go to conferences   Creativity builds on creativity. Great ideas are often adaptations of one or many existing ones. Going to conferences is a great way to collect a lot of ideas !   Share trainings and lectures   Different people might react differently to the same information. When a team member finishes reading a book or comes back from a training, it’s a great idea to have him present what he learned to the others. This will reinforce his own learning, but it might also trigger new ideas in his team mates.   A dash of turnover   Too much turnover can be fatal to a team, but not having any will bring other kind of problems too. Newcomers will challenge the status-quo, and the “this is how it’s done here” motto won’t be enough for them. That’s just what’s needed to trigger a sane re-examination of the current practices. Oh … and turnover between teams is fine too ! If your company is large enough, you don’t need to hire or fire people to create turnover, just encourage them to move to other teams !   The tricky part of complexity   By leaving time for other things than just cranking out stories, life will come back into the project, and zombies should go away. But wait, there’s even more !   Software projects are pretty complex beasts. One of the most counter-intuitive thing with these complex system is that they make planning very difficult. Focusing too much on your main goal might be slowing you down !      In the face of complexity your project landscape is like a maze of tunnels ! Who said you’re choosing the best ones ? By keeping free time to explore other, seemingly unrelated, topics you might discover opportunities that will remove a lot of the work to get to your final destination !  ","categories": ["agile","management","coaching"],
        "tags": [],
        "url": "/how-to-kill-scrum-zombies/",
        "teaser": null
      },{
        "title": "Video for our talk 'Extreme Practices' is on YouTube",
        "excerpt":"As I already wrote about before, my colleague Ahmad from Beirut gave a talk at Agile Tour Beirut about how we adopted XP to a distributed team at work. I was the remote buddy during the talk.   It was recorded and is now available on YouTube.    ","categories": ["agile","extreme programming","remote","rexp"],
        "tags": [],
        "url": "/video-for-our-talk-extreme-practices-is-on-youtube/",
        "teaser": null
      },{
        "title": "Overclocking a Scrum Team to 12",
        "excerpt":"From Wikipedia :     Overclocking is configuration of computer hardware components to operate faster than certified by the original manufacturer …    It is said that Scrum teams work best at 7 people, and that they break at about 10. The trouble is that sometimes there is just too much work for 7 people, but no enough for a full Scrum of Scrums. What if there was a simple way to hack this number up to 12 ?      An Idea   The Surgical Team   In his classic The Mythical Man Month Fred Brooks presents an idea to organize software development the way surgeons work. The master performs the surgery while the rest of his team (intern or junior surgeon and the nurses) are there to support him. Fred Brook imagined an organization where master developers could be the only ones with access to the production code, while other more junior developers would have the task to provide them with tools and technical libraries.   I admit that this idea sounds out-of-fashion in contrast with modern agile teams of generalists … Still …   Tools   At work, we are working on a pretty technical and complex product which requires some time getting into both the code and the domain. We took a few interns during the past years, and a bit like Fred Brooks, we came to the conclusion that internships yield more results when focused on building supporting tools rather than joining the team and working on production code.   We’ve also been doing retrospectives for 3 years now, we’ve stolen a lot of best practices from the industry and the team is working a lot better than it used to. The pending side of this is that nowadays, the opportunities for improvement that we discover are a lot more specific, and they often need us to take some time to build new tools to support our work.   The Agile Surgical Team   Agile method such as Scrum or XP are all about creating real teams instead of a collection of individual. That means that if we wanted to adopt the surgical team idea, we could use teams instead of individuals : a team of experts, and a tooling team of apprentice developers !   Why not, there’s not nothing really new here, but the challenge is to run such a tooling team efficiently !      3 people or less : there’s evidence in the industry that micro teams can self organize in an ad-hoc manner   Mandate ScrumBan, Continuous Delivery and Devops : on site customer makes this possible, it should reduce project management overhead to almost nothing, and enforce quality   A sandbox for junior developers : there’s no risk of messing up production code here, the domain (tools for software developers) is straightforward and the fast feedback provides a great environment for learning   Obviously, for this to work, you’ll also need to have enough tooling work to do for a 3 people team. That’s usually the case, the CI alone can take quite some time (see Jez Humble’s talk Why Agile Doesn’t Work) and any team will have its own custom tools to do. For example, in our team, we built our own benchmark framework and we could benefit a lot from Saros on IntelliJ.      Not quite there yet   I promised to scale up to 12. Let’s do the maths :      3 people in the tooling team   8 people in the product team if we push Scrum a bit   That’s only 11, 1 is missing. This one is more specific to each team’s context.   As I said earlier, the product we are building is pretty technical and complex. Sometimes, we simply don’t know how we are going to do something. We need to try different ways before finding out the good one. The typical agile way of doing that is by conducting time-boxed spikes. Spikes are fine for code and design related issues but way too short to deal with hard R&amp;D problems. These need a lot of uninterrupted time for research and experiments, so it’s not really possible to split them in backlog stories that any one can work on either …   The R&amp;D Role   Here is what you want : some uninterrupted time to learn and experiment different ways to do something difficult.   Here is what you don’t want :      specialists in the team   people out of sync with the daily production constraints   a never ending ‘research’ topic   Here is a simple solution in the context I describe : add someone in the product team, and do some 2 month round robin on hard subjects. This should leave plenty of time to study something different, but not so much time that one looses connection with the rest of the team. Plus it brings a bit of diversity in every one’s daily work. One issue I can think of is that working on isolation might leave someone on a bad track, regularly presenting what was done to the rest of the team might alleviate this concern.   A final word   Obviously, this has a smell of specialization, we’re bending Scrum principles a bit. So take it for what it is : just like overclocking, it’s a hack to get a bit of extra juice before investing in something a lot more expensive (Scrum of Scrums, Less or whatever).  ","categories": ["agile","software"],
        "tags": [],
        "url": "/overclocking-a-scrum-team-to-12/",
        "teaser": null
      },{
        "title": "How I got my feet wet with machine learning with 'The First 20 Hours'",
        "excerpt":"I’m currently wrapping up an alpha of a unit testing ruby gem that allows to assert the complexity of a piece of code. It’s the result of an experiment to learn some Machine Learning skills in 20 hours … not bad for a first a try at Data Science ! This is the story of this experiment.   How it all started ?      A few months ago, I read The First 20 Hours. The book describes a technique to get up to speed and learn some practical skills on any particular subject in only 20 hours. As examples, the author details how he managed to teach himself a pretty decent level of Yoga, Ukulele, Wind Surfing, Programming, Go and touch typing.   I decided to give it a try. In order to get a boost, I found a few motivated people at work to do it with me. I started by presenting them the technique described in the book, and asked everyone what they wanted to learn. After a quick vote, we set out to learn more about Machine Learning.   The technique   The method is meant to allow anyone to learn necessary skills to accomplish a specific task in about 20 hours. I my case, I could expect to get a basic understanding of the Machine Learning concepts, as well as some practical skills to do something involving Machine Learning. Here are the details of the technique :      H0 : Deep dive in the main concepts and theory of machine learning   H6 : Define an ambitious and practical goal or skill level to achieve by the end, and an outline of how to get there   H6 to H20 : Learn by doing   As you see, the technique is pretty simple !   How did it work ?   For the group   The plan for the group was :      to meet weekly for 2 hours   to share what we learned at the end of every session   to bound by similar goals   At first, people were enthusiastic about learning something like machine learning. After a while, I started to get the following remarks :      “I don’t really see the point of doing this together rather than independently”   “I’m feeling a bit lost by not having a concrete goal and a plan from H0”   “I picked up a target that’s too large for me”   The learning curve must have proven too steep, because as time went by, a lot of people droped out, and we ended up being only 2 !   For me   The first phase was the toughest. As the author had warned in his book, “You’ll get deep above your head in theory and concepts you don’t know”, “You’ll feel lost”. He had some reassuring words though : “The steeper the learning curve, the more you’ll be learning !” I actually like this feeling of unknown things to learn, and that’s why I stuck to it.   I took me 8 hours, and not 6 to get a good overall grasp of Machine Learning techniques. The theory was just too wide and interesting and I could not cut the learning after just 6 hours :-). I studied Machine Learning for developers plus a few other pages for details on specific points. I took and kept notes about what I learned. I chose my subject “unit testing algorithm complexity” for the following reasons :      I could imagine some utility   I had been writing benchmarks at work for 3 years, and I knew the practice well enough   It’s pretty easy to generate data for this subject : just run your algorithm !   It seems a good first step, doable with basic Machine Learning techniques like linear regression   It seems small enough to get something working in 12 hours   I could use ruby, which I find both fast and pleasant to program   This is the plan I set out :      Generate data with a linear algorithm (O(n))   Run linear regression on the data   Compute the the RMSE of the model   Deal with Garbage Collection in order to make reduce its noise   Deal with interpreter warm-up for the same reason   Generate data for a constant (O(1)) algorithm and build a model for it   Find a way to identify if an algorithm is constant or linear from it’s execution timings   Generate data for a quadratic (O(2)) algorithm and build a model for it   Identify if an algorithm is constant, linear or quadratic   Package all this in an RSpec library   It started well, and I made good progress. Unfortunately, as people dropped out of the group and I got more urgent things to do at work, I had to pause my project for a while. It’s only been since last week that I got some time during my holidays to finish this off. I’m currently at H18, and I’ve completed all steps from 1 to 9.   As I said the project is still in early alpha. They is a lot of points in which it could be improved (more complexities, faster, more reliable …). Even though I did not tackle the more advanced machine learning techniques, I now understand the overall process of ML : explore to get an intuitive grasp of the data, try out a model, see what happens, and repeat … I feel that learning these more advanced techniques would be easier now.   My opinion on the method   Overall, I found the experiment really effective, it’s possible to learn quite a lot by focusing on it for 20 hours. A word of warning though : you need to be really motivated and ready to stick through difficulties.   It’s also been very pleasant. I’ve always loved to learn new things, so I might be a little biased on that aspect. I liked the first part when I felt that there was a lot to learn in a large subject I knew almost nothing about. I loved the second part too, although this might be more related to machine learning, because I felt like exploring an unknown (data set) and trying to understand it.   I’ve never been afraid to learn something, doing this experiment taught me I can learn anything fast ! I’ll definitely re-use it again.   One last word about doing this in group. My own experiment did not work very well. Most people were not comfortable with the first ‘explore’ phase. I guess one could make it work better by starting 6 or 8 hours before the rest of the group, enough to grasp the basic concepts and come up with a few end goals. Having concrete targets from day 1 should help people to stick through and to collaborate. The ‘guide’ could also help the others through the first phase.   Stay tuned, I’ll present my gem in a following post  ","categories": ["learning","software","machine learning"],
        "tags": [],
        "url": "/how-i-got-my-feet-wet-with-machine-learning-with-the-first-20-hours/",
        "teaser": null
      },{
        "title": "Verify the Big O Complexity of Ruby Code in RSpec",
        "excerpt":"It might be possible to discover performance regressions before running your long and large scale benchmarks !   complexity_assert is an RSpec library that determines and checks the big O complexity of a piece of code. Once you’ve determined the performance critical sections of your system, you can use it to verify that they perform with the complexity you expect.   How does it work ?   The gem itself is the result of an experiment to learn machine learning in 20 hours (you can read more about that experiment in my previous post if you want).   Suppose you have some a method, let’s call it match_products_with_orders(products, orders) which is called in in one of your processes with very large arguments. Badly written, this method could be quadratic (O(n²)), which would lead to catastrophic performances in production. When coding it, you’ve taken particular care to make it perform in linear time. Unfortunately, it could easily slip back to a slower implementation with a bad refactoring … Using complexity_assert, you can make sure that this does not happen :   # An adapter class to fit the code to measure in complexity assert class ProductsOrdersMatching      # Generate some arguments of a particular size     def generate_args(size)         # Let's assume we have 10 times less products than orders         [ Array.new(size / 10) { build_a_product() }, Array.new(size) { build_an_order() } ]     end      # Run the code on which we want to assert performance     def run(products, orders)         match_products_with_orders(products, orders)     end end  describe \"Products and Orders Matching\" do      it \"performs linearly\" do         # Verify that the code runs in time proportional to the size of its arguments         expect(ProductOrdersMatching.new).to be_linear()     end  end   That’s it ! If ever someone changes the code of match_products_with_orders and makes it perform worse than linearly, the assertion will fail ! There are similar assertions to check for constant and quadratic execution times.   Internally, the code will be called a number of times with different (smallish) sizes of arguments and the execution times will be logged. When this is over, by doing different flavors of linear regressions, it should determine whether the algorithm performs in O(1), O(n) or O(n²). Depending on your code, this can take time to run, but should still be faster than running large scale benchmarks.   Just check the README for more details.   Did you say experiment ?   It all started like an experiment. So the gem itself, is still experimental ! It’s all fresh, and it could receive a lot of enhancements like :      Allow the assertion to specify the sizes   Allow the assertion to specify the warm-up and run rounds   Robustness against garbage collection : use GC intensive ruby methods, and see how the regression behaves   Find ways to make the whole thing faster   O(lnx) : pre-treat with exp()   O(?lnx) : use exp, then a search for the coefficient (aka polynomial)   O(xlnx) : there is no well known inverse for that, we can compute it numerically though   Estimate how much the assert is deterministic   …   As you see, there’s a lot of room for ideas and improvements.  ","categories": ["open-source","ruby","performance","rspec","machine learning"],
        "tags": [],
        "url": "/verify-the-big-o-complexity-of-ruby-code-in-rspec/",
        "teaser": null
      },{
        "title": "What Happens to Non-Enthusiast Programmers in the Long Run ?",
        "excerpt":"A few months ago, after receiving good feedback from my regular readers, I posted my latest article Is There Any Room for the Not-Passionate Developer ? on Hackernews and Reddit. I got a huge number of visits, a lot more than I typically get !   I also got a lot more comments, some nice, some tough, some agreeable and some challenging !      First, a summary   In this previous article, I wanted to contrast the different views about work/life balance in the software industry.   Some, like agile gurus and companies like Basecamp, and studies, strongly advocate for sane work hours. They explain that it results in greater productivity and healthy life.   On the other hand, the software field is always bubbling with novelty, and keeping up to date with technologies is by itself a challenge that takes time. For some companies, which might already be fighting for their survival against competition, it is almost impossible to grant some extra training time to their employees. The problem becomes particularly difficult when engineers get older, become parents and cannot afford to spend some extra time learning the latest JavaScript framework.   As a conclusion, I said that for most of us, it’s really difficult to remain a developer in the long run without the grit that only passion for programming brings. I encourage you to read it for more details.   What I learned from the comments   First of all, thanks a lot for all these, they were very valuable, they forced me to think even more about the issue.   People have been burnt !   The word ‘passion’ in particular, triggered engaged comments. As some pointed out, ‘enthusiast’ or ‘professional’ should be favored. It seems that some companies have asked their employees for unquestionable passion for their business (and not for engineering or programming) at the cost of the people’s own lives. As a commenter said, a lot of shops do not integrate the absolute necessity for their programmers to learn continuously in their business model. It made me kind of sad to feel once more this state of our industry.   As a result, people are weary of any statement of ‘passion’ in the workplace, and would prefer to be seen as very skilled professional, dedicated to keeping their skills up to date.   The particular question of France      I received some comments from all over the world, but my observations came from where I work : in France. Here, all in all, we have at least 7 weeks of paid leaves per year. It’s a lot more than in other parts of the world. I think it’s around 2 weeks in the US (other sources point the same fact). Imagine two companies, one from France, and one from the US. The one in the US can invest 5 weeks per year in exploratory learning (which can result in good things for both the business and the employee) while still producing as much as the french one.   Obviously, there are other parameters to take into account for overall productivity like hours per day, the effects of holidays or long hours on creativity, or funding … but here are some facts about software engineering in France :      20% time policy, hackathons and other exploratory learning are extremely rare (I’ve seen it once in 15 years)   It’s slowly getting better, but if you remain a programmer in your thirties, you’re seen as a loser   France has no software powerhouse like Microsoft, Google, Apple …   This lead me to this open question : What’s the effect of the 7 weeks of paid leaves on the french software industry ?   By no means will I try to give an answer, I just don’t know. Plus, for those who might be wondering : I love my 7 weeks of holidays !   The conclusion I came to   Yet, I can try to draw a conclusion at the individual level. In France, if you’re not really enthusiastic about programming, you won’t put the extra effort off-the-job to learn the latest technologies. Within a few years, you’ll be ‘deprecated’, which will leave you with mainly 2 options :      become a manager   stick to your current codebase (and become completely dependent of your employer)   To me, the sad truth is that if you want to make a career as a professional developer in France, you’d better be ready to spend some of your free time practicing !  ","categories": ["software","programming","career"],
        "tags": [],
        "url": "/what-happens-to-non-enthusiast-programmers-in-the-long-run/",
        "teaser": null
      },{
        "title": "A Plain English Introduction To Paxos Protocol",
        "excerpt":"A few weeks ago, I had to have a look at the distributed consensus protocol Paxos. Even though I know its purpose and I’ve built and used distributed systems and databases in the past, Paxos remains mind boggling at first !   The hard way   The best overall description I found is this answer by Vineet Gupta on Quora. After turning my head around it for a while, I finally gained the instinctive understanding which comes when you ‘get’ something.   As a way to both help others to understand Paxos faster and to burn all this in my own memory, I though it would he a good idea to illustrate it as a story (I was inspired by A plain English introduction to CAP Theorem which I found really instructive; I also later discovered that the original Paxos paper itself related the protocol using the metaphor of a parliament).      Once upon a time …   … there were 3 brothers and sisters, Kath, Joe &amp; Tom, living happily. They lived far away, and it was not easy for them to meet and spend some time together. Neither did they have phone or internet, for this was a long time ago. All they had to discuss and share news was good old mail …   Unfortunately, one day, the worst happened : their parents die. All 3 are informed by a letter from the notary, telling them that they need to sell the family house in order to pay for their inherited debts. It also advises to use Paxos to agree on a price (Note : I never said the story was going to be chronologically sound !).   The happy end   As the oldest in the family, Kath decides to take things in hand, and starts the whole thing. She knows Paxos consists of 2 phases : ‘prepare’ and ‘accept’.   Prepare Phase   Kath sends a signed and dated price value proposal to her brothers, by mail.   Joe and Tom both receive the letter from Kath, they think the price is fair. In order to send their agreements back to Kath, they make a copy of the proposition, mark it as agreed, date it, sign it, and send it back.   Accept Phase   Joe lives a bit further away from Kath than Tom does, so correspondence between Kath and Tom is usually faster. Kath indeed receives the agreement from Tom first, she knows she can go on with the protocol straight away, because Paxos relies on majority, and not unanimity. In his letter, Tom agreed to the same price she proposed, so she just picks this one as the final price to agree on.   She sends new letters, called accept letters this time, to her brothers to finalize the agreement. In these letters, she specifies the price that they are agreeing on, plus the date at which it was first suggested (see Prepare Phase). When Tom and Joe receive the accept letter, they simply need to check that the time and price of the proposal to make sure it is what they agreed on, before sending back their final accept letter.   At the time when Kath receives the accept letters from her brothers, everyone knows that the price has been agreed.      After   She then informs the notary on the agreed price. This one sends an information letter to the Kath, Tom &amp; Joe. The house is sold pretty quickly, leaving the family out of financial problems for the rest of their lives …   Shit happens   That story went unexpectedly well ! Let’s see different variations about what would happen in real life.   Joe is particularly slow to answer   Joe has never been good at paperwork … he’s always out partying and having fun, and he does not want to bother answering letters. When Joe receives the prepare letter from Kath, he does not reply straightaway but leaves it on his desk to handle later. Meanwhile, Tom answers as soon as he got the letter. As mentioned before, Paxos relies on majority, as soon as Kath gets Tom’s answer, she can continue to the next phase. In fact, the accept phase also relies on majority, so she can continue to the end of the protocol if Tom continues to answer.   In this case, Joe would receive the accept letter before he sent his answer to the prepare letter, and would know that the consensus is moving on without him. He can try to catch up or not, but the consensus can be reach without him.   Tom wants to speed things up by becoming the master   Tom has always been the hurried brother. He does not like when things linger forever but prefers things to be done quickly. As soon as he receives the letter from the notary, he starts waiting impatiently for the prepare letter from his sister. Kath, on her part, takes a lot of time to settle on a price. Not knowing what is going on, Tom decides to take action, and to takes on the master role : he sends his own copies of the prepare letters. While these letters are in the mail, Kath finally settles on a price, and sends hers.   Joe gets Tom’s proposal first. Thinking that it’s a change in the plan, he responds straight away by signing the proposal and taking a copy for himself. The following day, he receives Kath’s proposal ! He’s a bit surprised, but hopefully, Paxos tells him exactly what to do in this situation. By agreeing to Tom’s proposal, he made a promise to stick to it whatever happens later. Here the date on Kath’s proposal is later than on Tom’s, so Joe is going to answer to Kath that he agrees but to to Tom’s proposal, of which he’ll join a copy.   After receiving the Joe’s agreement on his proposal, Tom has the majority, and should be able to end the protocol.   What about Kath ?   She should have received Tom’s proposal, and rejected it, because she had already proposed a later value. That will not prevent Tom to reach a consensus.   She should have received Joe’s agreement to Tom’s proposal. The same way, she might as well have received Tom’s agreement to his own proposal as an answer to hers. She’d get the majority of agreements, so she might then want to push on. For the accept letter, she must pick a value that has been accepted, in this case, it’s Tom’s proposed value ! Everything ends as expected as she’ll reach the same price as Tom.   Tom wants a higher price an becomes the master   Imagine Tom is obsessed about money ! When he receives Kath’s proposal, he’s outraged ! Believing the house has a lot more value than the proposed price, he sets on to act as a master in Paxos and sends his own proposal letters to his brother and sister.   Unfortunately, when they receive his proposal, they have already agreed to Kath’s older proposal, so they send him back a copy of it as an agreement. Having received agreements to Kath’s value only, he cannot push forward his value. Whether he continues his Paxos or not does not really matter, as he would reach the same value as Kath would.   River flood split between brothers and Kath      There’s a wide river that separates Kath from Joe and Tom. While they were trying to reach consensus, the river flood, cutting all communication between the brothers and their sister. Kath might abort the consensus as she won’t be able to get answers from the majority. On their side, Joe or Tom can takeover the consensus, take on the master role, and still reach a price, as they form a majority. As soon as the river would settle, the messages would arrive to both parties, eventually informing Kath that a price was accepted.   Lots of others   You can imagine zillions of ways in which the consensus between Kath, Joe and Tom could go wrong. For example :      Mail is so slow that Kath sends new proposals   One letter gets lost and arrives after Kath made a new proposal   Kath is struck by a lightning   Go ahead and execute Paxos step by step on all of them, you’ll see that whatever happens, Kath, Joe and Tom will reach a price.   More Formally   Now that you have an instinctive understanding of Paxos, I encourage you to read out the full explanation I found on Quora. Here is a extract with the protocol part :      Protocol Steps:     1) Prepare Phase:                 A node chooses to become the Leader and selects a sequence number x and value v to create a proposal P1(x, v). It sends this proposal to the acceptors and waits till a majority responds.                  An Acceptor on receiving the proposal P1(x, v1) does the following:                  If this is the first proposal to which the Acceptor is going to agree, reply ‘agree’ – this is now a promise that the Acceptor would reject all future proposal requests &lt; x         If there are already proposals to which the Acceptor has agreed: compare x to the highest seq number proposal it has already agreed to, say P2(y, v2)                        If x &lt; y, reply ‘reject’ along with y             If x &gt; y, reply ‘agree’ along with P2(y, v2)                                         2) Accept Phase                 If a majority of Acceptors fail to reply or reply ‘reject’, the Leader abandons the proposal and may start again.                  If a majority of Acceptors reply ‘agree’, the Leader will also receive the values of proposals they have already accepted. The Leader picks any of these values (or if no values have been accepted yet, uses its own) and sends a ‘accept request’ message with the proposal number and value.           When an Acceptor receives a ‘accept request’ message, it sends an ‘accept’ only if the following two conditions are met, otherwise it sends a ‘reject’:                Value is same as any of the previously accepted proposals         Seq number is the highest proposal number the Acceptor has agreed to                 If the Leader does not receive an ‘accept’ message from a majority, abandon the proposal and start again. However if the Leader does receive an ‘accept’ from a majority, the protocol can be considered terminated. As an optimization, the Leader may send ‘commit’ to the other nodes.      And here are the key concepts to map my story to this formal description of Paxos.   | Story | Paxos |——-|——- | proposal letter (and copy of) | P(x,v) | Date (and time) | sequence number     At the time of slow mail based communication, using the date and time down to the second is enough to build up unique sequence numbers. In our current time of digital messages, it’s another story, typical Paxos implementation assigns a different and disjoint infinite set of integers for every participant, it does not exactly follow ‘time’, but it’s enough for the algorithm to work.  ","categories": ["software","distributed systems"],
        "tags": [],
        "url": "/a-plain-english-introduction-to-paxos-protocol/",
        "teaser": null
      },{
        "title": "5 minutes hack to speed up RSpec in Rails 5 using in-memory SQLite",
        "excerpt":"Here is the story : you have a Rails 5 app that uses RSpec, but your RSpec suite is getting slower and slower to run. You’ve already considered some solutions :      Use SQLite in memory for your test env.   test:   adapter: sqlite3   database: \":memory:\"   That’s the most straightforward thing to do, but unfortunately, if you are sharing your test env with Cucumber, you might want to use a production like DB with Cucumber (PostgreSQL or whatever). So unless you are ready to setup a new env for cucumber (which I tried and don’t recommend) you’re stuck.      Use mocks. That’s surely going to work, it’s going to make your test hell of a lot faster ! It will also make your tests a lot more fragile and more expensive to maintain … If you want to read more about why I think mocks are a bad idea, just have a look at these posts.   The hack   Here is a third alternative, I’ve already written about it, but here it comes updated and tested for Rails 5 :      Don’t change anything to your config/database.yml   Obviously, you’ll need to add sqlite3 to your Gemfile   At the beginning of your spec/rails_helper.rb, replace   # Checks for pending migration and applies them before tests are run. # If you are not using ActiveRecord, you can remove this line. ActiveRecord::Migration.maintain_test_schema!   with   # In order to keep the same RAILS_ENV for rspec and cucumber, and to make rspec # faster, patch the connection to use sqlite in memory when running rspec ActiveRecord::Base.establish_connection(adapter: 'sqlite3', database: ':memory:') ActiveRecord::Schema.verbose = false load \"#{Rails.root.to_s}/db/schema.rb\"   That’s it ! Run your specs … not bad for a 5 minutes investment !   Rails 5.1 (2017-03-29 Edit)   My fresh hack started to fail on Rails 5.1 ! If schema.rb is generated with the Postgres adapter, it is now incompatible with this injected Sqlite adapter. Here is a patch that removes the glitches :   # In order to keep the same RAILS_ENV for rspec and cucumber, and to make rspec # faster, patch the connection to use sqlite in memory when running rspec ActiveRecord::Base.establish_connection(adapter: 'sqlite3', database: ':memory:') ActiveRecord::Schema.verbose = false # load db agnostic schema by default. Needed to remove the \", id: :serial\" from # the table definitions to make it load on sqlite eval(`cat #{Rails.root.to_s}/db/schema.rb | sed 's/,[^:]*: :serial\\//g'`)   I admit this is getting a bit crappy, and I don’t know how long it is going to work …   One more thing …   If you need even more speed, you can now run your specs in parallel in different processes ! Each in-memory SQLite DB is bound to its process, so unlike a real PostgreSQL dev DB, you won’t get any conflicts between your tests ;-)  ","categories": ["ruby","rails","rspec","testing","cucumber","sqlite"],
        "tags": [],
        "url": "/5-minutes-hack-to-speed-up-rspec-in-rails-5-using-in-memory-sqlite/",
        "teaser": null
      },{
        "title": "How I fixed 'devicemapper' error when deploying my Docker app",
        "excerpt":"A few months ago, I started continuously deploying my latest side project to a Digital Ocean box. If you are interested, here is the full story of how I did it. All was going pretty well until last week, when the builds unexpectedly started to fail. I wasn’t getting the same error at every build, but it was always the Docker deployment that failed. Here are the kind of errors I got :   # At first, it could not connect to the db container PG::ConnectionBad: could not translate host name \"db\" to address: Name or service not known  # Then I started to have weird EOF errors docker stderr: failed to register layer: ApplyLayer exit status 1 stdout:  stderr: unexpected EOF  # Eventually, I got some devicemapper errors docker stderr: failed to register layer: devicemapper: Error running deviceCreate (createSnapDevice) dm_task_run failed   You can read the full error logs here.   That’s what happens when you go cheap !   After searching the internet a bit, I found this issue which made me understand that my server had ran out of disk space because of old versions of my docker images. I tried to remove them, but the commands were failing. After some more search, I found this other issue and came to the conclusion that there was no solution except resetting docker completely. Hopefully, Digital Ocean has a button for rebuilding the VM.      Once the VM was rebuilt, the first thing that I did was to try to connect from my shell on my local machine. I had to clean up my known host file, but that was simple enough.   nano ~/.ssh/known_hosts   Once this was done, I just followed the steps I had documented in my previous blog post   Was I all done ?   Almost … I ran into another kind of errors this time. Processes kept getting killed on my VM.   INFO [cc536697] Running /usr/bin/env docker-compose -f docker-compose.production.yml run app bundle exec rake db:migrate as root@104.131.47.10 rake aborted! SSHKit::Runner::ExecuteError: Exception while executing as root@104.131.47.10: docker-compose exit status: 137 docker-compose stdout: Nothing written docker-compose stderr: Starting root_db_1 bash: line 1: 18576 Killed   After some more Google searching, I discovered that this time, the VM was running out of memory ! The fast fix was to upgrade the VM (at the extra cost of 5$ / month).      After increasing the memory (and disk space) of the VM, deployment went like a charm. Others have fixed the same issue for free by adding a swap partition to the VM.   The end of the story   I wasted quite some time on this, but it taught me some lessons :      I should have taken care of cleaning up the old images and containers, at least manually, at best automatically   I should write a script to provision a new server   The cheap options always come at a cost   For an open source side project like this one, it might be a better strategy to only use Docker to setup my dev env, and use free services like Travis-ci and Heroku for production   Doing everything myself is not a good recipe to getting things done … I well past time I leave my developer hat for an entrepreneur cap   In order to keep learning and experimenting, focused 20h sessions of deliberate practice might be the most time effective solution  ","categories": ["docker","exploitation"],
        "tags": [],
        "url": "/how-i-fixed-devicemapper-error-when-deploying-my-docker-app/",
        "teaser": null
      },{
        "title": "Developer ! Are you losing your rat race ?",
        "excerpt":"   A rat race is an endless, self-defeating, or pointless pursuit. It conjures up the image of the futile efforts of a lab rat trying to escape while running around a maze or in a wheel.    Are we building our own maze self defeating landscape by our exacerbated focus on technology ? Let me explain.      The context   As Marc Andreessen famously said “Software is eating the world”, which means that there is more and more demand for software. At the same time, giant countries like China, India, Russia or Brazil are producing more and more master’s degrees every year. This also means more and more software engineers. The consequence is that there has never been so many new technologies emerging than these days. The software landscape his huge, growing and complex.   That’s great for progress, but it’s a puzzle for hiring. In this chaotic environment, years of experience with a particular technology is something that remains easy to measure, that’s why employers (and developers) tend to use keywords to cast for a job.   The effects   As a result, developers tend to pick a few technologies to become masters at, to put them on their CV and get job offers. There’s a danger with specializing on a particular technology : eventually, it will become deprecated, in this keyword driven world, it’s almost like if you’ll have to start from zero again. Even if a specialization is wide enough now, as time goes on and more and more technologies are created, any area of expertise will become a tiny spot in all the landscape. One might think this is only an issue for old guys that did not stay up to date … I strongly believe this is wrong, it happened to all past technologies, I don’t see why today’s latest .js framework wouldn’t be legacy stuff one day.   One could think that sticking to a good employer is a good fix against that. It is … for some time ! Sticking to an company actually means betting on this company. What would happen if it went out of business, or through difficult times and you’re asked to leave ? When you reach the job market after so long with a single employer, you’ll be a de-facto specialist, on proprietary stuff that no one is interested about.   Finally, you might work hard not to specialize, but it’s going to be a lot more difficult to get a job as a generalist, only a few shops actually hire this way.   To summarize, we are forced into specialization, which is great in the short term, but risky in the long run.   1€ advice   So what can we do about this ? Obviously, we cannot change the world … The only ones we can act on are ourselves !   Learning   In our fast moving tech world, learning remains key ! But instead of trying to keep up with all the cool new techs that are invented every day, we should study fundamental skills, and only learn just enough specific skills to get the job done. To me fundamental skills are all the things you’ll apply whatever the language and technology you are using, for example :      design   architecture (whatever that is …)   clean code   refactoring   legacy code   testing   tooling   mentoring &amp; coaching   programming paradigms (functional, dynamic, static, imperative, OO, concurrent …)   process flow   communication   product definition   concurrency   performance   I wrote this post that explains how I did learn some of these skills (by no mean would I say that this is the only way). Good mastery of these skills should be enough to quickly get up to speed in any project you are involved. This other article In 2017, learn every language, which I found through the excellent hackernewsletter, explains how this is possible.   Unfortunately, knowing is not enough …   Selling   How do you convince others that you are up to the job in a particular technology ? Unfortunately, I don’t have a definitive answer yet …   Regularly, people try to coin a word to describe the competent generalist developer : polyglot, full stack, craftsman … If it’s good enough, it usually gets taken over quite fast by the industry and just becomes yet another buzzword (the only exception being eXtreme Programming, but who would like to hire an eXtreme Programmer ?).   In Soft Skills, John Somnez says the trick is to explain to people that you might not have experience in a technology ‘yet’. This might work, if your resume gets through, which is not sure.   Here’s my try : the next time I’ll polish my resume, I’ll try to put forward my fundamental skills first, for example with 5 stars self-assessments. Only after will I add something like “By the way, I could work with tech X Y Z …”.   Independence      Being your own boss could be a solution in the long term. I recently listened to The End of Jobs in which the author explains that entrepreneurship is an accessible alternative these days, and that like any skill, it’s learnable. The catch is that there are no schools, no diplomas, and that it seems a lot riskier in the short run. Despite that, he makes the point that the skills you’ll learn makes it quite safe in the long run !   Questions   I feel like my post asks more questions than it provides answers :-). Honestly, I’d really love to read other people’s opinions and ideas. What are your tricks to market yourself on new technologies ? As a community, what could we do to fight our planned obsolescence ? Do you think I’m totally wrong and that the problem does not exist ? What do you think ?  ","categories": ["software","programming","career"],
        "tags": [],
        "url": "/developer-are-you-losing-your-rat-race/",
        "teaser": null
      },{
        "title": "How I finally use Docker on small open source side projects",
        "excerpt":"A few months ago, I started Philou’s Planning Poker, an open source side project to run planning poker estimate sessions remotely. The main technology is Rails, and I’d been planning to use Docker as much as possible as a way to learn it. Indeed, I learned that Docker is no Silver Bullet !      The Docker love phase   At first everything seemed great about Docker. I’d used it on toy projects and it proved great to quickly setup cheap and fast virtual machines. I even created the Rubybox project on Github to clone new ruby VMs in a matter of seconds. I also used Docker to host my Octopress environment to write this blog. As a long time Linux user, my dev machines have repeatedly been suffering from pollution : after some time, they get plagued with all the stuff I installed to do my various dev experiments, and at some point, re-install seems easier than cleaning up all the mess. If I could use containers for all my projects, Docker would be a cure for this.   Going through all these successes, when I started my planning poker app, I decided to go all into Docker, development, CI and deployment. You can read the log of how I did that in these posts. Fast forward a bit of searching, experimenting and deploying, all was setup : my dev env was in containers, my CI was running in containers in CircleCI and the app was pushed to containers on DgitalOcean.   Reality strikes back   At first, everything seemed to be working fine. Even if there were a few glitches that I would have to fix down the road like :      Whenever I wanted to update my app’s dependencies, I had to run bundle update twice, and not incrementally. Surely, I would manage to fix that with a bit of time   Obviously, the CI was slower, because it had to build the containers before deploying them to Docker Hub, but that was the price to pay in order to know exactly what was running on the server … right ?   And … Guard notifications did not appear on my desktop. I was accessing my dev env through ssh, so I would have to fix that, just a few hours and it should be working   After a while, I got used to my work environment and became almost as productive as I used to be … but you know, shit happens !      I had to install PhantomJS on my CI, and if that comes out of the box on TravisCI, you’re all alone in your own containers. Installing this on the Debian container proved unnecessarily complex, but I figured it out   Then all of a sudden, my CI started to break … You can read a summary of what I did to fix it here. Long story short : I had forgotten to clean up old docker images, and after enough deployments, the server ran out of space, and that corrupted the docker cache somehow. I eventually re-installed and upgraded the deployment VM. That made me lose quite some time though.   Finally, as I started to play with ActionCable, I could not get the web-socket notifications through my dev host. There must be some settings and configuration to make this work, for sure, but it’s supposed to work out of the box.   Eventually, this last issue convinced me to change my setup. All these usages of Docker where definitely worth it from a learning point of view, but as my focus moved to actually building the app, it was time to take pragmatic decisions.   My use of Docker now   There were 2 main ideas driving my changes to my dev env for this open source side project :      Use the thing most people do   Use commercially supported services &amp; tools   These should avoid losing my time instead of being productive. My setup is now almost boring ! To summarize I now use TravisCI, Heroku, and rbenv on my physical machine. I kept Docker where it really shines : all the local servers required for development are managed by Docker Compose. Here is my docker-compose.yml   db:   image: postgres:9.4.5   volumes:     - planning-poker-postgres:/var/lib/postgresql/data   ports:     - \"5432:5432\"  redis:   image: redis:3.2-alpine   volumes:     - planning-poker-redis:/var/lib/redis/data   ports:     - \"6379:6379\"   This saves me from installing Postgresql or Redis on my dev machine, and I can start all the services required for app with a single docker-compose up command !   My future uses of Docker   More generally, in the near future, here is when I’ll use docker      As I just said, to manage local servers   To boot quick and cheap VMs (check rubybox)   To handle CI and deployment of large or non-standard systems, where Docker can provide a lot of benefits in terms of price, scaling or configurability   Docker came from the deployment world, and this is where it is so great. As of today though, even if it is usable as dev VM, it is still not up to a standard dev machine. Despite that, all the issues I ran into could be fixed, and I’m pretty sure they’ll be some day.  ","categories": ["side-project","open-source","continuous-integration","docker","travis","heroku","rails"],
        "tags": [],
        "url": "/how-i-finally-use-docker-on-small-open-source-side-projects/",
        "teaser": null
      },{
        "title": "How to subscribe to an ActionCable channel on a specific page with custom data ?",
        "excerpt":"In my spare time, I’m writing a Planning Poker App. As a reminder, planning poker is a group estimation technique designed to eliminate influence bias. Participants keeps their estimates secret until everyone unveils them at the same time (See Wikipedia for more details).   The driving idea behind my app is for team members to connect together and share a view of the current vote happening in their team. Each team has an animator, who is responsible to start new votes. This is the aspect I’ve been working on during the last few days. I want all team members to be notified that a new vote started by displaying a countdown on their page.   I am building the app with Rails 5 but I did not have a clear idea of what technology to use to build this feature. After some googling, I found that ActionCable provides just the kind of broadcasting I am looking for (Have a look at the ActionCable Rails guide for more details).   A Specific Page   The Rails guide is pretty clear, as usual I would say, but all the examples show subscriptions at any page load. As explained above, I only want participants to subscribe to their own team’s votes : until they have joined a team, it is not possible to subscribe to a particular channel.   As my app is currently behaving, once identified, participants get to a specific team page. I wanted to use this page as the starting point to my subscription. After some more googling about page specific JavaScript in Rails, I found this page from Brandon Hilkert that explains how to do this cleanly. The idea is to add the controller and action names to the body tag, and to filter out js code at page load. This is what I ended up doing :   First, I adapted the app layout to keep track of the controller and action names in the HTML body :  &lt;!-- app/layouts/application.html.erb --&gt; &lt;!DOCTYPE html&gt; &lt;html&gt;   ...   &lt;body class=\"&lt;%= controller_name %&gt; &lt;%= action_name %&gt;\"&gt;     ...   &lt;/body&gt; &lt;/html&gt;   Then I replaced the default channel subscription with a function :  # app/assets/javascripts/channels/team.coffee window.App.Channels ||= {} window.App.Channels.Team ||= {}  App.Channels.Team.subscribe = -&gt;   App.cable.subscriptions.create \"TeamChannel\",     received: (data) -&gt;       # Do something with this data   As a reminder, here is what the server side channel would look like :  class TeamChannel &lt; ApplicationCable::Channel   def subscribed     stream_from \"team_channel\"   end end   Finally, I called this subscribe function from some page specific Javascript :  # app/assets/team_members.coffee $(document).on \"turbolinks:load\", -&gt;   return unless $(\".team_members.show\").length &gt; 0    App.Channels.Team.subscribe()   That’s it. By playing around in your browser’s js console, you should be able to test it.   Custom Data   That’s just half of the story. The code above subscribes on a specific page, but it does not specify any particular team channel to subscribe to. This means that all participants would receive notifications from all teams !   In his article about unobtrusive JavaScript in Rails, Brandon Hilkert also suggests using HTML data attributes to pass parameters to the a JavaScript button event handler. There’s no button in our case, but we can still use the same technique. Let’s add data specific attributes to the HTML body.   To subscribe to specific team channel, the plan is to add the team name to the HTML body tag through a data attribute, then to capture and use this team name when subscribing.   Again, let’s enhance the layout :  &lt;!-- app/layouts/application.html.erb --&gt; &lt;!DOCTYPE html&gt; &lt;html&gt;   ...   &lt;body class=\"&lt;%= controller_name %&gt; &lt;%= action_name %&gt;\" &lt;%= yield :extra_body_attributes %&gt; &gt;     ...   &lt;/body&gt; &lt;/html&gt;   I had to adapt my views. In the team members show view (the one doing the subscription), I added an extra data attribute for the team name :   &lt;!-- app/views/team_members/show.html.erb --&gt; &lt;% provide(:extra_body_attributes, raw(\"data-team-name=\\\"#{@team.name}\\\"\")) %&gt;  ...   With this done, it is possible to capture the team name from the page load event and feed it to the subscribe method :  # app/assets/team_members.coffee $(document).on \"turbolinks:load\", -&gt;   return unless $(\".team_members.show\").length &gt; 0    App.Channels.Team.subscribe($('body').data('team-name'))   I then used the team name to subscribe to a specific channel :  # app/assets/javascripts/channels/team.coffee window.App.Channels ||= {} window.App.Channels.Team ||= {}  App.Channels.Team.subscribe = (teamName) -&gt;   App.cable.subscriptions.create {channel: \"TeamChannel\", team_name: teamName},     received: (data) -&gt;       # Do something with this data   The last piece is to actually start a specific channel :  class TeamChannel &lt; ApplicationCable::Channel   def subscribed     stream_from \"team_channel_#{params[:team_name]}\"   end end   Same as before, hack a bit with your browser’s console, you should be able to check that it’s working.   Last thoughts   This is not exhaustive, depending on your situation, there might be other things you’ll need to do, like unsubscriptions for example.   I’d also like to give a word of feedback about ActionCable after this first look at it. Overall, it worked great both in development and production. Everything seemed to work almost out of the box … Except testing : I did not manage to write robust unit tests around it. There is pull request for that that should be merged in Rails 5.~ sometimes soon. For the moment, I’m sticking to large scale cucumber tests.  ","categories": ["rails","javascript","action-cable"],
        "tags": [],
        "url": "/how-to-subscribe-to-an-actioncable-channel-on-a-specific-page-with-custom-data/",
        "teaser": null
      },{
        "title": "My Ultimate Jira Personal Kanban",
        "excerpt":"A few years ago, I wrote about how I started to use Jira as my personal Kanban board at work. A lot of things have changed since then, which brought me to update my board and make it even more productive !   The context   During the almost 18 months since I wrote this first post, a lot of things have changed in my daily work (hopefully : I’m not doing the same thing again and again !). Essentially, I got involved in more projects, some of which involve people from all around the company and some of which don’t require any code to be written. For example, I’m now engaged in our Agile Community of Practice, where I sometimes contribute content.   Here are the consequences on my work :      I have more tasks to deal with, not necessarily more work, but still more tasks   I have more sources of tasks : tasks can come from any of the projects I am involved in   I have more tasks depending on other people, and that are in a WAITING state meanwhile   I had to adapt my personal Kanban to this new workload.   The changes   As I explained in the previous description of my Jira Personal Kanban, I am using a custom project and Kanban board to aggregate all my tasks from various projects, in order to see everything in a single unique place. Here are the changes I’ve made since, so if you haven’t yet, it might be a good idea to read that previous version first.   Quick filters   In his post Maker’s Schedule, Manager’s Schedule Paul Graham explained the challenge of having a lot of non-programming work to do everyday for programmers. He then advises to use slots for different activities during the day, in order to keep uninterrupted chunks of time to do creative work. To apply this technique, I reserved ‘Unbookable except for X’ slots in my calendar everyday.   I had previously been using Swim-lanes to track work from different projects. This turned out not to scale very well to more projects : it made the board messy, and I kept being distracted by all these other tasks. I ditched all the Swim-lanes (not exactly, I kept one for urgent issues only). Instead of Swim-lanes for tracking projects, I now use Quick Filters. I created such filters as With Project X and Without Project X. During the day, when I want to focus on Project X, I use quick filters to only show tasks related to it.      Day markers   I have a daily routine of checking what’s on my plate and deciding what I’d like to achieve during the day (picking the right time to do this is an art in itself). In order to keep track of this, I use special day marker tasks : as ^^^ TODAY ^^^, ^^^ TOMORROW ^^^ and ^^^ THIS WEEK ^^^. This tasks are always in my TODO column, and will never be completed. I move them around to mark what I expect to finish at different time horizon. Ex : everything above ^^^ TODAY ^^^ should be finished before the end of the day.   Again, this helps me to focus on today’s activities, and to do just enough prioritization.      One last thing here, you’ll have noticed the Epic for these special tasks. It’s a way to identify them in JQL queries.   WAITING flag   Quite often, you have tasks waiting for someone else. That’s surely not the most efficient situation, but once you leave the comfort of your focused dev team, handoffs are often the norm (at least until the lean principles spread in every part of the business). Status of waiting tasks is worth checking regularly, but very certainly not many times per day !   Again, leaving them in my board created useless distraction. I have now taken the habit of renaming the tasks I’m waiting for with a [WAITING] ... prefix. On top of that, I created 2 quick filters WAITING and Without WAITING to quickly check and then forget about waiting tasks.      Watching tasks I’m not the assignee of   On some occasions, we might be two of us working on the same task, or I might want to keep an eye on a task to know if something is needed from me. As there is only a single possible assignee in Jira, I changed my global filters to also include tasks with a custom label pbourgau-watch. Any time I want to add a task in my board, I just add this label to it.      Getting the Lean reports back   In order not to have too many old tasks in my board, I used to filter out old items in the global filter. This did the job, but at the cost of invalidating the lean reports (cumulative flow and control charts). In order to get these back, I removed this constraint from the global filter, and created yet another quick filter Without Old which I almost always keep on.      Scripts   Global Filter  project in (POP, POPABTODO, \"Development Engineering Program\", COPA) AND type != Epic AND (Assignee = pbourgau OR      Co-Assignees in (pbourgau) OR      mentors in (pbourgau) OR     labels in (pbourgau-watch)) ORDER BY Rank ASC   Quick Filters  -- With \"Project X\" + Day marker tasks (Epic link ...) + tasks containing \"BRANDING\" project = \"Project X\" or \"Epic Link\" = POPABTODO-410 or summary ~ \"BRANDING\"  -- Without \"Project X\" project != \"Project X\" and summary  !~ \"BRANDING\"  -- Without Old status not in (DONE,CLOSED) OR updated &gt;= -14d  -- WAITING summary ~ 'WAITING'  -- Without WAITING summary !~ 'WAITING'   Things that did not change   I still use a WIP limit on the In Progress column, display the Epic in on the cards and special use custom color coding for tasks :   -- Tasks with an imminent due date become red duedate &lt;= 1d or priority = \"1-Very High\"  -- Tasks with a due date are orange duedate is not EMPTY   The result   Overall, this is how my board looks like :      I guess I’m a kind of personal productivity geek … but I believe it’s a skill of utter importance for developers, especially when they get a bit of experience and are not fed ready made tasks to do.  ","categories": ["kanban","jira","personal-productivity"],
        "tags": [],
        "url": "/my-ultimate-jira-personal-kanban/",
        "teaser": null
      },{
        "title": "Almost 15 years of using Design By Contract",
        "excerpt":"I first read about Design By Contract in 2002, in Object Oriented Software Construction 2. As soon as I read it, I was convinced, today, I still believe it’s a great and fundamental technique. That’s why, I almost never write a contract ! Let me explain.   Phase 1 : DbC ignorance   I started to code professionally in 2001. This was a time where design and quality software meant Rational Rose (a UML design and code generation tool) while I, at the contrary, was just Cow Boy Coding my way out of any problem I was given.   I wasn’t really doing Object Oriented programming, but rather imperative programming, using objects as structs, getters, setters, and classes as a way to organize the code … In this context, my design skills were improving slowly, and I was at the risk of falling in love with a local-optimum practice that would prevent me from growing further.   That’s why I started to read books such as the Gang Of Four Design Patterns, or OOSC2.   Phase 2 : DbC enlightenment      Reading this book was a profound experience to me. My programming changed fundamentally before and after reading it. The chapter about contracts, taught me what objects are.   One the one hand, Pre and Post conditions can be used in any kind of programming and are just a kind of C assert macro on steroids. Class invariant, on the other hand, is a completely different kind of beast. The invariant of a class is a predicate about an instance of this class that should always be true. For example : field X should never be null, or the value of field N should always be greater than 0.   In some way, grasping the concept of invariant is close to understanding what a class is.   Phase 3 : DbC everywhere   That’s when I started to write contracts everywhere. I was writing C++ code at the time, and my code must have looked something like that :   class MonkeyWrench {     bool m_isStarted;     std::vector&lt;Part&gt;* m_movingParts;  protected:          virtual void invariant() const     {         assert(m_isStarted == (m_movingParts != NULL));     }      public:          MonkeyWrench()     {         this-&gt;m_isStarted = false;         this-&gt;m_movingParts = NULL;                  invariant();     }          bool isStarted() const     {         return this-&gt;isStarted();     }          void start()     {         assert(!this-&gt;isStarted());         invariant();                  this-&gt;m_movingParts = ...                  invariant();         assert(this-&gt;isStarted());     }          const std::vector&lt;Part&gt;&amp; movingParts() const     {         assert(this-&gt;isStarted());         invariant();                  return *this-&gt;m_movingParts;     }     ... };   I definitely over-abused contracts, it made the code unreadable. Plus sometimes, I was using excessively long and intricate assertions which made the problem even worse.   Hopefully, overusing contracts also taught me a lot in a short time. Here are some of the lessons I learned :      DbC is not very well supported, it’s never something built in the language, and edge cases like inheriting an invariant or conditions can become messy pretty fast.   Checking for intricate contracts at every method call can be pretty slow.   Checking everything beforehand is not always the simplest thing to do, at times, throwing an exception on failure just does a better job.   It happened that removing the contract made the code do just what I wanted. It’s easy to write unnecessary strict contracts.   Command Query Separation Principle is great ! Having ‘const’ or ‘pure’ queries that don’t change anything make writing contracts a lot simpler.   Preconditions on queries are painful. When possible, returning a sensible ‘null value’ works better, nothing is worse than getting an error when trying to call a const query from the interactive debugger.   Finally, the more immutable a class is, the simpler the invariant. With a lot of mutable fields, you might resort to have the invariant check that fields are synchronized as expected. If fields are immutable, this simply vanishes.   Phase 4 : DbC hangover   At the same time I discovered all these small subtleties about contracts, I fell upon Martin Fowler’s book Refactoring, improving the design of existing code and started to use Unit Tests extensively. This lead me to the following conclusions :      Tests are more efficient at producing quality software   Contracts can be an hindrance when trying to do baby steps refactorings as described in Martin Fowler’s book   On top of that, as DbC is not natively supported by languages, no documentation is generated, meaning that most of the time, the callers still have to look into the code. As a result, I was using contracts less and less often.   Phase 5 : DbC Zen   Looking back, I might not be writing a lot of asserts in my code, but I am still thinking in terms of contracts all the time. In fact, there are a ton of ways to use DbC without writing assertions :      Use as much immutability as possible. An immutable class does not need to check its invariant all the time, just throwing from the constructor if arguments are not valid is enough.   Use conventions as much as possible, for example, constructor arguments should be set for all the life of the object (cf Growing Object Oriented Software Guided by Tests which describes the different ways to inject something in an object)   Looking back at my DbC assertions, most relate to null values. Again conventions work better ! At work, we simply forbid passing null values around. If something can be null, it means it’s optional, Java has an Optional&lt;T&gt; class for just that (I’m pretty sure it is possible to do something even better with C++ templates). In this case, if the contract is broken, NullReferenceException will eventually be our assertion.   Replace as many pre &amp; post conditions with invariants on the callee, the arguments or the return objects as possible. It makes sense as it’s just making sure we are using ‘valid’ objects everywhere. Again, if these objects are immutable, it makes the whole thing even simpler !   To take further benefit of the invariant of immutable objects, introduce new types. For example, instead of changing an object’s state through a command with associated involved contracts, split the class in 2 and make the method a query returning an immutable object, potentially making the initial class immutable as well. Remember, immutable classes mean almost no assertions !   Use your language. Ex, instead of asserting that 2 lists remain of the same length, refactor to a list of pairs ! (I know that’s an obvious example, but you get the point)   If you are using a statically typed language, use types ! For example, I remember at one project I worked on, we had an bug involving a duration : somewhere in the code milliseconds got mistaken for seconds … We fixed that by replacing the integer by TimeSpan all over the place. Again, that’s so obvious !   Eventually, when all else fails, or when it’s just too much overhead, use the simple asserts provided by your language or common libraries.   To come back at the previous code section, this how it could be written without assertions :   class MovingMonkeyWrench {     const std::vector&lt;Part&gt; m_parts;      public:     MovingMonkeyWrench() : m_parts(...) {}          const std::vector&lt;Part&gt;&amp; parts() const     {         return this-&gt;m_parts;     }     ... };  class MonkeyWrench {    public:     MovingMonkeyWrench start() const     {         return MovingMonkeyWrench();     }     ... };   Details are omitted, but it’s easy to see how shorter the code is.   Conclusion   When applying all the techniques above, you’ll see that cases for explicit assertions are rare. Less assertions also workarounds the issues coming from the poor support for DbC : no documentation and intricate cases.   In the end, assertions made my code more ‘functional’. I’m not alone to have done the same journey, and if you are interested you should read Eric Evans’ DDD book where he presents things like immutable value objects and specification objects.  ","categories": ["programming","contracts","ddd","functional-programming"],
        "tags": [],
        "url": "/almost-15-years-of-using-design-by-contract/",
        "teaser": null
      },{
        "title": "How to mock your browser's timezone with Jasmine and MomentJS",
        "excerpt":"Last week, I’ve been working at adding a distributed countdown to my Online Planning Poker App. As our team works from Paris and Beirut, I wanted to unit test that it would work well through different timezones. I found a surprisingly simple solution.   What Google told me   I first searched Google to see how to do it. I found 2 answers that looked promising :      You can use moment-timezone to mock timezones in Jasmine which I unfortunately did not manage to use   How to mock the browser’s timezone? which seemed a bit of hack   Known results for such a simple situation were disappointing !   What I ended up with   After a good deal of dabbling around, I eventually found a pretty simple solution using Jasmine and Moment Timezone :   jasmine.clock().install(); ... jasmine.clock().mockDate(moment.tz(\"2017-03-23 10:00:00\", \"Europe/Paris\").toDate())   Obviously, the drawback is that it implies setting both the timezone and the time. This should be ok in most of unit tests though, but might be an issue in some cases.  ","categories": ["testing","javascript","jasmine"],
        "tags": [],
        "url": "/how-to-mock-your-browsers-timezone-with-jasmine-and-momentjs/",
        "teaser": null
      },{
        "title": "A seamless way to keep track of technical debt in your source code",
        "excerpt":"I eventually stumbled upon a way to keep track of technical debt in source code that is both straightforward and already built-in most tools : simple TODO comments !      How it happened ?   Some time ago, we tried to add @TechnicalDebt annotations in our source code. Unfortunately, after a few month, we came to the logical conclusion that it was too complex to be effective :      It involved too much ceremony, which frightened people   It made people uneasy to change anything around the annotation instead of sending a call to action   As a result, it was always out of date   After a bit of discussion with my colleagues, we decided to replace all these annotations with simple TODO comments.   When the refactoring to do seems fairly obvious (but also premature) we’ll use a straightforward //TODO (example) introduce a factory message. Next time a pairs gets to work on this part of the code, they get the silent opinion of their peers to help them decide what to do about this piece of the code. Other times, the code might be smelly, yet without us knowing what to do about it yet, in this case, we agreed to use //TODO SMELL (example) responsibilities are not clear in this class which is still a TODO comment, but not a clear call to action.   When I started my current side project, I naturally started to use them. They display nicely in CodeClimate.   The pros      The great thing about TODO comments is that, as a very old programming trick, they are already supported out of the box by most tools IntelliJ, SonarQube, Rails, CodeClimate and I guess many others. Only one day after I refactored to TODO comments, a team mate fixed one that had appeared in his IDE’s TODO tab !   The cons   Some tools, IDEs in particular, tend to assume that you should fix all your TODOs before you commit anything. That’s not exactly how we are using them to track lasting technical debt. So that’s one thing you need to keep in mind.   Tools like Sonar on the other hand, assign a fixed remediation cost to any TODO you have in the code, which is usually not the case at all !   How to set it up in your project   As you might guess, this is pretty easy. Just start adding TODO comments in your code …   Teamwise   It is worth first validating the practice with your colleagues though. There are many ways to do that, depending on your team’s work habits :      Use your team Slack (or whatever chat room you use) to share a link to this post (for example) and create a yes/no poll   Or if you think you need it, create some wiki page explaining the practice and detailing its rationals in your context, add a yes/no poll, and finally share this page with your team   Eventually, if you think that this topic deserves it, setup a meeting with everyone and discuss the point. It might be worth sharing information about the practice beforehand to make the meeting more efficient. You can end the vote with a thumb vote (up : yes, down : no, side : whatever)              Photo from Plays in Business        Don’t wait for unanimity to start the practice, majority is enough ! Make sure that people who voted the other way will follow the team practice in the end though. Remember that whatever the answer, discussing team practices is good.   Once all the team agreed on using (or not) TODO comments, mention the practice in your team’s coding conventions or working agreements (which I strongly recommend to have written somewhere). If you don’t have any yet, create some !   Toolswise   Most tools will handle TODO out of the box.      Rails comes with a rake notes task to list TODO comments.   CodeClimate and SonarQube both lists TODOs as issues in their default config   Most IDEs have a ‘TODO’ tab which will display the TODO comments in the project   Otherwise, good old grep will very happily find TODO comments in your code   Some tools might require small tweaks to improve the experience :      In IntelliJ, in the commit window, uncheck the ‘Check TODO’ checkbox to avoid getting a warning at every commit         SonarQube uses the same fixed remediation cost for every TODO comment. It’s up to you to adapt this remediation cost to your context.   What’s next ?   TODO comments are a good starting point to track technical debt. Once you start using them, there are a few things you can do :   First, remember to fix some regularly. Very old TODO comments are technical debt of their own ! Using code quality dashboards like SonarQube or CodeClimate help to continuously improve your code.   If your tools allow it, you might consider setting up a simpler //SMELL ... instead of //TODO SMELL ... or whatever other special comment that might be useful in your context.   Finally, there is a lean continuous improvement practice which consists of logging problems as they occur. Doing this could help your team to decide which technical debt hotspots are the most important to fix. When appropriate, link the problems with the TODO comments. After a few weeks of this, walking through all the problems during a retrospective should shed light on what parts of the code are causing the most troubles.   Edit 2017-04-19   Thanks a lot for your comments ! People have suggested a ton of great improvements over my basic setup :      plugins to other tools that also support TODO comments   activating automatic sync between issues in CodeClimate and your issue tracking system   using custom comments markers   adding an ‘X’ to your comment every time you are bothered by the technical debt, tools can configured to assign a higher severity to issues with a lot of ‘X’  ","categories": ["programming","technical debt","lean"],
        "tags": [],
        "url": "/a-seamless-way-to-keep-track-of-technical-debt-in-your-source-code/",
        "teaser": null
      },{
        "title": "How to get your team to do code reviews",
        "excerpt":"As software developers, we very always often get to work in code bases that are not perfect. In this situation we have 3 choices : leave, grumble, or make some changes ! Team wide code reviews are a recognized way to increase the quality of the code.   Unfortunately, installing code reviews as part of the daily work habits of a team can be very challenging. When I joined my team 3 years ago, no one was doing any kind of code reviews. With a small push here and there, I managed to get the team to adhere to a strict 4 eyes principle (full story here).   Here are a few strategies that I have either used or seen that should get your team mates to do code reviews.   Overall principle   Even if you are at the bottom of the org chart, you have far more influence than you would first think. My favorite way of bringing change is to demonstrate a valuable practice :      First, you need to be trustworthy   Then, do the practice you want to introduce   Make sure it is seen as valuable   Be ready to forgo the credits of the introduction of the practice   Keep on until people start to copy what you are doing   As someone famous said     A man may do an immense deal of good, if he does not care who gets the credit    I won’t go in the details about how to be trustworthy, which could be a post of its own. Basically, putting our customers interests first, speaking the truth and avoiding to appear dogmatic can get us a long way already. The Clean Coder is an excellent read on the subject.      Strategies   If you have retrospectives in place   In this case, you already have a place and time dedicated to discussing changes to your working agreements. Expressing your concerns about code quality (or another problem related to code reviews) and suggesting code reviews as a way to fix that problem might get a quick team buy-in.   If you don’t manage to get a definitive buy-in, try to get the team to ‘beta-test’ code reviews for a while. If the experiment demonstrates value, it will convert into a full fledged working agreement.   If you practice collective code ownership   Unfortunately, if you don’t have retrospectives in place, or if you did not manage to get your team to discuss code reviews in retrospectives, yo’ll need to find another way to introduce them.   If you have collective code ownership, it should be ok to comment on your team mates code (if not, jump directly to the next strategy). In this setting, just start to do some code reviews for others ! Make sure your reviews are helpful and ‘nice’.   You’ll need to stick to doing code reviews long enough before people actually start to mimic you. Reserve some time in your daily agenda for code reviews. Your goal is to win over people, so it might be a good idea to start with a selected few at the beginning, preferably people who are more likely to jump in. If asynchronous (tool based) reviews don’t get answered, be ready to fallback to face to face discussions : review on your own, then just ask the author for a few minutes so that you can discuss his change. When you feel someone is interested by your reviews, ask him to review your own code in return.   Remember to always try to get some feedback : ask people what they think of the exercise, keep note of the good points, and adapt to smooth out the rest.              Photo from emotuit        Once you won over your first team mate, involve him in your grand plan to spread the practice, explaining how much you think this could make a difference. As more and more people get convinced, the practice will eventually tacitly become part of your working conventions.   Depending on your context, this might take more or less time. I said it was possible, I never said it would be easy ! Grit, patience and adaptation are key here.   Otherwise   This is the worst starting point, basically, you have nothing yet. The strategy is very similar to the one with collective code ownership, with a different first move.   Instead of providing code reviews to your team mates, start by walking over to them to ask for a face to face code review of your own commits. Use the same tactic as stated before : stick to the same people at first. Once the practice starts to stick within this group, bring in a basic tool to ease up the process.   At some point, you should be asked to review others code, that’s a good sign ! If not, try again with other people.   Continue using the same strategy as with collective code ownership and you should eventually get there !   When it does not seem to stick   There could be many reason why the practice is not adopted. The key for you is to understand why and to adapt your strategy. The reason is often that the perceived value is not big enough, for example :      the team is not aware of its problems that reviews would fix : try to make them more visible   reviews are seen as too expensive or painful : try better tools or taking more on yourself   the team has bigger problems to fix first : spend your energy on these first !   reviews just don’t work in your context (ex: your job is to write one time, throw away code) : it’s up to you to stay or leave !   Tools   There are a ton of tools and best practices to run code reviews. It’s important that you know them, so that you know where you are going.   Don’t expect to use the best tools from the start though. At the beginning, your goal is to win over your team mates. In this context, only 2 things matter :      It should have almost no adoption curve, so that others start using it   It should have almost no maintenance cost, as you don’t want to spend your time doing that   That’s why at the beginning, low tech tools are so great. Spending a month setting up a top notch code review system before the first review won’t work. If your VCS has code reviews built-in, by all means use it ! Otherwise, diff in mails and face to face conversations are a good starting point. You’ll later hook something in your VCS to automatically send mails with commit diffs …   As people gradually get convinced of the value of code reviews, regularly meet and discuss a better setup. This is how you’ll introduce state of the art tools and agree on refinements such as pre or post commit reviews.   Best practices   As a code review champion, it’s very important that you provide great reviews to your team mates. You must become the local expert on the subject ! You don’t want all your efforts to be ruined because one of your reviews has been perceived as aggressive.      There is a ton of resources on the internet about how to perform good code reviews, here are a few :      Maria Khalusova’s talk at Devoxx 2016 How to stop wasting your time and start performing useful code reviews   Tim Pettersen from Atlassian talked about Code Reviews vs Pull Request at JavaOne 2016   Marco Troisi wrote an extensive blog post about How to run code reviews in your dev team’s workflow   Joel Kemp’s post Giving better code reviews   Gareth Wilson’s post Effective Code Reviews – 9 Tips from a Converted Skeptic   What’s next ?   Congratulations ! Your team will start to reap the benefits of code reviews. Keep on improving the practice !   To end the story, after a few months of code reviews, during a retrospective, my team (at work) decided to take it one step further and started to do almost full time pair programming ;-)  ","categories": ["code reviews","best practices","programming","continuous improvement"],
        "tags": [],
        "url": "/how-to-get-your-team-to-do-code-reviews/",
        "teaser": null
      },{
        "title": "Incremental architecture, a cure against architecture astronauts",
        "excerpt":"Back in 2001, when I started to code for a living, fresh out of school, I was mainly doing a form of cowboy coding. After a few months of maintaining my own mess, I started to recall my university lessons : we should be doing design before coding …   When I was asked to re-engineer the ‘wizards UI’, I paused my coding to design something clean from scratch. It worked quite well at first : the overall code was a lot simpler and contained a lot less duplication than before. Seeing this new shiny UI, product people asked for new features. Unfortunately, I hadn’t thought of them when designing this little framework. I was almost back at my initial situation.   That’s how I started to look for another way to design software. At about the same time the eXtreme Programming book fell into my hands. That’s where I discovered the idea of incremental design and architecture.      What is Incremental Archi   Let’s start with the antithesis of incremental architecture :   Astronaut Architecture   The term “Architecture Astronaut” was coined by Joel Spolsky back in 2001. If you haven’t read this classic post yet, I strongly encourage you to do so. Basically, he explains that we should not be impressed by architects going over their heads talking about too abstract stuff.   Incremental is the exact opposite of astronaut architecture   Two Schools to Software Architecture   Traditional architecture is about taking up-front choices that will be difficult to change. Incremental architecture is about preparing for non-stop change and taking decisions as late as possible.   The idea in incremental architecture is really simple : keep your code simple, clean and automatically tested in order to be able to simply adapt your code and architecture when definitely needed.   Pros and Cons of incremental architecture   The first reaction of most software engineers (me included, remember how my story started) is that it can only work on trivial stuff. After practicing it for about a decade, I am now convinced it works most of the time. I’m not alone, James Shore (who wrote the more on the subject) also shares my view:      Common thought is that distributed processing, persistence, internationalization, security, and transaction structure are so complex that you must consider them from the start of your project. I disagree; I’ve dealt with all of them incrementally.       Two issues that remain difficult to change are choice of programming language and platform. I wouldn’t want to make those decisions incrementally!    I would add published APIs to this list.   Granted, there are situations that incremental architecture alone cannot handle, what about its good points then ?   In all the other cases (and that means most of the time), here is what you get :      As you won’t need to deal with future use cases, you’ll do less work   That in turn, will keep your code simpler, decreasing time to release new features   As change is built-in, you’ll be able to improve your architecture in ways you could not have imagined from the start !      If you cannot see how this could possibly work ? Read on !   How to do it   eXtreme Programming   As I said earlier, incremental architecture emerged from eXtreme Programming. It won’t come as a surprise that in order to work well incremental architecture requires the XP practices to be in place. In particular, the code base should be automatically tested, the continuous integration cycle should take less than 10 minutes, the design should be simple. The team should be good at doing refactoring.   Don’t expect to be able to do incremental architecture without these practices in place. But this alone might be enough already !      Architecture Vision   At work, where our team consists of 9 developers, it’s not always that simple to coordinate and all pull in the same direction. That’s why we find it useful to share a very long term architecture vision (Enabling Incremental Design and Evolutionary Architecture). This will help people to make coherent decisions when hesitating between 2 alternate designs.   The vision can be the result of the work of a pair, or a mob brainstorming or whatever. Building this vision is typically an activity where experienced programmers can contribute a lot of value.   Once this vision is shared and understood by the team, every time a pair has to work on a story, they can orient the design towards it. But always as little as possible to finish the work at hand, remember the XP motos KISS (Keep It Simple &amp; Stupid) &amp; YAGNI (You Ain’t Gonna Need It.   One final word … a vision is just that : a vision ! It might turn out true, or false, be ready to change it as circumstances change.   Spikes   At times, even with a story in your hands and a long term architecture sketch on the whiteboard, you might have difficulties to know how to change your design to fulfill both.   As always in XP, in case of uncertainty, use Spikes ! Spikes are short time-boxed experiments of throwaway code, which goal is to answer a specific design question.   How to mitigate the risks   What about these topics that don’t yield to incremental architecture ? What if you discover late that you need to change your platform ? Or your API ?   Obviously, you should think about these questions up-front. Hopefully, there are usually not that difficult to answer. But, over time, Non-Functional-Requirements and technologies change. Large and long living systems are particularly likely to need to change to a new platform someday.   Unix had the answer : build your system out of small tools, that do only one thing well, and that communicate through a standard protocol. Systems built that way can be re-written one piece at a time.              Ken Thompson and Dennis Ritchie, the creators of Unix. Photo from WikiMedia        The modern version of this is the micro-services architecture. Incremental architecture allows you to start with a monolith, split it when you need to, and replace micro-services as needed.   With the safety of simple code and a great automated test harness. Interestingly, successful software systems that were architectured up-front also take this road … without the safety !   The Architect   Good news : no more PowerPoints and a lot more coding with the team ! Here is what’s expected from an incremental architect :      To code with the team. As Bertrand Meyer once said “Bubbles (aka. diagrams) don’t crash”, it’s plain too easy, and wrong, to mandate architecture without living with the consequences   To come up with more ideas when drafting the long term vision   To keep an eye on the ‘long term’ while being the navigator in pair programming   In the second edition of the XP book Kent Beck suggests that the architect should write large scale tests to stress the system and demonstrate architecture issues to the team   To delegate as much as possible to the team. However smart the architect, the team as a whole is smarter ! Delegating architecture increases motivation and the quality of the outcome.   End of the story   I’ve been practicing incremental architecture and design for a long time now. It made my life a lot simpler ! Most architecture questions become backlog items to prioritize.   One last advice : be prepared to re-read Joel Spolsky’s article whenever you get caught up in architecture meetings …  ","categories": ["architecture","software","agile","extreme programming"],
        "tags": [],
        "url": "/incremental-architecture-a-cure-against-architecture-astronauts/",
        "teaser": null
      },{
        "title": "A Straightforward Way to Scale to More Than 1 Scrum Team",
        "excerpt":"How come the “agile scaling” landscape seems so daunting and bloated ?   Being agile should be about taking baby steps, doing things incrementally, starting with the simplest thing that could possibly work, slowly but continuously improving and trying out experiments.   Here’s a real-life 2 hours change that kicks-off a very efficient organization for a few scrum teams.              Safeless: everything will go fine as long as you follow the principles        A classic story   Without doing anything about it, there are a lot of situations in where Scrum teams will collaborate inefficiently. For example if you are a developer within a medium or large company, suffering from communication loss between your team and others. Or you could be the owner of a software startup, needing to dramatically grow your dev team.   Whatever your situation, without good collaboration, the output of two Scrum teams is bound to be a fraction of the sum of their individual outputs.   Our own situation   We are a group of 4 teams that spun out of the R&amp;D division. We are working to build a highly reusable component for the future versions of Murex’s main product.   While in R&amp;D, teams were aligned with technologies. Now shifting to a product focus, this layered team structure makes it hard for us to organize efficiently. To make things even more tricky, the component we are building is something huge by itself, and we really need to find a way to deliver it incrementally.   This tech-oriented, low synchronization organization of team backlog was really underdelivering. It was not rare to see stories jump from one team to another, as dependencies were discovered, taking one full sprint every time. Integrating end to end features often took a few sprints instead of a few hours …   Clearly, we needed to do something.   The simplest things that could possibly work   Hopefully, all teams were following Scrum. We first had a look at scaling frameworks, especially since other parts of the company are moving to SAFe but we found that they required too much budget and re-organization for our means (remember, we are just a few motivated developers).   While we were discussing what to do, someone had an idea that was simple, cheap and easy : “Let’s start by doing all our scrum ceremonies together”.   We took the opportunity to add a ‘product’ retro after the teams retros, and a ‘product’ planning just before the teams plannings. We were hoping that regular higher level retros would bring continuous improvement at the product scale and bring in all the other practices required to make it work.   The situation now   It’s now been 2 sprints that we have set this up. Every 2 weeks, Friday is what we call Demo-Day. It’s a meeting heavy day where all the teams have their scrum ceremonies together      Product demo   Team retros   Product retro   Product planning   Team planning   Sure as hell, 1 day of meetings is exhausting, but unfortunately, it is not possible to efficiently grow a team without increasing the communication overhead. The good side of the coin is that it allows the teams to focus on building valuable stuff during the 9 other days.   Here are the first effects we have seen.   Improved team spirit &amp; product focus   The first noticeable effect was on team spirit. Having a demo and planning for the whole team showed to everyone that we are all pulling towards the same goal. It helped everyone to understand what his current role is in this greater scheme, but also ways to tweak our individual roles to bring more value.   Visible problems   The second good effect is that problems are now visible. If teams are not working in the same direction, it’s visible at the demo and plannings. If a team delivers a story too late for another one to integrated it, it’s again visible at demo and planning.   Another example : after the product planning, Product Managers had to admit that they had not managed to feed high value stories to all teams. They asked to do a pre-planning preparation meeting (which is a standard practice in LeSS for example)   Better continuous improvement   During our first team retro in this setting, we directly stumbled upon on a recurring systemic issue that we never managed to do anything about. Instead of going around in circle on the topic again, we pushed it to the product retro. We worked on another team related problem, for which we scheduled improvement actions.   During the product retro, we raised our recurring unsolved issue. With everyone in place, people higher in the organization had the chance to understand its consequences. Eventually, we managed to come up with concrete actions.   How to do it   Pre-requisites   There’s only one thing required : that the teams are already following Scrum. All the rest is easy stuff.   This technique is a bottom-up agile adoption. If you want to switch your whole company from waterfall to something more agile, take a look at SAFe.   Organizing the Demo-Day   Here is our planning for our demo day                  Time       Meeting       Who       Details                       10h - 12h       Demo       Everyone       30m per team                 12h30 - 14h       Team Retro       Teams       Every team has a different retro in a different room                 14h30 - 15h30       Product Retro       Scrum master, volunteers, and decision makers       Decision makers are important to be able to take actions                 16h - 17h       Product Planning       Product managers or owners, volunteers       Product people present what they would like to see in the product in one sprint                 17h - 18h       Team Planning       Teams       Every team has a different planning in a different room           We had to negotiate a bit with other people in the company to get enough rooms for all these meetings at the same time, but all in all, it did not take more than 2 hours.   All meetings are open to anyone, everyone should have the right to come to any of them !   Obviously, that’s a long day ! It’s also full of team energy. In 2 sprints, we found the following improvements :   Food   We pre-order food to be delivered at work, so that we can all eat together. Did you know that food makes retros more efficient ?   Product Retro   You might have noticed that the product retro is only 1 hour long. To make it all fit in one day, we had to cut some time here and there … In order to gain some time on the product retro, we decided to pre-fill its ‘gather data’ phase.   During team retros, if people find product scale issues, they can directly save them for the product retro items (As we are distributed, we use Trello for retrospectives). This way, when the product retro starts, the gather data phase is almost done, pre-filled with genuine points.   Keep the energy high   One full day of meetings is long. People get tired. In order to keep the energy high and the overall experience fun, we deliberately added some fun throughout the day :      Energizers at beginning of meetings   Board games during breaks   End of day celebration outside the office. You could go out and have a drink all together for exemple.              We had fun playing this game between the meetings        Preparing the product Backlog   Depending on your situation, you might (or not) have a prioritized product backlog. My advice is to start with what you have.   If you are scaling your single team to 2, keep a unique backlog for both teams and create 2 feature teams. You’ll be heading to the LeSS organization, which you’ll be able to refer to.   If you are applying this on existing scrum teams with their own backlogs, it might be easier to create a product backlog for the product managers. This is more akin to what SAFe suggests. It’s not as straightforward as having a single backlog for everyone, you’ll need to add some links between team and product stories to be able to track progress. Nevertheless, it provides visibility to everyone. As a side note, this is what we actually did.   Keep in mind that it’s only a starting point anyway, product retrospectives might transform all this down the road any way.   Start where you are now !   The most important thing to do is to start ! There are always a ton of reasons why things are not ready and need more preparation. Remember the agile principles : integrate early and often, adapt, experiment … Here are few examples of bad reasons  not to start      You cannot get all the good people in the room : start, invite them anyway, and send them a report of what has been decided without them. I cannot promise that’s it’s going to work at the end, but at least, it will create some discussions   The product backlog is not ready : start, and see what happens ! It will make it clear to everyone that product backlog is super important. People will organize to provide enough product features next sprint.   You need some other regular meetings for X, Y or Z. For example, Scrum of Scrum has daily SoS Meetings. Start anyway, if there’s a need, people will ask for these extra meetings, which will save you some goodwill. You might also discover that you don’t need these meetings at all and save some time !   With a product retrospective every sprint, important issues will get addressed !      Does ‘Agile Scaling’ even exist ?   We did this on 4 teams. I have no ideas how it would work with more teams. We did not use any framework, even though we took ideas here and there. What we really did is to follow the agile principles, for example :      KISS   Do the simplest thing that could work   Baby steps   Continuous improvement   Experiment   Others have noticed similar things :      LeSS : “Truly scaled scrum is Scrum scaled.”   In the Nature of Software Ron Jeffries explains that for a company is agile if it just focuses on delivering software “feature by feature”   In A Practical Approach to Large-Scale Agile Development […], the authors explain that they did not use any frameworks at HP, but just followed the agile principles   Agile scaling frameworks might be a good starting point (the Shu in Shu-Ha-Ri), but only as long as the agile principles and values have not been understood by the organization.  ","categories": ["agile","scrum"],
        "tags": [],
        "url": "/a-straightforward-way-to-scale-to-more-than-1-scrum-team/",
        "teaser": null
      },{
        "title": "Most Scrum teams are not agile",
        "excerpt":"Being agile is about adapting to change and continuously improving. I’ve seen (and been) in too many teams blindly following Scrum (Scrum Zombies) without figuring out how to continuous improve. The most obvious symptom is a boring (or no) retrospective.   Usually, it did not start that way, people had good intentions, and tried to follow the Scrum cookbook. Unfortunately, without any guidance or extreme perseverance, it is plain too easy to mess up the retrospective. If no concrete actions are scheduled at the end of the retro, if the same problems keep coming up at every retro, or if no problems at all are raised during the retro : your Scrum is somehow broken !   One of the promises of Scrum is to keep code quality high, in order to be able to adapt to late changes. Without a good retro to update the coding standards, the working agreements, or to spot and organize large scale refactoring, this will not happen.   One of the agile principle explicitly states the need for motivated team members. In Drive Daniel Pink explains that one of the ingredient for motivation is autonomy. As a way to fix recurring problems, effective retrospectives will create autonomy, and enable motivation. Dysfunctional retros will slowly kill both …   Although continuous improvement at all levels is one of the most important element in your process, it needs a bit of practice to setup right. After trying different approach in different teams, I compiled a list of the things that worked for me :      How to do successful retrospectives   Start !   Sounds obvious ! The first step is to schedule a recurring meeting and invite all the team. Someone should take care of facilitating. If you want to improve things, do this yourself at the beginning ! Once the practice is in place and everyone sees the benefits, you’ll be able to get some help.   Here is a crash-course about how to organize and drive your first retrospective. If your team is distributed, you’ll need a slightly different setup. In my previous team, we had success using Trello, you can read more about it here.      Make sure everyone talks   Retros should not be “just-another-boring-meeting” ! People should be excited to be there and to solve their recurring problems. I’ve seen retros where no one would speak, as if there was absolutely nothing to improve ! I’ve also been in retros where people raised collaboration issues, and came up with drastic improvements to the way they work, like adopting pair-programming for example.   There can be many reason why people don’t speak : habit, organizational culture … Whatever the cause, if you show them the path, if you demonstrate that retro work and that it is safe to speak, you’ll get them to participate.   Here are a few tricks to guide a team there :      It is said that if someone does not speak in the first 5 minutes of a meeting, he’ll remain silent for the rest. That’s why energizers work. They force everyone to participate to a Fun activity right from the start. Fun Retrospective contains a lot of engaging energizers.   Bring food ! Having food at work was one of Kent Beck’s main advises in eXtreme Programming, Embrace Change. Food is social and create a more relaxed and safer atmosphere.   Before you start the meeting, it might be useful to repeat the Retrospective’s Prime Directive.      Regardless of what we discover, we understand and truly believe that everyone did the best job they could, given what they knew at the time, their skills and abilities, the resources available, and the situation at hand.       As the animator, you should make sure the discussions don’t degenerate into some kind of trolling or blaming. If it’s the case, remind that this is not the way the retro is meant to work. In worst case, give a 5 minutes break to everyone to calm down.   If needed, have a private discussion with people outside the retro to make sure that they understand the “Blame the process, not the people” principle of the retro well enough.   Dealing with “bad” behavior in retros is a wide topic which I am only scratching the surface here. I guess I could write a full post about it.      Get out of with actionable tasks   If you want a sure way to screw up your retros here it is : end the first one with no actionable things to do ! People will learn that it’s a useless pscho-blabla meeting for agile softies.   This should be the absolute priority during the first retros. There are various kind of actionable tasks. Teams can adopt new conventions, decide to tackle some specific refactoring, or build a small new tool …   Actionable items should be small enough to be completed in one sprint. This is fine for refactoring a class, adding a SonarQube rule or asking for something from another team. What about larger tasks ? Suppose you just identified a large refactoring to do, how do you get started ? I know 2 strategies for that :      Only identify the first step of what you want to start. At least, that’s enough to get started and learn what to do later.   Agree on a Mikado discovery task to understand what needs to be done. Code is not the only thing that can be refactored with the Mikado method ! People have used it to transform organizations !   It’s also a good habit to book the rooms for extra time after the official retro time. Nothing is as frustrating as being interrupted while investigating a promising improvement !   Finally, I think it’s a good practice to review what happened of the improvements that were selected during last retro. It stresses the importance of doing what was scheduled in retrospective. It also gives a chance to investigate the reason why they were not done !      Put them in the sprint   When actions are selected in the retro, you should add them to the coming sprint before it starts. Obviously, you’ll add non business related items in your sprint backlog. Whatever you might have hear from agile zealots, do it ! Process improvements are fist class backlog citizens, just don’t discuss them with your product owner.   If you want a chance to deliver what you committed to in your sprint backlog, you’ll need bandwidth for that ! They are many ways to do get that. Some teams use slack time, others reserve an ‘improvement day’ every sprint. My personal favorite (and the one we’ve been doing at work) is to estimate the improvements in story points and reserve a fraction (ex 20%) of your story points for improvements.   You can try to negotiate your improvement ratio with your product owner … or you might also just set it yourself ! The team is responsible for the quality of its work. Better be sorry than ineffective. If you stick to your ratio and only work on the most important improvements, it’s usually pretty easy to justify working on them. Plus if you manage to go under the radar for a few sprints, the results of the improvements should speak by themselves.   Do them as soon as the sprint starts   One last thing. Start to work on your improvements as soon as the sprint starts.   Improvements are similar to investments : you invest in process, tools or code in exchange for more value creation downstream. Once you’ve decided to invest in something, what’s the point of waiting 1 week ? Plus if you wait for later in the sprint, you run the risk of having unfinished improvements at the next retro, which might interfere when prioritizing new ones.   Continuously improving teams   Being part of a continuously improving team is easier to live than to describe. It feels like the future is bright. Once a team has mastered continuous improvement, people can be optimistic that they’ll manage to adapt to change later down the road :      They could switch gears and go full speed on a particular re-architecture   They could change their working agreements to adapt to new constraints   They could re-focus their efforts to handle a maintenance pike   At the end of the day, bottom-up continuous improvement makes everyone’s lives easier. Developers experience more autonomy while improving their productivity, they can do good work (which also means spending less time bug-fixing). Product owners learn that they can trust the team to do their best. Sponsors get more value for their money.      Agile teams bend so they don’t break !    ","categories": ["continuous improvement","agile","scrum","retrospectives","mikado-method"],
        "tags": [],
        "url": "/most-scrum-teams-are-not-agile/",
        "teaser": null
      },{
        "title": "From apprentice to master, how to learn TDD (Test Driven Development)",
        "excerpt":"I started to learn Test Driven Development a long time ago. Since then, even though I’m still not a master, it’s been my most useful programming skill, by far. TDD is the first trick every new programmer should learn. It made my whole career as a developer easier.   I’ve been working as a professional software engineer for more than 15 years, including around 10 years in the trading industry. Only once did I need to work on week ends or late into the night to fix emergency issues. I believe I largely owe this work-life balance to the high test coverage that TDD ensures by design.   TDD also enables safe refactoring. Refactoring enables incremental design which lets you decide late. Deciding late is how you make your customers happy by accepting late changes. Deciding late is how take up-front design easy, and improve your code as you go. Deciding late is how you build trustful and harmonious relationships with the stakeholders of your project.   Finally, writing tests before the code is both easier and more fun. Among compilation errors and never ending customer demands, the recurring green barre tastes like pure candy to the poor programmer !              From Pinterest winding road catalog        The TDD Road   Here are the steps I took to learn TDD.   Learn the principles   The principles of Test Driven Development are fairly basic. 5 minutes is enough to know them forever :      Write a failing test   Make it pass by doing the simplest thing possible   Refactor your code while keeping the tests passing   Repeat until your acceptance test is green   TDD newbies might ask a few questions like :      Is making the code compile in step 1 or 2 ? Honestly, that does not matter. Write the test as if you had the code, then make it compile, make sure it fails, and continue.   How much should I refactor ? It often takes a good amount of experimentation to find your good balance. If you don’t refactor enough, you going to drown in dirty code. If you refactor too much, you’re going to waste your time (YAGNI). As a rule of thumb, Kent Beck’s rules on simple design are a very good starting point :              From Martin Fowler : Beck Design Rules        Find your tools   Back in 2003, after reading Martin Fowler’s Refactoring: Improving the Design of Existing Code book, I decided to give TDD a serious try. I was using C++ at work but we did not even have a nightly build. I had to download and setup CppUnit on my own. As solo endeavor, this was a required step to get into TDD.   Find and setup a tool. Which one should be rather obvious, if your team already uses a unit test framework, stick to it, otherwise, pick the most standard for your language, ex : JUnit for Java (you might as well start searching for a new job where they use a unit testing tool).   Train at a TDD coding dojo   Coding dojos were first invented as a tool to learn TDD. They were started by two french eXtreme Programmers Emmanuel Gaillot and Laurent Bossavit. It happens that I live in Paris, and that Emmanuel came to work as a contractor at the same company I was. I learned that he was organizing a weekly coding dojo open to any developer. That’s how I started attending coding dojos. Looking back, the TDD skills I learned at the coding dojo are just enormous !   If you are serious about learning TDD, attend coding dojos. Whether you find one in your city (ask meetup.com) or in your company does not really matter. If you cannot find any, just start one at work ! Send a public call for interested people (use wiki, mail, posters or whatever), book a room and start hacking ! Emily Bache’s Coding Dojo Handbook seems a very good reference about how to start a coding dojo (Disclaimer : I did not read the book, I only know it by reputation.)      Use it   If you’re a professional programmer, you must be spending most of your time writing code. Take this opportunity to use TDD. You’ll be slowed down a bit at the beginning, but as both your code and your skills improve, you’ll get your time back manyfold.   I remember one of my first professional use of TDD was to write a small parser. A few weeks later I was asked to make it faster. Having tests around it made it easy to optimize it without breaking it.   Deliberate practice   In The first 20 hours, Josh Kaufman explains that deliberately practicing specific topics is a more time effective way of learning than simply crunching hours of practice.   My friend Thomas Pierrain is used to regularly practice short 30 minutes katas to sharpen his programming skills. Katas can be found at codingdojo.org, at cyber-dojo.org or in The Coding Dojo Handbook.   Read   Meanwhile I was doing all this, I also spent a lot of time reading on the subject. At the time, there was a lot of things about TDD in the C2 Wiki, so I spent a lot of time there. Since then, I stumbled upon a few books that helped me to understand some aspects of TDD :      Kent Beck’s classic TDD by example remains a great reference for beginners   I got very useful advices from TDD, a practical guide especially the section about UI testing, which really saved me at some point   Finally, Growing Object Oriented Software Guided By Tests is now a classic about what is called “The London school of testing”, which uses mocks as a design tool. Definitely a must read.      Practice advanced topics   The TDD road goes on forever, testing has a lot of tricky topics. Once you’ve mastered the basics, it’s quite interesting to explore and practice more complex subjects. Here are a few things worth trying out :      Use automated IDE refactorings to keep the code compiling all the way through a kata   Do the same kata twice, using top-down and bottom-up styles   Do refactoring katas to learn how to work with legacy code   Do UI katas, to learn how to test the UI   Learn how to deal with DB   Learn how to handle remote services   If needed, invent your own katas to deliberately practice these topics and others.   The pitfalls   As any road worth walking, the TDD path is not linear and smooth. Before fully mastering TDD, you’ll regularly wonder if you’re on the right track. As any practice, TDD has pitfalls. I’ve fell into some quite a few times. I guess that’s part of the learning process.   Emerging Design   There’s always been a lot of misunderstanding around this topic.   Bad smells in your code make your tests harder to write. If adding a test is painful, that’s an indication that something could be improved in your design. It’s then up to you to know what. Once you’ve figured out what you want to change, use your existing tests to refactor your design first. Only then, add this new test.   As you can see, the tests will give you more feedback about the design of your code, but they won’t tell you what to do. That’s where your coding skills will help. This is particularly true about algorithms. Don’t expect a good algorithm to magically appear as you do the simplest-thing-that-could-possibly-work …   As with any rules, there are exceptions. At times, you’ll walk into a problem which emerging design is great. For example, I’ve done the Arab to Romans kata many times, and that repeatedly doing the simplest-thing-that-could-possibly-work yields a good solution.   Mocks   Mocks are useful at times, but be careful not to overuse them. I already wrote about mocks. Too much mocking makes your test suite brittle, ineffective and difficult to maintain ! I’ve been bitten hard by this one on a personal side project, the day I decided to get rid of all the mocks, I shrank the test code size by 25%. Since then I learned about alternate techniques such as :      Test data builders which allow to easily build test data   Value objects that are immutable and don’t need mocking   test spies and proxies as a way to mock and track calls while still calling the real code   in-memory fake implementations which make the tests independent and fast without unmaintainable proliferation of mock setup   As an indicator, less than 5% of my tests use mocks.   Metrics   In his keynote at RailsConf 2014 DHH explains the danger of testing metrics.      By using TDD correctly, you should get a high code coverage as a side effect. If you get below 80%, you must be doing something wrong. The other metric you should keep an eye on is the total build time. Original eXtreme Programming had the 10 minutes build rule which states that if your build + tests takes more than 10 minutes, you should refactor it.   That’s it ! Things like 100% test coverage or test ratio are complete nonsense.   Pushing it even further   My promise, at the beginning of this post, was that Test Driven Development would make your life as a developer easier. Now let’s imagine that your whole team or company was using TDD. It’s a foundation on which to build a sustainable agile organization.   The mastery of automated testing at organization scale is a key element to continuous delivery, making releasing software a non-event, and as stress-free as possible.   Full adoption of TDD can yield to incremental architecture which delivers features faster, by skipping the conflictual arguments about supposed future needs.   Finally, TDD can simplify your processes and tooling. Team-wide TDD results in a steady flow of features on top of which it is easier to create simple and effective processes and tools.   The road is long, sinuous and at times rocky, but so are the rewards.  ","categories": ["tdd","software","testing","learning"],
        "tags": [],
        "url": "/from-apprentice-to-master-how-to-learn-tdd-test-driven-development/",
        "teaser": null
      },{
        "title": "20 Bad Excuses For Not Writing Unit Tests",
        "excerpt":"   I guess we always find excuses to keep on with our bad habits, don’t we? Stephen King       I don’t have the time. But you’ll have the time to fix the bugs …   I don’t know how to write tests. No problem, anyone can learn.   I’m sure the code is working now. The competent programmer is fully aware of the limited size of his own skull …   This code is not testable. Learn or refactor.   It’s UI (works with DB as well) code, we don’t test it. Because it never crashes?   Because I need to refactor first … and I need tests to refactor! Damn, you’ve fallen into the test deadlock!   It’s multithreaded code, it’s impossible to test. Because it’s fully tederministic?   The QA department is already testing the code. Is that working well?   I should not test my own code, I’ll be biased. Start testing other people’s code right now then!   I’m a programmer, not a tester. Professional programmers write tests.              From todayiwillbefit.com           I’m using a REPL, it replaces unit tests. Sure, and you’re running your REPL buffers on the CI? and keeping your them for the next time someone modifies your code.   My type system is strong enough to replace tests. Does it detect when you use ‘+’ instead of ‘*’?   We don’t have the tooling to write unit tests. Get one.   Tests aren’t run automatically anyway. Install a Continuous Integration Server.   I’m domain expert developer, writing tests is not my job. Creating bugs isn’t either!   We’d rather switch to the Blub language first! You’re right, let’s do neither then!   We don’t test legacy code. Specifically because it is legacy code.   Adding tests for every production code we write is insane! As shipping untested code is unprofessional.   I find more issues doing manual testing. Exploratory Testing is a valuable testing, even more so on top of automated tests.   Because my teammates don’t run them. Time for a retrospective.     ","categories": ["joke","testing","software"],
        "tags": [],
        "url": "/20-bad-excuses-for-not-writing-unit-tests/",
        "teaser": null
      },{
        "title": "Speed up the TDD feedback loop with better assertion messages",
        "excerpt":"There is a rather widespread TDD practice to have a single assertion per test. The goal is to have faster feedback loop while coding. When a test fails, it can be for a single reason, making the diagnostic faster.   The same goes with the test names. When a test fails, a readable test name in the report simplifies the diagnostic. Some testing frameworks allow the use of plain strings as test names. In others, people use underscores instead of CamelCase in test names.      A 4th step in TDD: Fail, Fail better, Pass, Refactor   First, make it fail   Everyone knows that Test Driven Development starts by making the test fail. Let me illustrate why.   A few years ago, I was working on a C# project. We were using TDD and NUnit. At some point, while working on a story, I forgot to make my latest test fail. I wrote some code to pass this test, I ran the tests, and they were green. When I was almost done, I tried to plug all the parts together, but nothing was working. I had to start the debugger to understand what was going wrong. At first, I could not understand why the exact thing I had unit tested earlier was now broken. After more investigation I discovered that I had forgotten to make my test public. NUnit only runs public tests …   If I had made sure my test was failing, I would have spotted straightaway that it was not ran.   Then make it fail … better !   I lived the same kind of story with wrong failures many times. The test fails, but for a bad reason. I move on to implement the code to fix it … but it still does not pass ! Only then do I check the error message and discover the real thing to fix. Again, it’s a transgression to baby steps and to the YAGNI principle. If the tests is small, that might not be too much of an issue. But it can be if the test is big, or if the real fix deprecates all the premature work.   Strive for explicit error message   The idea is to make sure to have good enough error messages before moving on to the “pass” step.      There’s nothing groundbreaking about this practice. It’s not a step as explicit as the other 3 steps of TDD. The first place I read about this idea was in Growing Object Oriented Software Guided By Tests.   How to improve your messages   Readable code   Some test frameworks print out the failed assertion code to the test failure report. Others, especially in dynamic languages, use the assertion code itself to deduce an error message. If your test code is readable enough, your error messages might be as well !   For example, with Ruby RSpec testing framework :   it \"must have an ending\" do   expect(Vote.new(team: @daltons)).to be_valid end   Yield the following error :   expected #&lt;Vote ...&gt; to be valid, but got errors: Ending can't be blank    Pass in a message argument   Sometimes, readable code is not enough to provide good messages. All testing frameworks I know provide some way to pass in a custom error message. That’s often a cheap and straightforward way to clarify your test reports.     it \"should not render anything\" do     post_create     expect(response.code).to eq(HTTP::Status::OK.to_s),                              \"expected the post to succeed, but got http status #{response.code}\"   end   Yields   expected the post to succeed, but got http status 204   Define your own matchers   The drawback with explicit error message is that they harm code readability. If this becomes too much of an issue, one last solution is the use of test matchers. A test matcher is a class encapsulating assertion code. The test framework provides a fluent api to bind a matcher with the actual and expected values. Almost all test framework support some flavor of these. If not, or if you want more, there are libraries that do :      AssertJ is a fluent assertion library for Java. You can easily extend it with your own assertions (ie. matchers)   NFluent is the same thing for .Net.   As an example, in a past side project, I defined an include_all rspec matcher that verifies that many elements are present in a collection. It can be used that way :   expect(items).to include_all([\"Tomatoes\", \"Bananas\", \"Potatoes\"])   It yields error messages like   [\"Bananas\", \"Potatoes\"] are missing   A custom matcher is more work, but it provides both readable code and clean error messages.   Other good points of matchers   Like any of these 3 tactics, matchers provide better error messages. Explicit error messages, in turn, speed up the diagnostic on regression. In the end, faster diagnostic means easier maintenance.   But there’s more awesomness in custom test matchers !   Adaptive error messages   In a custom matcher, you have to write code to generate the error message. This means we can add logic there ! It’s an opportunity to build more detailed error messages.   This can be particularly useful when testing recursive (tree-like) structures. A few years ago, I wrote an rspec matcher library called xpath-specs. It checks html views for the presence of recursive XPath. Instead of printing   Could not find //table[@id=\"grades\"]//span[text()='Joe'] in ...   It will print   Could find //table[@id=\"grades\"] but not //table[@id=\"grades\"]//span[text()='Joe'] in ...   (BTW, I’m still wondering if testing views this way is a good idea …)   Test code reuse   One of the purpose of custom test matchers is to be reusable. That’s a good place to factorize assertion code. It is both more readable and more organized than extracting an assertion method.   Better coverage   I noticed that custom matcher have a psychological effect on test coverage ! A matcher is a place to share assertion code. Adding thorough assertions seems legitimate, contrary to repeating them inline.   Avoids mocking   We often resort to mocks instead of side effect tests because it’s a lot shorter. A custom matcher encapsulates the assertion code. It makes it OK to use a few assertions to test for side effects, which is usually preferable to mocking.   For example, here is a matcher that checks that our remote service API received the correct calls, without doing any mocking.   RSpec::Matchers.define :have_received_order do |cart, credentials|   match do |api|     not api.nil? and     api.login == credentials.email and     api.password == credentials.password and     cart.lines.all? do |cart_line|       cart.content.include?(cart_line.item.remote_id)     end   end    failure_message do |api|     \"expected #{api.inspect} to have received order #{cart.inspect} from #{credentials}\"   end end   Care about error messages   Providing good error messages is a small effort compared to unit testing in general. At the same time, it speeds up the feedback loop, both while coding and during later maintenance. Imagine how easier it would be to analyze and fix regressions if they all had clear error messages !   Spread the word ! Leave comments in code reviews, demo the practice to your pair buddy. Prepare a team coding dojo about custom assertion matchers. Discuss the issue in a retro !     ","categories": ["testing","tdd","software"],
        "tags": [],
        "url": "/speed-up-the-tdd-feedback-loop-with-better-assertion-messages/",
        "teaser": null
      },{
        "title": "Side Projects Matter",
        "excerpt":"As a manager, you could benefit a lot from helping your developers with their side projects.   I finished my latest side project. That’s the fifth serious one I bring to an end :      2010-2014 www.mes-courses.fr, an improved UI for online groceries. This was both a technical project and a wannabe business   Since 2011, this blog   2015 www.agileavatars.com, a custom magnet shop for agile team boards. This was a lean startup style business project   2016 complexity-asserts a unit test matcher to enforce algorithm complexity. This was a technical project time boxed to 20h.   2016-2017 Philou’s Planning Poker, a technical product, that I built to solve my own problem.   The more I do side projects, the more I am certain of their value to my employer.              Designed by Freepik        Reasons companies discourage side projects   Unfortunately, most companies discourage their employees to have side projects. It boils down to fundamental fears :      they might get less done   they might leave   While these are legitimate, most of the time, they are also unlikely or short sighted.   Why don’t they work extra hours ?   Said another way : if developers want to code, why don’t they add new features to the company’s products ?   From my own experience, having a side project has always been an energy booster. Side projects have made me more effective at work !   For a compulsive hacker, a side project is a hobby ! As painting, piano or soccer is to others. Working on smaller software, being in full control, renews the joy of programming.   They’ll quit once they’ve learned new skills !   Simply said, if a company’s retention strategy is to deprecate its developers … It’s got problems a lot worse than a few people doing side projects at night !   They won’t be as productive !   You could think that developers will be less focused on the company’s issues while at work. Indeed, passionate side-project hackers always have it on top of their heads.   Most of the time though, the extra energy provided by the side project out-weights this focus loss.   In the end, we should trust people to be professional. Let’s deal with the problem later, when someone actually starts to underdeliver.   They’ll leave if it turns into a successful product !   Building a product company is pretty damn hard. A time starved side project is pretty unlikely to turn into a successful business. Not much to worry about here ! If it happens, the company is lucky to have had such a productive employee.   They might steal our intellectual property !   This one is true. Only a very small minority of people might do that, but the risk remains.   You might conclude that it’s easier to play it safe and prohibit side projects … at the same time, it’s always sad to punish the majority for a minority’s bad behavior.   It boils down to a tradeoff between risks and rewards. How sensible the company is to IP theft vs the benefits of having a side-project friendly policy.   If you are wondering what these benefits are, read on !   Side projects made me a more valuable employee   As developers, side project teach us a lot. What is less obvious, is how these new skills benefit our employers !   Keep up with technology   A side project is an occasion to work on any subject, with any technology we want. That’s the perfect time to try that latest JS framework we cannot use at work.   This will help us and our companies to transition to these new technologies in the future.   Experimenting different platforms also widens our horizons. It teaches us new ways of addressing problems in our daily stack. For examples, learning LISP macros pushed me to use lambdas to create new control structures in Java, C++ or C#.   The conclusion is that side projects make people more productive and adaptive. Which in turn makes companies more productive and resilient   Understand what technical debt is              Image from Vector Software        The bottleneck in a side project is always time. In this context, to deliver fast enough to keep my motivation high, I tend to take technical debt. Particularly because I ignore how long I’ll be maintaining this code.   Even so, if I later decide to stick to this side project, this technical debt will be an issue.   That’s what technical debt is : a conscious choice to cut a corner and fix it later. Without keeping track of the cut corners, it’s not debt anymore, but crappy code ! That’s why I ended up using #TODO comments in my side projects.   Later down the road, at any moment, I can decide to invest in refactoring some technical debt out. We can apply the exact same principles at our day jobs.   Understanding what a business is   Trying to make money from your side project taught me other kind of lessons. To sell my product or service, I had to learn a ton of other skills than the typical developer has. Nothing will sell without marketing or sales. I also had to dip my toes in design, web content creation and project management.   Once I went through this, I was able to better understand a big picture at work. It became easier to discuss with product, project and sales people. I’m able to make better tradeoffs between engineering, product and technical debt. Non technical colleagues appreciate that. As developers, it increases our value and trustworthiness.   Discover new ways of doing things   While progressing towards my own side project goals, I had to search the internet for help on some tasks. I ended up using SaaS tools, and discovered alternate practices to the ones I was using in my daily job.   That’s great for employers ! Developers will gain perspective about which company processes work well and which don’t. If you have some form of continuous improvement in place at work, they’ll suggest great ideas ! If you don’t, then you should start doing retrospectives now !   Companies should sponsor side projects   I hope I convinced you that side projects are at least as efficient as a formal training. The topics are unknown at the beginning, but that’s the trick : they deal with the unknown unknowns !   There are many ways a company can help its employees with their side projects :      Slack time is a great way to spark the interest in a topic. Developers might start something in their slack time, and continue as a side project. Provided the topic as value for the company, they could continue using their Slack time on it.   Hosting a Startup Weekend or a Hackathon. Most company offices are empty on Saturdays and Sundays. You could ask your company to lend its premises for such an event. It’s very likely that some employees will take part.   Even better, some companies, like Spotify, organize regular Hackathons on office hours ! That’s Slack time, on steroids !   Sponsoring internal communities can enable employees with side-projects to help each other. Sponsorship could be free lunch, premises or a regular small slice of time on work hours.   Providing a clear legal framework around side projects reduces the risks for everyone. Questions like the ownership of intellectual property are better dealt with upfront.      If you are a developer looking for a side project idea, suggest slack time in retrospective ! You could also ask for sponsorship and organize a startup week-end or a lunch time community.   Finally, if your company is side-project friendly, communicate about it ! It’s a great selling point and it will attract great programmers.  ","categories": ["side project","software","management"],
        "tags": [],
        "url": "/side-projects-matter/",
        "teaser": null
      },{
        "title": "13 Tricks for Successful Side Projects",
        "excerpt":"As I said last week, I released the v0.1 of Philou’s Planning Poker, my latest side project. Although I have a day job, a wife, a family and a mortgage to pay, I still manage to finish my side projects. In the past 7 years, I published 5 of these as open source projects, website, or wannabe businesses.   Side projects rely on 2 things : time and motivation. If motivation goes down, you’ll  stop working on it, and it will die. If you don’t manage to find enough time for it, it will also die.   Over the years, I accumulated best practices that increase the chances of success. Here is a shortlist of 13 of these.      1. Know your goal   As I said before, side projects are time constrained. If you try to follow many goals at once, you’ll spread too thin and won’t deliver anything. That will kill your motivation.   To avoid this, you need to decide on a unique goal for your project. It can be anything : learning a new tech, building a tool, sell a simple product, maintain a blog.   Depending on the nature of your goal, your side project can take different forms. 20 hours experiments are great for learning new techs. As a side note, MOOCs can also be very effective for this. If you want to start a business, start a lean startup concierge MVP. Finally, if you already know users who need a tool, build a minimalistic version for them.   2. Time box your work   Time boxing will force you to make the choices that will keep you going forward. The risk is to take on too many topics : more refactoring, more UI polish, more options, more bells and whistles. All these can be very interesting and valuable, but are usually not the main priority.   20 hours programs are time boxes, that’s one of the reasons they work. For other kinds of side projects, I do a quarterly prioritization. “This is what I’d like to have in 3 months”. I often slip a bit, but that’s not a problem as long as I stay focused on my goal.   3. Setup a routine   You’ll need to dedicate time to your side project. Think of what you could do if you worked one hour per day to it. 365 hours per year, or 90 hours per quarter ! That’s 2 full weeks of work !   In the long run, having a routine is more effective than anything else. After a few weeks of sticking to a routine, it will become part of your daily life, and won’t be an effort anymore. It will also help to forecast what you’ll be able to do in the coming month or so.   To setup a routine, block a slot in your day to work on your project, and stick to it. My own routine is waking up early to have some focused time. I have entrepreneur friends who did the same. GrassHopper founder says the same in this Indiehacker podcast.   4. Keep delivering to sustain motivation   Nothing kills motivation as not delivering. At work, I can go on without user feedback for a while (not too long though). Unfortunately, that does not work on a time constrained side project. We have only one life and we don’t want to spend our time on things that don’t matter. Things that don’t deliver don’t matter …   To get the technical aspect of delivery out of the picture once and for all, I use Continuous Delivery. Continuous Delivery is pretty easy to start with on a new project :      automate all your tests   setup a CI server   deploy when the CI passes   Once this is up and running, as long as I split my work in baby steps, I’ll be delivering.      5. Use SasS tools   Setting up a CI and a deployment server can take some time. In 2017 though, online platforms make this very easy. Use as many as you can.   For Philou’s Planning Poker, I save my code on Github, test through Travis CI and deploy to Heroku. I also use Code Climate for static code analysis.   Most of these tools have some free plans for small or open source projects. That alone is a great advantage of making your project open source !   6. Pay for good tools   If you don’t want to make your project open source, consider paying for these services. How much you value your time will tell you whether to buy or not.   There are other things you should pay for as well. I definitely recommend paying for a good laptop and IDE.   Remember, anything that helps you to deliver also helps you to keep your motivation high. You have a day job that earns you money, so use it !   7. Pick a productive language    Depending on your project, you’ll have a choice in which programming language to use.   Paul Graham advices to use dynamic languages. I tend to do the same, especially after watching “The Unreasonable Effectiveness of Dynamic Typing for Practical Programs”.      In the end, I guess it’s a matter of personal preference. Pick the language you’ll be the most productive with.   8. Use a popular platform   Use a popular open source platform to build your side project on. Useless to say, if your goal is to learn X, use X, even if it is not popular !   There are many advantages to using a popular platform :      you’ll have something that has already been production proofed   you’ll suffer less bugs (remember Linus’s Law “Given enough eyeballs, all bugs are shallow”)   you’ll get help from the community   you’ll find compatible libraries to solve some of your problems   The end goal is always the same : sustain your motivation by delivering fast enough !   9. Walk the edge   We don’t start side project to spend time updating dependencies. The saying goes “If it hurts, do it more often”. To save your productivity and motivation, always keep your dependencies up to date.   This is easy with automated test and continuous integration in place. I use no version constraint, but update all dependencies at least every week. I  commit if all tests pass. Sometimes I fall into small 5 to 10 minutes fixes, but that’s all it takes.   10. Take technical debt   When starting a new side project, you have no ideas how long it will last. Could be one week, for example if you started a 20h experiment at the beginning of holidays. Could also be 20 years, if you managed to transform this side project into a full fledged business.   Starting with all the practices that make large software systems manageable will fail. You won’t deliver fast enough. By now, you know the story, if you don’t deliver, you’ll lose your motivation.   I used TODO comments in my latest side project to keep track of the shortcut I took. I found it had 2 main advantages in my situation :      I had a quick view of how much total technical debt I took   if things get more serious, it will be easy to find improvement points   I know that TODO comments are controversial in the software community. In the context of new side projects though, they make a lot of sense.   My advice is to take technical debt !   11. Use your day job   I’m not saying to use time from your day job to work on your side project. That would be like stealing. Your day job can help your side project in many other ways.   One I already mentioned is using your income to buy better tools.   If you have Slack Time at your day job, you could use it to start a side project that benefits your company. You’ll need to make sure that this kind of arrangement does not pose any IP issues. It can result in a win-win situation.   Another way is to find subjects at work which will grow some skills that are also useful for your side project.   12. Talk about it   Talking about your side project serves many purposes :      it’s an unofficial engagement to work on it   it provides feedback   it could attract early users   To summarize, the more you’ll talk about it the more it will become ‘real’. You can share your side project anywhere : blog, Meetups, work, with friends or family. Depending on your topic, some places will work better than others.   Don’t be afraid that one might steal your idea. A side project is small, not yet rocket science. It’s usually too small to be on the radar of serious businesses, and too big for individuals.   Let me explain that. Very few people have the grit to turn their ideas into something real. If you encounter someone who has the grit and the interest, ask her to join forces !   13. Find real users   Deploying your software is nice, but it’s useless until you have users. Find some ! It’s never too early to find testers. If your first demo does not embarrass you, it was too late ! At the beginning, it can be as basic as walking through an unfinished feature to get feedback.   Real user feedback always results in both high motivation and value. There are many places to get beta users : at work, through friends … have the courage to ask !   That’s again a case for building your system in baby steps. The faster you get to something you can show, the faster you can have beta users.   Do it !   If I needed a 14th best practice it would be to start today ! As with most things, just do it !     ","categories": ["side project","software","personal-productivity"],
        "tags": [],
        "url": "/13-tricks-for-successful-side-projects/",
        "teaser": null
      },{
        "title": "Don't stick to TDD's Red-Green-Refactor loop to the letter",
        "excerpt":"As long as you are writing your tests before your code and doing regular refactoring, you are doing TDD !   The Red - Green - Refactor loop is useful to introduce TDD to new developers. Different loops can be more effective in real world situation.   The Red - Green - Refactor loop is not a dogma !      Refactor - Red - Green   When I work on a story, I very often keep a TODO list next to my desk. I use it to keep track of the next steps, the edge cases to test, the code smells and refactorings to do.   When I get to the end of the story, all that remains of this list is a few refactorings. Very often, I don’t do them !   With the feature working, doing these refactorings feels like violation of YAGNI. Next time we’ll have to work on this part of the code, we’ll have a story to serve as guide to which refactorings to do.   The same thing is effective at the unit test scale. It’s easier to refactor when you know the test you want to add. Refactor to make this test easy to write !   Here is an example with Fizz Buzz   static int fizzBuzz(int number) {    return number; }  @Test public void it_is_1_for_1() {    assertThat(fizzBuzz(1)).isEqualTo(1); }  @Test public void it_is_2_for_2() {    assertThat(fizzBuzz(2)).isEqualTo(2); }   Here is the test I’d like to add.    @Test public void it_is_Fizz_for_3() {    assertThat(fizzBuzz(3)).isEqualTo(\"Fizz\"); }   Unfortunately, fizzBuzz needs to return a String instead of an integer for it to compile. That’s when I would refactor before adding the new test.   static String fizzBuzz(int number) {    return Integer.toString(number); }  @Test public void it_is_1_for_1() {    assertThat(fizzBuzz(1)).isEqualTo(\"1\"); }  @Test public void it_is_2_for_2() {    assertThat(fizzBuzz(2)).isEqualTo(\"2\"); }   In the end, this loop is very like the classic TDD loop :   red-green-refactor-red-green-refactor-red-green-refactor............. ..........refactor-red-green-refactor-red-green-refactor-red-green...   A bit more YAGNI, that’s all.   Red - Better Red - Green - Refactor   A few weeks ago, I wrote about error messages in unit tests. To summarize, extra work on error messages reduces the testing feedback loop.   We can translate this focus on error messages into an extra TDD step. Whatever the TDD loop you are using, you can add this step after the Red step.   Red - Green - Refactor - Red - Green   Sometimes, it makes sense to refactor before fixing the test. The idea is to rely on the existing tests to prepare the code to fix the new test in one line.   Let’s take our Fizz Buzz example again. Imagine we finished the kata, when we decide to tweak the rules and try Fizz Buzz Bang. We should now print Bang on multiples of 7.   Here is our starting point :   static String fizzBuzz(int number) {    if (multipleOf(number, 3)) {       return \"Fizz\";    }    if (multipleOf(number, 5)) {       return \"Buzz\";    }    if (multipleOf(number, 3*5)) {       return \"FizzBuzz\";    }       return Integer.toString(number); }  ...  @Test public void it_is_Bang_for_7() {    assertThat(fizzBuzz(7)).isEqualTo(\"Bang\"); }   I could go through all the hoops, 7, 14, then 37, 57 and finally 357 … By now, I should know the music though !   What I would do in this case is :      first to comment the new failing test to get back to green   refactor the code to prepare for the new code   uncomment the failing test   fix it   In our example, here is the refactoring I would do   static String fizzBuzz(int number) {    String result = \"\";    result += multipleWord(number, 3, \"Fizz\");    result += multipleWord(number, 5, \"Buzz\");    if (result.isEmpty()) {       result = Integer.toString(number);    }    return result; }  private static String multipleWord(int number, int multiple, String word) {    if (multipleOf(number, multiple)) {       return word;    }    return \"\"; }  ...  //@Test public void //it_is_Bang_for_7() { //   assertThat(fizzBuzz(7)).isEqualTo(\"Bang\"); //}   From there, fixing the test is dead simple.   In practice I find this loop very useful. At local scale as we saw but it’s also a great way to refactor your architecture at larger scale.   One downsize is that if you are not careful, it might lead to over-engineering. Be warned, keep an eye on that !   Last caveat : not all TDD interviewers like this technique …   Don’t obsess   It’s not because you are not following the Red Green Refactor loop to the letter that you are not doing TDD.   An interesting point is that these variations to the TDD loop are combinable ! Experienced TDD practitioners can jump from one to the other without even noticing.   This paper argues that as long as you write the tests along (before or after) the code, you get the same benefit. That’s not going to make me stop writing my tests first, but it is interesting. That would mean that even a Code - Test - Refactor loop would be ok if it is fast enough !  ","categories": ["testing","software","agile","tdd"],
        "tags": [],
        "url": "/dont-stick-to-tdds-red-green-refactor-loop-to-the-letter/",
        "teaser": null
      },{
        "title": "5 SPA Conference takeaways that could make us better software professionals",
        "excerpt":"Last week, my colleague Ahmad Atwi and I went to the London SPA Conference to give our Remote eXtreme Practice talk.   The London eXtreme Programming is one of the most active in the world. You could feel an XP atmosphere at the conference. For example, people like Nat Pryce and Steve Freeman, authors of GOOSGT book were speakers.      To summarize, we had the chance to attend a lot of very interesting sessions during the 3 days of the conference. Here are 5 pearls of wisdom I took back with me.   What connascences are   Identifying code connascences helps to rank refactorings and keep the system maintainable.   Continuous refactoring is one of the core practices of XP. For me, knowing what to refactor next has been a matter of code smells, discussing with my pair and gut feeling.   A connascence is a coupling between parts of the system. Two parts of your code are connascent if changing one implies changing the other. For example, a function call is connascent by name with the function definition. If you change one, you need to change the other.   Connascences are more formal than code smells. We can detect and rank them to pick the most important refactoring to do. People have listed 9 types of connascences. Some are visible in the source code, others are dynamic and difficult to spot before runtime.   The lowest form of connascence is ‘of name’, like in the function call example above. The worst form is ‘of Identity’, when different parts of the system must reference the same object.      The higher the connascence, the more difficult it is to evolve the parts involved. Instead of relying on intuition, you can use a connascence based refactoring algorithm :      Detect the highest connascence   Reduce or remove it   Repeat.   Thanks Kevin Rutherford and Adrian Mowat for your Red Green then what ? session about connascence.   Tips for pairing with junior developers   Irina Tsyganok and Nat Pryce gave a very fun session about this topic. A lot of valuable points discussed, from which I saved a few pearls of wisdom.   Cat eating session from @natpryce at #spaconf17 (unless @irinatsyganok stops him) pic.twitter.com/XSiaPDp9kJ &mdash; Andy Longshaw (@andylongshaw) 27 juin 2017   It was reassuring to hear Nat saying that “As we gain experience, we are not expected to know everything”. Pairing with developers out of college is an occasion to “exchange” skills. Hard learned design skills versus updates on the latest technologies.   I also learned about the Expert’s Amnesia and why experts often have a hard time teaching. Expert level knowledge is by nature instinctive. At this level of skill, it becomes very difficult to detail the logic of things that seem obvious.   We engineers are more mentors than coaches   In the first XP book, there were only 3 roles in the team : team members, on site customer and XP coach. The XP coach should be a developer who can help the team to learn the practices and principles of XP.   About the same time, the personal or professional coach jobs appeared. The Scrum Master, is to Scrum what the XP coach is to XP, without the developer part. Remember the joke “Scrum is like XP, without everything that makes it work” (Flaccid Scrum).   It looks like the Agile Coach job title appeared out of all this. The problem is no one exactly knows what this is. Should he be an experienced developer like the XP coach ? A great people person ? Or someone good at introducing change ? or a mix of these ?   Portia Tung and Helen Lisowski ‘s  talk “The power of coaching” clarified that.   There is no knowledge transfer from the coach to the coachees ! On the other side, a mentor does transfer knowledge to his mentees. The coach helps his coachee take a step back and take decisions in full consciousness. The goal of the mentor is to inspire and train to new techniques.   I’m fine being a mentor and not a coach ;-)   Servant leaders need to be tough at times   We hear a lot about servant leadership nowadays. Scrum Master should be servant leaders, as well as managers in agile organizations.   Angie Main gave a very interesting session about servant leadership. She made an interesting point I had not heard about before. We all know that servant leaders should trust the team to get the work done most of the time. In spite of that, servant leaders must also be ready to step in and remove people who don’t fit in and endanger the team !   This reminded me of what Jim Collins says in Built to last : “People who don’t fit are expelled like viruses !”      1/3000 ideas succeeds   Thanks to Ozlem Yuce’s session, I learned about the “Job To Be Done” technique to understand the customer’s real needs.   Studies measured that only 1 idea out of 3000 ends up as a successful product ! Here seems to be the original research.   I’ll remember this fact next time I’m ask for a funky feature !   To conclude   At the end, we had a very good time at SPAconference. The talks were insightful, we had interesting discussions, the premises were comfortable and on top of that, food was great !   I’m already eager to go to SPA conference 2018 !  ","categories": ["extreme programming","pair programming","refactoring","coaching","management"],
        "tags": [],
        "url": "/5-spa-conference-takeaways-that-could-make-us-better-software-professionals/",
        "teaser": null
      },{
        "title": "7 reasons why learning refactoring techniques will improve your life as a software engineer",
        "excerpt":"This post is a bold promise. Mastering incremental refactoring techniques makes our lives as software engineers more enjoyable.   I have already made the same statement about TDD before. As refactoring is a part of TDD, one could think I am repeating myself. At the same time, a recent Microsoft blog post argued that refactoring is more important than TDD. Even though I’m a TDD fan, that’s an interesting point.   Incremental refactoring is key to make releases non-events ! As early as 2006, using XP, we were releasing mission critical software without bugs ! We would deliver a new version of our software to a horde of angry traders and go to the movies without a sweat !   What’s so special about incremental refactoring ?   Avoid the tunnel effect      Mastering incremental refactoring techniques allows to break a feature down to baby steps. Not only smaller commits, but also smaller releases ! You can deploy and validate every step in production before we move to the next !   Small releases are also a lot easier to fix than big bang deployments. That alone is a good enough reason to deploy in baby steps.   There are a lot of other advantages to small deployments. Merges become straightforward. Someone can take over your work if you get sick. Finally, it’s also easier to switch to another urgent task if you need to.   Deliver early   When you know that you will be able to improve your work later on, it becomes possible to stick to what’s needed now. After spending some time working on a feature, it might turn out that you delivered enough value. Between enhancing this feature and starting another one, pick the most valuable. Don’t be able to switch. Incremental refactoring, makes it easy to resume later on if it makes sense.   Real productivity is not measured through code, but through feature value. This explains why incremental refactoring is more productive than up-front / big-bang development.   Know where you stand   As you’ll work through your feature, you’ll have to keep track of the done and remaining steps. As you go through this todo list and deliver every successive step, you get a pretty clear idea of where you stand. You’ll know that you’ve done 3 out of 7 steps for example. It helps everyone to know what’s the remaining work and when you’ll be able to work on something else.      A few times, I fell in the trap of features that should have taken a few hours and that lingered for days. I remember how stupid I was feeling every morning, explaining to my colleagues that it was more complex than I had thought, but that it should be finished before tomorrow … Learning incremental refactoring techniques saved me from these situations.   Deliver unexpected feature   Incremental refactoring techniques improves the code. As a systematic team wide effort, it keeps the code healthy and evolutive. When someone requests an unexpected feature late, you’ll be able to deliver it.   This should improve your relation with product people. They will be very happy when you build their latest idea without a full redesign.   Avoids rewrites   Joel Spolsky wrote a long time ago that rewriting a large piece of software is the number 1 thing not to do ! All my experiences in rewriting systems have been painful and stressful.   It always starts very rosy. Everyone is feeling very productive with the latest tools and technologies. Unfortunately, it takes a lot of features to replace the existing system. As always with software, the time estimates for the rewrite are completely wrong. As a result, everyone starts grumbling about why this rewrite is taking so long. The fact that the legacy system is still evolving does not help either. Long story short, the greenfield project ends up cutting corners and taking technical debt pretty fast … fueling the infamous vicious circle again.   Incremental refactoring techniques offer an alternative. It enables to change and improve the architecture of the legacy system. It looks longer, but it’s always less risky. And looking back, it’s almost always faster as well !   Ease pair programming   eXtreme Programming contains a set of practices that reinforce each other. As I wrote at the beginning, refactoring goes hand in hand with TDD. Pair programming is another practice of XP.              From www.extremeprogramming.org/        TDD and Refactoring simplify pair programming. When a pair is doing incremental refactoring, they only need to discuss and agree on the design at hand. They know that however the design needs to evolve in the long term, they’ll be able to refactor it. It’s a lot easier to pair program if you don’t have to agree on all the details of the long term design …   In turn, pair programming fosters collective code ownership. Collective code ownership increases the truck factor. Which reduces the project risks and makes the team’s productivity more stable. In the long run, this makes the work experience more sustainable and less stressful.   Simplify remote work   Refactoring will also save you from the commutes and allow you to work closer to the ones you love !   Refactoring techniques enable small commits. Small commits simplify code reviews, which are key to remote or distributed work. Even if you are doing remote pair programming, small commits help to switch the control between buddies more often.      To be continued   I hope that by now I persuaded you to learn incremental refactoring techniques. My next post will dig into the details about how to do that.  ","categories": ["programming","refactoring","extreme programming"],
        "tags": [],
        "url": "/7-reasons-why-learning-refactoring-techniques-will-improve-your-life-as-a-software-engineer/",
        "teaser": null
      },{
        "title": "How to start learning the tao of incremental code refactoring today",
        "excerpt":"In my last post, I explained why incremental refactoring techniques will make you both more productive and relaxed.   As anything worth its salt, the path to full mastery is long and requires dedication. The good news is that you’ll start to feel the benefits long before you are a master.   Dedicated Practice   The quickest thing that will get you up to speed is dedicated practice. Take some time to do some exercices outside of any ‘production’ code.   TDD Coding Katas   The most famous practice to learn TDD also works very well to learn refactoring. That shouldn’t be a surprise as incremental refactoring is an integral part of TDD.   There are many ways to do your first coding kata. You could find a coding dojo near you (ask meetup.com). Or you could find motivated colleagues to start one at your company … I wrote in more details about how to attend a coding dojo in this post.      You can also practice katas on your own. My friend Thomas Pierrain rehearses the same katas to discover deeper insights.   Refactoring Golf   The goal of incremental refactoring is to keep the code production ready all the time. Smaller commits is one happy consequence of that.   You can stretch your refactoring muscles by doing coding katas and keeping the code compiling all the time. You’ll need to master your IDE and its automated refactoring. Most of all, it will shift your attention from the goal to the path !   I learned at SPA conference that we call this ‘Refactoring golf’. The name comes from Golf contests, popular in the Perl community. Their goal is to write the shortest program possible to do a specific goal. The goal of a Refactoring Golf is to go from code A to code B in the fewest transformations possible.   They are a few refactoring golf repos on Github, I tried one and found it fun ! Give it a try too !   Study some theory   Real mastery does not come by practice alone. Studying theory alongside practice yields deeper insights. Theory enables to put your practice into perspective and to find ways to improve it. It saves you from getting stuck in bad habits. It also saves you from having to rediscover everything by yourself.   Develop your design taste   In Taste for Makers Paul Graham explains why taste for is fundamental to programming. Taste is what allows you to judge if code is nice or bad in a few seconds. Taste is subjective, intuitive and fast, unlike rules which are objective but slower. Expert designers use taste to pinpoint issues and good points in code on the spot.   Within the fast TDD - Refactoring loop, taste is the tool of choice to drive the design. Guess what : we can all improve our design taste !   Code smells are the first things to read about to improve your design taste. Once you know them well enough, it will be possible to spot things that might need refactoring as you code.   Spotting problems is nice, but finding solutions is better ! Design Patterns are just that … There has been a lot of controversy around Design Patterns. If overusing them leads to bloated code, using them to fix strong smells makes a lot of sense. There is even a book about the subject :      Finally, there’s a third and most common way to improve our design taste. It’s to read code ! The more code we read, the better our brain becomes at picking small clues about what nice and what is not. It’s important to read clean code but also bad code. To read code in different languages. Code built on different frameworks.    So, read code at work, read code in books, read code in open source libraries, good code, legacy code …   Learn your refactorings   As with most topics in programming there is a reference book about refactoring. It’s Martin Fowlers’s Refactoring, improving the design of existing code. Everything is in there, smells, unit testing and a repository of refactoring walkthroughs.      The book is said to be a difficult read, but the content is worth gold. If you have the grit, give it a try ! At the end, you should understand how your IDE does automated refactoring. You should also be able to perform all the refactorings that your IDE does not provide by hand ! This will enlarge your refactoring toolbox, and help you to drive larger refactorings from A to B.   Develop a refactoring attitude   Practice makes perfect. Whatever our refactoring skill, there is something to learn by practicing more.   Make it a challenge   As you are coding, whenever you find a refactoring to do to your code, make it a challenge to perform it in baby steps. Try to keep the code compiling and the tests green as much as possible.   When things go wrong, revert instead of pushing forward. Stop and think, try to find a different path.   If you are pairing, challenge your pair to find a safer track.   This might delay you a bit at first, but you’ll also be able to submit many times per day. You’ll see that your refactoring muscles will grow fast. You should see clear progress in only 1 or 2 weeks.   Team up against long refactorings   If your team prioritizes a user story that will need some re-design, try to agree on a refactoring plan. The idea is to find a coarse grain path that will allow you to commit and deliver many times. This plan might also help you to share the work on the story.   Having to question and explain your assumptions will speed up your learning.    Legacy code   Refactoring is most useful with bad legacy code. Unfortunately, it also where it is the most difficult. Next week’s blog post will be about what we can do to learn how to refactor legacy code.   That was my second post in this mini-series about refactoring. First one was 7 Reasons Why Learning Refactoring Techniques Will Improve Your Life as a Software Engineer. The third and last is 10 things to know that will make you great at refactoring legacy code  ","categories": ["programming","refactoring","extreme programming","learning"],
        "tags": [],
        "url": "/how-to-start-learning-the-tao-of-incremental-code-refactoring-today/",
        "teaser": null
      },{
        "title": "10 things to know that will make you great at refactoring legacy code",
        "excerpt":"We write tons of legacy code everyday. Experienced developers understand that legacy code is not something special. Legacy code is our daily bread and butter.   Should we abandon all hope as we enter legacy code ? Would that be professional ? In the end, code is only a bunch of bytes, somewhere on a drive. We are the software professionals. We need to deal with that.      1. Master non legacy refactoring first   Please calm down before this “Bring ‘em out” energy goes to your head.   I did not say that refactoring legacy code is easy. Legacy code can bite … bad. I’ve been in teams which literally spent nights fixing a bad refactoring gone to production …   Before you can refactor legacy code, you need to be good at refactoring new code. We all learned to swim in the shallow pool, it’s the same with refactoring. Mastering green code refactoring will help you when tackling legacy code.   First, you’ll know the ideal you’d like to get to. Knowing how productive a fast feedback loop is will motivate you to keep on refactoring.   Second, you’ll have a better idea of the baby steps to take you through a tricky refactoring.   If you are not yet at ease with greenfield refactoring, have a look at my previous post.   2. Understand that refactoring legacy code is different   The next thing to remember is that refactoring legacy code is different. Let’s assume Michael Feather’s definition of legacy code : “Code without tests”. Getting rid of legacy code means adding automated tests.   Unfortunately, trying to force push unit tests in legacy code usually results in a mess. It introduces lot’s of artificial mocks in a meaningless design. It also creates brittle and unmaintainable tests. More harm than good. This might be an intermediate step, but it is usually not the quickest way to master your legacy code beast.   Here are alternatives I prefer.   3. Divide and conquer   This is the most straightforward way to deal with legacy code. It’s an iterative process to repeat until you get things under control. Here is how it goes :   (1) Rely on the tests you have, (2) to refactor enough, (3) to test sub-parts in isolation. (4) Repeat until you are happy with the speed of the feedback loop.   Depending on the initial state of your tests, this might take more or less time. Your first tests might even be manual. This is the bulldozer of refactoring. Very effective, but slow.      4. Pair or mob program      Given enough eyeballs, all bugs are shallow.       Linus’s Law    Changing legacy code is a lot easier when you team up. First, it creates a motivating “we’re all in this together” mindset. Second, it guards us against silly mistakes.   Mob programming, might seem very expensive, so let me explain why it is not. Suppose you want to introduce some tests in a tricky section of code.   With mob programming, all the team gathers for half a day to work on this change. Together, they find and avoid most of the pitfalls. They commit a high quality change, which creates only one bug down the road.   Let’s see the alternative.   Using solo programming, a poor programmer tries to tackle the change all by himself. He spends a few days to understand and double check all the traps he can think of. Finally, he commits his change, which results in many bugs later on. Every time a bug pops up, it interrupts someone to fix it ASAP.   The savings in interruptions are greater than up front cost of mob or pair programming.      5. Seams      A software seam is a place where you can alter behavior in your program without editing in that place.       Michael Feathers    This is one of the many interesting things I learned from Michael’s book about legacy code.      Object polymorphism is only one kind of seam. Depending on your language, many other types of seams can be available.       Type seam for generic languages   Static link seam for static libraries   Dynamic link seam for dynamic libraries   …   Finding seams in your program is something opportunistic. Keep in mind though that testing through seams is not the end goal. It is only a step to bootstrap the test-refactor loop and start your refactoring journey.   6. Mikado Method   How do you get to your end then ? How to you refactor only what’s useful for your features ? How do you do large refactorings in baby steps ?   Over time, I found that the mikado method is a good answer to all these issues. The goal of the Mikado Method is to build a graph of dependent refactoring. It can then use it to perform all these refactorings one by one. Here is the mikado method by the book.   Before anything else, you’ll need a large sheet of paper to draw the graph. Then repeat the following :      try to do the change you want   If it builds and the tests pass, great, commit and you’re done   Otherwise, add a node for the change you wanted to do in your mikado graph   Write down the compilation and test errors    Revert your change   Recurse from 1 for every compilation or test error   Draw a dependency arrow from the nodes of errors to the node of your initial change   Once you built the full graph, tackle the refactorings from the leaves. As leafs have no dependencies, it should be easy to do and commit them.      When I first read about the mikado method, it seemed very simple and powerful. Things got more complex when I tried to apply it. For example, the fact that some changes don’t compile hide future test failures. That means that very often, the “Build the graph” and “Walk the graph” phases overlap. In real life, the graph evolves and changes over time.    My advice about the Mikado Method is not to take it to the letter. It’s a fantastic communication tool. It helps not to get lost and to avoid a refactoring tunnel. It also helps to tackle refactoring as a team.   It is not a strict algorithm though. Build and tests are not the only way to build the graph. Very often, a bit of thinking and expert knowledge are the best tools at hand.      7. Bubble Context   Refactoring needs to be opportunistic. Sometimes there are shortcuts in your refactoring path.   If you have access to a domain expert, the Bubble Context will cut the amount of refactoring to do. It’s also an occasion to get rid of all the features that are in your software but that are not required anymore.    The Bubble Context originated from the DDD community, as a way to grow a domain in an existing code base. It goes like that :      Find a domain expert   (Re)write clean code for a very tiny sub domain   Protect it from the outside with an anticorruption layer   Grow it little by little   I have friends who are fans of the bubble context. It is super effective provided you have a domain expert. It is a method of choice in complex domain software.   8. Strangler   Bubble Context works great when refactoring domain specific code, what about the rest ? I had good results with the Strangler pattern.   For example, we had to refactor a rather complex parser for an internal DSL. It was very difficult to incrementally change the old parser, so we started to build a new one aside. It would try to parse, but delegate to the old one when it failed. Little by little, the new parser was handling more and more of the grammar. When it supported all the inputs, we removed the old one.   The strangler is particularly well suited for refactoring technical components. They have more stable interfaces and can be very difficult to change incrementally.   9. Parallel Run   This is more of a trick than a long term strategy. The idea is to use the initial (legacy) version of the code as a reference for your refactoring. Run both and check that they are doing the same thing.      Here are some variations around this idea.   If the code you want to refactor is side effect free, it should be easy to duplicate it before refactoring. This enables running both to check that they compute the same thing.   Put this in a unit test to bootstrap a test-refactor loop. You can also run both in production and log any difference. You’ll need access to production logs … Devops teams have a refactoring advantage !   Here is another use of your logs. If the code writes a lot of logs, we can use them as a reference. Capture the logs of the old version, and unit test that the refactored version prints the same logs out. That’s an unmaintainable test, but good enough to bootstrap the test-refactor loop.   The Gilded Rose kata is a good exercise to practice this last technique.   10. Dead code is better off dead   You don’t need to refactor dead code ! Again, access to production logs is a great advantage for refactoring.   Add logs to learn how the real code runs. If it’s never called, then delete it. If it’s only called with some set of values, simplify it.   No silver bullet   That was a whirlwind tour of the legacy code refactoring techniques I know. It’s no promise that refactoring will become easy or fast. I hope it is a good starting point to set up and walk a refactoring plan.   This was the last post of a series of 3 about how to learn refactoring techniques. If you didn’t already, check part 1 7 Reasons Why Learning Refactoring Techniques Will Improve Your Life as a Software Engineer and part 2 How to Start Learning the Tao of Incremental Code Refactoring Today.  ","categories": ["programming","refactoring","learning","technical debt","mikado-method"],
        "tags": [],
        "url": "/10-things-to-know-that-will-make-you-great-at-refactoring-legacy-code/",
        "teaser": null
      },{
        "title": "Forget unit tests, only fast tests matter",
        "excerpt":"Don’t worry if your unit tests go to the DB, that might not be so bad.   When I started writing unit tests, I did not know what these were. I read the definition, and strived to follow the recommandations :      they should be independent from each other   they should not access the DB   they should not use the network   they should only cover a small scope of your code   I started to write unit tests on my own and became test infected pretty fast. Once I got convinced of the benefits of unit testing, I tried to spread the practice around me. I used to explain to people that it is very important to write real unit tests by the book. Otherwise, Bad Things would happen …   How I changed my mind   A few years ago, I spent a few years working on a Rails side project called mes-courses.fr. I was using a small test gem to enforce that no unit tests were accessing the db. I had to write a lot of mocks around the code. I ended up hating mocks : they are too painful to maintain and provide a false sense of security. I’m not alone in this camp, check DHH’s keynote at RailsConf 2014.     At some point, the mock pain got so bad that I stopped all developments until I found another way. I found a pretty simple workaround : use in-memory SQLite. I got rid of all the DB access mocks. Not only were the tests easier to write and maintain, but they were as fast as before, and they covered more code.   That changed something fundamental in my understanding of testing   It’s all about speed baby   The only thing that makes unit tests so important is that they run fast.   Unit tests as described in the literature run fast. Let’s see what happens when you remove one of the recommandations for unit tests.      If tests depend on each other, their outcome will change with the execution order. This wastes our time in analyzing the results. On top of that, independent unit tests are easy to run in parallel, providing an extra speedup. We lose this potential when our tests are dependent.   Tests that rely on an out-of-process DB run slower. Tests need to start the DB before anything else. Data needs to be setup and cleaned at every test. Accessing the DB implies using the network, which takes time as well. There’s also a risk of making the tests dependent by sharing the same DB. A last issue is troubleshooting the DB process when things don’t work.   Tests that use the network are slow too ! First, Network is slower than memory. Second, data serialization between processes is slow as well. Finally, these tests are likely to use some form of sleep or polling, which is slow, fragile, or both !   Finally, there is always a scope past which a test will be too slow.   This means that not only unit tests are fast, but also that fast tests usually show the features of unit tests.   My guess is that ‘unit tests’ were explicitly defined as a recipe for fast tests ! If you stick to the definition of unit tests, you’ll get fast tests and all their benefits.      Fast tests   That also means that we should focus first on having fast tests rather than unit tests. Here is my real check to know if tests are fast enough :      Is the build (and the tests and everything) less than 10 minutes ?   Can I continuously run my tests while coding and stay in the flow ?   If both answers are yes, then I won’t question myself too much whether my tests are unit, integration or end to end.   So what ?   I’ve been experimenting with these heuristics for some time. Side projects are great for experimenting since you don’t have a team to convince ! Here are my main takeaways :      Stick to end to end tests at the beginning of your project. They are easy to refactor to finer grained tests later on.   In-memory DBs are great to speed tests up without wasting your time with mocking.  We can use a unique DB for every test to keep them independent.   Large scope tests are not an issue provided 2 things.            The code contains very few side effects.       It provides good exceptions and assertions messages           On the other side, there are things that I still recommend :      Independent tests are easy to write from the beginning, difficult to fix later on. As they save a lot of headaches in diagnostic, I stick to them from the start.   Avoid network, it makes the tests slow, fragile and tricky to diagnostic. But please, read this before jumping to mocks.   These rules have served me well, particularly in my side projects, where I don’t have a lot of time. What about you ? Do you have your own testing rules ?  ","categories": ["testing","tdd","software"],
        "tags": [],
        "url": "/forget-unit-tests/",
        "teaser": null
      },{
        "title": "5 Remote energizer tips that will make your remote retrospectives rock",
        "excerpt":"Do you remember how people who are not used to the phone tend to shout in it, for the message to get far ? Read on and I’ll explain how this silly habit will make your remote retrospectives great !   A typical retrospective starts with an energizing activity, or energizer. It’s important for two reasons. First, people who don’t speak during the first 5 minutes of a meeting are more likely to remain silent until the end. Second, getting everyone to do an energizing and fun activity sets the tone for a peaceful and creative time.      Our experiences with remote energizers   When we started to do retrospectives at work, all the team was co located in Paris. There are tons of activities available on the internet to run effective energizers. We could do games like Fizz Buzz, or drawing based activities like face drawing and visual phone. It was easy and fun.   A few years ago, Ahmad Atwi joined our team from Beirut. Our catalog of energizer shrank to a few activities that we could run remotely. On top of that, going through the remote medium made it more challenging for energizers to … energize ! With time and trial, we managed to understand what works and how to pick the right energizer for a remote team.   Principles for remote energizers   We have an Agile Special Interest Group at Murex, where volunteers meet to share things they find interesting. A few weeks ago, during one of these sessions, we discussed remote energizers in a Lean Coffee.   Here are the points we came up with.      Question activities work great through Trello. Agile Retrospectives, making good teams great details such a Check-In activity. The typical questions sound like “Coming into this retrospective, if you were a car, what kind of car would you be ?”         If they are enough teammates at every place, energizers that play in small groups will work well. For example, it would be easy to organize a balloon battle or a back to back.         It’s also easy to use variations on an activity that proved effective. For example, explore new kinds of questions. It’s even ok to repeat verbatim an activity from time to time.   Replace energizing by team building. Team building is particularly important for remote teams. Instead of engaging activities, it’s ok to have everyone share a personal anecdote. By knowing each other better, the team can build trust. For example, you could introduce such an activity with : “What book would you bring on a desert island ? Why ?”   One last thing we came up with my colleague Morgan Kobeissi to energize a remote meeting is to YELL. The idea is to ask everyone to answer a question while standing and yelling. A question could be “How long have you been working and what companies did you work for ?”      Remote work is here to stay. More and more teams are facing similar difficulties. We need to invent new work practices. If you discovered new ways to run remote energizer, please share them with a comment.  ","categories": ["agile","retrospectives","remote"],
        "tags": [],
        "url": "/5-remote-energizer-tips-that-will-make-your-remote-retrospectives-rock/",
        "teaser": null
      },{
        "title": "eXtreme eXtreme Programming (2017)",
        "excerpt":"What would XP look like if it was invented today ?   A few days ago, I stumbled upon these two talks that got me thinking about this question.       So I looked deeper into the question, while sticking to the same values and principles. Here is what I came up with.   Practices   Continuous Deployment   Why should we only deliver at every iteration in 2017 ? Lot’s of companies demonstrated how to deploy every commit to safely production. Amazon for example, deploys new software every second !   DevOps++   As a team starts doing continuous deployment, devs get more and more involved in ops. This privileged view on the users’s behaviour can turn devs into product experts. Why not push the logic further and make them the product guys as well ?   Test in prod   Continuous deployement opens up many opportunities. Deploying safely requires bulletproof rollbacks. Once devs take care of product, code and ops they can shortcut testing and directly test in prod with some users ! Rollback remains an option at any time.   #NoBugs   That seems like wishful thinking. The idea is to fix bugs as soon as they appear, but also to prevent them in the first place. Continuous Deployment requires great engineering practices, which enable this way of working. A story cannot be “finished” until test in prod is over, and bugs fixed.   Kanban   At its core, Kanban is a way to limit the work in progress. It’s almost a side effect of the previous practices. #noBugs almost kills interruptions for rework. On top of that, devs have full control on their end to end work, so why would they multitask ?   #NoBacklog   In Getting Real, basecamp guys said that their default answer to any new feature request was “No”. Only after the same thing got asked many times would they start thinking of it. Maintaining a long backlog is a waste of time. Dumping backlog items but the current ones saves time. Important ideas will always come up again later.   #NoEstimates   This one is famous already. Some teams have saved time by using story count instead of story points. What’s the point anyway if the team is already :      working as best as it can   on the most important thing   and deploying as soon as possible   Data Driven   This is how to drive the system evolution. Instead of relying on projects, backlogs and predictions, use data. Data from user logs and interviews proves if a new feature is worth building or dumping. Exploring logs of internal tools can help to continuous improve.   Lean startup &amp; Hackathons   Incremental improvements, in either product or organization, is not enough. As Tim Hardford explained in Adapt, survival requires testing completely new ideas. Lean startup &amp; hackathons experiments can do just that.      Improvement kata   Improvement kata is a way to drive long term continuous improvement. It’s the main tool for that at Toyota (read Toyota Kata to learn more). It provides more time to think of the problem than a retrospective. It also fits perfectly in a data driven environment.   Mob programming   Pair programming is great for code quality and knowledge sharing. Mob programming is the more extreme version of pairing where all the team codes together.   Throw code away frequently   An alternative to refactoring with unit tests is throwaway and rewrite once it gets too bad. Companies have been doing that for years. I worked at a bank that used to throwaway and rewrite individual apps that were part of a larger system. It can be a huge waste of money if these sub systems are too large. Scaling down to individual classes or microservices could make this cost effective.   Remote   With access to a wider pool of talents, remote teams usually perform better. It also makes everybody’s life easier. Finally, teams have reported that mob &amp; remote programming work great together.   Afterthought   What’s striking from this list is that it’s not that far from the original XP ! For example, continuous deployment and generalists have always been part of it. Another point is that is not science fiction ! I found many teams reporting success with these practices on the internet ! A well-oiled XP team might very well get there through continuous improvement.   The more I look at it, the more XP stands as a unique lighthouse in the foggy Agile landscape.   As for me, I’m not sure I’d dare to swap TDD &amp; refactoring for throwaway &amp; rewrite. I’m particularly worried about the complex domain specific heart of systems. Nothing prevents from using both strategies for different modules though.   I should give it a try on a side project with a microservice friendly language, such as Erlang or Elixir.  ","categories": ["extreme programming","agile","pair programming","remote"],
        "tags": [],
        "url": "/extreme-extreme-programming-2017/",
        "teaser": null
      },{
        "title": "Throwing code away frequently",
        "excerpt":"Here is the main feedback I got about my previous post eXtreme eXtreme Programming.      What do you actually mean by throwing code away ? Does it mean stoping unit testing and refactoring ?       So I guess it deserves a bit of explanation.   What is it all about ?   When programming, I don’t throw code away a lot. I tend to rely on my automated tests to change the code I already have. That might be a problem.   As with everything, there is no one size fits all. We should choose the best practice for our current situation. Same thing applies for incremental changes versus rewriting.   TDD makes incremental changes cost effective, and as such, is a key to get out of the Waterfall.   The idea of throwing code away frequently is to make rewriting cost effective, so we can do it more often.   Why would we want to do so ?   In “When Understanding means Rewriting”, Jeff Atwood explains that reading code can be more difficult than writing it. There is a point where rewriting is going to be cheaper than changing.   The more unit test you have, the later you reach that point. The more technical debt you take, and the sooner. The bigger the part to rewrite, the more risky it becomes.   Let’s imagine you knew a safe way to rewrite the module you are working on. You could be faster by taking more technical debt and writing less unit tests ! Mike Cavaliere framed it as F**k Going Green: Throw Away Your Code.   This would be great for new features, that might be removed if they don’t bring any value. It would also be a good way to get rid of technical debt. Naresh Jain also makes the point that without tests, you’ll have to keep things simple (here and here) !   Wait a minute, isn’t that cowboy coding ?   How to make it work      How to ensure quality without unit tests ?   TDD and unit testing is a cornerstone of XP. If we remove it, we need something else to build quality in. Could Mob Programming and Remote teams do the trick ?   “Given enough eyeballs, all bugs are shallow”. Pair programming and code reviews catch a lot more bugs than solo programming. Only a few bugs are likely to pass through the scrutiny of the whole team doing mob programming.   What about Remote ? Martin Fowler explains that remote teams perform better by hiring the best. Skills of programmers have been known for a long time as a main driver of software quality.              Photo from Steve McConnell on Construx        Finally, the Cucumber team reported that Mob Programming works well for remote teams.   How to make this safer ?   Even with the best team, mistakes will happen. How can we avoid pushing a rewrite that crashes the whole system ?   The main answer to that is good continuous deployment. We should deploy to a subset of users first. We should be able to rollback straight away if things don’t work as expected.   As the system grows, microservices can keep continuous deployment under control. We can deploy, update and roll them back independently. By nature, microservices also reduce the scope of the rewrite. That alone, as we said earlier, makes rewrites safer.   As a last point, some technologies make building microservice systems easier and incremental. The Erlang VM, for example, with first class actors, is one these. Time to give Erlang and Elixir a try !   Is this always a good idea ?   There are good and bad situations.   For example, a lean startup or data driven environment seems a good fit. Suppose your team measures the adoption of new features before deciding to keep or ditch them. You’d better not invest in unit tests straight away.   On the other side, software for a complex domain will be very difficult to rewrite flawlessly. I have worked in the finance industry for some years, I know what a complex domain is. I doubt I could rewrite a piece of intricate finance logic without bugs. I would stick to DDD and unit testing in these areas.   How to dip your toe   Here is how I intend to dip my toe. I won’t drop automated testing completely yet. What I could do instead (and that I already did on side projects) is to start with end to end tests only.   From there every time I want to change a piece of code, I have many options :      Add a new end to end test and change the code.   If the code is too much of a mess, I can rewrite it from scratch. I’ll have the safety of the end to end tests.   If I see that the module is stabilizing, has not been rewritten for a while, and yields well to modifications. I could start splitting end to end tests into unit tests, and embark on the TDD road.   Maybe later, when I have a team doing mob programming to find my mistakes, we’ll skip the end to end tests.   Other interesting links      Throwaway Code by M. Scott Ford   Rewrite Code From Scratch on c2 wiki  ","categories": ["extreme programming","tdd","remote","refactoring"],
        "tags": [],
        "url": "/throwing-code-away-frequently/",
        "teaser": null
      },{
        "title": "How we used the improvement kata to gain 25% of productivity - Part 1",
        "excerpt":"If you are serious about continuous improvement, you should learn the improvement kata.   Retrospectives are great to pick all the low hanging improvements. Once you’ve caught up with the industry’s best practices, retrospectives risk drying up. Untapped improvement opportunities likely still exist in your specific context. The improvement kata can find those.      Here is how we applied the improvement kata to gain 25% of productivity in my previous team.   The Situation   Thanks to repeated retrospectives, the team had been improving for 2 years. Retrospectives seemed like a silver bullet. We would discuss the current problems, grasp an underlying cause and pick a best practice. Most of the time, that fixed the problem.   Sometimes it did not work though. Even if the issue came back in a later retrospective, it would not survive a second scrutiny. In the previous two years, the team had transformed the way it worked. It adopted TDD, incremental refactoring, pair programming, remote work, automated performance testing and many others.   Lately though, things did not work so well. The team was struggling with productivity issues. The team was not slowing down, but the scope and complexity of the product had grown. Features were not getting out of the door as fast as they used to. We had the habit of prioritizing improvements and bug fixes over features. That used to improve the flow enough to get more and more feature done. It did not seem to work anymore.   We tried to tackle the issue in retrospectives. We would change the way we prioritized features … To be later bogged down by bugs, technical debt or bad tools. We would discuss that in retrospective, and decide to change the priorities again … The loop went on and on a few times.   We were getting nowhere.   The improvement kata    That’s why I started to look for other ways to do continuous improvement. I stumbled upon a book called Small Lean Management Guide for Agile Teams. The book is in french, but I wrote an english review. I fell in love with the way the authors dug into the hard data of how they worked to understand and fix their problems.   To learn more about this technique, I read Toyota Kata. It details two management tools used at Toyota : the improvement and coaching katas. Some say these are Toyota’s special weapon. The thing that grew them from a small shop to the largest car manufacturer in the world.   They are katas because they are routines. They must be re-execute many times. The improvement kata should improve the flow of work. The coaching kata helps someone (or yourself) to learn the improvement kata. Every time we go through the kata, we also understand it better.   Here is how the improvement kata goes :      Describe your end goal   Understand where you stand about this goal by measuring facts and data   Based on your end goal and the current situation, define where you’d like to be in 3 months or less   Use Plan-Do-Check-Act to experiment your way to this new situation            Plan an experiment       Do this experiment       Check the results of this experiment       Act on these results.                     Either drop the experiment and plan a new one (go back to ‘Plan’).           Or spread the change at a larger scale.                                      Image from Mike Rother on Toyota Kata Website        The coaching kata is a way to coach someone into applying the improvement kata. The fun thing is that you can coach yourself ! The idea is to ask questions to the coachee to remind him of where he stands in his improvement kata.      You’ll find tons of details and material about these katas on the Toyota Kata website.   Our end goal   That’s how I started to apply the improvement kata in my team. I already had an idea of our end goal : to be more productive. To be more precise :      Generating enough profit for the business while sticking to a sustainable pace.    Retrospectives could not get us there. Would the improvement kata would succeed ?   This is the first part of a series of 4 posts relating our first use of the improvement kata. In the next post, I’ll explain what we did to understand the current situation.  ","categories": ["agile","lean","retrospectives","continuous improvement","improvement kata","first-improvement-kata-series"],
        "tags": [],
        "url": "/how-we-used-the-improvement-kata-to-gain-25-percent-of-productivity-part-1/",
        "teaser": null
      },{
        "title": "How we used the improvement kata to gain 25% of productivity - Part 2",
        "excerpt":"In my previous post, I described the productivity issue our team was facing. How retrospectives did not work, and how I started looking at the improvement kata.   We had gone through the first phase of the improvement kata : set the end goal.      Generating enough profit for the business while sticking to a sustainable pace.    Time to start the second phase : Understand.      Understand   Where we really slower ? Or was it an illusion ?   When trying to understand, you have to start with the data you have. You continue digging until you get a deeper understanding of the situation.   Velocity   We started with available data : story points and velocity. For sure this is a pretty bad measure of productivity. (Note : we should never use velocity for performance appraisal) In our case though, it felt like a good starting proxy measure.   Here is our velocity curve over 2 years.      It’s obvious that something changed. There are 2 parts to this graph. The velocity dropped between sprint 54 and 16.01. That’s a clue that our gut feeling was not completely wrong. Our productivity did change.   Man days   Our first hypothesis was that team members turnover was the cause. As with any team, some people came, and some people left. Let’s superpose the man days and velocity curves.      That could only explain part of the problem !   We tried to fine tune the man days curve. We took people’s involvement in tasks outside of programming into account. We used coefficients depending on the developers’ experience. That did not provide a better explanation.   We had to find another explanation.   Velocity computation   As I said earlier, velocity is not a measure of productivity. Any change in the way we were computing velocity would impact this curve.   We had kept photos and Trello boards of our retrospectives meetings. We searched them through for anything that could impact velocity. Here is what we found :      At sprint 55, we decided to ditch the focus-factor   At sprint 61, we started to do regular exploratory-testing. Exploratory testing discovers more bugs on top of user reported bugs. This made us spend more time on fixing bugs.   At sprint 62, as we opted for a No Bug policy we decided not to count story points for bugs      💡Keep Photos and Trello boards of Retrospectives as a log of your working agreements changes    The timings almost perfectly matched what we had observed in the first place. The question that came to our minds was :      Are we spending too much time on bugs ?    Halfway through understanding   This is how we started to dig into our situation. It’s a good time to give you a bit of feedback about how we felt at that point.   It was the first time we tried the improvement kata. More than that, we did not find any tutorial or guides about how to run it. The only instructions we had were theoretical descriptions or super concrete examples. We had to bridge the gap and come up with our own way.   To summarize, we felt a bit lost, we had gathered data from here and there, and we did not know what to look at next. On top of that, the quality of the data we were collecting was not great. We were wondering if we would get anything out of these investigations.      It felt a bit like when I did the 20 hours experiment to learn anything. We did exactly what had worked with the learning experiment : we pushed through !      💡If you feel lost when doing something for the first time. Push through !    In next week’s post, I’ll continue to detail the ‘understand’ phase. The series also gained an extra post, and will now be 5 posts long.   More to read next week.  ","categories": ["agile","lean","continuous improvement","improvement kata","first-improvement-kata-series"],
        "tags": [],
        "url": "/how-we-used-the-improvement-kata-to-gain-25-percent-of-productivity-part-2/",
        "teaser": null
      },{
        "title": "How we used the improvement kata to gain 25% of productivity - Part 3",
        "excerpt":"This is the third post on a series of 5 about the improvement kata. If you haven’t read the beginning of the story, I recommend you start from part 1.   In the previous post, I explained how we started to understand what was going on. We were now questioning our way of handling bugs.      Are we spending too much time on bugs ?       More understanding   Types of tasks   To answer this question, we decided to plot the different types of tasks we had completed per sprint.      Think again of the velocity curve we started with. We see an almost exact correlation between story count (green bars above) and story points (blue curve below).      💡#NoEstimates works       We can also see that after sprint 56, we were spending more time on bugs and improvements. Improvements are supposed to improve productivity, so we decided to focus on bugs first. Here is what we get if we compute the average number of bugs per sprint :                  Periods       Sprints       Bugs       Average bugs fixed per sprint                       2015, Before sprint 56       15       21       1.4                 After sprint 56       34       210       6.1           Starting sprint 56, we were fixing 4 times as many bugs as we used to do before !   What is going on with bugs ?   At this point, we felt we’d made a great step forward in our understanding. We almost thought we were done with it …   After a bit of thinking though, it was clear that we were not ! We still needed to understand why we were in this situation.   We started by listing more questions :      Could it be that we just got a lot better at testing ? Since sprint 56, we had been doing regular exploratory testing. Exploratory testing sessions were very effective at finding bugs.   Were we paying back a bug debt ? The created versus resolved trend seemed to show so. But it could also be that we weren’t testing as well as we used to !         If we were paying back a bug debt, how close were we to the end of the payback ?   Were we creating too many flaws in the software ?   Are we fixing too many bugs ? If so, what should we do to fix less ?   Are the bugs coming from other teams using our component or from our own testing ?   Are bugs on new or old code ?   A lot of questions, all difficult to answer. We decided to first see if we were paying back a bug debt. If this was the case, most other questions would become more or less irrelevant. With a bit of thinking, we came up with a measure to get the answer.   Are we paying back a bug debt ?   We first started to do exploratory testing at sprint 56. To do this, we would run a 1 hour session, where the pair finding the more bugs would win fruits. (Later on, we streamlined exploratory testing as part of the workflow for every story) At that time, we used to find more than 10 bugs in 1 hour.      💡Gamification transforms nice developers into berserk testers !                   Explo Test Sesssion       61       62       63       64       66       16.01                       Bugs found       16       6       16       10       11       11           We would do another such a session. If we found significantly less than 10 bugs, let’s say less than 6, it would mean that :      we improved the quality of our software   our streamlining of exploratory testing works   if we continue to search and fix bugs as we do currently, we’ll reach a point where we won’t find any more bugs   Otherwise, none of these stand, and we’ll have to continue our investigations.   So we did a 1 hour, fruit-powered, exploratory testing session. And we found only 3 bugs ! Indeed, we were paying back a bug debt. The question became      When should payback be over ?    A linear regression on the created vs resolved bug trend showed that we still had 15 more months to go !      Target condition   At that point, the target condition became obvious :      We’d like to be done with bugs within 3 months.    Currently, around 1 pair (25% of the team) was busy fixing bugs. If we’d manage to bring this down, we’d have a 25% productivity boost.   This was post 3 in a series of 5 about the improvement kata. Next post will be about PDCA.  ","categories": ["agile","lean","continuous improvement","improvement kata","first-improvement-kata-series"],
        "tags": [],
        "url": "/how-we-used-the-improvement-kata-to-gain-25-percent-of-productivity-part-3/",
        "teaser": null
      },{
        "title": "How we used the improvement kata to gain 25% of productivity - Part 4",
        "excerpt":"This is the fourth post on a series of 5 about the improvement kata. If you haven’t read the beginning of the story, I recommend you start from part 1.   We ended the previous post with a target condition :      We’d like to be done with bugs within 3 months.    Currently, 1 pair (one quarter of the team) is constantly busy fixing bugs. If we manage to find a way to spend less time on bugs, we can expect a productivity increase of about 25%.      Next step in the improvement kata is to run Plan-Do-Check-Act (PDCA) experiments. Before you run experiments, you need ideas !   Idea 1 : Stop exploratory testing   We’d like to spend less time fixing bugs. At the same time, we know that we started to spend more time on bug fixing when we started exploratory testing.   We thought that one easy way to do less bug fixing was to stop exploratory testing altogether ! We listed the pros and the cons of exploratory testing.   The obvious cons      It discovers bugs. Fixing these bugs costs time. We’d rather spend this time delivering new features   But the pros are many      It helps us discover technical debt and improve the quality of our code, which makes us faster in the long run. Clean code has less bugs. When we discover many bugs related to a particular region of the code, it means we should refactor it.   It speeds up integration with other teams’ work in many ways :            It saves other teams from debugging their time away trying to use broken things       Fixing bugs is sometimes just a matter of providing clear ‘not supported yet’ errors. Doing this saves a tremendous amount of time to other teams.       Avoids blocking other teams as they wait for your bug fixes       Reduces interruptions from bug reports and fixes bouncing between teams.       By reducing unplanned rework, it makes any commitment you do more likely           In the light of all this, stoping exploratory testing did not seem like a great idea. We had to find something else.      💡Saving time by not fixing bugs might not be a great idea       Idea 2 : Review what a bug is   We needed to find a middle ground between where we were and stoping exploratory testing. We wanted :      not to let bugs escape   raise clean errors on things that are not yet supported   prevent scope creep from bug reports   A few months ago, when we decided to prioritize bugs before features, we had defined a definition for bugs. Here is what it looked like. Something is a bug if any one of these is true :      It used to work   It corrupts data   It silently returns wrong results   It returns an internal error (like an internal exception stack trace)   We examined this definition, discussed, and decided to dump the first clause. Our logic was that it was very difficult to know if something had already been working. The best thing we could refer to was our extensive suite of tests. If we discover that something is not working, it is very likely that it has never been. If it had been working, it means there was a hole in our test suite.      💡A hole in a test suite is a bug to fix    We decided to test this new bug definition for a while.   Not there yet !   We’re getting closer to the end, but you’re not done with PDCA yet.   This was post 4 in a series of 5 about the improvement kata. In the next and last post of this story you’ll learn how we ended the PDCAs for great results.  ","categories": ["agile","lean","continuous improvement","improvement kata","first-improvement-kata-series"],
        "tags": [],
        "url": "/how-we-used-the-improvement-kata-to-gain-25-percent-of-productivity-part-4/",
        "teaser": null
      },{
        "title": "How we used the improvement kata to gain 25% of productivity - Part 5",
        "excerpt":"This is the fifth (and last) post of a series of 5 about the improvement kata. If you haven’t read the beginning of the story, I recommend you start from part 1.   In the previous post, we decided to adjust our definition of a bug to limit the time lost on nice-to-have bug fixes.   It would take a while to know if adjusting the definition of a bug would help us or not. At the same time, we knew it would not help us to reduce the number of bugs we escaped to other teams.      Idea 3 : More exploratory testing   We decided to push on this matter as well. This means that we would be running two PDCAs (Plan-Do-Check-Act) at the same time. This is not the improvement kata procedure by the book. That could have been an error from our side, as first time users of the kata. Or maybe it’s a pragmatic habit to adapt the kata to real life … I guess we’ll know better as we apply the kata more often. The danger is that the different experiments can conflict with each other. When measuring the results, it becomes difficult to link observations with experiments. Anyway, back to our own situation, as you’ll see, it ended up well for us.   The first thing was to know a bit more about our bugs. Checking the recently closed bugs yielded suspicions about a recent features. Analyzing further proved our gut feeling.   Curve of how bugs were fixed on the last 2 months      Curve of the origin of bugs on the last 2 months      Ignoring the Christmas drop at the middle of the curve, we concluded 2 things from these graphs :      We were leaking bugs to the product   Bugs mostly came from newly added features      Despite all our automated tests and regular  exploratory testing, we were leaking bugs.    We decided to do more exploratory testing for a while ! We were already doing exploratory testing at the end of every story. We added an extra 1 hour team session of exploratory testing every sprint.  Do, Check &amp; Act   We used these new conventions for a few weeks. We did more exploratory testing, and would be more strict about what a bug was. We stuck to our prioritization : first improvements, then bugs and only after, stories.   After a few weeks of that, we were able to update our bug trend and do a linear regression on it again. Here were the results :      Hurray ! As you can see, we were to be done with bugs around April 2017, which was 3 months away at that time.      💡 Quality is free, but only for those willing to pay for it ! [Tom DeMarco in Peopleware]       We confidently adopted these practices as part of our working agreements. This brought our first improvement kata to its end.      💡 The improvement kata not only brings improvements, it also teaches you why they work.    3 months later   As you know, April 2017 is long gone. I can now give you a more up to date report of the actual effects on the team’s daily work.    First, the backlog does not contain bugs anymore. We payed the bug debt back. Second, we still discover some bugs from time to time, but a lot less than we used to. To summarize, there is now a pair of developers (25%) of the team that can work on user stories instead of fixing bugs.   As we are still fixing bugs as they appear, the 25% productivity gain claim might be an overstatement, but 20% is not. At the same time, less bugs are now escaping. This means that the whole organization is saving on interruptions and rework. 25% might not be such a bold claim after all !   This is it !   This was post 5 in a series of 5 about the improvement kata. I’m not completely done writing about this improvement kata though. In the coming weeks, I’ll post about the lessons learned and how to start your own improvement kata.  ","categories": ["agile","lean","continuous improvement","testing","improvement kata","first-improvement-kata-series"],
        "tags": [],
        "url": "/how-we-used-the-improvement-kata-to-gain-25-percent-of-productivity-part-5/",
        "teaser": null
      },{
        "title": "Lessons learned from running our first improvement kata",
        "excerpt":"During the past few weeks, I blogged the story of our first improvement kata.   Doing this first improvement kata taught us many lessons. We re-discovered best practices of the software industry. We also learned more general things about the improvement kata itself.      Rediscovered best practices   As we went through the kata, we ‘proved’ many known best practices. We did not have to believe them, we had data explaining that they worked. For example :      #NoEstimate works. Back in part 3 we saw that the story point and story count curves were very similar, almost identical   #NoBug policy was part of the solution to our problem   Doing more exploratory testing resulted in less bug fixing and increased our productivity. This proves principles like “Build quality in” and “Quality is free”   We also pushed the #NoBug policy further than it is usually done. We defined a clear definition for bugs that anyone could use. Doing so, we removed the product owner (or on-site customer) from the picture. Very often, the PO is the only one who can sort stories from bugs out. We created what Donald Reinertsen calls a distributed rule in the flow book. It increased empowerment, removes bottlenecks, while ensuring alignment.      The improvement kata   The first general lesson that we learned is that the improvement kata works !   At the beginning, we were very uneasy not to have perfect data. Remember how we had to resort to velocity as a proxy measure for productivity. In the end, that was not a severe problem. It still allowed us to understand what was going on.   We also learned that retrospective is not the only road to continuous improvement. In fact, the improvement kata and retrospectives are very different :      The time frame is different. A retrospective lasts for 1 or 2 hours and yield immediate results. An improvement kata is a background task that could take months in theory !   But the improvement kata also digs a lot deeper in the topic and brings true understanding. In our case, it fixed a problem that retrospectives where failing to address.   Ownership is also different. Retrospectives are a team activity. The improvement kata needs one or a few owners. It would be very difficult to align everyone on the same path of thoughts if we did it as a group activity.   Being a team activity, retrospectives have built-in alignment. The conclusions of the improvement kata must explained and agreed for a team to act on them. A good practice is to have regular (short) team meetings to share the current state of an improvement kata.   As the improvement kata is a more individual activity, it is more remote friendly. Team members can run the kata on their side, sharing everything through a wiki, or a blog for example.   Keep in mind that this was our first try at the kata. Some of our difficulties might disappear with a bit of practice !   What’s next ?   Hybrid continuous improvement   I clarified the differences between the improvement kata and retrospectives. That’s not the end of it. I’m sure a mixed format could be very interesting ! Start with a retrospective to collect the team’s problems, and vote on the more important. Add a corresponding improvement kata task to the backlog. Someone would then handle this improvement task, sharing with the team along the way.   This might be a great opportunity to reduce meeting time with shorter retrospectives.      💡 Reduce meeting time with mixed retrospective &amp; improvement kata    Data science for software   Going through this improvement kata made me realize something else. It is very difficult to get quality data to investigate. We had to resort to what was available all the way.   What’s striking is that we use software tool for all our work ! These tools have logs and could record usage data somewhere. Imagine all we could learn by mining all this data ! Our IDEs, CI servers, quality monitors, test tools, version control and project management tools should store everything we do at the same place !   With all this data, could we improve our estimations ? Could we find creative ways to be more productive ? Could we estimate the speed up that fixing some technical debt would bring ?      💡 What are we waiting to apply data science to the development process ?    As the saying goes, “The cobbler’s children go barefoot”. We are building such systems for others, but not for ourselves.   Hopefully, new tools like CodeScene are emerging to fill this gap. You can learn more about CodeScene on their website, or from the founder’s book. It analyses the version control history to find hot spots and other things in your code.      While we keep dreaming of such tool, I’ll continue to blog. Next week, I will write a short guide of how to run your first improvement kata.  ","categories": ["agile","lean","continuous improvement","testing","improvement kata","retrospectives"],
        "tags": [],
        "url": "/lessons-learned-from-running-our-first-improvement-kata/",
        "teaser": null
      },{
        "title": "How to run your first improvement kata",
        "excerpt":"The improvement kata can solve problems that typical retrospectives fail to address. Although there is a halo of mystery around it, it’s actually not that difficult to get started ! Here is a guide.   During the last few weeks, I’ve been blogging about the improvement kata. You can read the full story of the first time we applied it in our team to gain 25% of productivity. If you are more interested by what it taught us, check this other post.      A 5 bullets summary   Here is how I would explain what the improvement kata is :      It’s a continuous improvement technique. It relies on the scientific method to reach a target state.   It involves running experiments to know if your ideas are valid.   It can be long to run through, but it works on tricky situation where retrospectives don’t.   It’s 100% scientific. It uses data analysis and deduction. Not gut feeling and community best practices.   It can be part of the backlog, as any other item. It does not have to be a special event like retrospectives usually are.         💡 The improvement kata is 100% scientific, it uses data and deduction, not gut feeling and best practices    Let’s give it a try !    Is that enough for you to give it a try ? If so, great, read on ! If you need a bit more convincing, check the full story of how we gained 25% of productivity with it.   Here is how to get started :      The first thing is to read about it. If you have the time, Toyota Kata is a good read. If you read French, the “Petit guide de management lean à l’usage des équipes agiles” is a very pleasant and easy read. Finally if you want to cut it as short as possible, read the Toyota Kata website.   Pick a topic to try it on. Best candidates are clear and important problems. They might have emerged out of a retrospective for example. The scope should be small enough not to get lost.   Once you’ve identified a topic, someone or a pair should take ownership of the kata. It’s very unlikely that you’ll be able to do the full kata in one afternoon. Understanding happens when the brain is at rest, and experiments take time. The owners need to dedicate some time to follow up on the kata.        Repeatedly ask yourself the coaching kata questions. This will help you and your pair to stay on track.              What’s the target condition ? (Describe what are you trying to achieve)       What’s the actual condition ? (Describe your current situation)       What obstacles do you think are preventing you from reaching the target condition ? Which one are you addressing now ? (Describe the first problem you are about to try to fix)       What is your next step ? What do you expect ? (Describe the experiment you are going to run to test a solution)       When can we go and see what we have learned from taking that step ? (Run the experiment and learn from the results. Decide on a process change or repeat from an earlier step)           Going through the kata in pair is a great way to spread the practice within the team. At some point you might be able to run many improvement katas in parallel ! Just make sure not to walk on each other’s toes …   Expect the first time to be a bit rocky, and to feel lost from time to time …   Start today !      Many practices and techniques seem daunting at first. Remember the first time you wrote a test before the code. The first time you tried to program using only immutable data structures. Or the time you wrote your first “hello world” program !      💡 We can learn anything on our own by just doing it    The improvement kata is no different. Give it a go, and you’ll learn a powerful technique.   Whether you have already used the kata, you plan to use it or you have questions about it, I’d like to hear from you ! Leave a comment.  ","categories": ["agile","lean","continuous improvement","improvement kata"],
        "tags": [],
        "url": "/how-to-run-your-first-improvement-kata/",
        "teaser": null
      },{
        "title": "What Rails teaches us about building platforms and frameworks",
        "excerpt":"More than ever, the cheapest way to build a framework is to refactor it out of a specific app.   Rails is a web development server side framework built in Ruby. It’s been and remains very popular. It set the tone for all the MVC web frameworks that followed. People have ported it to many other languages. Rails is now in it’s 5th version, what is less known is how it was built in the first place.      The story of Rails   David Heinemeier Hansson, the creator of Rails, is a cult in the Ruby community. You can find the history of Rails at many places. If you have a bit of time, read this great article from Wired. If you don’t have a lot of time, here is a summary.   Jason Fried and DHH were working at 37signals. They were working on project management app for small businesses called Basecamp. After releasing the first version of Basecamp, DHH extracted and open sourced Rails out of it.   37signals later then re-used Rails to build other apps like Campfire, Highrise and Backpack.   We all know the end of the story, Rails adoption exploded. Many successful companies like Github, Airbnb, Twitter &amp; Shopify have used it. It evolved a lot, through the darwinism of open source. Today, Rails might not be the latest and coolest web framework, but it is still very productive and popular.      Economic sense   To summarize, here is how they did it :      Built a specific app, and monetized it   Extracted an open source framework from it   Built other apps on the framework   If you are not a programmers, you’re likely to assume that the cost of reusing software is negligible. If you are a programmer, I’ll ask you to assume that it is for a while.  Under this hypothesis, all they did makes a lot of business sense :      Building a specific app first is the fastest path to paying customers. Building a framework first would need more work.   Once you have paying customers, you’ve got money to fuel further work.   In particular, you have money to finance the extraction of an open source framework.   Putting the framework open source increases its reusability through bug reports and contributions.   Building the next app is even easier thanks to a mature framework and the revenues from the original app      💡 Once you have paying customers, you’ve got money to fuel further work.    How can it work ?      Ok, enough for common sense. Let’s see what happens when you take into account the real cost of changing software ? All software developers know that changing software is far from cheap. Very often, it’s more expensive to adapt than to rebuild   So how did the Rails guys manage it ? Rails has 3 specificities that explain that :      Rails has automated testing built in and out   Basecamp guys are agile with a lower ‘a’. They don’t follow Scrum or any method. Read their books and you’ll understand how agile they are. They follow principles like KISS, YAGNI, Lean startup discovery …   Finally, it was open sourced !   Agile principles and technical practices is what allowed them to take the common-sense path.      💡 Thanks to agile practices, the cheapest way to build a framework is to extract it from a specific app.    Open question   If it is possible to do the thing that makes the most economic sense, why isn’t it the default way ? Why are so many of us still losing a ton of money writing large frameworks and platforms up-front ? How could we get large companies to adopt the more nimble ‘basecamp’ way ?   If you have remarks, answers or different point of views, I’d love to read your comment !  ","categories": ["agile","rails","architecture","testing"],
        "tags": [],
        "url": "/what-rails-teaches-us-about-building-platforms-and-frameworks/",
        "teaser": null
      },{
        "title": "Mobbing around the world for 48h ... who's in ?",
        "excerpt":"In Far From the mobbing crowd the Cucumber guys explain how they combined mob programming and remote work. Matt and Steve also explain that a mob is both resilient and fast. They explain that they sometimes have to leave the mob for 15 minutes or so. When they come back the code has changed a lot ! It’s as if the mob continues no matter who’s in.   One of them then suggests an idea.       With enough people around the world, we could run a mob that never stops!       What’s the point ?   In The Ultimate Guide To Remote Work the Zapier team explains how to take advantage of time zones and remote work. We can get a feature done faster by having a pair of programmers from different time zones work on it ! Instead of a single day of work in 24h, you get 2 ! We could get features in half the time.      In Lean Management, the time to get a feature is called the cycle time. A short cycle time has a lot of benefits. You get feedback faster. The needs of the users have less risk of changing. You’ll get less interruptions, which increases your focus and effectiveness.   Zapier found that time zones and remote work can divide the cycle time by 2. This is already a major improvement. Let’s replace this pair of programmers by a mob ! The cucumber guys explain that 4 hours of mobbing is about the most someone can do in a single day. With a mob of 5 people at any time, that’s around :   24/4 * 5 = 30   30 developers working on the same feature, full time !   What would be the effect of this on cycle time ? I don’t expect it to cut the cycle time by 30, but from my experience with pairing, I’d still expect a drastic reduction. Imagine a complex feature that you estimated to 30 days to finish. Could it be done in less than a week ?   Working on a single story at a time, we could forget merges and complex CIs ! More generally, it would lead to an important Work In Progress (WIP) reduction. Very related to cycle time, reducing the WIP also brings a lot of improvements. One of the most important is an increased throughput. Here is a great article detailing the benefits of reducing the WIP.      💡 Mob programming reduces WIP and cycle time    And what about the team structure ? Could a team of 30 be effective in this setting ? 7 is the magic count for a team size. If we could grow past this number, we could imagine completely new org charts :      More decentralized   More resilient    Less synchronization and coordination overhead      Let’s try it !   As any new idea, it’s also very likely to fail miserably … The communication overhead could turn out overwhelming. The mob might not be resilient to frequent changes after all. It could work but not bring any reduction to cycle time. There is a ton of reasons for this to fail.   The best way to find out is to try !      💡 Stupid ideas that work become great ideas. [XP moto]    We just need to find enough people spread around the world to give it a try. Could we do 48 hours straight of mobbing and go twice around the globe ?   We would need people distributed across all time zones. Pairing and incremental design would be de-facto practices. It would work better with people used to XP.   We’d need at least a mob of 3 doing 5h of mobbing per day for 2 days. That would be :   24/5*3 = 15   We could go with 15 people but 30 would be ideal. Once we have enough volunteers, we’ll have to agree on a few other things :      A date for the experiment   A subject we can all understand   Technologies and tools to use   A time-schedule   There’s nothing impossible here !   Call for volunteers   If you want to participate, leave a comment below or contact me through twitter. I’ll keep you updated about the status of the experiment.   One last thing … as you’ve guessed this is an experiment, not paid work. Anything we would produce will be open sourced on Github.  ","categories": ["agile","extreme programming","pair programming","remote"],
        "tags": [],
        "url": "/mobbing-around-the-world-for-48h-dot-dot-dot-whos-in/",
        "teaser": null
      },{
        "title": "Are Software Developers Overworked or Undecided ?",
        "excerpt":"During the last 15 years, I’ve worked in many different teams. Every team has its own way of working. I’ve been at places where everyone seemed busy all the time. Did we deliver more ? No. Often quite the opposite ! The most productive teams I’ve been in used to do no more than 6 hours of solid work every day …      What’s the reason behind that ?   In Lean Management theory, lead time is the time it takes to get a feature from customer request to delivery. Here is a graph that draws typical lead time over factory load (1 - idle time percentage).      What does all this tell us ?    When utilization becomes too high, people start multitasking. As multitasking increases, productivity drops, delaying projects. As a consequence, products are late to hit the market. On the other side, a low utilization means paying people to be idle. That might put any company out of business. Factory managers’ best practice is to limit Work In Progress by loading their assembly lines around 80%. Not too high to avoid over-utilization, but not too low to avoid wasting time.   Every software product is new product development (Craig Larman). As such, they are way less predictable than any assembly line. It means we need more slack to absorb unexpected variations (Donald G. Reinertsen). This tells us that we should have more than 20% of slack in our schedules and plannings. Unfortunately, overloading and multitasking is a wide spread disease in the software world.   How to get rid of task switching ?   The answer is obvious : prioritization ! Only work on the most important item at a time. It’s not because we start everything now that they’ll sell them earlier ! It’s quite the opposite in fact !      Here is another advantage to prioritization. It decreases the time between start and completion of a feature. This, in turn, decreases the likelihood of building something obsolete.   One last thing about prioritization …              “Black Swan Farming Using Cost of Delay” Joshua J.Arnold and Özlem Yüce        This graph tells us that a small fraction of the features bring most of the value, while a very large portion of the features bring almost no value … As puzzling as this graph may be, it’s also a wonderful opportunity. If we manage to focus one the most valuable features only, we can save a ton of work, while being more effective ! Lean startup and market research are techniques to identify these features up-front.      💡 You don’t need more time, you just need to decide ! Seth Godin    Culture issue   Becoming more effective by banning overtime, enforcing monotasking and being serious about prioritizing ? These ideas are heresy in some workplaces ! Have a look around you. Are people overworked ? Are they fire fighting all the time ? Are they drowning in multitasking ? If it’s the case, I bet there’s a wide scale lack of prioritization issue at your workplace.   People at the top of the company need to control the overall WIP. If they don’t, multitasking will catch you up, however you try to escape it. Multitasking can be obvious e.g., when we need to work on different projects at the same time. It can also sneak in as maintenance, or unrelated meetings marathons. Another symptom is when developers need to take on “submarine” refactoring initiatives. From experience, that’s often the case in sales driven companies. Where any customer request is agreed and sold, regardless of the company’s capacity.      💡 Require​ ​everyone​ ​in​ ​product​ ​and​ ​engineering​ ​to​ ​read​ ​​Principles​ ​of​ ​Product​ ​Development​ ​Flow by Donald Reinertsen. John Cutler    There’s an interesting parallel in the Stephen R. Covey’s 7 Habits of Highly Effective People. He says we should drop the urgent and not important for the important and not urgent. In software term, it means spending time on important and long term work rather than short term stuff. We should do more refactoring, hiring, training and continuous improvement. We should stoping building features we are not sure of the value.   Let’s summarize. Here are my advices to work less and be more productive. Enforce hard WIP limits at all levels. Do serious prioritization. Provide enough slack. In my next post, I’ll show how XP addresses these issues.  ","categories": ["agile","lean"],
        "tags": [],
        "url": "/are-software-developers-overworked-or-undecided/",
        "teaser": null
      },{
        "title": "Sustainable productivity in eXtreme Programming",
        "excerpt":"eXtreme programming will not improve your short term productivity. But it will drastically improve your long term productivity.   My last post detailed how prioritization and Work In Progress limits are the keys to less work and higher productivity.      XP is an experiment in answer to the question, “How would you program if you had enough time?” Kent Beck               From “Extreme Programming Explained: Embrace Change” by Kent Beck        The best jam is made in old jars   As I was writing the post, I could not stop thinking :      Once again, XP has been addressing these WIP and prioritization issues for 20 years now !    Let’s see exactly how.   WIP limitation   XP has a drastic way to reduce WIP by 2 : pair programming. Instead of 6 developers taking on 6 stories at the same time, they only tackle 3. I won’t dive into the ton of other advantages to pair programming here. If you want to learn more, these blog posts might help.   Although it is now seen as a Scrum practice, Planning Game was part of original XP. Sticking to a basic planning game will give room for slack at the end of iterations. The trick is to leave the infamous focus factor away.      💡 A simple planning game will give room for slack.    Suppose a team member takes vacations for a sprint. The team should deliver less user stories that sprint. They’ll have less story points to schedule on next sprint. This leaves room for slack and long term improvements such as refactoring, learning … If half the team goes on holiday, that’s a different story, adapt your schedule with gut feeling.   A funny aspect of this is that XP teams are now the most likely to switch to the more extreme #NoEstimates.   Priorities   XP’s on-site customer is a straightforward way to work only on the most valuable features. Who better than an end user would be able to make this estimation ?   The YAGNI (You Ain’t Gonna Need It) and simple design principles avoid building unnecessary features. TDD’s strict point of not writing code before a failing test prevents over-engineering.   Modern XP     In this wonderful talk from Rachel C. Davies, she presents how her team has been improving XP for 15 years. She explains how they organize as 2 mobs and 1 solo. At any moment, one mob is working on stories. The other mob works on important long term technical improvements. The solo does some learning. This is an even greater WIP reduction technique than pair programming.   Having a dev doing learning all the time is a form of continuous slack. If the team feels that they cannot deliver something by an important date, he can join the mob to help them.   There’s even more to this learning. It can be about anything. Developers work on their technical skills, but they also learn about the domain. Little by little this turns them into domain experts. Guess what : domain experts are great at optimizing value !      💡 With time and efforts, developers can become domain experts !    Afterthoughts   Unfortunately, there is no easy way to prove the long term productivity of XP. We cannot split the world to run the same project with and without XP. Another difficulty is that after a years of XP, work remains smooth and sustainable. Nothing like crunch mode. People used to the hero culture are hard to convince that they can be more productive by doing less.   Hopefully, lean theory and gut feeling of programmers who have switched to XP can back up my claims.  ","categories": ["agile","lean","extreme programming"],
        "tags": [],
        "url": "/sustainable-productivity-in-extreme-programming/",
        "teaser": null
      },{
        "title": "Why and how to start a #ZeroBugs policy - part 1",
        "excerpt":"Some teams spend 95% of their development time fixing bugs … An entrepreneur I worked with reported an even scarier story. He went bankrupt because bugs were taking all the development time. He had no time left for new features !   At the other end of the spectrum, some eXtreme Programming teams reported bug rates as low as 1.5 per month !      💡 Some XP teams reported bug rates as low as 1.5 per month !    Wouldn’t it be great for everyone if we had less bugs to fix ? Programmers would do more interesting work. Business people would get more valuable features. People would do less overtime. All this while users get a better product !      Last year, in my team, we reached and maintained a #ZeroBugs state. Here is how we did it.   What’s the #ZeroBugs policy   Before we jump into our story, let’s take a second to understand what this is all about.   Bugs waste time. They waste the time of users, who can’t do what they need. They waste everyone’s time with interruptions from unhappy user. They steal time from new features development. If you have a large number of known bugs, you’ll also waste some time organizing them : eliminating duplicates, prioritizing them, ignoring others …   A #ZeroBugs policy is the convention of having no bugs in your backlog. This means that when someone finds a defect, it should be immediately :      fixed before any other feature   prioritized as a new story   or ignored (for the moment)   Obviously, the less bug you create the easier this will be.   Our journey to a #ZeroBugs policy   Context   We were an R&amp;D team, building a data management system. Our product was moving outside of pure research and development. Other teams were starting to use it and often reported bugs back to us.   We’d first read about #ZeroBugs in The Art of Agile Software Development. Given enough automated testing, XP teams could almost stop doing bug fixing !      As we had very solid automated testing in place, we decided to take the extra steps to save time on bug fixing.   First attempt   There is a straightforward way to install a #ZeroBugs policy. Make sure to have a customer or a Product Owner to prioritize defects as soon as they appear (check this story for an example).   Unfortunately, this would not work for us. Our Product Owner was great at providing us with a long term vision. He could not be available for day to day bug prioritization though.   Our first try was to propose a definition for bugs that we could use ourselves to classify them. We decided that something was a bug if either of the following was true :      It corrupts data.   It returns wrong results. As we were building a data management system, this was rather easy to verify.   It used to work but is now broken   Whenever we found an issue meeting any of these points, we would start working on it straightaway.      💡 Exploratory testing goes hand in hand with #ZeroBugs policy    To speed up downstream integration of our product by other teams, we also started Exploratory Testing. Exploratory testing is a practice that goes hand in hand with a #ZeroBugs policy.   Unearthing a bug debt   During the first exploratory testing session, we found more than 10 bugs. We thought that with a bit of focus we could fix all lingering bugs in a few weeks max … After 2 other sessions and more than one sprint of intense bug fixing, we had even unearthed more bugs. We were wrong, this would take a lot more than a few weeks.   If we kept on our 1 hour exploratory testing sessions every sprint, we would fill our backlog with bugs. We would also be fixing bugs for a few months before we could tackle any new feature !   We agreed on the following strategy to fix all these bugs little by little :      stop these intense exploratory testing sessions   do shorter exploratory testing sessions at the end of every story   fix 5 to 6 bugs every sprint   We hoped to fix all our bugs in a few months.   Next week   That was the first part of a series of posts about #ZeroBugs policy. In the next post, I’ll explain how we dealt with this bug debt and dig into the consequences it had on our work.  ","categories": ["continuous improvement","quality","zero-bugs-series"],
        "tags": [],
        "url": "/why-and-how-to-start-a-number-zerobugs-policy-part-1/",
        "teaser": null
      },{
        "title": "Why and how to start a #ZeroBugs policy - part 2",
        "excerpt":"This is the second post of a series about the #ZeroBugs policy. In the previous post, I detailed what a #ZeroBugs policy is and how we tried to apply it. I encourage you start there.   Here is a quick summary of where we stood. We had tried to apply a #ZeroBugs policy with exploratory testing. We had discovered a lot of bugs, and we had agreed to tackle them little by little. We hoped to finish in a few months.      The final bug rules   At about the same time, we started an improvement kata to increase our productivity. I wrote in details about the full story, but to make it short, here is what we discovered :      It would take us years, not months to fix all our bugs   Our definition for bugs was part of the problem   After some experimentation, here is the bug definition we came up with :      It corrupts data   It returns wrong results   It crashes with an unexpected error message   It stopped scope creep by removing the subjective “it used to work” rule. This rule also sped up bug fixing a lot. We closed many bugs with descriptive error messages.   Consequences   Within 2 sprints, we reached a #ZeroBugs state. Backlog ‘management’ was simpler as it did not contain bugs anymore. We were also spending less time fixing bugs and more building stories. Even as we resumed our intense exploratory testing sessions, our bug backlog did not grow.   After a while, we discovered other benefits to the #ZeroBugs policy.   Spending less time on bugs made our velocity more stable. Our product owner was happier to see us deliver new features more reliably. In the end, this improved our relationship with him.       💡 #ZeroBugs policy makes developers happier    It also made the team happier. First, because we had a lot less of stressful emergencies to answer to. Second, because programming new features is more interesting than fixing bugs.      It also made the other teams happier and more productive ! They encountered a lot less bugs when using our product. When they had problems, they usually got a clear error message. They would fix it on their side, or suggest us a new feature.   A #ZeroBugs policy and exploratory testing also have a side effect on technical debt ! Bugs tend to appear in the more brittle parts of the code. When fixing bugs, you took the opportunity to refactor technical debt at the same time.      💡 #ZeroBugs policy reduces technical debt    Finally, having a definition for bugs was very helpful. It allowed decentralized prioritization of bugs. The alternative to that is to have a domain expert available to sort bugs from stories. A decentralized rule prevents all sort of coordination and people issues. Our system was rather technical, which made it easy to define what a bug is. In other cases, it might be worth spending some time with the domain expert to define similar rules.      “Use decision rules to decentralize economic control.” From Principles of Product Development Flow by Donald G. Reinertsen    To summarize, sticking to #ZeroBugs is a win-win and sustainable situation for all.   Next week   This was part 2 of a series of posts about the #ZeroBugs policy. In next week’s post, I’ll provide a step by step guide about how to inject a #ZeroBugs policy in your team.  ","categories": ["continuous improvement","quality","zero-bugs-series"],
        "tags": [],
        "url": "/why-and-how-to-start-a-number-zerobugs-policy-part-2/",
        "teaser": null
      },{
        "title": "Why and how to start a #ZeroBugs policy - part 3",
        "excerpt":"This is the third post of a series about the #ZeroBugs policy. In the first 2 posts, I detailed how we applied it in our team, and what were the consequences.   I hope that by now, I have convinced you that applying a #ZeroBugs policy is good for you and your team. I’ll surprise you by arguing that it’s a lot easier than people first think. This post will be about how to inject it in your team.      Build quality in   Obviously, you need a low bug rate for this work. If your team creates too many bugs, fixing incoming bugs only will consume all your time. You won’t be able to deliver new features and it will make everyone miserable.   If you are in this situation, I’d advice you to start investing in coding best practices such as :      Continuous Integration   Test Driven Development   Code reviews   Pair programming   Refactoring   With grit and time, your bug rate will get down.   New bugs   Before dealing with the old bugs, you should first put in place the target policy for new bugs. Don’t let the situation get worse !   The way to deal with new bugs is to setup a regular (at least weekly) routine to decide what to do about them :      If it’s causing real pain to someone and needs a fix as soon as possible, then it’s very likely to be a bug   If it’s something important you can still live without for a few sprints, change it to a story in the backlog.   If it’s not that important now, delete or archive it as “won’t fix”   From then on, the new convention is fix all bugs before working on features. This ensures that the bugs backlog will not grow anymore. If you need more details about how to categorize issues, take a look at what Sam Hatoum says about that.   Different people will provide different perspectives on issues. Business people will know their business value. Developers will know fix’s impact on technical debt. Finally, service desk people will know how much time they would save with the fix. Depending on your situation, involve the right people in the classification.   Another approach is to do as we did, and come up with clear rules that define what a bug is. This has the advantage of allowing developers to categorize issues on their own. The drawback is that it can be very tricky to come up with these rules.      💡 Agree on rules for categorizing bugs with your users to streamline your flow    Old bugs   Ok, that was the easy part. How can we deal with the zillions bugs we all have waiting in the backlog ? Depending on the size of your bug backlog, you should adopt different strategies.   If you have a rather small bug debt, as we did, you can get away by prioritizing X bugs every sprint.   Let’s suppose you have a larger bug debt. You could do as ConceptShare did and crank out a feature team dedicated to eliminate bugs. That should work, but it will take some time. It took the ConceptShare teams 18 months to get rid of a 350 bugs backlog !   It’s a good idea to take a look at all existing bugs to re-classify some as stories and delete others. Just as you should now be doing with new bugs. This should drastically reduce the number of bugs, but it will take some time from very busy people … Again, rules to define what a bug is would make this easier.   Finally, there’s a last, very effective solution : archive all the bugs ! Some bugs must be months if not years old. Deleting them should not do much harm. Users will report important bugs again anyway. One caveat though : this won’t work if your bug rate is too high ! If quality is too low, you’ll drawn under old bug.   Going further      It’s great to fix the bugs very fast. It’s even better if to fix them before the users see them ! Exploratory testing is the way to do that. It will also provide you a measure of how many bugs escape your development process. I encourage you to try this very interesting practice. I wrote a guide about how to start exploratory testing.      💡 Exploratory Testing gives you a quantifiable measure of your workflow    Next week   This was part 3 of a series of posts about the #ZeroBugs policy. In next week’s post, I’ll provide arguments and advices to convince the people around you to give it a try.  ","categories": ["continuous improvement","quality","zero-bugs-series","how to guide"],
        "tags": [],
        "url": "/why-and-how-to-start-a-number-zerobugs-policy-part-3/",
        "teaser": null
      },{
        "title": "Why and how to start a #ZeroBugs policy - part 4",
        "excerpt":"This is the forth and last post of a series about the #ZeroBugs policy. In the previous posts, I detailed how we applied it in our team, what were the consequences and how you could do the same.   There’s a catch though, even if you are sure that #ZeroBugs is a good thing, people around you might not agree ! As with any kind of change, one of the trickiest thing is overcoming people’s fear about your idea. This post will be about dealing with people’s resistance.      A few general change advices   With time, I discovered general best practices to bring change. The first is that you are much more likely to get your ideas through if you are nice with people ! In a bit more details, this means that :      Try to help people, for real !   Listen to their concerns instead of pushing your idea   Don’t blame people if they don’t see the value of your idea      💡 Change agent’s advice #1 : be nice with people    Another invaluable trick is to be patient : change takes time. Finally, if you are stuck, suggest testing your idea for a while, people are more likely to adopt it this way.      Winning the devs over   Back to our particular #ZeroBugs issue. In this particular case, I would  try to convince the devs first. Reason A is that without them onboard, the practice won’t work as well as expected. Reason B is that it is more difficult for management to say ‘no’ to a full team than to a single developer.   Developers will fear spending too much time fixing bugs. Sell them long term fixes instead of dirty quick fixes. Stress that it will be a chance to refactor important, poorly designed parts of the system. Also explain that once the bug backlog is clean, they’ll spend more time on new developments.   A last argument for developers is about clarifying prioritization. Pushing bug triage to business people will save devs from feature creep. That should save them from some stress of not delivering.   Winning business people over   They’ll fear that fixing bugs will reduce the delivery of valuable features. Indeed, if done without care, this is a real danger. Highlight the value of bug fixing : it improves the product and image of the company. Fixing bugs also increases feature delivery in the long run. It does so by removing some recurring support issues and fixing technical debt.   Also point out that they’ll have more visibility and control over what is being worked on by the team. This is an opportunity for them to increase the value throughput.   They might also have concerns about the time to spend categorizing bugs. Make sure to get this as streamlined as possible. For example, if they are very busy, you could pre-categorize the issues to make it easier for them.   In this blog post, Andrew Fulton explains how he convinced his boss to adopt a #ZeroBugs policy.   Last words      💡 #ZeroBugs policy makes work more sustainable    I’m done with this series about #ZeroBugs policy. I hope I convinced you to give it a try. The developer’s life is better without bugs ! If you do, or if you already did, I’d love to read about it. Thanks a lot for reading me so far.  ","categories": ["continuous improvement","quality","zero-bugs-series","how to guide","psychology"],
        "tags": [],
        "url": "/why-and-how-to-start-a-number-zerobugs-policy-part-4/",
        "teaser": null
      },{
        "title": "Why Agile transformations usually don't work - part 1 - The situation",
        "excerpt":"In Aesop’s famous tale, a farmer kills his goose that lays a golden egg every day to get all the eggs at once. He finds no eggs, lost his goose, and remains poor ever after.   I’ve seen the same pattern happen many times in the software industry. A team decides to become more agile. After a few years of continuous improvement, it shows largely above average performances. Managers start to notice and ask how the team got there, to replicate this success. Unfortunately, they don’t want to wait a few years for the other teams to find their own way. As a result, an “Agile” process is copy printed to all teams. Follows only ephemeral improvement, at the same time that the initial team gets drowned in a company “Scaled Agile” initiative …      Examples   In the 60’s, Toyota was innovating new ways to build cars. Other car manufacturers used to visit the Toyota plants to understand how they did it. Toyota was welcoming them with open arms ! (You can still visit Toyota plants today) Fortunately for Toyota, none of the visitors managed to copy their results.      💡 Competitors managed to copy Toyota’s practices, but not their results !    Let’s think of other agile companies like Google, Apple or Spotify. Did they copy an existing Agile framework or recipe ? No, they rather learned to embrace change in their own way. The HP LaserJet team did not become more agile by sticking to best practices either, they also did it their own way.      Thinking about it, I never read a single story of how a team succeeded by sticking to method X, Y or Z.   It’s just not possible to be agile by strict adherence to a method. Unfortunately, that’s what most companies do. Have a look at the job posts, everyone is looking for agile devs, but do these companies look agile once you are in ?   What are the actual results ?   With a move to “Agile”, management usually envisions some of the following benefits :      better quality   increased productivity through waste reduction   more innovation   People who want these changes fast don’t understand what they are doing. If they were, they would not expect this to be fast. It’s all too easy to see “Agile” as a method, or a process.   Following the traditional “machine” organization metaphor, processes are to be applied ! Deploying the new “Agile” process, leaves little room for self-organization and empowerment. People throughout the organization perceive it as yet another reorganization. People still feel like cogs in the machine. Cogs don’t show a lot of autonomy and motivation ! That’s when the vicious circle kicks in. To fix the lack of drive, management adds more top-down process !   Long story short, these transformations bring very little improvements. At the beginning, when everyone is keeping a close look at the process, things will go a bit better. A few month later, things will get back to where they were.            Lean Enterprise - Enabling Innovative Culture  de ThoughtWorks    A specific danger of “Agile” is that it can serve micro-management. Short iterations, code reviews and other feedback practices puts everyone under the radar. This makes decision makers feel more in control, at the cost of motivation and innovation. As an example, check this post from a pair programming sceptic.      💡 “Agile” can easily slip into micro-management    Next Week’s post   This was part 1 of a series of posts on making large organizations more agile. Next post will dig into why the traditional approach described above does not work. Further posts will propose what to do.  ","categories": ["agile","coaching","large scale","agile-transformations-fail-series"],
        "tags": [],
        "url": "/why-agile-transformations-usually-dont-work-part-1-the-situation/",
        "teaser": null
      },{
        "title": "Why Agile transformations usually don't work - part 2 - The problem",
        "excerpt":"This is the second post in a series about making large organizations more agile. I encourage you to start with the beginning.   Let’s summarize the previous post. I illustrated the typical disappointment with large scale “Agile” transformations. At best they bring a small short term productivity improvement. At worst, they turn into micro-management.   Why doesn’t it work ?   An organization is agile if it is able to respond and take advantage of changes fast. An organization where decisions need to go up and down the ladder will be slow. To thrive in this complex world, everyone needs to be able to take faster and bigger decisions. To do so, people need the good data, but also the right to fail, and the chance to learn new ways of working.      People with the right authority could make data available fast enough. Learning new habits and convincing every one of their right to fail takes time though.   A safe environment   In “Turn the shop around” Navy captain David Marquet explains how he empowered his crew. They would take decisions and inform him with “I intend to …”. Doing that, he also took on him the responsibility of his subordinates errors !      There are always people who put the organization’s interests before theirs. They will try new experiments, even if could backfire in their faces, because it’s the right thing to do. Unfortunately, these people are pretty rare. The vast majority will not try things that would get them troubles. They need a safe environment to unleash autonomy.   The typical adoption technique is to “fake it until you make it”. Apply the practices until the principles and values soak in everyone’s mind. There’s a catch though ! People need great models. People buy-in values, not actions. Think of Apple and other technology brands. People buy Apple hardware because they “Think Different” not for the products ! (Though I admit I love their products too :-))   Although people buy-in values, they measure them through actions. Leaders need to walk their talk for people to trust them.      💡 People buy-in values, but they measure them through actions.    Very often, leaders stick to actions that communicate that it is not ok to fail. Like compensation by objectives for example. Or highlighting the commitment of sprints. Praising overwork is another one !   Place to learn   Real learning comes from experimentation.      💡 “Never help a child with a task at which he feels he can succeed.” Maria Montessori       This is still true for adults at work. Every time management pushes top-down incentives to speed up change, general understanding suffers. There is a limit to what an organization can withstand before it falls prey to cargo cult.   Leaders must provide a safe environment. They must also provide the conditions for the teams to learn and improve their unique way of working.   Too often, leaders are in a hurry. “Why should we waste our time rediscovering what others have already understood ? Let’s apply Scrum (or SAFe or LeSS) !”   Did you ever learn something by copying the results of others ?   Let’s look at SAFe for example. It’s a collection of proven best practices. That’s only half of the story though. What’s not clear is how a practice is proven to work ? By definition, a team had success with it before adding it to SAFe. It can only be the result of experimentation and learning, as it was not yet part of SAFe !   In the end   Many leaders embark their organization in an Agile transformation for some business benefits. Unfortunately, people often perceive that “It’s just the latest way to squeeze a bit more out of us all !”.   It does not have to be that way. What if they followed massive decentralization of power. What if they made more data available ? What if they adapted the compensation scheme to encourage risk taking. What if they gave authority to their teams to experiment and learn from their own problems. Would people still feel it’s just another re-org ? As a reference, see how unions have been fighting lean manufacturing for decades.   That’s what we’ll look into the next posts. This was part 2 of a series of posts on making large organizations more agile. Further posts will propose what to do.  ","categories": ["agile","coaching","large scale","agile-transformations-fail-series"],
        "tags": [],
        "url": "/why-agile-transformations-usually-dont-work-part-2-the-problem/",
        "teaser": null
      },{
        "title": "Why Agile transformations usually don't work - part 3 - What to do",
        "excerpt":"This is the third post in a series about making large organizations more agile. I encourage you to start with the beginning.   Let’s summarize the previous posts. Large scale “Agile” transformation programs usually fall short of their promise. One reason is that leaders forget to provide a safe context to learn.      What should we do ?   First, we should take the time ! Rome wasn’t built in a day. The road to an agile organization is a never ending path. We should also remember what we are after :      Super fast adaptation to business changes through massive decentralization   Low process overhead thanks to people’s buy-in   Continuous innovation in both products and organization, relying on everyone’s creativity   This is far more ambitious than a mere 5% productivity increase !   Be patient for people to learn   The goal is for people to be able to take bigger decisions on the spot. No one can learn this in a few hours. All over the company, people will have to improve their problem solving skills. They might also need to learn how to do root cause analysis. They’ll need to take into account the point of views of other people in the organization. Some will need to beef up their communication skills as well. All will need to master new collaboration practices to stay up to date with what is going on.      💡 An organization cannot be agile if all decisions have to go up and down the management ladder.    That’s a lot, and it takes time. The visible part of the iceberg, the collaboration practices, can be setup in a few weeks. But it won’t yield much until the people are bold enough to take decisions by themselves.   First, I need to have a word about the workload in this section. If people are Busy to Death, They won’t have the time to learn. They won’t even have the mood to think. Again, lasting productivity goes through sustainable pace. Slack time or a 20% policy are very good investments here !   Communication skills alone take months or years to improve. A communication training campaign can help to make the company ready to be more agile. In a previous team, new joiners used to attend a Non-Violent Communication training. Being transparent a constructive is a foundation of an agile organization.      Finally, root cause analysis and problem solving skills take practice to get around. Some organizations, used to carrots and sticks, have been avoiding this for years ! Good coaches can teach this to teams. People first need to learn that it’s OK to look into problems and to experiment solutions though. That’s where a safe environment helps.   The Safe environment enhancer   If people remain afraid to fail, they won’t do enough experiments. As a result, they’ll learn slower. They’ll also keep delegating more decisions ! The organization won’t become very agile. Sure, leaders will get small productivity and predictability improvements. Unfortunately, exclusive focus on these puts innovation in danger.   People should have the means to experiment. By far the greatest experimentation killer is carrots and stick management. If people live in fear of losing their job, we have no chance of becoming more agile.      💡 Encouraging experimentation while sticking to compensation by objectives is schizophrenic.    The situation is only slightly better with management by objectives. One of the reason is that as people start to expect their bonus, they also start to fear not having it next year ! We’re back at the fear problem. There’s a more direct issue with MBO though. A good experiment should be ambitious and have a high likelihood of failure. Unfortunately, people will negotiate unambitious objectives, to secure their bonus. In the end, people do far too few bold experiments and don’t raise problems to safeguard their bonuses. Eventually, both people and the organization fail to learn.      Instead of using MBO, we need to find a different way to deal with compensation. The management 3.0 literature has plenty of ideas for that. For example :      Salary formula. Some companies, like Buffer use a transparent formula to compute everyone’s compensation. This base salary is independent from any subjective performance evaluation. On top boosting motivation, it provides plenty of room for experimentation.      After disclosing employee salaries, Buffer was inundated with resumes       Badge-based compensation. Holocracy describes how to take valuable skills into account in a salary formula. To get a badge on a topic, you need to be recognized by the people who already own this badge. Examples of badges can be “UX Expert” or “Senior Software Engineer”.   Merit money. Company profits redistribution to employees should be both motivating and fair. Instead of typical yearly bonuses, merit money is a crowd-sourced money allocation scheme. Everyone regularly receives a fixed amount of monkey money. You should give some back to a colleague who helped you. You could also give some to acknowledge someone else’s great job. Every month a lottery is played to know if employees should receive their share of profits. If so, everyone gets his share in proportion with the monkey money they received. Otherwise, profits accumulate until the next lottery.   We could worry that dropping MBO will drop people’s motivation as well. It turns out it’s the exact opposite ! Check out this talk by Daniel Pinks to understand how bonuses kill motivation.     To be continued   This was part 3 of a series of posts on making large organizations more agile. We’re almost at the end. The last post will dig into how to provide the data to everyone.  ","categories": ["agile","coaching","large scale","management30","agile-transformations-fail-series"],
        "tags": [],
        "url": "/why-agile-transformations-usually-dont-work-part-3-what-to-do/",
        "teaser": null
      },{
        "title": "Why Agile transformations usually don't work - part 4 - Transparency",
        "excerpt":"This is the fourth and last post in a series about making large organizations more agile. I encourage you to start with the beginning.   In the previous posts, I presented why large scale “Agile” transformation usually fail. Part of the solution is about giving people the time to learn and a safe environment. That’s not enough though.      Provide the data   I explained how Management by Objectives harms experimentation. I presented alternatives like Salary Formula, Badge Based Compensation and Merit Money.   An interesting commonality between these compensation practices is that they are transparent. When all this is public, people know what to do if they want to increase their income. For example : get a badge by learning something new, or collaborate with others to get monkey money. That’s great, but they should also know how to increase the profits to get a bigger cake for everyone !   Management By Objectives is not only about compensation. It also cascades business objectives all the way down to everyone in the company. If we get rid of MBO, people still need to know what is important.   Let me introduce Objectives and Key Results, aka OKRs. Intel invented OKRs. Nowadays, among many other companies, Google is famous for using them.         💡 OKRs have nothing to do with compensation !    OKRs match this experimentation mindset. OKR objectives should be ambitious, with room for overachievement. OKRs also provide Key Results, which let people measure how well they are doing ! As OKRs are public, teams get a full understanding of the context of their actions.   To max out the value of OKRs, we should make as much data public as possible. Financial figures can really help teams to know if they are doing a good job.   The end result is that teams can take informed decisions. They can measure the impact of their work at their scale, but also on the full organization.   Compelling mission   A last good point of OKRs is that they make very good higher level backlog items. Both Team Epics and full products can be OKRs. Even the company’s mission can be an OKR. Check Toyota’s mission, for example, “Better cars for more people”. This provides invaluable guidance for everyone to take day to day decisions.   In fact, a clear and motivating company mission statement is not only good but mandatory. Without it, people will go in all direction, resulting in very slow progress. It’s an absolute pre-requisite for self organization to work. This is particularly a challenge for business-to-business companies. One symptom is when developers succumb to Resume Driven Development.      💡 Self organization does not work without a clear and compelling mission.    To summarize   Organization’s initiatives to become agile fast are doomed. They can at best expect small productivity and visibility improvements.   The benefits of being agile include lasting continuous improvement and business resiliency. Unfortunately, this cannot happen in a day. The road starts with the following :      Creating a safe environment by getting the compensation issues out of the picture   Providing a clear goal and all the data people need to take informed decisions   Finally, leave the time for the people to learn   I discovered another fun fact while looking for references. People from lean manufacturing have been arguing the same thing for decade ! The good news though, is that we ripe improvements all along the way !   This was the last post of a series about making large organizations more agile. Next week, I’ll present an idea to apply the Mikado Method to refactor organizations one step at a time !  ","categories": ["agile","coaching","large scale","management30","agile-transformations-fail-series"],
        "tags": [],
        "url": "/why-agile-transformations-usually-dont-work-part-4-transparency/",
        "teaser": null
      },{
        "title": "Reducing the risks of large organization changes with the Mikado Method - part 1",
        "excerpt":"Large scale agile transformations are often painful, stressful and … failed ! Mixing the Improvement Kata and the Mikado Method can make them more successful.      The Mikado Method is a programming technique to reduce the risks of large code changes. Let’s see how to apply it to organization changes.   “If it’s not broke, don’t touch it !”   A lot of programmers have learned this maxim the hard way. Here is how it goes.   It all starts with a shitty piece of code that gets into everyone’s way as soon as they have to deal with it. It’s badly written, difficult to understand, and very difficult to change. One day, a brave programmer suggests to rewrite or revamp it so that it gets easy to work with. This is not a small task, but after some discussion and negotiation, he gets a ‘go’ from his team.   A few hours in this refactoring, he discovers that he needs to adapt other pieces of the code before he moves on. He leaves his work in progress as it is, and starts to change these pre-requisites. Unfortunately, these too have pre-requisite. Little by little, he builds up work in progress(WIP) on different parts of the code. He still has nothing working though ! He’s beginning to have troubles keeping track of all his WIP. On top of that, he’s wasting time integrating the work done by his colleagues with his WIP. As days pass by with nothing to show, his team begins to doubt that he’ll be able to bring this to its end ! The situation as a whole accumulates an awful lot of stress on the developer. In the end, it is pretty likely that the team will abandon the whole thing.      💡 By never changing the code, programmers make it even harder to change    The problem with this maxim is that by never changing the code, programmers make it even harder to change. In the end, this makes new features too expensive to build, which is pretty bad for the business.   Let’s see how this related to large organization changes.   The typical large scale agile transformation   It all starts when a leader decides that Agile is the way to go. It could be because everyone is doing it or that he got convinced by someone. It could also be because consultants sold him a 10% productivity increase. The reason does not matter. What matters is that in a few months, all the company should switch to an Agile method. Large groups of people will have to switch to Scrum of Scrum, LeSS, SAFe or any other framework. Even if the transformation is split in product teams, these will still be big.   One great thing about these frameworks is that they show problems. After a few sprints people discover that they have troubles delivering incrementally. After analysis, they understand another deeper problem. Examples : bad management, legacy code, outdated tooling, bad underlying processes … As Agile is the new Golden Hammer, the teams responsible of these problems in turn jump to Agile X method. Unfortunately, these too might fall into deeper problems … and the pattern repeats itself. Meanwhile, the top down transformation team sticks its own objectives. It continues to onboard new product teams to Agile, fueling the mess that is spreading.      At any moment, a very large proportion of the people in the company are in a state of transformation. This is pretty bad for productivity. What is even worse is that they are all facing the same root problems. They are all blocked in a sub-optimal state, impeded by the root problems. Following agile principles, they all start similar initiatives to workaround these issues ! They then need to coordinate with other teams, to avoid duplicate efforts. On top of that, as agile newbies, all these teams need coaches to help them at the same time.      💡 Large scale Agile transformations are too often stressful for everyone …    Needless to say that this situation is pretty stressful for everyone. It costs a lot, in productivity and consulting, for results yet to come. Despite reassurance that things will get better, the leader’s trust is eroding.   The story ends bad when it goes on for too long. When the leader loses faith in the initiative, he cancels it, and goes back to the old ways. That’s what we call the Agile hangover : “We tried it, but it was a mess. It does not work for us.”   To be continued   As you can see, the two situations have a lot in common. Programmers have invented the Mikado Method to deal with large scale code changes. This was the first post of a series about the Mikado Method and large scale organization changes. In next post, we’ll detail this technique, and see how to apply it on organization changes.  ","categories": ["agile","lean","coaching","large scale","mikado-method","mikado-method-organization-series"],
        "tags": [],
        "url": "/reducing-the-risks-of-large-organization-changes-with-the-mikado-method-part-1/",
        "teaser": null
      },{
        "title": "Reducing the risks of large organization changes with the Mikado Method - part 2",
        "excerpt":"In the previous post, I presented both large scale code and organization changes. I highlighted how they face similar difficulties of huge Work In Progress. Let’s see how programmers deal with these problems.      The mikado method algorithm   The mikado method is a programming technique to perform large code refactoring one step at a time. It fixes the problematic situation we raised at the beginning of these posts. It enables developers to integrate their work with their teammates’ continuously. It allows to build features and do the refactoring in parallel. Developers can also pause a mikado refactoring for a while if needed. On top of all this, it makes it easier for the full team to collaborate on the same refactoring.      💡 Developers can pause a Mikado Method refactoring for a while if needed.    Here is how it goes, but you can get a more detailed (and technical) description here.      Try to do the change you want   If it the tests pass, great, you’re done   Otherwise, add a node for the change you wanted to do in your mikado graph   Write down the errors you faced   Revert your change   Recurse from 1 for every error   Draw a dependency arrow from the nodes of errors to the node of your initial change   The catch is that developers should revert the work they could not finish to move on !   How to apply it to an organization change   In some way, the Mikado Method is very similar to the improvement kata. The flow of the Improvement kata goes on like that :      Determine a target condition   Try to work with the target condition in place   If it works, you’re done   Otherwise, understand the main impediment, and revert to the old way of doing things   Work to remove the impediment   Start again from step 2   Have a look at these blog posts for a more practical presentation of the improvement kata.      💡 The Mikado Method and the Improvement Kata are more than similar.    Granted, the Mikado method and the improvement kata look similar. They are in fact more than similar. The improvement kata does not prescribe how you should fix your impediments. What’s not explicit is that we can use it recursively to fix impediments ! That makes it almost identical to the Mikado Method, but for organization instead of code. It won’t be a surprise to know that some companies have used the improvement kata for that. For example, HP used it to drive its Laser Jet team transformation. Others have already used the Mikado Method for organization transformation.              Original Post by Sandy Mamoli        Suppose your organization wants to move to full Continuous Integration. It could start by trying to integrate all their devs in 10 minutes on a small group of teams. If it works, fine, other large teams can try it as well. Let’s see what to do if it fails though. Thanks to a retrospective teams understand that they need to master CI themselves first. In this case, they could revert large scale CI, and have one of the teams try to do continuous integration. If it works, fine, another team can try it too. If it does not, they’ll do a post-mortem of some sort to understand the root issue, and work on it. For example, here the main impediment could be a coupled architecture. The fun thing would be to apply the Mikado Method to do this refactoring 😉.   As with the Mikado Method, it is possible to draw an organization change Mikado graph along the way :      The graph is a living artifact. As we try and revert new experiments, we should expand and collapse the graph. This creates a global and shared view of the state of the change. Needless to say that this is great for collaboration.   To be continued (again)   This was part 2 of a series on applying the Mikado Method to organization changes. I’ll end this series up with next post, where I’ll go through the pros and cons of the approach.  ","categories": ["agile","lean","coaching","improvement kata","large scale","mikado-method","mikado-method-organization-series"],
        "tags": [],
        "url": "/reducing-the-risks-of-large-organization-changes-with-the-mikado-method-part-2/",
        "teaser": null
      },{
        "title": "Reducing the risks of large organization changes with the Mikado Method - part 3",
        "excerpt":"In the previous posts, I presented how to use the Mikado Method to large organization changes. Drawn from the programming world, this technique keeps a low transformation WIP. In theory, this should reduce risk and the time to see return on investment. Let’s see the pros and cons in more detail.      An experiment culture   A first interesting point is the “Try - Revert” attitude. No one is signing with his blood that the change will have to succeed ! It’s liberating for everyone to have to try its best instead of having to succeed at all costs. In fact, it’s an opportunity to show a “right to fail” mindset. (You can read more about the topic here). It’s a clear message that leaders are proponents of experiments.      💡 Transforming large organization with the Mikado Method shows a “right to fail” mindset.    The flip side of that, is that people might not like to try Scrum 7 times before adopting it definitely. Plus switching process every month will not be very productive. Here is what we can do about these problems.    As I read in Freedom, Inc. “It’s not that people don’t like to change, its that they don’t like to be changed !”. If a transformation teams manages all the change initiative top down, people will hate it. Hopefully, the mikado graph is a unique collaboration tool. Start by presenting the method and sharing a blank graph. Ask everyone to build it together. Then let the teams handle their own part of the graph. Here is an example. Suppose a team needs to be able to integrate every hour. Add this goal to the mikado, and hand the responsibility to get there to this team. As Daniel Pink said, autonomy and mastery are key motivators.   We should also pay special attention to how we name mikado (sub) goals. If the general goal is to adopt LeSS, jumping in and out of LeSS will be very painful ! Instead of “How” goals, we should use specific “What” goals. For example, we could replace “Adopting LeSS” with “All teams integrate in less than 10 minutes”. It is a lot easier (and faster) to try to integrate in 10 minutes for a day. In one day, we should know if it works, and if not, what’s blocking. In this case, experiments will be standard improvement items in the backlogs of teams. Teams could try to integrate every 10 minutes by hand for example !              Original Poster By Jurgen Appelo        Reduced WIP   In fact, it lets people work on smaller changes, one at a time. I can think of two direct advantages of small changes. First, it does not disrupt the rest of the organization, letting it deliver as it used to. Second, it’s a lot less stressful for the people, who then have more energy to focus on the experiment.   Another point is that less things are being changed at the same time. After reverting previous attempts, less parts of the organization are being changed. There’ll be no teams halfway in the change waiting for fixes to their impediments. As usual, reducing WIP brings many advantages :      It requires less coaches, as they will only concentrate on areas where they can drive the change to its end.   It removes the synchronization overhead of different teams working on the same problem.   What about speed ?   I can hear you think “But that’s going to take forever !” It is quite the opposite. Here is why :      The first completed changes will be root problems. Focusing on these will bring general improvement to all the organization. The state of the rest of the organization does not matter. The path you used to discover this root problem does not matter either. Fixing this root problem is likely to bring improvement to all the teams !   As the graph unravels, we can use it to start independent change initiatives in parallel ! This is a classic way to speed up code refactoring that transposes to organizations. The graph gives you a clear map of which problems are independent. Perform changes that won’t conflict in parallel.   There are quick wins also. You don’t need to be dogmatic about reverting. Suppose you tried something that brought improvement, but that uncovers a deeper problem. If the people felt an improvement and prefer to continue the new way, let them !      💡 Transforming large organization with the Mikado Method helps to parallelize work.    Granted, if the goal is just to move to “Agile”, then it might be slower than switching all teams to “Agile method X”. This is only faster on paper though. If you want more details about why becoming agile takes time, I encourage you to read these other blog posts. Plus, as I said above we should prefer “What” goals.   Continuous improvement   One final advantage I see with this technique is that it’s sustainable. As it’s a lot less stressful than a typical large re-org, it is possible to keep it going all the time ! It’s a gateway to continuous improvement. It’s no wonder Toyota (see Toyota Kata) people say that the improvement kata is their main management tool !      Closing thought   It’s funny how two practices like the Mikado Method and the Improvement Kata are the same idea ! I also noticed similarities with thefirst “Be Proactive” practice of Stephen R. Covey’s 7 habits of highly effective people. Could it be the same idea again ?  ","categories": ["agile","lean","coaching","large scale","mikado-method","mikado-method-organization-series"],
        "tags": [],
        "url": "/reducing-the-risks-of-large-organization-changes-with-the-mikado-method-part-3/",
        "teaser": null
      },{
        "title": "Why you should start a team coding dojo Randori right now",
        "excerpt":"Coding Dojos are easy to start and have a high return on investment. They will improve everyone’s technical skills. That’s only the start though. Practiced as a team Randori, they will also be a tremendous boost to team work.      My own story   Coming up with team coding conventions is always a challenge. A few years ago, I remember organizing a meeting to discuss this with my team mates. In about 1 hour, we managed to agree on the standard C# code style, but not much more. Important points like the usage of singletons were still far from any agreement. I was disappointed. At least I learned that this is not the good way to build coding conventions.   A few years later, in another team, the coding conventions topic came up again. I did not want to repeat my previous failure. I created a wiki page where we could suggest, comment and vote for conventions. It was a lot better than my previous attempt, but it was slow. It turned out that we had also been doing team coding dojos for a while. A colleague suggested to try to fix a Sonar issue from our production code during a Randori session. It took us 2 hours to fix not one but a bunch of errors and to agree on 3 or 4 coding conventions. That was far more effective than my wiki page ! Looking back at the past few months, I realized what other topics the Randoris had helped the team with.      💡 Team Coding Dojo Randori are great at defining coding conventions.    Classic Team Problems   Coding conventions is only one of the team work problems that Randoris help to solve. Here is a non-exhaustive list :      People waste time by ignoring how to use their tools as best as they could.   People lack skills that they could easily learn from one of their colleagues.   Team mates don’t agree on the same design principles. This harms collective code ownership as the design goes 1 step in a direction and then 2 in the opposite.   People don’t know how to work in baby steps and, as a result, perform large commits. This makes code reviews difficult and breaks the Continuous Integration more often.   Because they are not at ease to give and receive feedback, people don’t pair. Reviews suffer from this as well, either not going in the depth of things, or ending bad !   Some team members might have difficulties explaining what they are doing to others.   Believe it of not, Randoris can help you with all these issues.   What is a Randori ?   A Coding dojo   A coding dojo is a meeting where programmers deliberately practice their skills. To be effective, the coding dojo should be regular, and safe. Created to spread TDD (Test Driven Development), it’s an effective way to teach other skills as well. Whatever the goal, it should use TDD. TDD sets up the fast feedback loop necessary to collaboration and effective learning. The dojo should end with a short retrospective : everyone stops what they are doing to discuss how it went.   People have invented many formats to the coding dojo, but there were only 2 at the origin. The Kata, where someone live codes a prepared solution to a problem using TDD. The Randori which I’ll detail right away.   A Randori   In a Randori, you’ll be using a single machine, a wide screen and a timer. Pairs of programmers round-rob at the keyboard every 5 minutes or so. Particularly here, we should stick to TDD for collaboration and fast feedback loop. When the timer rings, the co-pilot gets the keyboard, and a new co-pilot comes in.   5 minutes are very short, and force people to use real baby steps. If they try to do something too big, they will not reach a stable state before the end of their turn. If their change is too complex, others will get lost, and remove the code. The pair needs to collaborate a lot for others to understand and continue in the same direction. They have to explain what they are doing, and take feedback into account as much as they write code.              From a YouTube video by globo.com        Your team   A team Randori is a Randori with your team. It improves all the problems I listed above.      People will share coding conventions and design best practices. They are likely to agree on something for the sake of progress in a 2 hours session. The opportunity to see how the convention works with dojo code often leads to a production code agreement.   People will share IDE shortcuts and tricks.   The constant feedback will make people accustomed to it.   On top of that, people will refactor or have their code refactored from time to time. This is a great lesson in egoless programming.      💡 Coding Dojo Randoris are a great lesson in Egoless Programming    I hope I convinced you that you should start team coding dojo Randoris. In my next post, I’ll go into more details about how to setup and run your first Randori.  ","categories": ["tdd","team building","coding dojo","team-randori-series"],
        "tags": [],
        "url": "/why-you-should-start-a-team-coding-dojo-randori-right-now/",
        "teaser": null
      },{
        "title": "How to start a team coding dojo Randori today",
        "excerpt":"In my previous post, I explained why you should start a team coding dojo Randori as soon as you can. Here is a step by step guide to set one up today.      Logistics   This the most important, and most easy, part !   A Time Slot   For a regular team coding dojo, practice showed that 2 hours every 2 weeks works great. Most teams I’ve worked with had 2 weeks sprints, which made this rhythm natural.   Try to find a day and a slot that fits your own constraints. It could on the first, last or mid day of the sprint. It could be in the morning where people are usually fresh. People might have a bit less energy in the afternoon. Don’t put it in the middle of the afternoon, or you’ll ruin the programmers’ focus. Some teams use the lunch break, and bring food in to make the coding dojo even more fun.   Once you’ve found the perfect slot, book a recurring meeting with all your team. It’s now official, you are going to have your first team coding dojo !   Material Stuff   You now need only 4 other things :      a room   a laptop with a programming environment   a large visible screen to display the laptop   a timer   It used to need a bit of preparation to get all these, but nowadays, it should be easy.   Your first session   The good thing about the Randori is that it is almost preparation-free. Here is the typical agenda for a Randori session :      Introduction (0:05)   Problem Selection (0:05)   Coding (1:40)   Retrospective (0:10)   Introduction   Start by reviewing last session’s retrospective. This will bring good resolutions back to everyone’s minds. Obviously, you cannot do that at the first session. Present the rules of the Randori instead (as stated by Emily Bache in her book):           If you have the keyboard, you get to decide what to type     If you have the keyboard and you don’t know what to type, ask for help     If you are asked for help, kindly respond to the best of your ability     If you are not asked, but you see an opportunity for improvement or learning, choose an appropriate moment to mention it. This may involve waiting until the next time all the tests pass (for design improvement suggestions) or until the retrospective.      Don’t hesitate to repeat the rules when you have a newcomer or when you see fit.   The introduction is also a moment where participants can share things together. Just make sure it does not eat on coding time.   Problem Selection   There is a ton of coding dojo subjects out there. For the first session, pick in 1 or 2 problems for the team to choose from. After a few sessions, let people bring in problems and dot vote on their favorite subjects.   As a first problem, I like the Roman Numerals kata. It’s not too difficult and has some interesting YAGNI and “Do the simplest thing that could work” properties. That’s only a personal preference though. Here some resources where to find kata problems :      codingdojo.org   cyber-dojo.org   kata-log.rocks   The Coding Dojo Handbook (Where you’ll also find tons of other valuable advices)      Whatever the topic you chose for your first session, make sure it is not too difficult.      💡 As a first team Randori coding dojo problem, I like Roman Numerals kata.    Coding   That’s why we are here ! The Randori is a dojo format where everyone works together, on the same problem, through the same computer. Start the timer for 5 minutes as the first pair works on the problem. When the timer rings, the co-pilot takes the keyboard, and someone new becomes the co-pilot. Start the timer again, and repeat after 5 more minutes. If you did not already, have a look at my previous post for more details.   You should be using Test Driven Development during the Randori. Even if you are not (yet) using it on your production code. Here is why :      It provides a fast feedback loop which enables the fast paced 5 minutes round robin   It teaches how to design testable code, which is always useful   It teaches how to do baby steps refactorings   🎁 Bonus : it demonstrates agile principles in practice (focus on outcome, incremental delivery, YAGNI …)   I’ve written a lot about TDD, have a look at my posts for more. Applying TDD means sticking to the Red-Green-Refactor loop. During first sessions, be particularly careful that people don’t refactor on red tests.      💡 Team Randori coding dojo demonstrates agile principles in practice (focus on outcome, incremental delivery, YAGNI …)    Retrospective   Stop coding 10 minutes before the end of the session. It might be difficult, but remind everyone that you are here to learn and that you can resume on next session. Use these 10 minutes to think about how it went. As the facilitator, take the keyboard, write these 4 questions in a text document :      What did we do ?   What did we learn ?   What still puzzles us ?   What did we decide ?   Ask everyone for their answers and write them down. You’ll use this at the beginning of next session.   Next Post   I thought 2 posts would be enough to cover coding dojos, but it seems I have more to say ! My next post will be about tricks and advices to make your dojos a success.  ","categories": ["tdd","team building","coding dojo","team-randori-series"],
        "tags": [],
        "url": "/how-to-start-a-team-coding-dojo-randori-today/",
        "teaser": null
      },{
        "title": "Coding Dojo Troubleshooting",
        "excerpt":"In my last 2 blog posts, I’ve detailed why and how to start a team Randori Coding Dojo. That’s the easy part. As soon as you start your first dojo, you’ll face trickier issues, especially people issues.      What if my team (or my boss) does not want to ?   Very often some of your team mates won’t see the value of the coding dojo upfront and will prefer to work on other tasks. It can also be your boss, who thinks you should be delivering features instead. Here are a few tricks you can do to make it work.      Try to find another time slot. Ask people for their preferred moment. If you can negotiate food sponsorship with your boss, you might get everyone happy. He won’t feel you’re not delivering features, you’ll have a free lunch and you’ll improve your skills.   If your boss or colleague doesn’t want to spend 2 full hours on a dojo. Get them to start with smaller problems and a shorter time slot.   Your colleagues might have doubts about the value of the dojo. Get them to try it once or a few times before committing to a recurring event.   As a general rule of thumb, the more you manage to involve people in the preparation, the more they’ll adhere.   If you have 1 or 2 inveterate laggards, do it without them anyway. With time, they’ll understand what they are missing !      💡 If you cannot get people to adopt a new practice, get them to try it once. You’ll be more than halfway there.    Dealing with TDD complaints      As you’ll start your first Randori, you’ll have some complaints about Test Driven Development. Whether they come from newbies or skeptics, they usually look like :      Why do we stick to TDD ? We’d go so much faster if we coded this straight away.   We don’t need TDD for such a simple problem.   We don’t need such small baby steps on this simple problem.   …   My answer is more or less always the same. I try to re-frame everyone in the context of a learning exercice of deliberate practice. It could sound something like :      Yes, sure. I know you are professional developers and that you could easily solve this little problem. Keep in mind that we are here to deliberately practice TDD and friends. Solving the problem is only a side effect.       We are going to apply TDD by the book, for the sake of learning. It’s a lot easier to learn to swim in 1 meter of water than in the middle of the sea. Once we’ll master it in the safe dojo environment, you’ll know how to adapt it to your production code.       Please, play by the rules here !    As you can see, I don’t try to convince them. The last thing I want is to get into a pro vs cons of TDD. 95% of the time, this answer is enough to get people over their skepticism and try it for the time of the dojo. Unfortunately, the last 5% might result in a difficult session. There’s no single way to deal with these 5%. You can try to discuss with them in private, or run next session without them.      💡 Reframe the coding dojo as a learning exercice relying on TDD to go beyond skepticism.    How to avoid getting bogged down in details   One last advice, especially for your first sessions. It’s a common rookie mistake to waste 80% of the coding time on error handling. The key is to focus on what you want to learn. You are not writing production code, so don’t hesitate to omit certain aspects. For example, assume that correct arguments are provided to skip error handling. This will save you time, be more fun and increase what you learn.   What’s next ?   This was part 3 of this series on team coding dojo. In the next post, I’ll write how to maximize the benefits we can get out of coding dojos.  ","categories": ["tdd","team building","coding dojo","psychology","team-randori-series"],
        "tags": [],
        "url": "/coding-dojo-troubleshooting/",
        "teaser": null
      },{
        "title": "How to get the max out of your Team Coding Dojo ?",
        "excerpt":"If you’ve read my previous posts about Team Randori Coding Dojos, you should know why and how to run a successful one.   Did you manage to setup the team Randori coding dojo as a recurring event ? Congratulations ! Your team is on the road to continuous learning and improvement. To close this series of posts, here are battle tested tricks for greatest impact. Let’s boost your teamwork, your production code and a few other things.      Boost your teamwork   I stated before that the team Randori is a perfect occasion to improve your teamwork. By itself, just doing it will already take you a long way towards better collaboration. As instigator of the coding dojo though, you can push the topic faster and further.   Coding and Design Conventions   Whenever you see the opportunity during the dojo, raise design discussions. It’s a good way to share best practices. It often ends up in new coding conventions for the team.   Also don’t forget to use the retrospective. It’s the perfect time to agree on best practices for the dojo and for production code. Push people to dig into what they are mentioning. Ask them if they are willing to adhere to a particular practice. You can use thumb vote to get a quick agreement. Once the team agrees on something, record it somewhere and make sure it is visible to everyone.   Egoless Programming   Egoless Programming makes collaboration a lot easier within a team. In the dojo, demonstrate Egoless Programming yourself. In particular if you already enjoy good peer recognition, adopt a “low attitude”. Don’t hesitate to encourage others to delete your code when they have a better idea. Yourself, don’t hesitate to delete code if it makes sense, but don’t make a fuss about it.      💡 Be a champion of Egoless Programming in Coding Dojo to bring this practice in your team.    Be careful if your workplace is too competitive or if your reputation is not yet strong enough. I’d go slow on this aspect in such situations.   During the dojo, you might notice people who have difficulties with egoless programming. In this case, remind its principes to everyone and that you are here to learn and practice. You can also mention that this is a TDD exercice and that deleting and changing code is the way to go.      Going further   After enough successful sessions, you’ll want to push further and experiment new things. Absolutely do it ! There’s a lot more to discover about the coding dojos.   Variations   You can try new formats like the Prepared Kata or Randori in Pairs. You can learn a new language by redoing your favorite problems in this language. You can add constraints like “No If”, “Always Compiles” or even exotic things like “No Heap Allocation”. You might also give Emily Bache’s book a read for tons of others ideas.   Production code   If you continue long enough, your team will get particularly good at Randoris. At that point, you might wonder how you could apply this to production code ? It turns you can !   One way I found, which I wrote about in my first post, is to try to fix a local smell or static analysis issue in the code. Get all the team to do a Randori to fix that, discuss the design and conventions, and commit at the end of the session.   Particularly difficult legacy code refactorings are also pretty good candidates for Randoris.       💡 Given enough eyeballs, all bugs are shallow. Linus’s Law    Once you are there, you might altogether jump into mob programming ! Randoris are by nature, like timeboxed mobs. Replace the Randori rule “Driver decides what to code” with Strong Style pairing (Make the driver code your idea) and that’s it, you are a mob !   Spread the word   One last thing before closing this series on team coding dojos. If the practice is useful to your team, spread it. Chances are that there are other development teams working next to you. Invite members of other teams to your dojo. This will build up their envy for their own team coding dojo. Propose your help to boot their first session !   In the long run, the improved practices of this team might benefit you ! For example, if your teams start collaborating. Or perhaps you’ll join this team some day !   Whatever happens, I wish you a lot of fun in your teams Coding Dojos. Happy Coding !  ","categories": ["tdd","team building","coding dojo","team-randori-series"],
        "tags": [],
        "url": "/how-to-get-the-max-out-of-your-team-coding-dojo/",
        "teaser": null
      },{
        "title": "How to learn a programming language in just 20 hours",
        "excerpt":"We should not panic when asked to work with a new language. We should be bold enough to answer to job openings requiring technologies we are not used to. In one word, we should not be afraid of new techs. Here is why : by scheduling 20 hours of Code Kata routine sessions, we can get a decent level of mastery on most topics.      How I learned some Haskell   Quite a few years ago, we used to do weekly Coding Dojos at Murex. Arnaud Bailly was among us, and as he is an Haskell fan, we ended up doing quite a lot of katas with Haskell. To my astonishment, after a few sessions, I understood of the fundamentals of the language. Without ever studying it !      💡 I learned a lot of Haskell by just going to Coding Dojos !    Many times afterwards, I learned new languages quickly by practicing them in the Dojo.   How I set out to refresh my javascript   Fast forward to the end of last year. Someone asked me if I could work at coaching a team doing some Javascript. I’ve done some Javascript in the past, but my skills definitely needed a serious update. I decided to use Code Katas to refresh my Javascript. To try to make this even more effective, I decided to mix in a bit of the “First 20 hours” technique.      I started by defining a plan of 10 sessions of 2 hours long code katas :      Roman Numeral - Mocha - JS 5   Game of Life - Mocha - JS 5   Mars Rover - Mocha - JS 5   Bowling Score - Mocha - JS 5   Median of a list of lists - Mocha - ES 6   LCD Numbers - Jasmine - ES 6   Kata Potter - Jasmine - ES 6   T9 - Jasmine - Typescript   Poker hand - Jasmine - Typescript   Egg cooker with React - Jasmine - Typescript   The plan felt pretty ambitious at the beginning. The first session was a bit hectic as I struggled to find a quick setup I could use to code my kata in. After only 3 sessions though, I could feel I’d already made a lot of progress. I had become confident I would get most of the plan done in 20 hours.   How to start ?   A good thing about the 20 hours technique is that it’s pretty easy to start with ! There’s a catch though ! At the start, it’s puzzling to be on your own with no clear track on how to tackle the topic. Here is the fix : start anyway, stick through, and you’ll work it out 99% of the time.   Here is, in more details, how to use code kata with the 20 hours technique :      Start by setting up a routine. It could be 2 hours at lunch time, 1 hour in the morning or 3 hours at night. Do whatever is best for you. It should be enough to finish in a few days or weeks.   Use the first hours of your 20 hours to setup a code kata plan. It might be very fast if you already have an idea of your learning space. It will take longer if you are a complete newbie. Skim through the literature for the main concepts until you have a plan. Try to keep this phase under 6 hours, otherwise you won’t have any time left for actual learning.   Test Driven Development plays a key role in fast learning ! Next step is to setup a development environment with TDD to use in your Code Kata sessions.   Finally, do you code kata sessions. Time-box them for something less than 2 hours and run a mini-retrospective at the end of every session. Don’t hesitate to adapt your plan if it makes sense.   When you reach 20 hours of learning, stop your sessions. It will be time to think a bit about what you accomplished.      💡 TDD plays a key role in fast learning.    If all went well, you should have learned quite a lot in only 20 hours. At that point, it’s up to you to decide what to do next. You can either decide to dig deeper. In this case, setup a new run of 20 hours of code katas ! It could also be a good time to read a bit of theory about what you just learned. Casting a different light on the topic will make it stick and deepen your understanding. Otherwise, you could stop there and start whatever is on your mind. I don’t recommend continuing on the same topic without rethinking a new plan though. That would kill your focus and be a less efficient use of your time.   To be continued   This was the first post in a series about applying the 20 hours technique and Code Katas to learn new programming languages. Here is the what to expect in the coming posts.      How to learn a programming languages in just 20 hours   Why 20 hours of code kata are so effective for learning new languages   Frequently asked questions about the 20 hours of Code Katas  ","categories": ["tdd","coding dojo","learning","personal-productivity","how to guide","20-hours-code-kata-series"],
        "tags": [],
        "url": "/how-to-learn-a-programming-language-in-just-20-hours/",
        "teaser": null
      },{
        "title": "Why 20 hours of code kata are so effective for learning new languages",
        "excerpt":"In my previous post, I described how I’ve been using 20 hours of Code Katas to learn new languages. If you did not read it yet, have a look at it first. Let’s now look at why it works so well.   In The First 20 Hours Josh Kaufman explains how he learned Ruby in 20 hours. He did not become a Ruby expert, but he was able to build and maintain a static website generator. For my part, I have succeeded to learn a bit of machine learning using the 20 hours technique.   The effectiveness of the 20 hours of Code Katas relies a few key points.      Time-boxing   Time-boxing has 2 main benefits. First, it forces us to stick to what is the most important for us to learn. There is no time to waste slacking around in only 20 hours. Plus it’s a lot easier to focus for 20 hours than over a very long period of time.   There’s a second great thing about time-boxing. The further you go, the less remains to do, and the less likely you are to drop the effort ! We are a lot less likely to abandon when we know we only need a few hours to finish the goal we had set to ourselves.      💡 Time-boxing creates focus    A plan   Again, the plan helps us to focus. We’ll need to choose what gets in a 20 hours plan. Building the plan itself forces us to get a grasp of the learning space. This will help to pick the good stuff to practice.   Routine   Routine is a magic trick to get things done. Once we have a routine in place, we don’t have to think or do extra efforts to find time to learn. The time is already there, we just have to use it !   Deliberate practice   Some exemples from “The first 20 hours” highlight the benefits of deliberate practice. When learning the Colemak keyboard, the author went through typing exercices. When studying the game of Go, he did practices specific situation puzzles. In both cases, deliberate practice made him learn faster. Code katas are typical deliberate practice exercices for programmers.      Test Driven Development   Coding Dojos are the programmers’ deliberate practice. Coding Dojos traditionally rely on TDD. TDD sets up a fast feedback loop that is key to efficient learning. Think of all the time saved by not having to run and debug a full program every time ! Even dabbling around in the REPL cannot beat running 20 or so test cases every few seconds.   We are already programmers   One last and obvious little detail : we don’t have to learn it all ! When the author learned Ruby in 20 hours, he was starting from scratch ! Unlike us, who already know how to program, but want to extend our knowledge to a few more topics. Most of the times, we don’t need to relearn everything, but to transpose what we know in a new context.   For example, if we already know an object oriented language, learning a new one will be easier. It’s a bit like with foreign languages, the more you know, and the easier it is to learn the next one. In fact, the more languages, frameworks, patterns and paradigms you know, the more the 20 hours code katas will work for you.      💡 The more you know about software, the easier it will be to learn your next programming language.    You might have a look at this post for advices about evergreen concepts to learn.   Next part   This was the second post on this series about the 20 hours of Code Katas technique. The next, and last, post will be compilation of answers to frequently asked questions.  ","categories": ["tdd","coding dojo","learning","personal-productivity","20-hours-code-kata-series"],
        "tags": [],
        "url": "/why-20-hours-of-code-kata-are-so-effective-for-learning-new-languages/",
        "teaser": null
      },{
        "title": "Frequently asked questions about the 20 hours of Code Katas",
        "excerpt":"In my previous posts, I explained how to use the 20 hours of Code Katas technique to learn new languages. If you did not read these yet, start by the beginning.      To close this series, here are a few tips and suggestions presented as questions and answers.   What if you don’t know TDD yet ?   The few Parisian guys who invented the Coding Dojo wanted to teach and spread TDD ! You should have no problem to use it to learn TDD yourself !      💡 The coding dojo was invented to teach and spread TDD    Pick your favorite language, and schedule a kata plan to practice TDD. Watch one or two videos to see how gurus are doing it. At first, you’ll have to be very careful to stick to baby steps and the red-green-refactor loop. If you need help, check meetup.com for local coding dojos where you’ll find help.   Can I apply this technique to learn something else than a new language ?   As you might have noticed, I used it to refresh my Javascript. I went on to learn different flavors of JS, but also different test libraries. I’ve used in to learn more advanced parts of other languages in the past.   Katas also work well to learn programming techniques like refactoring or DDD. Some nice people shared refactoring katas on the web. To practice DDD, we could repeat katas with the constraint of using Entities and Value Objects only.   You can even use the technique to learn other things like frameworks or tools, but you’ll need to tune it. As I explained before, you need an exercice for deliberate practice and a fast feedback loop. We typically use a Code Katas and TDD for that, but that’s not the only options. Whenever you can find a way to deliberately practice with a fast feedback loop, you’re ready to go ! These days, we should look for docker images with frameworks and tools pre-installed. Going through tutorials without looking at the solutions is deliberate practice. A small live environment can give us fast enough feedback.      💡 Find Deliberate Practice exercices and a fast feedback loop for efficient learning    What if I don’t find any kata ?   Build one yourself ! I’m not joking, building a kata, especially one where you start from scratch is not too difficult. Inspiration comes from anything you happen to do in your daily work. Trim down a programming challenge you had to work, and you might have a kata ! Went to a programming interview ? The question you had to answer might do a nice kata.      Once you’ve created and tested your kata, share it ! There are online kata repositories where you could get a chance to publish it.        codingdojo.org     cyber-dojo.org     kata-log.rocks   One last thing   I just remembered I did not finish my story about my Javascript kata plan. For those wondering, here is the end of story. In the end I did not join this team to do Javascript coaching. After thinking through it for a while, I decided to stop the katas there, and move to something else. I was only 6 hours in, and what was the point to study Javascript not to use it straight away ? The day I’ll need it, I’m likely to have forgotten 80% of it and some of it will be outdated. The knowledge is only another 20 hours away anyway !   That’s what we could call “Just In Time Learning” ! We are drowning in knowledge nowadays. It’s better to have a fast and effective way to learn anything than trying to know everything.  ","categories": ["tdd","coding dojo","learning","personal-productivity","20-hours-code-kata-series"],
        "tags": [],
        "url": "/frequently-asked-questions-about-the-20-hours-of-code-katas/",
        "teaser": null
      },{
        "title": "Careless Mocking Considered Harmful",
        "excerpt":"   💡 Mock hell : when excessive use of test mocks makes refactoring extremely slow or difficult.    A few years ago, I managed to get a side project out of mock hell. Since then, I’ve been using what I learned to avoid mocks in all the projects I’ve worked on. This is the start of a series of posts about my mock-avoiding techniques.      Escape from Mock Hell   Between 2010 and 2014, I was working on a side project I called http://mes-courses.fr. Which actually means “my house shopping” in English. I wanted people to be able to do their house shopping in 5 minutes, by using a better UI for online groceries. I was using Ruby, and I had just read Growing Object Oriented Software Guided by Tests. I got a bit too excited with mocking, and was using it way too much.   I’d been practicing Test Driven Development for more than 5 years and I was expecting great results with a language like Ruby. After a few months though, I could feel that something was not going as well as it should. The test initialization code was getting longer and longer, as it included a lot of mock setup. This made the tests more complex and less readable. It also made them unreliable, as it was not rare for all my unit tests to pass while the system was not working. I was taking the habit of running my end to end test more and more often. I was also losing a lot of time maintaining the mock setup code in line with the real classes. Mocks also tricked me into the bad practice of keeping a 1 to 1 mapping between code and test files. That again increased my maintenance burden when moving code from one file to another.   It reached a point where I could not take it anymore. All these problems were pointing at mocks, so I tried to remove them from a test file. Here are the techniques I ended up using to remove them mocks :       Value Objects   Test Data Builders   Test Matchers   Hexagonal architecture   In-memory fakes   Proxy doubles   The end result was beyond my hopes, as my problems almost magically disappeared. The code got simpler, I became a lot more confident about my unit tests, and they got easier to maintain. As an illustration, here is an excerpts from the diff of a rails controller test file which went through this mock diet.      What’s the long term risk ?   Basically, excessive mocking arms the maintainability of the tests. Here is what would have happened if I’d done nothing. Tests would have become so painful to maintain that I would have started to ignore or delete them. As coverage would decrease, more and more code would become untested. That’s exactly Michael Feathers’ definition of Legacy Code :      Legacy Code is code without tests. Michael Feathers    To summarize, excessive use of mocks leads to legacy code ! As most of us have learned the hard way, the further a system drifts into legacy, the lower the productivity.      💡 Excessive use of mocks leads to legacy code    Next posts   Others already spoke about the dangers of mocks :      Uncle Bob through his blog   DHH in the “Is TDD Dead” series   In this series of posts, I’ll go through the details of the different techniques I used to remove mocks. Here is my plan :      Careless Mocking considered Harmful   How Immutable Value Objects fight mocks   Immutable Value Objects vs Mocks : Fizz Buzz   How to use Test Data Builders to avoid mocks and keep your tests clear   How Custom Assertion Matchers will keep mocks away   Avoid mocks and test your core domain faster with Hexagonal Architecture   Get rid of mock maintenance with full fledged in-memory fakes   When is testing using mocks still a good idea ?  ","categories": ["tdd","mocking","testing","programming","how-to-avoid-mocks-series"],
        "tags": [],
        "url": "/careless-mocking-considered-harmful/",
        "teaser": null
      },{
        "title": "How Immutable Value Objects fight Mocks",
        "excerpt":"Excessive use of mocks makes tests very painful to maintain. If we stick painful mocks for too long, we’ll end up abandoning unit testing. Eventually, the system will degrade into legacy.    There are many techniques to avoid mocks. Some of the most effective involve architecture changes. Unfortunately, there are not the most straightforward to use. Re-architecting involves people and time that you may not dispose of right now. In the following posts, I’ll go over techniques that any developer can use in his day to day code to avoid mocks. These battle tested techniques that I’ve used on different projects in the past. Check the previous post if you’re interested to learn how I came to use them.   This is the second post of a series about how to avoid mocks in automated tests. If you haven’t yet, I recommend you to read my first post to understand the perils of mocks in more details.   The first mock fighting small-scale technique I’ll go over is Immutable Value Objects.      What are Immutable Value Objects ?   Behind this weird name is something very simple to understand. Immutable Value Objects :      Cannot change their state after construction   Only depend on other Immutable Value Objects   Don’t change the state of the whole system in any way   Don’t do side effects, like inputs and outputs for example   Eric Evans popularized the name in the Domain-Driven Design Blue Book. Immutable Value Objects have existed for decades in functional languages though. We say these objects are immutable (they cannot change) and pure (they cannot do side effects). Here are 2 interesting properties of Value Objects :      you can call a method any number of times with no risk of changing anything to the system   you’ll always get the same result every time you call the same method on the same object   These by itself, can already be handy when testing.      How do they prevent mocks ?   That was a bit theoretical, so let’s see how this helps to reduce mocking.   Simpler “init path”   Let’s take it the other way round and see how side effects can lead to mocking. Every test starts with setting the state in which to run the test. Side effects make this complicated, as many objects need to collaborate to set this state up. When this becomes too painful, people start hacking around with mocks. This in turn makes the tests more fragile :      We are not testing a “real” situation   We need to keep this setup in line with the real code      💡 Intricate state initialization encourage people to use mocks.    Isolates parts of the system   Unfortunately, that is not all the story ! Mutable state also, tricks us into using mocks. As soon as your test deals with mutable state, there is a chance that this state is changed in the ‘real’ system. This means that some bugs might ‘escape’ unit tests and appear in end to end tests or in production. That’s where the mocks strike ! In order to detect this bug in a fast feedback loop, we’re likely to add larger scope tests and use mocks to speed them up …      💡 Mutable state and side effects make unit tests less effective.    Reduces code with side effects   But there’s another reason why Immutable Value Objects help us to avoid mocks. As we’ll try to use them more and more for the previous two reasons, we’ll need to adapt our programming style. As we’ll push more and more code in Immutable Value Objects, the ‘imperative’ part will shrink. This ‘imperative’ part is where side-effect happen. This is the part where mocking out IOs makes sense. To summarize, the more Immutable Value Objects we use, the more isolated the IOs are, and the less mocking we need.   Javascript expert Eric Elliot also wrote about the immutability and mocks here.   Next week   This was the second post in a series about how to prevent mocks in your automated tests. Next post will be an example of using immutable value objects on the FizzBuzz kata.  ","categories": ["tdd","ddd","mocking","testing","programming","how-to-avoid-mocks-series"],
        "tags": [],
        "url": "/how-immutable-value-objects-fight-mocks/",
        "teaser": null
      },{
        "title": "Immutable Value Objects vs Mocks : Fizz Buzz",
        "excerpt":"In my previous post I explained how Immutable Value Objects help us to avoid mocks. In this post, I’ll illustrate this in practice with real code.   This is the third post on a series about how to avoid mocks. If you haven’t, you can start reading the full story here.      Fizz Buzz Example   As a simple example, I’ll go through the classic Fizz Buzz. I’ve implemented and tested it with and without immutable value objects. Please keep in mind that this is a toy example, where problems are obvious and easily fixed. I try to highlight at small scale the same problems that get hidden by the complexity of a large scale program.   Let’s start with a typical FizzBuzz implementation.   1.upto(100) do |i|   if (i%3 == 0 and i%5 == 0)     STDOUT.puts(\"FizzBuzz\\n\")   elsif (i%3 == 0)     STDOUT.puts(\"Fizz\\n\")   elsif (i%5 == 0)     STDOUT.puts(\"Buzz\\n\")   else     STDOUT.puts(\"#{i}\\n\")   end end   Suppose you need to add some tests around the code. The most straightforward way is to mock STDOUT :   require 'rspec'  def fizzBuzz(max, out)   1.upto(max) do |i|     if (i%3 == 0 and i%5 == 0)       out.puts(\"FizzBuzz\\n\")     elsif (i%3 == 0)       out.puts(\"Fizz\\n\")     elsif (i%5 == 0)       out.puts(\"Buzz\\n\")     else       out.puts(\"#{i}\\n\")     end   end end  # main fizzBuzz(100,STDOUT)  describe 'Mockist Fizz Buzz' do    it 'should print numbers, fizz and buzz' do     out = double(\"out\")     expect(out).to receive(:puts).with(\"1\\n\").ordered     expect(out).to receive(:puts).with(\"2\\n\").ordered     expect(out).to receive(:puts).with(\"Fizz\\n\").ordered     expect(out).to receive(:puts).with(\"4\\n\").ordered     expect(out).to receive(:puts).with(\"Buzz\\n\").ordered     expect(out).to receive(:puts).with(\"Fizz\\n\").ordered     expect(out).to receive(:puts).with(\"7\\n\").ordered     expect(out).to receive(:puts).with(\"8\\n\").ordered     expect(out).to receive(:puts).with(\"Fizz\\n\").ordered     expect(out).to receive(:puts).with(\"Buzz\\n\").ordered     expect(out).to receive(:puts).with(\"11\\n\").ordered     expect(out).to receive(:puts).with(\"Fizz\\n\").ordered     expect(out).to receive(:puts).with(\"13\\n\").ordered     expect(out).to receive(:puts).with(\"14\\n\").ordered     expect(out).to receive(:puts).with(\"FizzBuzz\\n\").ordered      fizzBuzz(15, out)   end end     Unfortunately, there are a few problems with this code :      With nested logic and complicated mock setup, both code and tests aren’t very readable   They both seem to violate the single responsibility principle as well   It’s depending on a mutable output. Within a larger program, something could be messing around with this output stream. That would break FizzBuzz.   Let’s now try to use as many immutable values objects as possible, and see what happens to the mocks.   require 'rspec'  # We extracted a function to do the fizz buzz on a single number def fizzBuzzN(i)   if (i%3 == 0 and i%5 == 0)     \"FizzBuzz\"   elsif (i%3 == 0)     \"Fizz\"   elsif (i%5 == 0)     \"Buzz\"   else     i.to_s   end end  # We replaced the many calls to STDOUT.puts by building a single # large (and immutable) string def fizzBuzz(max)   ((1..max).map {|i| fizzBuzzN(i)}).join(\"\\n\") end  # main, with a single call to STDOUT.puts STDOUT.puts fizzBuzz(100)  describe 'Statist Fizz Buzz' do    it 'should print numbers not multiples of 3 or 5' do     expect(fizzBuzzN(1)).to eq(\"1\")     expect(fizzBuzzN(2)).to eq(\"2\")     expect(fizzBuzzN(4)).to eq(\"4\")   end    it 'should print Fizz for multiples of 3' do     expect(fizzBuzzN(3)).to eq(\"Fizz\")     expect(fizzBuzzN(6)).to eq(\"Fizz\")   end    it 'should print Buzz for multiples of 5' do     expect(fizzBuzzN(5)).to eq(\"Buzz\")     expect(fizzBuzzN(10)).to eq(\"Buzz\")   end    it 'should print FizzBuzz for multiples of 3 and 5' do     expect(fizzBuzzN(15)).to eq(\"FizzBuzz\")     expect(fizzBuzzN(30)).to eq(\"FizzBuzz\")   end     it 'should print numbers, fizz and buzz' do     expect(fizzBuzz(15)).to start_with(\"1\\n2\\nFizz\").and(end_with(\"14\\nFizzBuzz\"))   end end   As we can see, using immutable value objects got us rid of the mocks. Obviously, this new code will not be as efficient as the original version, but most of the time, this does not matter. As a bonus though we get finer grain and more readable tests.   Other testing advantages   Appart from preventing mocks, Immutable Value Objects have other advantages related to testing.      We can directly assert their equality, without having to dig into their guts   We can call methods as many times as we want, without the risk of changing anything and breaking the tests   Immutable Value Objects are a lot less likely to contain invalid state. This removes the need for a whole range of validity tests.      💡 Immutable Value Objects simplify testing in many ways.    Convincing your teammates   We’ve seen that Immutable Value Objects have a ton of advantages when testing. People have found that they also have many other benefits :      6 Benefits of Programming with Immutable Objects in Java   5 Benefits of Immutable Objects Worth Considering for Your Next Project   Surprisingly though, it’s difficult to persuade programmers to use more immutability. It’s tricky to explain why returning a modified copy is simpler than just adding a setter.      💡 Why is it so hard to persuade other developers to use immutable data structures ?    I had the most success by far when encountering a bug resulting of share mutable state. When this happens, the long term benefits and safety of the immutable design wins people over. The good thing is that as you convince more people in the team, immutability will spread like a virus !   Outside of this situation, you might try some of the following arguments to move people :      Immutable values prevent bugs caused by different parts of the system changing the same mutable state   They make it easier to deal with the program in smaller parts and to reason about the system in general   Immutable values don’t need any synchronization and make multithreaded programming easier   When tempted to add a simple setter instead of keeping a class immutable, highlight the stressful debugging time to come   If you’re dealing with a Design By Contract adept, explain how immutability has it built-in   Admit that mainstream languages have bad support for Immutable Value. Point to patterns like Data Builders that work around these limitation   Next post   I’m done with immutable value objects. It was a far longer post than I thought, but there was a lot to say. This was the third post in a series about avoiding mocks. In next post, I’ll dig into another small scale mock fighting pattern : Test Data Builders.  ","categories": ["tdd","mocking","testing","programming","ruby","how-to-avoid-mocks-series"],
        "tags": [],
        "url": "/immutable-value-objects-vs-mocks-fizz-buzz/",
        "teaser": null
      },{
        "title": "How to use Test Data Builders to avoid mocks and keep your tests clear",
        "excerpt":"We are sometimes tempted to use mocks to shortcut test data initialization. Unfortunately, excessive mocking makes tests difficult to maintain. As Uncle Bob explained, it’s a road that leads to giving up on tests.   Hopefully, Test Data Builders both shortcut test data setup and avoid mocks.      This is the fourth post of a series about how to avoid mocks in automated tests. If you haven’t yet, I recommend you to start from the beginning.   The problem with test data initialization   Setting up the correct state for automated tests can be pretty verbose. This is especially true for software in complex domains or code with a lot of side effects.   The situation gets worse as tests need to setup similar but not exactly identical data. What I often see in code bases is a lot of test data setup duplication. For example, here are tests for a basic ticketing system.   require 'rspec' require 'date'  describe 'Ticket Tracking' do   context \"with test setup duplication\" do      it 'latest change date is the creation date when nothing changed' do       creation_time = DateTime.new(2018,4,26,13,9,0)        ticket = Ticket.new(\"Widget broken\", \"The widget is not loading when ...\", \"Philippe\", creation_time)        expect(ticket.latest_change).to be(creation_time)     end      it 'latest change date is the comment date when a comment is written' do       ticket = Ticket.new(\"Widget broken\", \"The widget is not loading when ...\", \"Philippe\", DateTime.new(2018, 4, 26, 13, 9, 0))       comment_time = DateTime.new(2018, 4, 26, 13, 16, 0)        ticket.add_comment(Comment.new(\"Should work now\", \"Dan\", comment_time))        expect(ticket.latest_change).to be(comment_time)     end      it 'latest change date is the comment date of the latest comment' do       ticket = Ticket.new(\"Widget broken\", \"The widget is not loading when ...\", \"Philippe\", DateTime.new(2018, 4, 26, 13, 9, 0))       ticket.add_comment(Comment.new(\"Should work now\", \"Dan\", DateTime.new(2018, 4, 26, 13, 16, 0)))       comment_time = DateTime.new(2018, 4, 26, 18, 36, 0)        ticket.add_comment(Comment.new(\"Should work now\", \"Dan\", comment_time))        expect(ticket.latest_change).to be(comment_time)       end      it 'latest change date is time of latest change if after comment' do       creation_time = DateTime.new(2018, 4, 26, 13, 9, 0)       ticket = Ticket.new(\"Widget broken\", \"The widget is not loading when ...\", \"Philippe\", creation_time)       ticket.add_comment(Comment.new(\"Should work now\", \"Dan\", DateTime.new(2017, 4, 26, 13, 16, 0)))        expect(ticket.latest_change).to be(creation_time)     end   end end   ## The code under test ## class Ticket    def initialize(title, description, reporter, creation_time)     @updated_at = creation_time     @comments = []   end    def latest_change     ([@updated_at] + @comments.map(&amp;:created_at)).max   end    def add_comment(comment)     @comments.push(comment)   end end  class Comment   attr_reader :created_at   def initialize(message, author, time)     @created_at = time   end end    It’s clear that there’s a huge amount of duplication in the tests data setups.   The straightforward fix against that is method extraction. This is the Object Mother pattern. Unfortunately, Object Mother breaks down under the number of variations. Every time you need a new change, you’ll add a parameter to the Object Mother method. Long story short, you’ll end up with code like that :   describe 'Ticket Tracking' do   context \"with object mother\" do      it 'latest change date is the creation date when nothing changed' do       creation_time = DateTime.new(2018, 4, 26, 13, 5, 0)       ticket = create_ticket(creation_time, [])        expect(ticket.latest_change).to be(creation_time)     end      it 'latest change date is the comment date when a comment is written' do       comment_time = DateTime.new(2018, 4, 26, 13, 16, 0)       ticket = create_ticket(DateTime.new(2018, 4, 26, 13, 9, 0), [comment_time])        expect(ticket.latest_change).to be(comment_time)     end      it 'latest change date is the comment date of the latest comment' do       comment_time = DateTime.new(2018, 4, 26, 18, 36, 0)       ticket = create_ticket(DateTime.new(2018, 4, 26, 13, 9, 0),                              [DateTime.new(2018, 4, 26, 13, 16, 0), comment_time])        expect(ticket.latest_change).to be(comment_time)     end      it 'latest change date is time of latest change if after comment' do       creation_time = DateTime.new(2018, 4, 26, 13, 9, 0)       ticket = create_ticket(creation_time,[DateTime.new(2017, 4, 26, 13, 16, 0)])        expect(ticket.latest_change).to be(creation_time)     end      def create_ticket(creation_time, comment_times)       ticket = Ticket.new(\"Widget broken\", \"The widget is not loading when ...\", \"Philippe\", creation_time)       comment_times.each do |comment_time|         ticket.add_comment(Comment.new(\"Should work now\", \"Dan\", comment_time))       end       return ticket     end   end  end   As you can see, we have less duplication, but the tests got both unreadable and intricate … Following my advices and using more Immutable Value Objects makes the situation worse ! When data is mutable, we can customize it after the call to the Object Mother method. If data is immutable, it all has to be setup at initialization …   That’s when the mock temptation strikes. Sometimes it’s so much easier to mock a method rather than to initialize your data properly. It can be 1 line of mock instead of dealing with all this mess.      💡 If you are not careful, messy test initialization code will trick you into using mocks.    Suppose we now want to make sure we can’t add comments that were written before the ticket was created. We’ll add the following   describe 'Ticket Tracking' do   # ...   it \"is not possible to insert a comment before creation data\" do     ticket = create_ticket(DateTime.new(2018, 4, 26, 13, 9, 0), [])     expect do       ticket.add_comment(Comment.new(\"Should work now\", \"Dan\", DateTime.new(2017, 4, 26, 13, 9, 0)))     end.to raise_error(ArgumentError)   end end # ... class Ticket   # ...   def add_comment(comment)     raise ArgumentError unless @updated_at &lt; comment.created_at      @comments.push(comment)   end   # ... end    Unfortunately, one test (latest change date is time of latest change if after comment) where we were doing just this, will now fail. The fix would be to find a real situation for this test. Here this could be that the ticket is modified after the latest comment. If the tests are too messy though, a mock can be a quick and dirty fix the setup and make the test pass :   it 'latest change date is time of latest change if after comment' do   creation_time = DateTime.new(2018, 4, 26, 13, 9, 0)   ticket = create_ticket(creation_time, [])   comment = Comment.new(\"Should work now\", \"Dan\", DateTime.new(2018, 4, 26, 13, 16, 0))   ticket.add_comment(comment)   allow(comment).to receive(:created_at).and_return(DateTime.new(2017, 4, 26, 13, 16, 0))    expect(ticket.latest_change).to be(creation_time) end   There is a third way : Test Data Builders   What are test data builders   As often, when design is not satisfying, adding an indirection solves the issue. Here the indirection takes shape of the Builder pattern.      Builder Pattern [Wikipedia] :       The intent of the Builder design pattern is to separate the construction of a complex object from its representation. By doing so the same construction process can create different representations.    The idea is to use the builder pattern to build the test data. Growing Object Oriented Software Guided by Tests covers this technique in great length.      Here is the previous code re-written using the test data builder pattern.   require 'rspec' require 'date'  describe 'Ticket Tracking' do   context \"with test data builders\" do      before :each do       @t = date_times.build     end      it 'latest change date is the creation date when nothing changed' do       ticket = a_ticket.at(@t[0]).build        expect(ticket.latest_change).to be(@t[0])     end      it 'latest change date is the comment date when a comment is written' do       ticket = a_ticket                    .at(@t[0])                    .with_comment(a_comment.at(@t[1]))                    .build        expect(ticket.latest_change).to be(@t[1])     end      it 'latest change date is the comment date of the latest comment' do       ticket = a_ticket                    .at(@t[0])                    .with_comment(a_comment.at(@t[1]))                    .with_comment(a_comment.at(@t[2]))                    .build        expect(ticket.latest_change).to be(@t[2])     end      it 'latest change date is time of latest change if after comment' do       ticket = a_ticket.at(@t[0])                    .with_comment(a_comment.at(@t[1]))                    .build        ticket.update_description(\"The widget is not loading when logged in as anonymous\", @t[2])        expect(ticket.latest_change).to be(@t[2])     end      it \"is not possible to insert a comment before creation data\" do       ticket = a_ticket.at(@t[1]).build        expect do         ticket.add_comment(a_comment.at(@t[0]).build)       end.to raise_error(ArgumentError)     end   end end  ## Test Data Builders ## class DateTimeBuilder   def build     seed = DateTime.now     (0..10).map {|i| seed + i}   end end def date_times()   DateTimeBuilder.new end  class CommentBuilder   def initialize     @at = DateTime.now   end   def at(time)     @at = time     self   end   def build     Comment.new(\"Should work now\", \"Dan\", @at)   end end def a_comment()   CommentBuilder.new end  class TicketBuilder   def initialize     @at = DateTime.now     @comments = []   end   def at(time)     @at = time     self   end   def with_comment(comment_builder)     @comments.push(comment_builder.build)     self   end   def build     ticket = Ticket.new(\"Widget broken\", \"The widget is not loading when ...\", \"Philippe\", @at)     @comments.each do |comment|       ticket.add_comment(comment)     end     ticket   end end def a_ticket()   TicketBuilder.new end  ## The code under test ## class Ticket    def initialize(title, description, reporter, creation_time)     @updated_at = creation_time     @comments = []   end    def latest_change     ([@updated_at] + @comments.map(&amp;:created_at)).max   end    def add_comment(comment)     raise ArgumentError unless @updated_at &lt; comment.created_at      @comments.push(comment)   end    def update_description(description, update_time)     @updated_at = update_time   end end  class Comment   attr_reader :created_at    def initialize(message, author, time)     @created_at = time   end end   As you can see, it provides default test values, and we only need to provide the custom values we care about. This makes the test code both readable and intention revealing. Making the tests more understandable helps a lot to find ways to avoid mocks. Here, we replaced the mock on the comment time by an update to the ticket after the last comment.   The pattern applies in many languages, even if implementations will be different. In Ruby, libraries like factory_bot avoid a lot of boilerplate code. Have a look at this article for examples in Java.   Other advantages   Test data builders have another second effect benefit. When setting up the data is complicated, we are likely to add more that one assertion in a test. Unit tests can end up looking like a mini scenario to avoid duplicating this test setup.   It’s easy to create a specific tests for every assertion with Test Data Builders. By doing so we get smaller and more focused tests, which bring :      Better names for tests   More readable tests   Faster diagnostic of the problem when a particular test fails   🎁 Better coverage ! In a large test, all assertions work on the same input values. When we have many small tests, we can use a different value in each.      💡 By simplifying the creation of new tests with different data, Test Data Builders increase code coverage in the long term!    Next week   This is the fourth post of a series about how to avoid mocks in automated tests. Next week I’ll dig into Custom Assertion Matchers and how they avoid mock expectations.  ","categories": ["tdd","mocking","testing","programming","ruby","how-to-avoid-mocks-series"],
        "tags": [],
        "url": "/how-to-use-test-data-builders-to-avoid-mocks-and-keep-your-tests-clear/",
        "teaser": null
      },{
        "title": "How Custom Assertion Matchers will keep mocks away",
        "excerpt":"Custom assertions are a handy compromise alternative to mocks when we don’t have the time to refactor to a functional style.   With the 3X model, Kent Beck almost explicity tells us that we should take technical debt at the beginning of a feature or project. Using no-brainer CRUD frameworks, like Rails, is very effective to get the first version of a new product out. When the project hopefully becomes a success, many things happen. Tests become important, but also messy and slow. Unfortunately, in this fast expansion phase, we’re lacking time for refactoring. Even if moving to a more functional style would be the test thing to clean up our tests, we often end up adding mocks.      💡 When we are in a hurry, we get lured into mocking to workaround complicated tests    Custom assertions are a handy compromise. They avoid most of the downsides of mocks, while preparing for a potential refactoring. If you don’t know what custom assertions are, here is pseudo code that uses a custom assertion :   assert.that(actual, VerifiesMyCustomAssertion(withCustomProperties))   For more details, have a look at these examples for your preferred language : Java, Ruby or Javascript.      I already blogged about the benefits of Custom Assertion Matchers. Here I’m going to dive in their advantages against mocking.   This is the fifth post in a series about how to avoid mocks. If you haven’t yet, I recommend you to start from the beginning.   Why would we end up with mocks when we don’t have matchers ?   Let’s walkthrough a small story. Suppose we are building an e-commerce website. When someone passes an order, we want to notify the analytics service. Here is some very simple code for that.   class AnalyticsService   def initialize    @items = []  end   attr_reader :items   def order_passed(customer, cart)    cart.each do |item|      @items.push(customer: customer, item: item)    end  end end  class Order  def initialize(customer, cart, analytics)    @customer = customer    @cart = cart    @analytics = analytics  end   def pass    # launch order processing and expedition     @analytics.order_passed(@customer, @cart)  end  end  describe 'Order' do   it \"notifies analytics service about passed orders\" do    cart = [\"Pasta\",\"Tomatoes\"]    analytics = AnalyticsService.new    order = Order.new(\"Philippe\", cart, analytics)     order.pass     expect(analytics.items).to include(customer: \"Philippe\", item: \"Pasta\")    expect(analytics.items).to include(customer: \"Philippe\", item: \"Tomatoes\")  end end   Let’s focus on the tests a bit. We first notice that the verification section is large and difficult to understand.  Looking in more details, it knows too much about the internals of AnalyticsService. We had to make the items accessor public just for the sake of testing. The test even knows how the items are stored in a list of hashes. If we were to refactor this representation, we would have to change the tests as well.   We could argue that responsibility-wise, our test should only focus on Order. It makes sense for the test to use a mock to verify that the Order calls AnalyticsService as expected. Let’s see what this would look like.   it \"notifies analytics service about passed orders\" do  cart = [\"Pasta\",\"Tomatoes\"]  analytics = AnalyticsService.new  order = Order.new(\"Philippe\", cart, analytics)   expect(analytics).to receive(:order_passed).with(\"Philippe\", cart)   order.pass end   Sure, the test code is simpler. It’s also better according to good design principles. The only glitch is that we now have a mock in place with all the problems I described before.   This might not (yet) be a problem in our example but, for example, the mock ‘cuts’ the execution of the program. Suppose that someday, the Order starts expecting something from the AnalyticsService. We’d then need to ‘simulate’ the real behavior in our mock. This would make the test very hard to maintain.   Matchers to the rescue   Let’s see how a matcher could help us here. The idea is to improve on the first ‘state checking’ solution to make it better than the mock one. We’ll extract and isolate all the state checking code in a custom matcher. By factorizing the code in a single matcher, we’ll reduce duplication. The matcher remains too intimate with the object, but as it is now unique and well named, it’s less of a problem. Plus, as always with matchers, we improved readability.   RSpec::Matchers.define :have_been_notified_of_order do |customer, cart|  match do |analytics|    cart.each do |item|      return false unless analytics.items.include?(customer: customer, item: item)    end    true  end end  describe 'Order' do  it \"notifies analytics service about passed orders\" do    cart = [\"Pasta\",\"Tomatoes\"]    analytics = AnalyticsService.new    order = Order.new(\"Philippe\", cart, analytics)     order.pass     expect(analytics).to have_been_notified_of_order(\"Philippe\", cart)  end end   Here is how we could summarize the pros and cons of each approach :                  Assert state       Mocks       Matchers                       👎 duplicated code       👎 duplicates the program behavior       ❤️ customizable error messages                 👎 breaks encapsulation               ❤️ more readable                                 👎 intimacy with the asserted object                                 ❤️ factorizes the assertion code           Design improvements   As I said in the introduction, customer matchers are often an alternative to mocks when we don’t have the time for a better refactoring. Later down the road, you might find the time for this refactoring. Functional style and the “Tell, don’t ask!” principles are often the solution here.   In our example, a publish-subscribe pattern might do. A better design should fix the encapsulation problem of the matcher. Here again, the custom assertion matchers will help. In most cases, it will be enough to change the implementation of the matchers only.      💡 Custom assertion matchers make refactoring easier by factorizing test assertions.    Summary of small-scale techniques   I’m done with small scale mock avoiding techniques. To summarize, the first thing to do is to push for more and more immutable value objects. Not only does it help us to avoid mocks, but it will also provides many benefits for production code. Practices like Test Data Builders and Custom Assertion Matchers simplify dealing with Immutable Value Objects in tests. They also help to keep tests small and clean, which is also a great thing against mocks.   Next post   In the following posts, I’ll look into architecture scale techniques to avoid mocks. I’ll start with Hexagonal architecture.   Thanks to Dragan Stepanovic who’s comments brought me to update this post.  ","categories": ["tdd","mocking","testing","programming","ruby","how-to-avoid-mocks-series"],
        "tags": [],
        "url": "/how-custom-assertion-matchers-will-keep-mocks-away/",
        "teaser": null
      },{
        "title": "Avoid mocks and test your core domain faster with Hexagonal Architecture",
        "excerpt":"As I’ve written in my last few posts, we can get a long way to avoid mocks with small scale coding best practices. Unfortunately, when systems reach a certain size, we need something at architecture scale.   This is the 6th post of a series about avoiding mocks. If you haven’t, you can start by the beginning.      Why do we end up with mocks in large systems ?   A few years ago, I joined a team working in a legacy system. We wanted to apply TDD and refactoring. As expected, adding tests legacy code proved a real challenge. With a lot of effort we could manage to add a few. Unfortunately, this did not seem to have any positive effect on our maintainability ! The tests we were writing all involved a lot of mocking. The system was such a large mass of spaghetti code that there was no clear place to mock. We were actually mocking where it seemed the easiest on a test by test basis. We were making progress at small scale, but the big picture was not improving at all !   Large systems are beasts with many faces. They  involve a lot of IOs. They write and read data from the disk and databases. They call 3rd parties and remote services.   As we test these large systems, we’ll need to stub out these IOs. Even if the tests are fast enough, we usually don’t want to call external services for real. Most of the time though, tests are slow. That’s 2 reasons why end up adding some mocks.   Here comes the nasty part. These large systems are so complex that we, developers, don’t have the full picture. When we test, we tend to mock at different places, depending on our knowledge. This is bad for maintenance. Mocks duplicate production code behavior. When many different mocks are in place to isolate an external dependency, we end up with ‘n’ versions of the code. That’s a nightmare to refactor !      💡 When many different mocks are in place to isolate an external dependency, we end up with ‘n’ versions of the code !    Hexagonal architecture to the rescue   Alistair Cockburn coined the term. The idea is pretty simple :  isolate a piece of code from all dependencies. This is particularly useful for the core bounded contexts. With this in place, it becomes straightforward (and fast) to test the core domain logic.   To main techniques to isolate a piece of code from any dependency are :      Dependency injection   Observers   Adapters   It’s also possible to split a system in many ‘hexagons’ and glue them together with adapters at startup. If you want to learn more on this style of architecture, have a look into the Domain Driven Design lore. This community has been building systems this way for years now.   Enough talk, show me the code !   This post was the occasion to try to inject a Hexagonal Architecture and a dash of DDD in a Rails application. There’s one caveat though : DDD shines on complex systems. Unfortunately, large and complex systems make very poor didactic examples. The following code highlights the gains about mocking. We would not use DDD for such a small app in real life.   The starting point   I chose a simple TODO app. I started by generating a scaffold for a Task with a description and a done/not-done status. As third party interaction, completing a task sends an automatic tweet. Here is the only specific code I wrote on top of the Rails scaffold :   app/models/task.rb   class Task &lt; ApplicationRecord   include ActiveModel::Dirty    validates :description, presence: true    before_save :tweet_if_done    private   def tweet_if_done     if done_changed?       TwitterClient::Client.update(self.description)     end   end end   Thanks Jason Charnes for the change attribute technique.   spec/models/task_spec.rb   require 'rails_helper'  RSpec.describe Task, type: :model do    it \"is valid with all attributes set\" do     expect(Task.create(description: \"Finish presentation\", done: false)).to be_valid   end    it \"requires a description\" do     expect(Task.create(description: nil, done: false)).to be_invalid     expect(Task.create(description: \"\", done: false)).to be_invalid   end    it \"tweets when a task is finished\" do     task = Task.create(description: \"Wash the car\", done: false)      expect(TwitterClient::Client).to receive(:update).with(\"Wash the car\")      task.done = true     task.save   end end   This is pretty simple and to the point !   5 years later   Now let’s imagine that the app grew to tens of thousands of lines. We added a lot of features to the app, which transformed the TODO domain into a very complex thing. Now suppose that, for the sake of maintenance, we want to isolate the domain logic into its own hexagon. Unlike traditional Rails ActiveRecords, we want to make it independent from the database. We also want it to be independent from the Twitter API.   Here is what the code might look like.   lib/core/task.rb   First, we have a core task class, independent from anything else. The Core module is our hexagon.   module Core   class Task      attr_reader :description     attr_accessor :db_id      def initialize(attributes = {})       @description= \"What do you need to do ?\"       @done = false       @done_subscribers = []        self.update(attributes)     end      def done?       @done     end      def mark_as_done       @done = true       @done_subscribers.each {|proc| proc.call(self) }     end      def update(attributes={})       self.description= attributes[:description] unless attributes[:description].nil?       self.mark_as_done if attributes[:done]     end      def notify_when_done(&amp;proc)       @done_subscribers.push(proc)     end      def description=(desc)       raise ArgumentError.new(\"Task description cannot be blank\") if desc.blank?        @description = desc     end   end end   As we can see, it contains only domain logic and nothing else.   ####### spec/lib/core/task_spec.rb   Here is the corresponding test, fast, mock-free and independent from the database and any external system.   require 'rails_helper' require 'core/task'  context 'Task' do    let(:task) { Core::Task.new}    specify 'is not done by default' do     expect(task).not_to be_done   end    specify 'comes with a default description' do     expect(task.description).not_to be_blank   end    specify 'it can be initialized from a hash' do     task = Core::Task.new(description: \"Old description\", done: true)      expect(task.description).to eq(\"Old description\")     expect(task).to be_done   end    specify 'can have a custom description' do     task.description= \"Clean up the house\"     expect(task.description).to eq(\"Clean up the house\")   end    specify 'forbids empty descriptions' do     expect{task.description = nil }.to raise_error(ArgumentError)     expect{task.description = \"\" }.to raise_error(ArgumentError)   end    specify 'can be done' do     task.mark_as_done     expect(task).to be_done   end    specify 'publishes when done' do     done_task = nil     task.notify_when_done {|t| done_task = t}      task.mark_as_done      expect(done_task).to be(task)   end    specify 'can be updated with a hash' do     task.update(description: \"New description\", done: true)      expect(task.description).to eq(\"New description\")     expect(task).to be_done   end    specify 'has no DB id by default' do     expect(task.db_id).to be_nil   end end   ####### lib/infrastructure/task_repo.rb   To read and save with the database, we now go through an adapter. This is not considered to be part of our core domain.   module Infrastructure   class TaskRepo      def self.all       Task.all.map do |db_task|         from_db(db_task)       end     end      def self.load(db_id)       from_db(Task.find(db_id))     end      def self.save(task)       if task.db_id.nil?         db_task = Task.create!(to_db_attributes(task))         task.db_id = db_task.id       else         db_task = Task.find(task.db_id)         db_task.update!(to_db_attributes(task))       end       task     end      def self.delete(task)       unless task.db_id.nil?         db_task = Task.find(task.db_id)         db_task.destroy!         task.db_id = nil       end     end      private      def self.to_db_attributes(task)       {description: task.description, done: task.done?}     end      def self.from_db(db_task)       result = Core::Task.new       result.db_id = db_task.id       result.description = db_task.description       result.mark_as_done if db_task.done?       result     end    end end   ####### app/controllers/tasks_controller.rb   Finally, all the pieces interact together in the controller. This controller basically does what the previous version was, it’s just using different classes. Obviously, we’ll need to adapt the views and the tests.   require 'core/task' require 'infrastructure/task_repo'  class TasksController &lt; ApplicationController   before_action :set_task, only: [:show, :edit, :update, :destroy]    # GET /tasks   def index     @tasks = Infrastructure::TaskRepo.all   end    # GET /tasks/1   def show   end    # GET /tasks/new   def new     @task = Core::Task.new   end    # GET /tasks/1/edit   def edit   end    # POST /tasks   def create     begin       @task = Core::Task.new(task_params)       Infrastructure::TaskRepo.save(@task)        redirect_to task_url(@task.db_id), notice: 'Task was successfully created.'      rescue ArgumentError       render :new     end   end    # PATCH/PUT /tasks/1   def update     begin       @task.update(task_params)       Infrastructure::TaskRepo.save(@task)        redirect_to task_url(@task.db_id), notice: 'Task was successfully updated.'      rescue ArgumentError       render :edit     end   end    # DELETE /tasks/1   def destroy     Infrastructure::TaskRepo.delete(@task)     redirect_to tasks_url, notice: 'Task was successfully destroyed.'   end    private     def set_task       @task = Infrastructure::TaskRepo.load(params[:id])       @task.notify_when_done do |task|         TwitterClient::Client.update(task.description)       end     end      # Never trust parameters from the scary internet, only allow the white list through.     def task_params       params.permit(:description, :done)     end end   The main gain here is that our core domain, our most valuable asset is now easy to test without mocks. This means that we are able to write and execute fast tests for this area of the code. This puts us in a great position to increase our competitive advantage in our core business !      💡 By keeping your tests around your core domain fast, Hexagonal Architecture increases your competitive advantage.    As you can see, we are now wiring everything together at the controller level. We could later build a facade to isolate the controller from the inside of our domain. A presenter might do, but it seemed over-engineered, even in this made up example. (I’ll post something about that some day)   Next post   As we can deduce from the controller code above, we still have to use fakes or mocks when testing the controller. The good thing though is that this is now more local which already makes mocking less of an issue. If a mock is used in less tests, it’s easier to use the same mock everywhere ! This is a great opportunity for simplifying test setup, as we’ll see in the next post about in-memory fakes.  ","categories": ["tdd","mocking","testing","programming","ruby","architecture","ddd","how-to-avoid-mocks-series"],
        "tags": [],
        "url": "/avoid-mocks-and-test-your-core-domain-faster-with-hexagonal-architecture/",
        "teaser": null
      },{
        "title": "Get rid of mock maintenance with full fledged in-memory fakes",
        "excerpt":"Last week’s post was about how hexagonal architecture results in fast, mock-free tests around your core domain. Unfortunately, that does not remove all mocks, yet it groups them in the same, less critical, zone. In last week’s code sample, this was the controller. I concluded that at least, this was easier to manage. Let’s see how.      This is the 7th post in a series about avoiding mocks. If you haven’t, you might start from the beginning.   Mock concentration   Let’s get back to the last post’s code sample. As a reminder, it’s a very basic TODO app built on Rails. I extracted the domain part, the tasks, in a core domain area. This allowed to push all mocks out of this section. A consequence though, is that all mocks gathered in the controller test. Here is the controller code :    require 'core/task' require 'infrastructure/task_repo'  class TasksController &lt; ApplicationController  before_action :set_task, only: [:show, :edit, :update, :destroy]   # GET /tasks  def index    @tasks = Infrastructure::TaskRepo.all  end   # GET /tasks/1  def show  end   # GET /tasks/new  def new    @task = Core::Task.new  end   # GET /tasks/1/edit  def edit  end   # POST /tasks  def create    begin      @task = Core::Task.new(task_params)      Infrastructure::TaskRepo.save(@task)       redirect_to task_url(@task.db_id), notice: 'Task was successfully created.'     rescue ArgumentError      render :new    end  end   # PATCH/PUT /tasks/1  def update    begin      @task.update(task_params)      Infrastructure::TaskRepo.save(@task)       redirect_to task_url(@task.db_id), notice: 'Task was successfully updated.'     rescue ArgumentError      render :edit    end  end   # DELETE /tasks/1  def destroy    Infrastructure::TaskRepo.delete(@task)    redirect_to tasks_url, notice: 'Task was successfully destroyed.'  end   private    def set_task      @task = Infrastructure::TaskRepo.load(params[:id])      @task.notify_when_done do |task|        TwitterClient::Client.update(task.description)      end    end     # Never trust parameters from the scary internet, only allow the white list through.    def task_params      params.permit(:description, :done)    end end    The controller is now dealing both with the Twitter connection and the database. This is visible in the controller test :    require 'rails_helper'  RSpec.describe TasksController, type: :controller do   before :each do    allow(TwitterClient::Client).to receive(:update)  end    # ...   describe \"PUT #update\" do    context \"with valid params\" do      let(:new_attributes) {        {done: true}      }       it \"updates the requested task\" do        task = Task.create! valid_attributes        put :update, params: new_attributes.merge(id: task.to_param)        task.reload        expect(task).to be_done      end       it \"tweets about completed tasks\" do        task = Task.create! valid_attributes         expect(TwitterClient::Client).to receive(:update).with(task.description)         put :update, params: {id: task.to_param, done: true}      end       it \"redirects to the task\" do        task = Task.create! valid_attributes        put :update, params: valid_attributes.merge(id: task.to_param)        expect(response).to redirect_to(task_url(task.id))      end    end     # ...     end end    We need to stub out the twitter API for most tests. We are also still using a mock to verify that the tweet is sent. Finally, as we can see from the test execution times, we are still using the database in some tests.      If the project grew large this would become an issue. Sadly, mocking is often the fix people jump on …      💡 Mocking is the unfortunate quick fix to slow tests.    From a mocking point of view, our current controller test can seem worse than before ! There’s something pretty effective we can do though !   In memory fakes   Instead of stubbing and mocking in every test, let’s write a full fledged in-memory fake that does the job we need. We could then install it once and for all, and forget about it !   Actually, this is nothing new. This is exactly what Rails provides out of the box with ActionMailer::Base.delivery_method = :test.   Here’s how we could do the same thing for our Twitter Client.   spec/rails_helper.rb    class FakeTwitterClient  def initialize    @tweets = []  end   attr_accessor :tweets   def update(message)    @tweets.push(message)  end end  RSpec.configure do |config|    # ...  config.before(:each) do    stub_const(\"TwitterClient::Client\", FakeTwitterClient.new)  end end   spec/controllers/tasks_controller_spec.rb    it \"tweets about completed tasks\" do  task = Task.create! valid_attributes   put :update, params: {id: task.to_param, done: true}   expect(TwitterClient::Client.tweets).to include(task.description) end   Simple isn’t it ?   Wait a sec …   There’s a catch though … How do we make sure that this fake is behaving the same way as the real thing ?   Let’s run the same tests on both ! We could mimic the twitter API in our fake, but that might not be a great idea. Do you remember the moto “Always wrap your 3rd parties” ? It takes all its meaning here, for 2 reasons.   The first is to make faking easier. We can build a minimal wrapper API that is just enough for our use. By keeping this interface small, we’ll make it a lot easier to fake.   The second reason is that we can write real integration tests on the 3rd party through this wrapper. They’d look like ordinary unit tests, except that they’d end up calling the real 3rd party in a sandbox. They are usually pretty slow, but as 3rd parties don’t change everyday, that’s ok. We can ensure up-front that integration will go well. As a bonus, we can be very fast to detect and contain changes to online services. (I’m looking at you Scrappers!)   Here is what it would look like for our Twitter client :   lib/infrastructure/twitter_client.rb    class FakeTwitterClient  def initialize    @tweets = []  end   attr_accessor :tweets   def tweet(message)    @tweets.push(message)  end   def search_tweets(text)    @tweets.select {|tweet| tweet.include?(text) }  end end  class RealTwitterClient  def initialize(&amp;block)    @client = Twitter::REST::Client.new(&amp;block)  end   def tweet(message)    @client.update(message)  end   def search_tweets(text)    @client.search(\"from:test_user #{text}\")  end end    As you can see, we renamed update to tweet in the wrapper. We’d have to update the calls accordingly. Let’s look at the tests.   spec/lib/Infrastructure/twitter_client_spec.rb    require 'rails_helper' require 'infrastructure/twitter_client' require 'securerandom'  RSpec.shared_examples \"a twitter client\" do |new_client_instance|  let(:client) { new_client_instance }  it \"sends tweets\" do    token = SecureRandom.uuid    message = \"Philippe was here #{token}\"    client.tweet(message)     expect(client.search_tweets(token)).to include(message)  end end  context FakeTwitterClient do  it_behaves_like \"a twitter client\", FakeTwitterClient.new end  context RealTwitterClient, integration: true, speed: :slow do  it_behaves_like \"a twitter client\", (RealTwitterClient.new do |config|    config.consumer_key        = \"TEST_CONSUMER_KEY\"    config.consumer_secret     = \"TEST_CONSUMER_SECRET\"    config.access_token        = \"TEST_ACCESS_TOKEN\"    config.access_token_secret = \"TEST_ACCESS_SECRET\"  end) end    We had to add a search method to our interface for the sake of testing. This should remain “For testing only”. We’d also adapt the controller test to use this search_tweets method.   Let’s look at where we stand now. We’re injecting each mock only once. Tests are fast yet straightforward, almost as if they were testing the real thing. Doing so, we’ve split our system in cohesive parts and we’ve wrapped our 3rd parties. We’ve actually done a lot more than removing mocks ! Mocking really is a design smell.      💡 Merciless mock hunting will improve the design of your system !    Last word about implementation   Sometimes, this 3rd party wrapper can become pretty complicated. Try to reuse as much of it as possible between the real and the fake. For example, an ORM, like ActiveRecord for example, is a wrapper around the database. Reimplementing a fake ORM would be real challenge. We’re far better plugin it on top of SQLite instead !   References   Smart people have already spoken and written about this subject. If you want to learn more, I recommend that you have a look at Aslak Hellesøy’s Testable Architecture talk. James Shore, the author of The Art of Agile Development, also wrote a pattern language called Testing Without Mock.   Next week   This was the 7th blog post in a series about how to avoid mocks. Hopefully, I’m reaching the end ! Next week’s post should be the last in series, and deal with a few remaining points. What to do when you really need a mock ? What about mocking and legacy code ?  ","categories": ["tdd","mocking","testing","programming","ruby","architecture","how-to-avoid-mocks-series"],
        "tags": [],
        "url": "/get-rid-of-mocking-maintenance-with-full-fledged-in-memory-fakes/",
        "teaser": null
      },{
        "title": "When is testing using mocks still a good idea ?",
        "excerpt":"In the previous 7 articles of this series, I’ve tried my best get rid of mocks. I’m pretty sure that using these techniques will get you a long way out of mock hell. Excessive mocking leads to unmaintainable tests. Unmaintainable tests lead to low coverage. Low coverage ultimately leads to legacy code. If you haven’t already, I encourage you to start reading from the beginning.   One question remains though : Is it realistic to get rid of all mocks ? An even better question would be : Are mocks always bad ? Are there situations when mocking is the best choice ?   When mocking still makes sense   Let’s to through a few examples.   Testing a generic wrapper   A few years ago, I had to write a service for an enterprise system. As any service, I had to ensure that it was returning nice errors. We decided to capture and wrap all errors from a few ‘gate’ points in the code. We built a generic wrapper that did only delegation plus exception wrapping. In this case, it made a lot more sense to test this with a mocking framework.    context ServiceErrorWrapper do   specify 'converts all kinds of exceptions' do    failing_object = object_double(\"Failing object\")    allow(failing_object).to receive(:long_computation).and_raise(Exception.new(\"Something terrible happened\"))     expect{ ServiceErrorWrapper.new(failing_object).long_computation }.to raise_error(ServiceError).with_message(\"Something terrible happened\")  end   # ... end    Not only did we reuse the wrapper many times in my service. We also ended up using it in other services as well !   Injecting a hand written in-memory fake   As you might have noticed, in the previous article, I recommended to use an in-memory fake instead of mocks. By nature, an in-memory fake is a kind of mock. Even if it is not defined by a mocking framework. (I actually think that by making mocking so easy, mocking frameworks often do more harm than good.)      💡 By making mocking so easy, mocking frameworks often do more harm than good.    Still, I used const_stub(...) to inject the in-memory fake.    config.before(:each) do      stub_const(\"TwitterClient::Client\", FakeTwitterClient.new)    end      I did this for 2 reasons :      Production code can continue to use a straightforward constant   I don’t risk forgetting to remove the mock at the end of its lifecycle, the framework does this for me   As I’m injecting the same fake for all tests, there is not much risk of test conflict (for the moment)   Testing a cache   The “raison d’être” of a cache is to avoid doing something twice. It should also return the same results as if it was not there. This is by nature almost impossible to test with state based assertions. Mock frameworks are great for this situation though. Here is an example :    context \"UsersController\" do  it 'caches users' do    expect(User).to receive(:load).once.and_return(User.new(name: \"Joe\"))     controller.login('Joe', 'secret')    controller.login('Joe', 'secret')  end end   The assertion could not be more explicit, we are checking that the expensive load was only done once.   Legacy code      In Working Effectively with Legacy Code Michael Feathers explains how to exploit “seams” in the code to put it under test. Mocking is straightforward way to inject behavior through a seam.   Mocking is a pretty good starting point but we need to be careful and keep a few things in mind. Legacy or not, we must not forget that too many mocks will make tests unmaintainable !      It’s a good idea to refer to a target design or architecture blueprint to know where to inject mocks. (I’ll write a post about this one day). This increases the chances to replace them with an in-memory fake later down the road.   Plan to replace the mocks with a better design as soon as possible.   It depends …   As with anything in software, there is no absolute rule about mocking. Even if I prefer not to 99% of the time, there are situation when testing using mocks is the thing to do. Knowing the risks, it’s up to you to decide !   If using a mock, prefer spy / proxies      As I explained in previous posts, mocks duplicate behavior. If we could use mocks without duplicating behavior, they would do less harm.   It turns out there is a flavor of mocks for that : spies and overlooked proxies. Proxies do the real thing but also record the calls and return values. It’s as non-intrusive as mocks can be.      💡 Proxy mocks are as unintrusive as mocks can be.    For example, here is how our cache test would look like using a proxy :    context \"UsersController\" do  it 'caches users' do    allow(User).to receive(:load).and_call_original     controller.login('Joe', 'secret')    controller.login('Joe', 'secret')     expect(User).to have_received(:load).once  end end   It’s more verbose, but simpler. Most mock frameworks provide some form of spy or proxies. A few years ago, I also wrote rspecproxies, a wrapper on top of rspec to make this easier.   This is the end   This was the 8th and last post in a series about how to avoid mocks. Before closing here is a list of other references about the topic.      In the RailsConf 2014 keynote, DHH  explains how mocking made their test harness unreliable.   Is TDD dead is a well known online discussion about the Classic vs Mockist TDD approach   Have a look at what Uncle Bob says about When To Mock   For JS expert Eric Elliott, Mocking is a Code Smell   In this talk  Testable Architecture talk, Aslak Hellesøy explains how to build a full architecture for fast tests   James Shore recently published a full pattern language entitled Testing Without Mock  ","categories": ["tdd","mocking","testing","programming","ruby","architecture","how-to-avoid-mocks-series"],
        "tags": [],
        "url": "/when-is-testing-using-mocks-still-a-good-idea/",
        "teaser": null
      },{
        "title": "A coding dojo exercises plan towards refactoring legacy code",
        "excerpt":"My current job at work is technical coach. I’m available for teams that need help to adopt incremental coding practices.   Problems with refactoring legacy code   A few months ago, a team which was struggling with a lot of legacy code asked for help. As you might know if you read my blog, I’m a big fan of Test Driven Development (TDD) because it has made my life as a developer so much more easy. I’m so used to TDD now, that even if I don’t have tests yet (as is the case when refactoring legacy code), TDD helps me :      To stick to baby steps which are a lot less likely to fail than larges changes.   Write testable code. I know what testable code looks like, and when refactoring, I’ll try to change it towards that.   That’s why we started to run regular, all team, coding dojo randoris. It was nice for the team dynamics, and the people where learning a lot of technical skills. I also got the feedback that they where not able to apply this directly on their day to day job though. After a bit more discussion, I understood that they did not know where this was going, what to expect, and when !      💡 Test Driven Development also teaches you what testable code looks like.    The coding dojo exercices   It turned out that a coding dojo exercises plan was enough to answer their questions. This is what it looks like.   Drawing      Mind Map   Here is another, more concrete, version, with sample names of katas we can find online.      Text   It starts with simple greenfield katas :      Fizz Buzz   Roman Numerals   Bowling   It goes on to intermediate katas, where we can use TDD to do design :      Mars Rover   Poker Hands   trading card game   From then on, it’s possible to tackle advanced katas and styles :      Refactoring fresh code            Continue design katas on 2 or more sessions       Always compile Constraint           Bottom-up TDD            Game of Life       Median of a list of lists (with no concatenation)       Langton ant           Top-Down TDD            Kata Potter       LCD           TDD on algorithms            Diamond       Kata Lags       anagrams           All this opens the gate to legacy code refactoring katas :      Gilded Rose   Race Car Katas   Ugly trivia game   Others from http://kata-log.rocks   At that point, the team can mob to refactor production code :      Real life, static analysis issue, mob programming session   Real life, code smell, mob programming session   Real life, larger mob Refactoring   What changed in practice ?   We wanted to split the teamwork and the coding dojos exercises. The team is now doing mob programming sessions on their usual stories twice a week (I’ll blog about that someday). But also doing regular coding dojos exercises in pairs.   Even if they did not go through all the TDD katas yet, mobbing on real stories helps the team to take on legacy code.      Given enough eyeballs, all bugs are shallow. Linus’s Law    Working in pairs on the code katas allows them to be more engaged in the exercises. In the end, it brings faster learning.      💡 A mix of Coding Dojos in pairs and Mob Programming sessions is a good way to teach TDD in a Legacy Code context.   ","categories": ["tdd","technical debt","coding dojo"],
        "tags": [],
        "url": "/a-coding-dojo-exercises-plan-towards-refactoring-legacy-code/",
        "teaser": null
      },{
        "title": "How to avoid unnecessary meetings (a takeaway from Devoxx France 2018)",
        "excerpt":"I had the chance to attend Devoxx France this year in Paris. Here is the most important lesson I learned :      How to avoid unnecessary meetings with asynchronous decision making    Bertrand Delacretaz, a member of the Apache foundation. He gave a great talk about how the open source community handles decision taking. Open source developers are often all over the world, often in different timezones. Meetings are not an option for them. Still, they manage to make great decisions !      Even if you don’t work remotely, avoiding unnecessary meetings is always a great thing !      You’ll have more time to do productive and interesting stuff   You’ll avoid interruptions and be even more productive   If you are an introvert, it’s going to be easier to contribute to the decision   As people have more time to think through the decision, the result is usually better     For a full walkthrough, I encourage you to watch the talk in full length. If you don’t speak french, an english version is available here. Finally, slides are also available in french and english.      💡 Even if you don’t work remotely, avoiding unnecessary meetings is always a great thing !    Crash course   For the hasty folks among you, here is a summary. The decision making follows 4 stages :      Open discussion and brainstorming. People discuss openly and suggest ideas in a free form manner.   Emergence of options. After enough discussion, a few options will start to make more sense than others.   Coming to a consensus. Someone will draft a formal proposal. People will discuss and amend this proposal until they reach consensus. Consensus is not unanimity !   Decision. After consensus, the benevolent decision owner validates the decision once and for all.   Until the decision is taken, the process can move forward but also backward.   Tooling   We need only two tools to make this possible :      For discussion, brainstorming and emergence of options, use a very open and chatty tool. The speaker called this a “shared asynchronous communication channel”. This can be an online chat, a mailing list or Github issues (ex). It could even be a real life whiteboard if you all had access to it.   From drafting the proposal to the end, prefer a structured and chronological tool. The speaker suggests using a “shared case management tool”. Draft the proposal in this tool, and use comments to log the latest steps of the decision taking. He had examples using Jira issues (ex) or Github pull requests (ex). To confirm the decision, close the case. The tool will record which version of the decision was exactly taken.   Architecture Decision Record      ADR is the practice of documenting architecture decisions. It makes sure we remember why we took a decision. This can be very useful to know how to deal with the existing software. A widespread practice for ADRs is to use simple text files in git. There are even tools for that. This looks like a perfect fit for decision making using git pull requests ! I’ll write a post about that when I get the chance to try.      💡 Git pull requests based asynchronous decision making is a perfect fit for Architecture Decision Records.    Currently experimenting   I am currently trying this whole decision making technique at work. We are still in the brainstorming phase. We are using our internal chat app for that. Options are starting to emerge, but we did not move to the consensus part yet. I’ll write a return on experience post when we reach the end.  ","categories": ["personal-productivity","remote","continuous improvement"],
        "tags": [],
        "url": "/how-to-avoid-unnecessary-meetings-a-takeaway-from-devoxx-france-2018/",
        "teaser": null
      },{
        "title": "How to convince your business of sponsoring a large scale refactoring",
        "excerpt":"Whenever I present or suggest a good practice to dev teams, I often get the same remark. Here is how it goes :           That’s a great idea and we would love to do this, but our code is in such a mess that we cannot !     Maybe you should start doing more refactoring then !     We would like to, but we don’t have the time. We are fire fighting all the time.      It’s a bit like the old adage of the lumberjack that is too busy to cut wood to sharpen his axe… The sad part here, is that most of the time, developers know they would be a lot faster if they could clean up their code. Unfortunately, they are usually not given the time.   How do we end up in this silly situation ?      Only developers see the bad code   As I’ve already been joking about, code is invisible. Mess in the code even more so, especially to people who don’t code. The code could look like that and no one would notice.              By A Tourist - Private photography CC BY-SA 3.0, Link        If someone put his own office in that state, he would get fired, but not for the source code. The good side is that we, developers, are safe, we can continue to wreak chaos without fear ! That’s pretty weird when we think that this is what we ship to customers …      💡 Is Diogenes syndrome for source code a recognized pathology ?    Business might also not see bad code because that’s the only thing they’re used to ! Maybe they’ve always been working in dysfunctional organizations that systematically create crappy code. Slow teams, late deliveries and fire fighting might be business as usual for them. From this point of view, trying to improve code is a pure waste of time and energy. The same goes for large scale refactorings.   The worse part of all this is that if devs don’t have the time to keep their code clean, it will only get worse. This will reinforce the view that software delivery is slow and that there is nothing to do about it !   Business has been burnt in the past !   Bad experiences are another reason why business is unwilling to sponsor refactoring. Did someone sell them an unrealistic productivity boost that turned in a never-ending tunnel project ? Badly managed large scale refactorings deliver late, create no value, and a lot of bugs. At one company I worked for, business gave devs 1 full year (!) to clean up the code … We took 2 !! Meanwhile, the CEO had to dilute the stocks a bit to keep the boat afloat ! I’d think twice before giving this kind of mandate myself.   Performing a large scale refactoring is not easy, and involves specific skills. These skills are about refactoring in baby steps, alongside feature delivery.   Usually, people acquire these skills through hard won experience … Unfortunately for us, our industry is not very nice to experienced engineers … It’s a lot easier to hire a fresh grad who knows the latest javascript framework than a 2 decades engineer. (Who, BTW, could learn this framework in 2 weeks …) It’s also a lot harder for the junior developer to succeed in negotiating a refactoring.   Again the twist of fate is that junior engineers are a lot more likely to start a submarine latest-framework.js rewrite supposed to solve all maintenance issues … which will only make things worse.   Overestimate, only as last resort   A quick fix is to systematically overestimate to get enough time to refactor. As any other ‘submarine’ initiative, I would recommend it only in last resort, after you’ve tried every other possible technique … and just before you quit.   Hiding things to the business people kills trust and hides problems. Trust and collaboration is what you need to get the business to sponsor large scale refactorings ! Plus, if ever you mess up (as submarine initiative often do) you’ll be the only one to blame …   That said, ‘overestimating’ so that you can write clean code is ok. It’s not overestimating, it’s estimating to do a good job.      💡 We should never ask the permission to do a good job. (Doc Norton)    To be continued   You might wonder what these other techniques are ! That’s exactly what I’ll go through with the next posts. This was the first one in a series about how to get sponsorship for a large scale refactoring. The series will cover topics like :      How to convince your business of sponsoring a large scale refactoring   Why we need Badass developers to perform large scale refactorings   5 mistakes badass developers never do   Principles That Will Make You Become a Badass Developer   Incremental Software Development for Large Scale Refactoring   Incremental Software Development Strategies for Large Scale Refactoring #1 : Constant Merciless Refactoring   Incremental Software Development Strategies for Large Scale Refactoring #2 : Baby Steps   Incremental Software Development Strategies for Large Scale Refactoring #3 : Manage it !   Incremental Software Development Strategies for Large Scale Refactoring #4 : a Pattern Language   Presenting a large scale refactoring as a business opportunity   5 Effective warning signals that will get you sponsorship for a large scale refactoring   Making the business case for a large scale refactoring - Part 1   Making the business case for a large scale refactoring - Part 2   Become a Business Partner and Stop Begging for Refactoring   How to maintain a business partnership about refactoring?  ","categories": ["refactoring","planning","large-scale-refactoring-sponsorship-series"],
        "tags": [],
        "url": "/how-to-convince-your-business-to-sponsor-a-large-scale-refactoring/",
        "teaser": null
      },{
        "title": "Why we need Badass developers to perform large scale refactorings",
        "excerpt":"My last post was about the challenge for dev teams to get sponsorship for large scale refactorings. I listed two main reasons :      The business doubts developers to have their interests in mind   They are also not aware of the cost of the current technical debt   This post (and the next) will be about how to gain the business’s trust. This is exactly where badass developers can help. Let me start with a story.      Back in 2002, at my first job, I started to read Refactoring, Improving the Design of Existing code. That’s where I read about unit testing. I found it so great that I made a demo to other programmers. Being the junior dev in the team, my co-workers reaction was something between “Meh” and “Maybe”. Fortunately, a more experienced and respected developer gave it a try. A few weeks after my demo, he announced to the team that unit testing worked great on new code. This time, people showed no questioning about his opinion : he if said so, it must have been true. Even business people blessed the practice !      I had given a good live coding demo, but it was this respected developer’s opinion that won the point. To convince business people of sponsoring a large scale refactoring, we need their trust. That’s why we need badass developers around.      💡 I had given a good live coding demo, but it was this respected developer’s opinion that won the point.    What is a badass developer              By Brooke Lark CC0, via Wikimedia Commons        Badass developers are first of all people who are credible to the business. This usually implies a track record of delivering features and refactorings. Badass developers understand the business constraints. That’s why they learned how to deliver refactorings alongside features. They also need to be responsible and bold enough to stand ground in front of the business. Finally, badass developers are able to train others.      💡 Badass developers are first of all people who are credible to the business    Unfortunately, there are not so many badass developers in the industry … It has a youngster bias, and tends to push experienced developers to other activities. As if 10 years of systems design was less valuable than knowing the latest language !   Learn more about Badasss developers   I tried to find other words before resorting to ‘Badass’. Unfortunately, I could find none that got the point so clearly. Uncle bob calls them ‘software professionals’ in The Clean Coder. Professionalism is not specific enough to me. Adam Nowak also calls them ‘responsible developers’ in a blog post.  That does not convey the idea that, sometimes devs need to stand guard in front of the business.      These concepts, though, are very close to my definition of a badass developer. Check by yourself :      How to be a responsible and badass developer by Adam Nowak   Developer under influence by Guillaume Duquesnay   The clean coder by Robert Martin   To be continued   This was why badass developers matter to the success of large scale refactorings. This was the second post in a series about how to get sponsorship for a large scale refactoring.  In the next post, we’ll look at what we can do to all become Badass developers.  ","categories": ["refactoring","badass-developer","large-scale-refactoring-sponsorship-series"],
        "tags": [],
        "url": "/why-we-need-badass-developers-to-perform-large-scale-refactorings/",
        "teaser": null
      },{
        "title": "5 mistakes badass developers never do",
        "excerpt":"Here is a one sentence summary of my previous post.      Badass developers negotiate large scale refactorings with the business better.    Unfortunately, not all of us are sitting next to a true badass developer … Hopefully, we can all become one ! Depending on our track record, it’s going to be more or less difficult, but with time and the good attitude, we can all do it. Becoming a badass developer is all about trustworthiness.   This post is the third in a series about how to get sponsorship for a large scale refactoring. If you haven’t, I recommend you to start from the beginning.   The first thing to become trustworthy is to avoid things that kill trust. Sounds obvious, but it’s very easy to forget. Here are 5 examples of trust killers you should never do if you want to become a badass developer.      Resume Driven Development   We should pick the best tools for the job. The best tools are often a bit old, precisely because they’ve been battle tested in production for a while ! That’s exactly the kind of technologies you want your business to rely on.   To keep their skills up to date, badass developers will not add a funky new tech in the production code. They would rather negotiate slack time with the business. They might also openly take their Friday afternoons to experiment the latest techs! They would simply explain that it’s to avoid polluting the production system.      💡 Badass developers will not add a funky new tech in the production code.    Over-engineering   Gold plating or over-engineering are similar anti-patterns. Badass developers always keep the business’s interest in mind. This means they know how to balance long term design and short term features. A good rule of thumb is to keep a vision in sight, but to get there in baby steps.   Build features with no agreed upon value   Product managers are here to select what should and what should not be in the product. As product experts, they are the ones who know how much a feature is worth. Except (maybe) when we are building tools for others developers, they know better than us. Adding something of dubious value in the product is bad in two ways.       First, it takes some time to build, time that we could use to build valuable features instead. Remember : Real developers ship !   Second, it creates unnecessary code to maintain. In the worst case, it can constraint the architecture. Which might eventually prevent us from adding other more valuable features afterwards.   Badass Developers only build features with clear value.   Hide in a tunnel   We should always be careful of the tunnel effect. Seeing their money vanishing with no visible output makes business people, understandably, creepy. As soon as things become too complicated, badass developera will raise the alarm. The fact is that they have been in this kind of situation before, and know how to recognize it. At that point, it’s still possible to discuss the problem with everyone, and adjust the plan.   As an interesting side note, I was at the Paris DDD Meetup last Thursday. We had the chance to welcome Eric Evans as a surprise guest ! When asked what were his worst mistakes, he said something along this line :      💡 Some of my biggest mistakes were not backtracking soon enough a few times as I was drifting in quagmire. Eric Evans               By Jérémie Grodziski, at Paris DDD Meetup        Let the team down   It’s not only about getting the business’s trust. We must also build trust from our fellow developers. Whenever we break the build and leave, or worse, deploy and leave, that trust is gone for a long while… We should not do that !   There’s more to a badass developer   I’m done with this list of things badass developers don’t do. Avoiding these is only the first step to become a badass developer. In next post, I’ll write about what we need to do if we want to build strong trust with the business.   This post is the third post in a series about how to get sponsorship for a large scale refactoring.  ","categories": ["refactoring","badass-developer","large-scale-refactoring-sponsorship-series"],
        "tags": [],
        "url": "/5-mistakes-badass-developers-never-do/",
        "teaser": null
      },{
        "title": "Principles that will make you become a badass developer",
        "excerpt":"There are some simple attitude principles that badass developers follow to gain the business people trust.   In my last post, I went over things we should avoid if we want to become badass developers. As I said though, this is far from enough. Once we’ve stoped losing trust from the business, it’s time to build some more ! This is the forth post in a series about how to get sponsorship for a large scale refactoring. If you haven’t, start reading from the beginning.   The badass developers gain the business’s trust by sneaking in as business partners. A good way to become one is to start acting like one ! Here are examples of principles for that.      Keeping the business best interests in mind   This is all about decision making. We should try to steer decisions according to the business. Whenever we talk with business people, we should stay away from technical topics. I still remember my younger self explaining threading details to a trader… Most of all, I remember the look on his face! We should avoid technical bla bla, but we should be clear about the business consequences.   Honesty and Candor   When we don’t agree with something, we should say so. When we don’t understand something, we should ask the question. We need to stick to facts and assume everyone has the business’s best interests in mind. Candor is a way to get our opinions and questions through, without sounding rude or pushy. There’s a whole chapter about candor in Creativity.inc, the book about Pixar.      With time, business people will think of us as a positive and pragmatic problem solvers. That is exactly the kind of people they want to work with !      💡 Candor is a way to get our opinions and questions through, without sounding rude or pushy.    Strong opinions, but weekly held   Jeff Atwood, already wrote about this. The idea is to fight for our opinions, but let them go without a fuss when we proved wrong. We know that we are all very often wrong. Only fools or self-centered people don’t admit this reality. Business people won’t trust us in either case. We need to show that we can go over our previous opinions. This grows our reputation of rational problem solver.   Acknowledging when we don’t know   The same logic goes with knowledge. None of us knows everything. We have to admit when we don’t know something and ask for help. This proves that we place the business’s speed over our personal ‘know-it-all’ reputation.   Here is a story that happened to me at my first job. I’m sure most developers go through it one day or another. I was assigned a new task involving technologies I did not know. I did not have the guts to state upfront that I would have to learn them. The result was that I sent 2 full weeks fiddling with this task to get something working. The more it went on, the more the product people were wondering why it was taking so long, and the more I got stressed !   Be bold and say No !   If we are sure something we should not do something, we need to say so. Badass developers are not afraid to say they won’t do it. Good software engineering requires merciless prioritization. If there are doubt about the value of doing something, it’s often a better idea to make sure before wasting time on it.   There are many ways to say ‘No’. Before giving a harsh ‘No’, we can try to challenge decisions. We can ask for clarifications and rationals through open questions. Very often, highlighting the risks makes people review their plans. As technical experts, we should also share as much of the consequences as possible.   In the end, badass developers are ready to leave a FUBARed situation. Great engineers don’t have troubles finding jobs these days … There’s no reason they should accept to be waisting their time.      💡 In the end, badass developers are ready to leave a FUBARed situation    What do to next ?   As we become badass developers, our reputation will grow. We’ll be in a better position to negotiate a large scale refactoring with the business. There’s a catch though : we’ll need to live up to our reputation ! Admitting that we are wrong 100% with candor will not make it !    When we manage to negotiate a large scale refactoring, the team will need to do a good job of it. This boils down to delivering it piece by piece, alongside features. This is exactly what my next post will be about.   This post was the forth post in a series about how to get sponsorship for a large scale refactoring.  ","categories": ["refactoring","badass-developer","large-scale-refactoring-sponsorship-series"],
        "tags": [],
        "url": "/principles-that-will-make-you-become-a-badass-developer/",
        "teaser": null
      },{
        "title": "Incremental Software Development for Large Scale Refactorings",
        "excerpt":"In the end, incremental software development techniques are almost always the safest way to refactor. Here is why.   My previous post was about the badass developer attitude. More specifically, how it can buy sponsorship for large scale refactorings. Unfortunately, attitude is not enough. We also need to be able to deliver in a way that builds trust with the business. Most of all, business is scared of the tunnel effect. Incremental software development techniques allows to deliver large scale refactoring step by step. Not only that, but it also allows to do so alongside business features. That’s how badass developers walk their talk and gain the business people’s trust.   This is the fifth post in a series about how to get sponsorship for large scale refactoring. If you haven’t, I encourage you to start from the beginning.      Why does incremental software development matter ?   A short story   A few years ago, I joined a team whose work involved a Domain Specific Language and a parser. The parser had grown in an ad-hoc way for a few years, and was now both very brittle and difficult to extend. We knew the way to go was to adopt a more solid parsing approach. We wanted to migrate to ANTLR and a multi pass parser.   As always, the business was very pushy for new features. There was no way we could have stoped everything for a full parser re-write. We explained to them that some of their features would be impossible to write without the new parser. We suggested that we work on migrating the parser as a background technical Epic. We did so using incremental software development techniques.   It took us a few months to migrate the parser. Meanwhile, we kept the software in a releasable state. This allowed us to validate our progress on the refactoring. We could continue to release features. We were able to share our progress on the refactoring with  the business people. They were very happy with the way we did this refactoring. In fact, it set a standard about how to prepare the software for big features later on.   The real problems   To understand why incremental software development works, let’s understand the alternatives’ problems. The main alternative it to do the refactoring in one massive task. This kind of initiative screams “Tunnel effect waiting to happen” ! The tunnel effect scares business people for 3 reasons :      Because they don’t know how much money they’ll need to put in the tunnel to get out of it   Because they don’t know when they’ll get the other features which they are also waiting for   To be blocked in the tunnel if something unexpected and important comes along      Delivering a large scale refactoring with incremental software development fixes these 3 points.      Every commit is a step forward while keeping the system in a releasable state. If ever something unexpected comes along, we can pause the refactoring for a while. (point 3)   Not all the team has to work on refactoring at the same time. Working on the refactoring does not block the delivery of other features. (point 2)   Finally, after working on a refactoring for a while, it becomes easier to have an idea of how long it will take. (point 1)      💡 Incremental software development fixes the business people’s fear of refactoring tunnel.    It is true that performing the refactoring in one team-wide batch would be more efficient. It would reduce the overall Work In Progress and get it done quicker. Unfortunately, it’s also a lot more scary for business people !   Incremental Software Development techniques   Like any skills, we can learn these techniques. Some are easy, and we can learn them from books. Others are more difficult and are usually learned through hard won experience. They can also be learned from a veteran developer in your team who’s been through all this already. If you aren’t or don’t have a veteran around, we can still manage. Deliberate practice is a great way to learn almost anything. Coding dojos are the thing here (I’ll write more about this later).   Once we master these skills, a lot of things change. First, we can do refactoring without harming our relationship with business people. Second, it builds enough self confidence among developers to negotiate with business people. This in itself, makes us more credible in their eyes. As a result, they are more likely to compromise on prioritizing refactoring.      💡 Mastering incremental software development builds self-confidence for developers.    To be continued   This was the fifth post about how to get sponsorship for large scale refactoring. In the next posts, I’ll deal headlong with the actual techniques. How to get bandwidth ? How to work in baby steps ? How to track the progress ? How to deal with the large scale ? Finally how to go further ? As you can see, there is still a lot to come, so continue reading !  ","categories": ["refactoring","incremental-software-development","large-scale-refactoring-sponsorship-series"],
        "tags": [],
        "url": "/incremental-software-development-techniques-for-large-scale-refactorings/",
        "teaser": null
      },{
        "title": "Incremental Software Development Strategies for Large Scale Refactoring #1 : Constant Merciless Refactoring",
        "excerpt":"Here’s everything you need to find the time for constant merciless refactoring … without asking for the permission!   My previous post advocated incremental software development for large scale refactorings. It’s less risky and it prevents tunnel effects. Most of all, it’s a lot easier to convince business people of engaging in a refactoring this way.   It’s one thing to understand why it’s the way to go, but it’s another to be able to do it ! In this post, I’ll start by explaining how to find the time to do constant merciless refactoring.   This is the sixth post in a series about how to get sponsorship for large scale refactoring. If you haven’t, I encourage you to start from the beginning.      Steal Take the time for constant merciless refactoring      If it hurts, do it more often ! Wisdom of the internet    As a child, I used to be a very untidy kid. Every few week, my room would get in a real mess, and my mum would order me to clean all this mess. I would then lose 1 or 2 hours tidying my room up. She used to tell me that if I kept things tidy as I used them, I would not have to lose this time. From the top of my 10 years old, I would judge this advice as nonsense.       Fast forward a few years, I am myself a parent and I’ve been working with legacy code for many years. These years have taught me how much I was wrong …      💡 The easiest refactorings to negotiate are the ones we don’t have to talk about !    The more refactoring we embed in ‘Business As Usual’, the more we’ll do, and the less we’ll argue with the business. We can wonder if this is still ‘professional’ ? In The Art Of Agile Development, James Shore explains that managing technical debt is the key to long term productivity. (Details in the Risk Management section of his book). As developers, we are the only experts in the code, the responsibility to keep it clean falls on us.      Never ask the permission to do a good job ! Doc Norton    There’s more to constant merciless refactoring ! It also keeps us in a good position to accept new features or refactorings later down the road.   Following are the 3 practices that make up constant merciless refactoring.   Yesterday’s weather and slack time   20 years ago, a promises of agile software development was to stick to a sustainable pace. When we are working with a flavor of Scrum, we can rely on it’s literature to inject some slack time.  Slack time is buffer time at the end of every iteration. We should not plan any work during the slack, to accommodate with the unexpected. It’s a way to deliver on forecasts, whatever happens.   In short, if your velocity chart looks something like that :      Scrum tells us to plan what you delivered in your worst iteration for the next one ! When things will work bad, we’re still pretty likely to deliver what we wanted. When things work well, we’ll have time to tackle refactoring.   There’s a lot more to say about slack time. How to take some when you are using Kanban ? How to make sure you keep some when your velocity becomes pretty stable ? How to do you increase your velocity in the long term ? (I guess I’ll have to write a full blog post about this some day.)   The Boy Scout Rule   I already blogged about the Boy Scout Rule. Here is how Uncle Bob wrote it :      Always leave the file you are editing a little better than you found it. Bob Martin    Following this simple rule goes a long way to keep the code clean and ready for larger refactorings. It works arm in arm with Yesterday’s weather principle. The extra time we take for clean up impacts our capacity to plan stories and features. This creates time to keep on doing the boy scout rule in future iterations.   How ‘clean’ the code should be is a team decision. Coding conventions and a static code analyzer are very important to track the boy scout rule. I learned that code reviews, pairing, mobbing and coding dojos are great to agree on coding conventions.   Embedding refactoring in features   The Test Driven Development loop goes as Red-Green-Refactor.      The same loop goes on at larger scale for Acceptance or Feature Test. When repeated many times, the loop could as well be Refactor - Red - Green. In fact, it’s a lot easier to refactor when you know what feature you want to build than at the end of the previous one. (Nat Pryce wrote about that in more details)      💡 “Disguise” refactoring as first technical sub tasks of features to get them prioritized.    All this to say, we should start our features with the pre-requisite refactoring ! We should not build features on shaky foundations. We should also impact our estimates. In fact, it’s a lot easier to justify to business people at that moment. We don’t need to mention ‘refactoring’ or ‘clean up’. We can use technical sub-tasks to track these technical refactorings. Technical sub-tasks are the team’s and don’t need to be understandable by business people.      To be continued   Finding the time for constant merciless refactoring is one thing, but how do we fit the work in these short slots ? In the next post, I’ll continue about how to actually work in baby steps.   This was the sixth post about how to get sponsorship for large scale refactoring.  ","categories": ["refactoring","incremental-software-development","planning","large-scale-refactoring-sponsorship-series"],
        "tags": [],
        "url": "/incremental-software-development-strategies-for-large-scale-refactoring-number-1-constant-merciless-refactoring/",
        "teaser": "/imgs/2018-08-02-incremental-software-development-strategies-for-large-scale-refactoring-number-1-constant-merciless-refactoring/recurring-cleanup-teaser.jpeg"
      },{
        "title": "Incremental Software Development Strategies for Large Scale Refactoring #2 : Baby Steps",
        "excerpt":"How can we exploit short time slots here and there to perform large scale refactoring?   My previous post was about how to get slots of time in your daily (or weekly) work to do some refactoring. I left my readers with the promise of techniques to fit the refactoring work into these small slots.   Obviously, it won’t be possible to perform any refactoring of any size in this way. With a bit of discipline and know-how though, it is possible to deal with quite a lot by splitting them up.   Baby steps are small increments of working software. The idea is that we test, commit, integrate and even deploy every small code change ! Using baby steps, we can perform large scale refactorings little by little. Refactoring in this way might seem involved, but it’s so much safer that it’s a no brainer once you’ve tried it ! Refactoring in baby steps can be challenging to master though.      10 years ago, I used to work in a large bank in Paris. I had been dabbling on my own with eXtreme Programming for a few years, when we started a small project. I was to become the informal XP coach. The project was about connecting to an online brokering service. It involved adapting an existing domain library. It went out pretty well. More precisely, we created very few bugs in production, which was very different from the norm at the time. I remember this feedback from the manager :      We managed to move the code from X to Y through a succession of working baby steps ! That’s pretty uncommon ! A manager in 2006    Keep in mind that this was 10 years ago. We had not done anything special except trying to apply eXtreme Programming. Nowadays, as Continuous Integration has become mainstream these skills are becoming more common. That said, we still need to learn how to apply incremental software development to large scale refactoring. This is what I’m going to write about today.   This is the seventh post in a series about how to get sponsorship for large scale refactoring. If you haven’t, I encourage you to start from the beginning.   Team TDD Coding Dojos   Learning to work in baby steps is not as complicated as it might first seem. The safest and easiest way is to setup a team TDD coding dojo. With its Red-Green-Refactor loop TDD sets up a baby steps rhythm. As I’ll explain in my next post, baby steps work best when all the team uses them. That’s another thing the team Coding Dojo helps with.      💡 TDD has a baby steps rhythm baked in.    We can push the learning further. For example, we can use the baby steps constraint during a few coding dojo sessions. With this constraint, we revert the code if tests fail for more than 2 minutes ! Here is a way to go at it :      Setup continuous testing : NCrunch in .Net, Guard in Ruby or Infinitest in Java   Only use automated refactorings or extremely simple code changes in order to ….   … keep the code compiling all the time …   … and cut the time the tests fail as much as possible   Mikado Method   One way to keep the tests green all the time is to use a slightly different TDD loop, as Nat Pryce suggests :              From Nat Pryce’s blog. Read the full post here        Here is how it goes.      Add a new failing test   If it’s trivial to fix, fix it. We are done   If not, see what’s missing in the code   Comment the test to get back to a green state   Refactor the code to add what’s missing (and use other tests if needed)   Uncomment the test   Repeat from step 2   When doing this at the scale of a real life story or feature, we’d use git stash instead of comments. This way of working has a name, it’s called the Mikado Method. It is at the heart of making baby steps work in real life.      💡 The Mikado Method is at the heart of making baby steps work in real life    Take a break   With TDD and the Mikado Method we can put the refactoring on pause. We can perform a small increment of the refactoring, commit and deploy it … and pause ! We’ll work on business features for a while, and resume the refactoring later on.      When done well, it feels slow. We have to remember that the alternative is to convince business people of prioritizing a refactoring … As we’ll regularly ship baby steps of a large scale refactoring, we’ll know we’re on the good track !   More to come   Unfortunately, even with bandwidth and skills, we are not there yet … It’s one thing for developers to do incremental software development of large scale refactoring on their own. It’s another to do it as a team !   This was the seventh post in a series about how to get sponsorship for large scale refactoring. Next post will be about how to manage constant merciless refactoring and baby steps as a team.  ","categories": ["refactoring","incremental-software-development","coding dojo","mikado-method","large-scale-refactoring-sponsorship-series"],
        "tags": [],
        "url": "/incremental-software-development-strategies-for-large-scale-refactoring-number-2-baby-steps/",
        "teaser": "/imgs/2018-08-07-incremental-software-development-strategies-for-large-scale-refactoring-number-2-baby-steps/baby-steps-teaser.jpeg"
      },{
        "title": "Incremental Software Development Strategies for Large Scale Refactoring #3 : Self-Organize !",
        "excerpt":"My previous posts where about how to find and use small time slots for large scale refactorings. Refactoring step by step is a series of slack slots, sub tasks of features and boy scout rule increments. Unfortunately, keeping track of all these is a challenge of its own. Here are some self-organization best practices for that.      Here is a little story of what can go bad. I used to work in a team which had a high ‘refactoring culture’. Everyone in the team wanted to apply the kind of practices I mentioned in my previous posts. To make things more tricky, we were working from 2 cities. We had introduced slack time and developers would tackle refactoring at the end of every iteration.   Unfortunately, we did not particularly organize or collaborate on slack time. As a result, we soon ran into conflicts. People wanted to refactor the same code, but in different ways! After a while we also had too many large-scale refactorings going on at the same time. This slowed down progress, increased the bus factor and the failure rate. Worst of all, it made it difficult to refocus on a newly discovered but urgent refactoring.   With a bit of self-organization though, we got things to work. Let’s see how we managed it.   This is the eighth post in a series about how to get sponsorship for large scale refactoring. If you haven’t, I encourage you to start from the beginning.   Self organize with a Design Vision   To succeed at anything, we need to know where we are going. It’s the same for large scale refactorings. We don’t need to have all the details of what we want to build. We do need a good enough draft to avoid going in the wrong direction though. That’s even more true when we work as a team. Without a shared design vision, people will refactor in conflicting directions.   It’s very important to share the vision with all the team. We can stick high level UML sketches on the walls for example. As Kent Beck suggests, we can also use metaphores to communicate the design. In this talk, Nat Pryce explains that it’s a great way start, but that we will have to drop the metaphore later.            By Nat Pryce - What we talk about when we talk about software from NEWCRAFTS Conferences on Vimeo      The idea is not to waste time in a Big Design Up Front. We just want to draft a vision :      We can run a Design level Event Storming. (Update: I wrote a full series about how to run your own)   We can grab a copy of Gamestorming and run any another kind of collaborative design game   Or a few team members could work on something the way they prefer   Whatever the technique we start with, we’ll be able to refine and evolve the vision down the road.   Self organize with Mikado Graphs   Remember the ‘Mikado Method’ from my previous post ? It’s a technique to code and deploy large scale refactorings in baby steps. If you had a look at the reference links, you might have seen mentions of a ‘Mikado Graph’. Here is what it looks like :              Sample mikado graph from http://mikadomethod.info/        As the number of refactoring steps grows, it becomes tricky to keep track of them all. A simple way is to draw them as nodes in a graph, and tackle the work from the leaves. If you are interested, check these posts about the Mikado Method. In my previous team, we became fans of the Mikado Method. We even built a tool to generate mikado graph from JIRA (our ticket management system) dependencies! Using colors, we could track where we stood in the refactoring.      A key benefit of mikado graphs is that we can stick them on the wall. This shows to all the team where we stand in a refactoring. This way, team members can collaborate during their slack. It can also make the boy scout rule more effective. When a developer touches a file that appears in the graph, he or she can move it further in the good direction.      💡 A key benefit of mikado graphs is that we can stick them on the wall for everyone to know where we stand in a refactoring.    Self organize with Metrics   I mentioned coding conventions and a clear Definition of Done in a previous post.  Having code quality constraints is the compass of constant merciless refactoring. To make this actionable and real, we should take the time to setup an automatic metrics system. For example :         Doc Norton suggests to track maintainability, coverage, complexity and coupling over time.   The A2DAM model suggests using specific rules to create Definition of Done constraints   Putting this in place will help everyone in the team to know if she or he should do more or less refactoring. The first benefit is that it prevents under and over engineering on new code. The second benefit is progress validation through metrics changes as we refactor.      Self organize with a bit of Planning   Granted, planning is not the most fun part of our job. It can save us a ton of work though. Joe Wright explains how they doubled their productivity by spending more time planning. If we want to make a good job of incremental refactoring, we’ll need to spend enough time preparing it. Important questions are :      What are the most important refactorings to work on ?   How many refactorings should we tackle at the same time ?   Are we making good progress on our refactorings ?   Why is this refactoring not yielding any visible results through our metrics system ?   Are there any news that should change our plans ?   Are we doing enough refactoring to keep things under control ?   What are the next steps in these refactorings ?   etc   I’m not talking about a big 6 month planning but rather regular short planning sessions. In Scrum this kind of planning happens every sprint. To make plannings more visual, engaging and fun, we might us something like Story Mapping. (I guess I should blog about this someday.)      💡 Keeping a Work In Progress limit on refactorings is essential.    Self organize through Time-Boxing   One last advice before I’m done. We must be very careful to time-box our work on refactoring increments. It’s all too easy, to get caught up in a refactoring during the slack at the end of the iteration. If we let the refactoring spill on features we risk loosing the business people’s trust.   Here again, using extra small baby steps helps to pause the refactoring. Another way is to do Kanban style slack. Replace end of sprint slack by a fixed number of people slacking all the time. But I’ll come back to this in more details in a future post.   Next post   Using this set of practices my team was able to deliver large scale refactorings in small steps.   That said, some refactorings remain very difficult to technically deliver incrementally. Fortunately, people have come up with patterns like the Strangler and the Bubble Context to cope with this. That’s what I’ll go over in the next post.   This was the eighth post in a series about how to get sponsorship for large scale refactoring.   In the previous post, I’ve been through why it’s so difficult to get sponsorship for a refactoring. Why a badass developer attitude is important? How to deliver refactorings steps by steps?   If you haven’t start by the beginning !  ","categories": ["refactoring","incremental-software-development","mikado-method","code-analysis","selforganizing","large-scale-refactoring-sponsorship-series"],
        "tags": [],
        "url": "/incremental-software-development-strategies-for-large-scale-refactoring-number-3-manage-it/",
        "teaser": "/imgs/2018-08-08-incremental-software-development-strategies-for-large-scale-refactoring-number-3-manage-it/refactoring-self-org-wall-teaser.jpeg"
      },{
        "title": "Incremental Software Development Strategies for Large Scale Refactoring #4 : a Pattern Language",
        "excerpt":"It can sometimes be a real challenge to integrate, let alone deploy, a refactoring step by step ! Here are some patterns that make this easier.   This is the ninth post in a series about how to get sponsorship for large scale refactoring. If you haven’t, I encourage you to start from the beginning. It’s also the fourth about incremental software development strategies for large scale refactoring. My point is that it’s not possible to sell a refactoring to business people until we master those :      How to find the time to refactor in our daily work   How to learn to work in baby steps   How to organize and manage this ongoing effort as a team   What about when it is not easy to split the work into incremental steps ?   Do you remember the DSL parser refactoring story in mentioned in another post ? Switching to a different parser technology incrementally sounds like an impossible mission. Even so, that’s what we did !    Here’s another story. A long time ago, I was working in a bank. We were to migrate imperative C++ financial contract models into declarative definitions in C#. We had to maintain an ugly adaptation layer. It made it possible migrate and deliver step by step. In the end, we suffered almost no bugs resulting from this transition.      Why the effort ?   Incremental refactoring implies going through Frankensteinesk intermediate situations. Situations where both the legacy and the new models exist in the software at the same time. This kind of adaptation layer costs time and energy, but doesn’t add value to the product either ! What’s the point of going through this ? Isn’t a Big-Bang change cheaper ? Here is why it is still worth doing :      It’s safer. With incremental delivery, we confirm that what we are doing is working in production. On top of that, if something goes wrong, as we only delivered a small increment, the problem is easier to diagnose.   It’s also safer in term of priorities. The system keeps working during all the refactoring. There’s no pressure to finish it before we can move on to the next ‘valuable’ feature. As I explained before it makes it possible to pause, and why not stop there for the moment. This can be helpful if we hit a new urgent priority.   Finally, it creates value earlier. Instead of having to wait 2 months to get all the value, you start getting a bit of this value every week. Even refactoring create value ! They reduce the time wasted to fix bugs. They increase our productivity. Sometimes, they even improve Non Functional Requirements of the system.      As we can see, the goal of incremental refactoring is not only to work in small steps. We also want to find a way to deliver value incrementally !      💡 Incremental refactoring is not only about baby steps, it’s also about early value delivery !    A Mini pattern language   As I said earlier, this is easier said than done. Some piece of code won’t let you refactor them step by step easily. Some will be too obscure to know where to start. Some will be just huge. Others will depend on an all encompassing third party. etc.   Here is a short pattern language to deliver large scale refactorings incrementally.   Discuss with a domain experts   Goal  We need to refactor code containing a lot of domain knowledge   Conditions  We have a domain expert available   Therefore  Have regular discussions with the domain expert to find the best modeling possible.   Consequences     💚 We get simpler code than by trying to replicate the twisted legacy logic   💚 Can save a lot of work by skipping deprecated aspects.   💚 Chance to get bug fixes or new features for free   ⚠️ The system does not exactly behave as it used to, which can cause integration problems   Difficulties  It can sometimes be difficult to find a domain expert …   A lot of the presentations at the July Paris DDD Meetup were about how to find domain experts. Who actually seem to be pretty rare beasts ! Here are my notes.      Bubble context      Goal  We want to refactor a large piece of code with no Big-Bang   Conditions  We have access to the internals of the code to refactor   Therefore     Create a new bubble of clean code (a namespace, a package …)   Rewrite a piece of legacy code in the bubble   From the legacy code, delegate to the bubble   Repeat until the legacy code is not used anymore   Consequences     💚 Enables a step by step continuous delivery of the new version   💚 It is possible to eventually transform the API of the system   💚 It’s easy to add new features in the bubble long before the refactoring is over   Difficulties     Need to understand the legacy enough to find good delegation points   Need to understand what the old small piece of code was doing to re-write it. A domain expert might be mandatory.   Strangler   The bubble context grows from the inside, but the strangler starts from the outside.   Goal  We want to refactor a large piece of code with no Big-Bang   Conditions  We can keep the same interface (API) for the legacy and the refactored versions   Therefore     Wrap the existing code   Re-implement calls in the wrapper   Delegate the rest to the legacy   Repeat until you support all the interface   Remove the legacy code   Consequences     💚 Enables a step by step continuous delivery of the new version   ⚠️ Maintenance of the wrapper and both versions of the code during all the refactoring   Difficulties     Interaction between the legacy and the refactored version is not always as simple. For example when the wrapped code is stateful   The granularity of the steps is the (method) calls to the interface. They need to be small enough for the whole process to be incremental   Remember my story about how we switched our DSL parser to ANTLR ? We used a Strangler to do this.      Feature toggles   Sometimes, we just don’t find a way to deliver a refactoring to users step by step. I’ve seen situations where all incremental paths implied a temporary impact on NFRs. Ex : Keeping both versions of the code implied an increase in resource consumption.      Goal  Incrementally build a refactoring that we cannot deliver piece by piece to all our users.   Conditions  When we cannot find a way to incrementally deliver our refactoring to bulk of our users   Therefore  From the code, dynamically switch to the different versions depending on runtime configuration. This way, we let most users stick to the legacy version. Yet, we can build, test, integrate and deploy the new version to beta testers.   Consequences     💚 We can build, integrate and test our refactored code in baby steps   💚 We can beta and A/B test our refactored code   ⚠️ We need to maintain and evolve both versions of the code for a long time   ⚠️ We need to maintain the switches in the code   ⚠️ We only deploy to beta testers, and don’t get as much early value   ⚠️ Duplicate the Continuous Delivery pipeline to test different feature toggle sets   Difficulties   Maintaining feature toggles is a mess. Thus, we need to      As much as possible, prepare the code to reduce the number of switches. Ref : Branch by abstraction   Hunt down the number of active feature toggles at any given time   Reduce the scope of toggles. Where possible, we should push things out of the toggle into stranglers or bubbles.   Feature toggles are an alternate to branches. Even if toggles are painful to use, branches are worse ! I’m not going to go over branches. If you want to see why we should not use branches, check this talk.        💡 Feature toggles are painful, but branches are worse !    Final word   I’m done writing about Incremental Software Development Strategies for Large Scale Refactoring. This is only what I currently know about this very important topic. There’s one last thing we need to do to be successful at it. We all need to keep an eye on new ideas from the community, and to share this with our teams as much as possible.   This was the ninth post in a series about how to get sponsorship for large scale refactoring. If you haven’t start by the beginning ! In next post, I’ll start to go over how to present a refactoring in financial terms to business people.  ","categories": ["refactoring","incremental-software-development","ddd","large-scale-refactoring-sponsorship-series"],
        "tags": [],
        "url": "/incremental-software-development-strategies-for-large-scale-refactoring-number-4-a-pattern-language/",
        "teaser": "/imgs/2018-08-14-incremental-software-development-strategies-for-large-scale-refactoring-number-4-a-pattern-language/pattern-teaser.jpeg"
      },{
        "title": "Presenting a large scale refactoring as a business opportunity",
        "excerpt":"Have you ever tried to talk about refactoring with business people ? Most of the time, the matter is pushed aside or received with rolling eyes … A few weeks ago, someone on Hacker News asked the question “As a CTO, what is your most frustrating problem with technical debt?”. Here is the most voted answer      I think a lot of the time when a developer shouts “technical debt” what they are really shouting is “code someone else wrote that I’d rather rewrite than understand”. (The rest of the time is the same but they’ve understood it enough to think it’s a disaster area.)       I have found it’s best to not take tech debt complaints very seriously and instead look at actual success metrics. For example if every change to a bit of code introduces new bugs then that might be a reason to tidy it up.       ghiculescu    We need a constructive dialogue with business people to get sponsorship for important large scale refactorings. Let’s see what we can do to have one. This is the 10th post of a series about how to get sponsorship for large scale refactoring. If you haven’t start by the beginning.      Presenting a business opportunity   We must be very careful about how we present refactorings. We don’t want them to be seen as unnecessary chores, or the latest tech fashion to follow. These don’t bring value, and business people will run away from such refactorings. Instead, we should present business opportunities for higher productivity to invest in.   Presenting a similar success story and its impact on the business   Most of our companies have been through similar refactorings in the past. We can try to find a successful one and draw parallels to forecast benefits for the business. If the company is too young to have any or they were all failures, we can have a look in the whole industry. When I was at Devoxx in Paris, Hervé Lourdin the CTO of VideDressing presented how they managed to do a large scale refactoring. Among other things, he went over how he managed to get sponsorship from his board. If you understand French, have a look at the full talk. In this case as in most, a likely promise of reduction in the costs of bugs and new features is what made the point.        💡 In most cases, a likely promise of reduction in the costs of bugs and new features is what gets a refactoring prioritized.    Be a Badass Developer   I wrote a lot about being a badass developer earlier in this series. This is when it becomes crucial. Being badass is a way to gain the trust of business people. Without this trust business people will react like the guy on Hacker News. Badass developers are way better at presenting large scale refactoring as business opportunities.   Find a path to do incremental delivery   Without incremental delivery, a refactoring risks delaying features for an unknown time. That scares the hell out of business people. Day to day incremental refactoring best practices will save the day here. They’ll actually help 3 times !      To already perform a lot of refactoring in day to day work   To learn how to find an incremental refactoring path to present to business people   To prove that we know what we are talking about : we’ve already been doing it for a while   If you haven’t, have a look at the articles about incremental refactoring techniques that I wrote earlier in this series.   Pitch It !   In “corporate” environments, building credibility is a lot about being convincing. The more we learn to be convincing, the more likely we are to have our refactoring prioritized. If you are ready to spend some time learning how to pitch, I recommend reading Pitch Anything. At least have a look at its summary. It contains many actionnable nuggets to deliver powerful pitches. Here are a few.      When we present an idea, we should put “frames” (time, authority …) in place to gain control of the discussion. For example, a time frame is a kind of deadline that will urge people to take action now. (Before overthinking it …)    We should create tension by alternating phases where we are giving and phases where we take a step back. Following the same idea, it also mentions the Tao of Steve to rule at negotiations :      Don’t want anything   Show that you are really good   Leave at the crucible moment   Giving a great pitch is a great way to present large scale refactorings as business opportunities.      💡 Pitching is a great skill for developers to get sponsorship for a refactoring.    More to come   This was the 10th post in a series about how to get sponsorship for large scale refactoring. In next week’s post, I’ll go over how to use a recent discovery about how our brain works to become even more convincing !  ","categories": ["refactoring","psychology","large-scale-refactoring-sponsorship-series"],
        "tags": [],
        "url": "/presenting-a-large-scale-refactoring-as-a-business-opportunity/",
        "teaser": "/imgs/2018-08-22-presenting-a-large-scale-refactoring-as-a-business-opportunity/refactoring-opportunity-teaser.jpeg"
      },{
        "title": "5 Effective warning signals that will get you sponsorship for a large scale refactoring",
        "excerpt":"In 2005, professors Bizer and Petty showed something interesting about human behavior. People make more efforts to avoid what they don’t want, than to get what they would like. The study itself is interesting, you can have a look at it here. For example, it explains why political campaigns are getting more and more nasty. There’s also a lesson for us, mere developers. We’ll get more sponsorship for our refactorings if we highlight the dangers of not doing them !      This is the 11th post of a series about how to get sponsorship for large scale refactoring. If you haven’t already, start by the beginning.   From the inside, many systems are in such a messy state that seem like a catastrophe waiting to happen. Unfortunately, this mess is completely invisible to non-developers ! Here are a few techniques to show how close we are from a total breakdown !   Effective Warning Signal #1 Pranks   If you have the guts and your company is fun enough, you can try one of these pranks.              What if we printed the whole source code and dumped it on non-developers desks ? By Kris Krüg, CC BY-SA 2.0, Link        They’re bound to have a big impact … but they might also get you fired ! We should be creative and find both effective and acceptable pranks. Pranks are a lot more effective than we first think. Non-Violent Revolution activists have actually used Laughitism to take dictators down ! For a good (and unexpectedly fun) read on the topic, have a look a Blueprint for a Revolution. It was written by Serb non violent activist Srdja Popovic member of OTPOR!, who brought Millosevic down .      Promised, as soon as I manage to use such a prank without getting fired at work, I’ll blog about it !   Effective Warning Signal #2 Dice of Debt Game   While doing my researches for this article, I found the Dice of Debt game. It’s aimed at making business people experience the long term legacy code drag. It has good reviews, have a try at it with your business people and post back your feedback ! I’ll do so as soon as I have a chance to test it myself.   Effective Warning Signal #3 Higher authority   Appealing to a higher authority works as long as people recognize this authority as so. Knowing the people in front of us, it’s our job to bring up the reference in an effective way. Here are   In this talk, Doc Norton, a recognized technical debt expert, shows the link between technical debt and productivity.      Showing similarities between our own code metrics and this graph might ring the alarm bell in business people.   Another interesting model out there is A2DAM. It was built through the Agile Alliance. It can be used to estimate the value of a codebase when buying a company. Maybe business people will listen if we tell them that their software would be worth 0 on the market ?   Effective Warning Signal #4 Metaphor   I was recently working with a team that is preparing a pitch to get sponsorship for a refactoring. They want to rework multithreaded code that uses locks and other low level synchronization. The hand written synchronization is becoming difficult to maintain. They would like to refactor it with the actor model. Business people will likely argue that this module is now stable enough and should stay as is. We all know that this is not the case with bogus multithreaded code : the more you use it, the more bugs you find ! They had the idea to use the email vs phone metaphor. No one in the room could have handled all his daily emails with a phone only ! Everyone understood why it was necessary to switch to actors.   In A Taxonomy of Technical Debt, Bill Clark adds another dimension to technical debt. On top of the classic principal and interests he adds contagion. In fact, it’s a bit as if he’s ditching the technical debt metaphor for the disease metaphor ! Disease have a cost to live with, a cost to heal from and a contagion rate. People at the agile alliance also noted this self reinforcing behavior. This metaphor might be better for “cruft”. Ward Cunningham’s original metaphor of technical debt only applied to tested code.        💡 A disease might be a better metaphor than debt for code cruft.    Effective Warning Signal #5 A horror story   We said that a successful refactoring story will be useful to frame ours as an opportunity. We can have more impact with the opposite ! We should relate a large software failure, that had impact on the business to bad code. The bigger the impact on the business, the better it is. If you (unluckily) have something like that at your company, it should be a very powerful argument. If you don’t, try to find a public story on the internet, or a public conference. For example here is one from The 10 Worst Programming Mistakes in History.       💡 The Therac-25 (a radiation therapy machine) killed 6 people because it was difficult to perform automated tests !    We can draw parallels and forecasts, to highlight the high risk of failure.   Almost there   This was the 11th post in a series about how to get sponsorship for large scale refactoring. Unfortunately, presenting refactoring in a good way only brings us so far. If we want to be really convincing, we need to use quantitative data. That’s going to be the topic of my next post.  ","categories": ["refactoring","joke","technical debt","psychology","large-scale-refactoring-sponsorship-series"],
        "tags": [],
        "url": "/effective-warning-signals-to-get-sponsorship-for-a-large-scale-refactoring/",
        "teaser": "/imgs/2018-08-27-effective-warning-signals-to-get-sponsorship-for-a-large-scale-refactoring/legacy-monster-teaser.jpeg"
      },{
        "title": "Making the business case for a large-scale refactoring - Part 1",
        "excerpt":"With a bit of discipline, we can make a factual business case for a large-scale refactoring that business people won’t resist.   The 2 previous articles were about how to present large-scale refactorings to business people. This might already get us a lot closer to having our large-scale refactoring prioritized. Unfortunately, many times this won’t be enough. Hopefully, we can add something to make the point to business people : numbers !   Business people love numbers. Presented well, they make decisions a lot easier to take. Making a business case is about estimating the costs of doing versus not doing a refactoring.      This is the twelfth post in a series about how to get sponsorship for large-scale refactorings. If you haven’t, I encourage you to start from the beginning.   Before anything   Before we start any computation, we need to know what we are talking about. This means that as a team, we must agree and specify enough the refactoring we want to do. A small group design session might be useful at this point.      💡 We must define exactly what refactoring we want to do before making a business case for it.    Refactoring Cost   We pay for a refactoring only once, when we do it. The cost of the refactoring is the time it takes to do it. We can do a team estimate on the refactoring and see the numbers.   From my experience large-scale refactoring estimates are pretty inaccurate. If the team is having a hard time agreeing on estimates, it’s a warning! Recipes against this are :      Do a bit more design to better define the refactoring   Build a safety margin for your estimation. Times 2 is not a bad ratio to start with (cf. the risk management section of The Art of Agile)      Eventually, be sure to convert your estimates in man.hours. If you don’t, it will be difficult to compare this with the non-refactoring cost.   Non-Refactoring Cost   Contrary to the previous cost, this one is recurring. We’ll only compute it for a single period of time, let’s say an iteration. It also works fine with a month, a week or whatever.      💡 The cost of not doing a refactoring is the sum of all the wasted or unproductive work generated by the current code.    The non-refactoring can be computed like:   non-refactoring cost = Bug fix time + Support time + Lower productivity   Bug fixes &amp; Support time   Bad code generates bugs and support time. By doing the refactoring well, we’ll almost kill these wasteful activities.   We want to get the real amount of time the team is spending on bug fixes and support time related to our refactoring. Hopefully, our ticketing software might already contain enough time tracking data. That’s what I used when we did our improvement kata.      If not, we’ll unfortunately need to do a bit of manual time tracking for a while. It should be fine to cover a few days, and extrapolate for a longer period. Unbelievably, some people made time tracking fun by using Lego Blocks!              Photo by Nancy Van Schooenderwoert. Read the full story here        Lower productivity   Bad code also makes us slow! By doing this large-scale refactoring, we should get a productivity boost.   Here again, we are only going to measure for an iteration, and extrapolate for the future. If we are working with iterations, we should have an ‘estimation’ meeting of some sort. At the end of it, we should have selected a list of ‘tasks’ for next iteration.   Next time we go through this meeting, we’ll need to estimate these tasks twice.      Do a normal estimation   Now, imagine we did the refactoring. Re-estimate.   The difference between the estimates is the non-refactoring productivity loss.   productivity loss = normal estimation - estimation after refactoring   Payback Period   We need to convert everything in man.hours. We should now have figures for both total refactoring and recurring non-refactoring cost.   payback period = refactoring cost / non-refactoring cost   The payback period is the refactoring cost divided by the non-refactoring cost. It’s the time after which the refactoring will pay for itself.   To be continued   We’re done with computations. Next week, I’ll go over what to do with these numbers. We want to make business case for this large-scale refactoring an unmissable offer!   This is the twelfth post in a series about how to get sponsorship for large-scale refactorings.  ","categories": ["refactoring","technical debt","business value","planning","large-scale-refactoring-sponsorship-series"],
        "tags": [],
        "url": "/making-the-business-case-for-a-large-scale-refactoring-part-1/",
        "teaser": "/imgs/2018-09-17-making-the-business-case-for-a-large-scale-refactoring-part-1/legacy-vs-refactoring-teaser.jpeg"
      },{
        "title": "Making the business case for a large scale refactoring - Part 2",
        "excerpt":"How to improve a factual business case for a refactoring to make it even more compelling to business people.    In last post, I explained how to make a business case for a large scale refactoring using real numbers. Numbers are great, but they won’t get us sponsorship by themselves. We need to understand them. We need to make sure they make sense. We need to make sure they are backing our refactoring up. In the end, we might need to improve them.      This is the thirteenth post in a series about how to get sponsorship for large scale refactorings. If you haven’t, I encourage you to start from the beginning.   What If Payback Period is too long?   If payback period is close enough, then great, we can go out and pitch the refactoring. Most of the time though, large scale refactorings take quite some time to payback.   Product Life Expectancy   The first thing to do is to put this in the perspective of the company’s or product’s life expectancy. Imagine a product which is 10 years old and expected to cash-in for another 10 years. In this context, a refactoring that pays for itself in 1 year is a great opportunity!      💡 The longer the life expectancy of your product, the more refactoring you should invest in!    Picking the age of the product as its life expectancy is realistic most of the time. New products have low life expectancy, but legacy systems seem to last forever!   Split the refactoring!   A strategy to reduce the payback period is to split this large scale refactoring. Can we make it smaller, more focused, or find sub steps that pay for themselves faster? Here again, incremental refactoring techniques will be critical.   Did we do an error?   It’s also possible that we did an error in the business case computation. Sampling and logging are error-prone techniques.   Is the iteration we took for sample is representative enough of the future work ? In doubt, we can re-do the sampling or the computation. Using better data leads to a better conclusion.   There are also other costs we did not take into account to be able to stick to man.hours. Let’s have a look at these.   Ideas for Improvements   Convert to Money   If we have access to money numbers, we should be able to improve the figures with new costs.   First, we’ll need the average wage of team members to convert our figures in real money.   If we have the figure, we can add the image cost of a bug to the non-refactoring cost.   Finally, if we have the revenue per feature, we can add the opportunity cost to the refactoring cost. Opportunity cost is the cost of not working on features !      Use a similar refactoring   Did someone do a similar refactoring in the past? If so, we can use it to improve your estimates about:      Refactoring time   Productivity improvement   Time saved on bugs and support.   Is it a good idea in the end?   Are the numbers still arguing against the refactoring? Maybe it’s not such a great idea afterwards … We ought not to argue for something of dubious value, our credibility is at stake.   At this point, it might be a good idea to look for another improvement to do. Maybe there is other code to refactor or a new productivity tool to build.      💡 Avoiding bad moves is a key benefit of making a business case for large scale refactoring!    Other References   I’m fond of this way of prioritizing software. It’s the way out of bickering about best practices, and towards sustainable pace. If you want to learn more about this, here are 2 helpful references :      A blog post Making Technical Debt Visible . It explains how to use the scrum sprint backlog to display the cost of technical debt.   A PluralSight course Making the Business Case for Best Practices. It’s just great. It contains a ton of practices to help us to get realistic numbers and estimates.              Visual notes from Making the Business Case for Practices. Large image        Next post   This is the thirteenth post in a series about how to get sponsorship for large scale refactorings. We’re reaching the end ! In next week’s post, I’ll go over Business Partnership. There are some practices we can put in place once we have earned the trust from business people. Stay tuned !  ","categories": ["refactoring","technical debt","business value","planning","large-scale-refactoring-sponsorship-series"],
        "tags": [],
        "url": "/making-the-business-case-for-a-large-scale-refactoring-part-2/",
        "teaser": "/imgs/2018-09-18-making-the-business-case-for-a-large-scale-refactoring-part-2/refactoring-trim-teaser.jpeg"
      },{
        "title": "Become a Business Partner and Stop Begging for Refactoring",
        "excerpt":"When we, developers, earn enough trust from business people, we become their business partners. As such we enjoy a lot more freedom to refactor.   A team I was in a few years ago had worked hard to earn the trust of business people. In a retrospective, we discussed about our failure to work on the improvements we had agreed to. We decided that we would dedicate 20% of our story points to improvements and refactoring. Better yet, we would start the sprint with these, before working on features! Here’s what our sponsor said:      As long you are sure it’s the best thing for the product, I trust you.    This is the kind of autonomy business partners can enjoy.      This the 14th post in a series about how to get sponsorship for large scale refactoring. If you haven’t already, I recommend you start from the beginning.   Becoming a business partner   All the tricks and advices from the previous posts should lead to the following outcomes:      Time for refactoring   Trust from business people   With time, it establishes a solid business partnership. This means that business people will acknowledge that we know what we are talking about. When we say we need to refactor something, they’ll let us do without much protest.   Once we reach this level of trust with business people, a new world of practices opens. Here are few I’ve used or heard people talk about.   Negotiate Technical Debt Interests   My friend Xavier told me once that he had discussed how to deal with technical debt with a Scrum coach. The coach looked puzzled, and asked why we did not simply negotiate interests up front! When someone asks for a shortcut, we should explicit the recurring cost to fix it.   Here’s an example: suppose we are asked to deliver a 8 points story in half the time for a sales show. We could say OK provided we’d also pre-plan 2 story points refactoring tasks for the next 4 sprints.   Agree on a decentralized decision rules   Boing had weight issues when designing the 777. Instead of a weight reduction project, management issued a decentralized decision rule.      Any engineer could swap a pound weight reduction with an $300 increase in production cost. (The Principles of Product Development Flow by Donald G. Reinertsen)       Pretty soon, the plane had lost enough weight to be viable. Here’s another example:      If we speed up the build of 1 minute in less than 16 hours of work, we should do it.    If you are wondering how to create such rules, have a look at this post. For yet another example, check this bug definition that came out of an improvement kata.   These rules are powerful because they are agreed to once, but used many times. They also don’t involve any management escalation or delay. Only high level of trust with business people can lead to such rules.      💡 Decentralized decision rules are agreed once, but used many times, without involving management.    Negotiate a refactoring bandwidth   Negotiating refactoring on a task by task basis takes a lot of time. Once we’ve become business partners, we can negotiate a permanent refactoring bandwidth.      💡 Becoming a business partner lets developers have a permanent refactoring bandwidth.    For example, in her talk What ever happened to being eXtreme? Rachel Davies explains how her long lived XP team was doing that. At any moment, among 7 developers, 3 were working on technical (or refactoring) tasks.    Rachel Davies - What Ever Happened to Being eXtreme? from NEWCRAFTS Conferences on Vimeo.   As with decentralized rules of thumb, the main advantage is that negotiation is done once and for all.   Almost there !   This was the 14th post in a series about how to get sponsorship for large scale refactoring. In the next post, I’ll dig into what we must be careful about to remain business partners.  ","categories": ["refactoring","planning","large-scale-refactoring-sponsorship-series"],
        "tags": [],
        "url": "/become-a-business-partner-and-stop-begging-for-refactoring/",
        "teaser": "/imgs/2018-10-03-become-a-business-partner-and-stop-begging-for-refactoring/business-hand-shake-teaser.jpeg"
      },{
        "title": "How to maintain a business partnership about refactoring?",
        "excerpt":"Becoming business partners grants us freedom to refactor. Careful though, we must not over-abuse this trust if we want to maintain this partnership.   Remember my team from the previous post? The one that had managed to build a great business partnership with its sponsors. I did not tell the full story though. After a while, we wanted to see something different and a few developers moved to other teams. Eventually, new devs and a new team lead where brought in. Unfortunately, the team’s practices did not survive the change. Not long after, a few misguided refactoring broke the business partnership.         With great power, comes great responsibility. Uncle Ben    As business partners, we can enjoy freedom to refactor. Maintaining this partnership should be one of our top priorities. Keep on doing what got us in there is the obvious first thing to do. We also need not to over-abuse this trust. Doing unnecessary refactoring is a tremendous trust killer. Here are a few practices to avoid this trap and maintain this business partnership.   This the 15th and last post in a series about how to get sponsorship for large scale refactoring. If you haven’t already, I recommend you start from the beginning.   Kanban-style timeboxing   Timeboxing is a good way to avoid wasting time on refactoring. Unfortunately, timeboxing is always a lot easier said than done. Pausing a refactoring between 2 commits or in the middle of a large find-replace wastes time too! Expanding the time box is not a solution either. It makes both measuring and enforcing the time spent refactoring very tricky.    Do you remember Rachel Davies talk I mentioned in my previous post? Her team managed to get this right. Their trick was to negotiate a refactoring bandwidth of n developers full time. This has many advantages:      First, it’s a lot easier to measure the time spent refactoring. That’s great for continuous improvement and trust with business people.   Second, it involves less task switching. Once we start a refactoring, we don’t stop until it’s done.   Finally, it’s kanban-style slack time. Under extraordinary deadline, the refactorers can temporarily switch to other more urgent work.      💡 Timeboxing with ‘x team members full time’ is a lot easier to do than with typical time constraints.    Focus on high return on investment refactorings   Obvious, but it’s very easy to waste time on the wrong refactoring. There are a data-driven techniques to stick to the most important refactorings.      💡 Use data to spot the best refactoring to work on.          Some tools, like CodeScene show us the hotspots in our codebase. Working on these cannot be a bad move !   FIXME XXXX comments is a trick to collaboratively identify the most painful problems. Whenever a developer is bothered by badly designed code, he would add a FIXME comment or a X to an existing one. After a few weeks, the team can simply start by refactoring the FIXME comments with the most stars!   Doc Norton suggests tracking 4 metrics to detect if a code base is degrading. As soon as we see these metrics degrade for a section of the code, we should think hard about refactoring it.      I’m done!   I hope these practices will help you to maintain your business partnership. Business partners have great freedom about how to tackle refactoring. If you’ve been in this situation before, you might have come up with innovative practices. Please share them below, I’m sure it will interest a lot of people.   This was the last post in a series about how to get sponsorship for large scale refactoring. I’ve been writing about refactoring for almost 4 months. I’ll be glad to write about something different next week! Don’t worry though, I’m sure I’ll come back to this topic again.  ","categories": ["refactoring","planning","large-scale-refactoring-sponsorship-series"],
        "tags": [],
        "url": "/how-to-maintain-a-business-partnership-about-refactoring/",
        "teaser": "/imgs/2018-10-04-how-to-maintain-a-business-partnership-about-refactoring/spiderman-teaser.jpeg"
      },{
        "title": "The unexpected contributions to a productivity increase by 25% of a fruit basket at work",
        "excerpt":"As software people, we live in a complex-system world. To leverage on its non-linear effects, we need to multiply weird experiments.   You might remember how we hacked up a weekly fruit basket in a previous team. The company would not provided us fruit baskets at work, so we decided to build our own. There’s a more interesting part to this story though. This fruit basket later contributed to increase productivity in unexpected ways!      A fruit basket at work   We initially invested 10€ to get started. Every week, one of us would buy some fruits, and sell them to teammates for 50c each. The sustainability of the process surprised us. After a few weeks, we got our money back and we were all eating fruits!   Exploratory testing   A few iterations later, we decide to perform regular exploratory testing sessions. The common belief is that developers make poor testers. Caring too much about their software, they stick to the happy path.   We used gamification to workaround this. We transformed exploratory testing into a bug competition. The pair that would discover the most bugs would become that week’s “best testers”. Money adds a bit of thrill to poker, and we decided to do the same. The “best testers” would win some free fruits. This turned out to be very effective. For the length of the session, we, developers, would transform in hardcore sadistic testers.   Improvement kata   Fast forward a few iterations again. We were feeling that we were somehow wasting some time, but could not understand where. Our retrospectives were not as effective as they used to be, so we tried our first improvement kata. Thanks to this kata, we discovered that we were losing a lot of time fixing bugs. To know where we stood, we had to measure how many bugs remained. Again, we used these time-boxed, fruit-powered exploratory testing sessions. Long story short, the improvement kata brought two improvements:      We defined, without ambiguity, what a bug was   We stuck to a #zeroBugs policy.   After a few weeks, we had almost eliminated bug fixing. We used to spend around 25% of our time on bugs. By slashing bug fixing time, we had increased productivity by 25%. The fruit basket played a surprising role in this productivity increase!   Costs and Benefits summary   Let’s start with the benefits      As people asked for a fruit basket at work during retrospective, we can assume that it made them happier   Studies have shown that happy people are also more productive   Eating fruits instead of biscuits or other food made people healthier   Exploratory testing increased the quality of our software   The #zeroBugs approach increased the software quality even further   Thanks to the improvement kata, we increased productivity by 25%   What about the costs?      We invested 10€ for the fruits that we got back a few weeks later   Someone would spend 30 minutes every week for the fruits (shopping, washing, accounting)   Not much else, as the rest was already part of our “Business As Usual” work   Non-Linear and Unpredictable!      Sure, the fruit basket was only a catalyst in continuous improvement. Still, 25% productivity boost for 30 minutes per week was a great deal! If we had had to ask for the permission to our manager up-front, he would not have given it. From a pure money perspective, who would blame him though? There was no way we could have guessed that a fruit basket would help us to reach a #zeroBugs policy!      💡 25% productivity boost for 30 minutes of weekly fruit management!?!    There’s more though! The more we dig, the more interesting the story gets… The company did not provide free fruit baskets at the time. If it had, gamification would have been more difficult. Exploratory testing and improvement kata would not have worked so well either. Nowadays though, most of us believe that free fruits at work are good for happiness and productivity…      💡 In a complex world, nothing is straightforward. Things only appear to make sense after the fact.    More than ever, eXtreme Programming’s mantra of “Embracing Change” makes sense. In practice, it means we should leave a lot of place for experimentation and adaptation. Benefits are non-linear and small improvements can be game changing.  ","categories": ["continuous improvement","complexity","improvement kata"],
        "tags": [],
        "url": "/how-we-increased-productivity-by-25-with-a-fruit-basket-at-work/",
        "teaser": "/imgs/2018-10-17-how-we-increased-productivity-by-25-with-a-fruit-basket-at-work/fruit-catapult-teaser.jpeg"
      },{
        "title": "The best 10 seconds I spent on my Kanban board setup",
        "excerpt":"Flipping your Kanban board columns is a cheap way to reduce multitasking and stress. Eventually, it contributes to a more sustainable pace.   2 weeks ago, I blogged about how seemingly small changes can have big consequences. Here is another experiment that could have great consequences.      The problem with current Kanban boards   As you might know, I use a personal Kanban. Through the years, I have been playing with and tuning it to become as productive as I can. Even this latest description of it is already out of date.   This personal Kanban has been serving me well. It has helped me to stay on top of all the things I have to do. It has also helped me to make my workflow visible, and to remain productive.   What’s the problem then?   Even though I have been using this board for years, I always tend to start too many things at the same time. I know the theory behind Kanban. I know that a high WIP (Work In Progress) is synonym with multitasking. I know it’s bad for me, that it will make me both miserable and unproductive. Yet, I still push too many things from TODO to DOING. I regularly experience stressful moments where I realize I put too much on my plate …   It’s Obvious, Stupid!   I grew up in France. Like most Western languages, French reads from left to right. Actually, we organize most documents along this left-to-right, line-by-line flow. What’s the first thing I see when I open my Kanban board? You guess it: TODO items!      💡 What’s the first thing I see when I open my Kanban board: TODO items! 😰    It’s not an accident I tend to stress on what’s left to do and to start too many things. Every time I need to access my board, it reminds me how slow I am, and all I still need to do… As a result, I start more things than I can handle. I start multitasking. I get less done, which makes the situation even worse, etc, etc, etc.      What did I do?   I heard once that inverting the Kanban board setup was good for morale. (Sorry, I cannot remember who suggested that first.) It’s a way to focus first on our accomplishments instead of the never-ending flow of things to do.   I gave it a try. Here is what my Kanban board looks like now.      First thing I now see when I open my board are my accomplishments. But the best part is the second thing I see: my work in progress. As these are things I can work on, I tend to stop there and do what I can to close these. I look at TODO items a lot less than I used to.   Takeaways   Here are the effects on cycle time after 2 months of this new Kanban board setup.      Column reordering is not responsible for all improvements because I also did some task cleanup at the same time. Nevertheless, it did reduce work in progress, multitasking and stress level. In short, it helped me to stick to a more sustainable pace.      💡 Inverting Kanban board columns contributes to a more sustainable pace.    Not bad for a 10 seconds configuration change! If you are using a Kanban board, my advice is to give it a go, there’s almost nothing to lose, but a lot to win!   Culturally oriented   Kanban and Kanban boards came from Japan. Traditionnaly, Japanese reads in columns, from right to left. Kanban boards use columns too, but flow from left to right! In next week’s post, I’ll try to imagine what culturally oriented Kanban board setup would be.   Thanks to Damien, Matthieu and Xavier for reviewing this post  ","categories": ["personal-productivity","kanban","scrum"],
        "tags": [],
        "url": "/its-time-to-flip-your-kanban-board-setup/",
        "teaser": "/imgs/2018-11-06-its-time-to-flip-your-kanban-board-setup/kanban-mirror-teaser.jpeg"
      },{
        "title": "Vertical Kanban Board Setups",
        "excerpt":"Turning Kanban boards vertically fits western cultures better. Improving UX of our process tools would eventually lead to a more effective and sustainable pace.   My previous post detailed how flipping Kanban boards columns makes Kanban more effective. I suggested that Kanban works in Japan because of the Japanese language’s orientation.   Culturally oriented Kanban   Kanban and Kanban boards came from Japan. Traditionally, Japanese reads in columns, from right to left. Kanban boards use columns too, but flow from left to right! Japanese people ‘pull’ items from left-TODO to right-DOING. It makes sense to them because they are pulling against the traditional word flow.   When I was at NewCraft this year, Nat Pryce gave a presentation about the metaphors we use in software. He mentioned widespread metaphors we rely on, that we are not even aware of.      Many metaphors are culture dependent. Let’s see what the Kanban board setup would look like if we adapted it to the Western text orientation.      Benefits   The first benefit is when we display this board on the wall. DONE and DOING areas would be at eyes height. We would need an extra effort to look at items that are still TODO. This would reduce Work In Progress, multitasking and stress. In the end, it would contribute to a more sustainable pace.   Another benefit is when we display such a board on a computer screen. TODO items would be at the bottom of the board. They are likely not to appear on the screen when we open the board. Again, we would need the extra scrolling to see what’s TODO. And again, this would result in a more sustainable pace.   Finally, it makes more sense vocabulary-wise. Kanban is a pull system. Did you ever try to pull something from the left with one arm? Pulling from a side does not work well. As my Aikido master would say, it’s a great way to hurt yourself. When we want to pull something heavy, we usually start by facing it. Most of the time, we’ll pull from in ahead, or from below.   Regional settings      💡 We need to adapt Kanban boards according to people’s cultures.    All this is culture dependent. As I said, I’m French, and this Kanban board setup works well for Western cultures. We need to adapt Kanban boards according to people’s cultures. People who read lines from right to left, might prefer this orientation:      Fortunately for digital boards, our computers have regional settings. The software could adapt Kanban boards orientations to the client’s regional settings!   For obvious reasons, orienting the physical board according to local culture should suit most people using it.   Humane tools      💡 Could we make these sad, sterile and mechanic process tools more humane?    Let’s dream for a while and push things even further. What we are talking about is actually UX for Kanban boards. Could we make these sad, sterile and mechanic tools more humane? Up to now, all we did has been to copy-paste physical boards on a screen. Could we use technology to build something better? Here is what such a Kanban could look like:      We could add animations, visual hints, interaction, gamification… All this to help us perform our work at a sustainable pace.   TODO cesspool   The TODO area is usually a mess made of in-the-shower ideas, half specified stories, fully detailed tasks, large or small epics, valuable or not stories… We could use visual clues to make all this jump to our faces:      bigger cards for larger items   shiny cards for high value items   jiggling cards for risky items   blurry cards for unclear items   we could orient all this mess over 2 axes, value and size for example, to spot items that are ready to start   DOING overload   The DOING section is often overloaded with too much work.      We could make the DOING bottleneck visible with a funnel shape   We could have small task slots to make sure we cannot start items that are too big   DONE Sisyphus tasks              Sisyphus by Franz von Stuck, 1920        Software too often feels like Sisyphus task. As soon as something is finished, a new task takes its place to keep us busy… until we retire.      We could have mini-rewards when things get done (quotes, songs, tweets, notifications, gifts, sustainable pace advices …)   We could use a metaphor, like air balloons, for Stories, Epics and products. When enough story balloons are done, they’ll lift their epic balloon…   Quantified OKRs would also make a great progress tracking system here. (I’ll blog about that some day)   Back to reality   Enough dreaming… I don’t know of such tool yet. If you know something like this, I’d be happy to learn about it. Otherwise, this would make a great open-source side-project. What do you think? Would you use it? Would you contribute?   Thanks to Damien, Matthieu and Xavier for reviewing this post  ","categories": ["personal-productivity","kanban","gamification"],
        "tags": [],
        "url": "/vertical-kanban-board-setup/",
        "teaser": "/imgs/2018-11-13-vertical-kanban-board-setup/humane-kanban-teaser.jpeg"
      },{
        "title": "Misadventures with Big Design Up Front",
        "excerpt":"Although a nice idea in theory, Big Design Up Front has many problems that I learned the hard way. We should avoid BDUF most of the time.   ℹ️ NOTE: An updated version of this post has been published on the Event Storming Journal blog   At school, I was taught  that Big Design Up Front is the only way to go. We even used to write all pseudo code on paper before touching a computer at all!      💡 At school, we were taught to write all pseudo code on paper before touching the keyboard at all!    Old style cowboy design   I entered my first job with a heavy bias towards doing design first. Real-life work surprised me a lot! My colleagues would design on the fly, at the same time they implemented features. Our best design diagrams were rough design sketch on paper napkins. This was almost 20 years ago and the tooling was not that of today. We were learning Version Control, we would not unit test, and a nightly build remained a dream! As a consequence, late design changes were a mess and resulted in an awful lot of regressions.      The general advice around the place was:      If it ain’t broken, don’t fix it!    I was far from pleased with this technique, and I decided to try something against it. I naively thought that applying what I learned in school would fix the problem. I started to spend more time designing what I wanted to do before “touching the computer”. For more or less a year, I tried. It seemed to work fine at first. Unfortunately, troubles came when I was to add a new feature on top of this initial design. I’d built some flexibility in, but no matter what, never where later needed. I was spending more time to design up front, and still had to hack last minute changes to meet deadlines… This was definitely not a sustainable pace!   In fact, I’d spent a year re-discovering the cons of Big Design Up Front.   Problems of Big Design Up Front      These problems have not disappeared with time. A few months ago, some colleagues asked me to animate architecture meetings. I discovered the same issues again, only worse. This time, it’s not me alone who needs to come with a design, but a group of people who need to agree on architecture. It’s no surprises the project has been struggling to get started.   Here are the typical problems of Big Design Up Front      Usually involves painful meetings and plenty of document reading   It can be a long process, especially if many people contribute   It can lead to a bad design by committee. People end up capitulating to a proposition to stop this painful process   It always misses things. It’s almost impossible to think of everything on paper. Remember, Powerpoints don’t crash!   It creates over-engineering. We tend to add future-proof flexibility in the design. We discover that it’s never used when it’s been in the code for years.   To make things worse, it sometimes takes so much time that it’s deprecated from the beginning.   Finally, it’s very difficult to get a collective buy-in for it. It’s impossible to get everyone in these BDUF meetings if we want to agree on anything. As a result, the only solution is to mandate the architecture in a top-down manner   You’ve guessed I’m not a big fan of Big Design Up Front…      💡 People capitulate to a design to end this painful BDUF process.    Alternatives to Big Design Up Front   In next post, I’ll continue my story and explain how I switched to incremental design. I’ll detail how it fixes almost all of the issues of Big Design Up Front.   Unfortunately, there is no silver bullet, and incremental design has its own weak points. The lack of a shared vision is the main one. Finally I’ll retrace how I discovered Event Storming, and how it can be used to draft a shared design vision in just a few days.   To be continued   This was the first post of a series about how to use Event Storming to kick start architecture on good tracks. Here are the topics I intend to go through in this series:      Misadventures with Big Design Up Front   How I learned to do Big Design Up Front in 2 days of Event Storming   How to prepare a DDD Big Picture Event Storming workshop   How to prepare the room for a DDD Big Picture Event Storming   Detailed Agenda of a DDD Big Picture Event Storming - Part 1   Detailed Agenda of a DDD Big Picture Event Storming - Part 2   Detailed Agenda of a DDD Big Picture Event Storming - Part 3   4 tips that will make your DDD Big Picture Event Storming successful   Drafting a Functional Architecture Vision with Event Storming and DDD   Build or Buy Software? Identify your core bounded contexts with Event Storming and DDD   Ensure core contexts have the upper hand with Event Storming and DDD   Focus on the Core with Event Storming and DDD Domain Relationships - 1   Focus on the Core with Event Storming and DDD Domain Relationships - 2   How to use Event Storming and DDD for Evolutionary Architecture   Using Event Storming and DDD to prototype (micro)services and NFRs - 1   Using Event Storming and DDD to prototype (micro)services and NFRs - 2   Rewrite vs Refactor? Get Insights from Event Storming and DDD   Feature Teams vs Component Teams? Decide with Event Storming and DDD   How to max out DDD Big Picture Event Storming with other Workshops   Continue Reading…   Thanks to Thomas and Xavier for reviewing this post  ","categories": ["architecture","squash-BDUF-with-event-storming-series"],
        "tags": [],
        "url": "/misadventures-with-big-design-up-front/",
        "teaser": "/imgs/2018-11-19-misadventures-with-big-design-up-front/jenga-tower-teaser.jpeg"
      },{
        "title": "How I learned to do Big Design Up Front in 2 days of Event Storming",
        "excerpt":"The combination of Incremental Design and Event Storming is a better alternative to Big Design Up Front. It’s faster, safer and creates natural buy-in.   ℹ️ NOTE: An updated version of this post has been published on the Event Storming Journal blog   In my previous post, I explained how I tried to apply Big Design Up Front at my first job. Long story short, I did not manage very well…      This is the second post in a series about how to use Event Storming to kick start architecture on good tracks. If you haven’t yet, I recommend you to start reading from the beginning.   Incremental architecture to the rescue   Here I was: I had tried to apply Big Design Up Front to keep the software tidy. Unfortunately, I was still facing regression at every new features. I started to look for ways to better design software. That’s how I discovered the refactoring book:      In it, I read about eXtreme Programming, which got me to read the XP book:      I got sold on XP. I jumped in. Within a few month I was unit testing a lot. I had swapped Big Design Up Front for incremental design. The idea is to start and keep the simple code. (Remember: Simple is not Easy).      Just get started, and refactor a lot as you go.    I have practiced incremental design since then. It’s better than Big Design Up Front in almost all aspects.      It’s fast to get started. This alone removes a lot of the risks:            Of demotivating people       Of building a deprecated design       Of falling into analysis paralysis           It remains open to change, because the code remains simple   It avoids over-engineering, because we only code what’s needed   Here is the main lesson I get by looking back all these years. We cannot achieve a sustainable pace without incremental design.      💡 We cannot achieve a sustainable pace without incremental design!    The main problem of incremental design   Without a clear design, you’ll waste your time refactoring over and over. That’s true.   In practice, this is not a big issue if you are working alone. Once you’ve spent enough time at a problem, you’ll form a mental design target. From then on, you’ll always refactor in this direction.   The problem is trickier for teams. A few years ago, I joined a team as technical lead. Because of turnover, I became the only experienced programmer in a team of juniors. We were doing incremental design and refactoring. Unsurprisingly, the team turned to me to provide them an architecture vision. Having this shared vision helped everyone in the team to know how to refactor the code at any moment. It avoided both conflicting and dead-end refactoring.      💡 Having a shared vision helped everyone in the team to know how to refactor the code at any moment.    As the team members grew in experience though, they started to suggest more design ideas. It became more and more difficult to agree on a vision. This translated directly in more time wasted in conflicting refactoring initiatives.   The Event Storming compromise   I did not know how to solve this problem until I discovered Event Storming.   I first heard about it through the internet. The first time I had the chance to practice it was when a team at work asked for help to organize one. They wanted to on-board newcomers. I then went through a few practice sessions. Finally, I had the chance to attend a workshop with Alberto Brandolini at a conference. That’s when I’d finish my rookie training at Event Storming. The main feedback I got through all these sessions was “Massive knowledge sharing”.   During his workshop, Alberto also said we could use it to identify the bounded contexts of the domain. This statement sparkled my interest. Could this provide the collaborative architecture vision that incremental design lacks?      After reading through the beta release of Alberto’s book and more experimenting, I can say it is. Whenever we start something new involving architecture:      A new product, project or startup   Almost any kind of legacy refactoring effort. For example:            To modularize your architecture to scale to more teams       To break a monolith into micro-services to support a continuous-delivery SaaS business model       To rewrite a poor-quality part of the system, which generates excessive support costs           Event storming let us draft a good enough functional architecture vision. (Context Map for Domain Driven Design aficionados). Even better: it lets us do this collaboratively, in a couple of days! It’s not bloated like the BDUF, but still provides a Rough Design Up Front. It complements incremental design to get something even better.   Next posts   This was the second post in a series about how to use Event Storming to kick start architecture on good tracks. In next post, I’ll explain how to run a Big Picture Event Storming. Later, I’ll go over how to draft a functional architecture from there.   Continue Reading…  ","categories": ["architecture","squash-BDUF-with-event-storming-series"],
        "tags": [],
        "url": "/how-to-squash-big-design-up-front-in-a-few-days-with-event-storming/",
        "teaser": "/imgs/2018-11-21-how-to-squash-big-design-up-front-in-a-few-days-with-event-storming/event-storming-teaser.jpeg"
      },{
        "title": "How to prepare a DDD Big Picture Event Storming workshop",
        "excerpt":"The massive domain knowledge sharing of a Big Picture Event Storming unlocks DDD. Good preparation makes running one easy. Here’s the 1st of a 4 posts step by step guide.   ℹ️ NOTE: An updated version of this post has been published on the Event Storming Journal blog   Every time I ran DDD’s Big Picture Event Storming workshop, the main feedback I got was “This was massive learning!”. Having the required people together for a few hours triggers the important conversations.      That’s why Big Picture Event Storming is so great to kickstart a new project. Once everyone knows enough about the problem domain, they work better together. They can discuss architecture, and draft a target vision. They can also brainstorm team organization. In the case of an existing system, they can compare different migration strategies. For example, is it better to rewrite or to refactor?      💡 Once everyone knows enough about the problem domain, they work better together.    Worse, that’s not all. Two things might happen as they explore their domain space. They will discover existing problems. They will also agree on domain specific definitions. These are quick-wins that pay off fast.   The fantastic thing is that it lets us do all this in a very short time! That’s the real power to DDD’s Event Storming. In a few days, we can get everyone to embark on sustainable path that leads to a shared, good-enough, vision!   This is the 3rd post in a series about how to use Event Storming to kick start architecture on good tracks. If you want to learn how I got into Event Storming, I recommend you to start reading from the beginning. You would also learn why Incremental Design is the perfect fit for Event Storming.   Homework first      You’ve decided that DDD’s Big Picture Event Storming is the way to go and you’re eager to start one. Not so quick though. As with many things, lack of preparation can turn your promising workshop into a total failure…       Before anything, preparation is the key to success. (Wikiquotes) Alexander Graham Bell    Let’s see how to start.   Sponsorship   At this point, the most important thing to do is to find good sponsorship. By good sponsorhip, I mean the support from influential people. What you want is your workshop to become the group’s initiative, and not just your initiative. Wining the influencers over to your strategy tremendously increases your chances of success.   Usually, private chats with these people is the best way to win them over. In the end, you want them to share and support a common goal for the Event Storming session. One of the first sessions I ran didn’t go very well because I had omitted this step. Some people wanted to draft a target architecture, to know how to refactor in the long term. Others wanted to understand the main blocking points and identify the first wins. The results of the session were mitigated.   Scope   With a clear goal in mind, you should have a rough idea of the scope of the session. DDD Event Storming is an exploratory (and rather chaotic) activity. People who are not used to it will feel a bit lost at first. To ease this out, I found that it’s better to boot the workshop around 1 or 2 uses cases. So, have a chat with your sponsors or other domain experts to come up with these use cases up front.   As I said, Event Storming is exploratory by nature. Discussions about these use cases will bring in other concerns during the session. It will be up to the group to decide whether to add them in the scope or not. It’s ok to start with precise and specific use cases, there’s no need to be afraid to miss things out.   It’s also a good idea to identify a first domain event. It will both serve as an example, and a way to trigger the Event Storming. From experience, this event should be somewhere in the middle of the story. It should also be clear enough for everybody to understand. Examples:      A trade was booked   An order was payed   If you are wondering what a domain event is, don’t worry, I’ll go over this later on.   Audience   It’s time to list the ideal audience. I found that sessions work better with a mix of 50% of domain experts and 50% of technical people.   With too few domain experts in the room, the workshop becomes a one-way teaching lesson. Ideally, we’d to have experts for all the bounded contexts you foresee in the scope of the workshop.   We also need to have a fair share of technical people in the workshop. In the end, it’s their domain knowledge we want to grow. Don’t forget that, as https://twitter.com/ziobrando says:   It’s developers (mis)understanding, not expert knowledge that gets released in production…. @ziobrando at #DDDx #DDDesign pic.twitter.com/O5mIHaMyEw &mdash; Mariusz Gil (@mariuszgil) 28 avril 2017   Having enough technical people, from all roles also creates the natural buy-in. This is on of the key advantages of DDD Event Storming. If possible send a call for volunteers among teams, to increase buy-in further.   Invitation   By now, you should have sponsors, a clear goal, a few starting use cases and the ideal audience. The next thing to do is to send enticing invitations. Depending on organizations, more or less efforts are needed to get people to attend.   Make sure that the sponsorship is visible in the invitation to maximize buy-in. For example, ask an influential sponsor to send the invite at your place.    If you can, sketch a visual invitation. (Unfortunately, I could not find references, maybe I should blog about this one day.) This will set the tone for a different kind of architecture meeting.      Briefing   When you have your final list of attendees, brief them about the goal of the workshop. This helps people in many aspects:      To understand that the goal is worth their time   To be ready for the initial disorientation   To get an idea of how the session will go   To get answers to quick questions   I found that a quick 15 to 30 minutes gathering works well, but again, you might need to adapt to your organization. Groups that are used to written communication might prefer email, chat or wiki. What is important is that people can ask public questions and get answers.   To be continued   This was the 3rd post in a series about how to use Event Storming to kick start architecture on good tracks. In next post, we’ll see how to prepare the room for a successful session.   Continue Reading.  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming"],
        "tags": [],
        "url": "/how-to-prepare-a-ddd-big-picture-event-storming-workshop/",
        "teaser": "/imgs/2018-12-03-how-to-prepare-a-ddd-big-picture-event-storming-workshop/ddd-on-event-storming-teaser.jpeg"
      },{
        "title": "How to prepare the room for a DDD Big Picture Event Storming",
        "excerpt":"Running a DDD Big Picture Event Storming is a great way to kick start a project. Fortunately, it only requires common supplies. Here is a detailed checklist.   ℹ️ NOTE: An updated version of this post has been published on the Event Storming Journal blog   This is the 4th post in a series about how to use Event Storming to kick start architecture on good tracks. If you want to learn how I got into Event Storming, I recommend you to start reading from the beginning.   In last week’s post, I went over the preparation for a successful DDD (Domain Driven Design) Event Storming workshop. Before we jump into the actual animation in next post, let’s see what material you’ll need.      A Visual Agenda   One thing I found useful to prepare before the session is a Visual Agenda. As I explained, DDD Event Storming can be very disturbing the first time, and people might feel a bit lost. Before the session begins, stick the Visual Agenda on the other walls of the room. Walking attendees through the different steps with a Visual Agenda will reassure them. It’s also a good way to highlight the few rules of Event Storming.      You can find explanations about how to prepare a Visual Agenda in the GameStorming book or website.      You must be wondering what the steps actually are. Don’t worry, I’ll cover this in the next post. For now, here is a list of other supplies you’ll need to run an DDD Big Picture Event Storming.      💡 DDD Event Storming can be a rather chaotic workshop. A Visual Agenda will reassure first time attendees.    Infinite Design Space      By far, the most exotic and difficult thing to get is a big enough wall to do the workshop. Alberto Brandolini, the inventor of Event Storming, recommends a 8 meters long wall. There are 2 goals:      not to feel limited in our design because of space   have enough place to let everyone walk around and collaborate   If you have a large enough room, this should be your first choice. Alberto says that corridors can be good candidates too. My own experience with this was not a great success. When we tried it, participants complained about other people coming and going all the time.   Once you have the room, you’ll need a large paper roll to stick your post-its on. This way, you’ll be able to move your design space. You’ll be safe in case you need to extend it, or if you decide to stick it in your workplace for a few days.   Post-its      Event Storming consumes a tremendous amount of Post-Its, especially Orange, domain events, Post-Its. To summarize, you’ll need:      Many orange post-its, around one stack per person   pink post-its   large yellow post-its   small yellow post-its   blue post-its   Sharpies      Obviously, people will need something to write on the post-its. Sharpies or small markers are the best. They are readable from a few meters, but still let you write enough words on a single post-it.   No Chairs, No Table   Typical meetings are boring and get people sleepy. In comparison, a successful Event Storming workshop keeps people energized and productive. Removing chairs and meeting tables in the area helps people to stay energized. We don’t want DDD Event Storming to become a long torture session either. As a consequence, we’ll need to schedule enough breaks.   A Small Table   We’ll need somewhere to place our post-its and sharpies. A small table will be the final addition to our setup.   Food      If you can, bring some food and drinks. The Workshop can be very tiring, having food around sustains attendees energy longer.      💡 I beg you to bring food to avoid #DeathByEventStorming    To be continued   This was the 4th post in a series about how to use Event Storming to kick start architecture on good tracks. In next post, we’ll see how what’s a typical agenda for the workshop.   Continue Reading…  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming"],
        "tags": [],
        "url": "/how-to-prepare-the-room-for-a-ddd-big-picture-event-storming/",
        "teaser": "/imgs/2018-12-07-how-to-prepare-the-room-for-a-ddd-big-picture-event-storming/event-storming-box-teaser.jpeg"
      },{
        "title": "Detailed Agenda of a DDD Big Picture Event Storming - Part 1",
        "excerpt":"Kick starting a project with a DDD Big Picture Event Storming can be chaotic. Here is a detailed agenda and a sample briefing to set it on the right track.   ℹ️ NOTE: An updated version of this post has been published on the Event Storming Journal blog   This is the 5th post in a series about how to use Event Storming to kick start architecture on good tracks. If you want to understand the full story, I recommend you to start reading from the beginning.   In the 2 previous posts (3rd and 4th), I went over all the preparation to do before the workshop. Let’s move into the real thing. Now that we have everything ready, how do we actually run this workshop?      1. Prepare the room      I learned from experience that I to come in the room around 30 minutes in advance to prepare it for the workshop. Previous post has all the room preparation details. Roughly, this includes:      Removing tables and chairs   Sticking the design-space to the wall   Sticking the Visual Agenda to the wall   Laying down the rest of the material somewhere   2. Energizer              Participants walking around during a Collaborative Face Drawing energizer. Picture from funretrospectives.com        As I’ve already said, DDD Event Storming is a different kind of architecture meeting. It won’t work if people don’t get out of their traditional way. Running a physical energizer with everyone is great for that.   It’s also very useful to raise the energy level. Event Storming can be pretty intense and tiring, so they’ll need all the energy they can. You can find many great physical energizers at funretrospectives.com. I’ve had success with many of them.   3. Briefing and Agenda   Now is the time to present the workshop. Start with the goal, scope and use cases. It’s the good moment to explain each step we will go through, and how they will help us to reach our end goal. It’s also a good time to introduce some general conventions. Here is a typical brief I could say.   General goal      Remember that DDD Event Storming is a way to shrink months of Big Design Up Front into a few days! It’s going to be intense, but we’ll do a lot in a very short time.     Event Storming might be chaotic. It might be rocky and go in unexpected ways at times, we might need to adjust. 💡 At the end of the day though, the success is all up to how much you want it!    Goal      The main outcome of this workshop is the shared knowledge between domain experts and developers. We’ll later build on this shared knowledge to get .     We must stick to domain language if we want to keep this collaboration alive. 💡 I’ve seen Event Storming sessions drifting into technical discussions, this leads nowhere.    Scope and use cases      Today, we’ll work in the scope of &lt;your scope&gt;. To make things a bit more concrete, we’ll start with these use cases in mind &lt;list your use cases&gt;. We’ll be modeling around domain events such as &lt;your 1st event&gt;.    Domain Events         A quick brief about post-it colors.     Orange post-its are for Domain Events. Here is an example &lt;You sample event&gt;. Here are a few points to help you understand what domain events are:                 You could read about them in domain books                  Domain experts understand them                  Writing them in past tense is a trick to create meaningful events. They are not actions of someone or something (not “The trader books the deal”). Even though some events will result from actions, we are not interested in actions yet.                  They are not technical, and should not be specific to our system’s implementation            Domain Definitions (aka Ubiquitous Language)         Whenever we come across or agree on a domain word, feel free to write a definition for it on a large yellow post-it. This is a way to build up a domain ubiquitous language. This is very helpful to improve the communication between all of us. This in turn improves how we work in many different aspects (ex: when choosing what to refactor).    Problems         Likewise, we use purple post-its to park “problems”. Whenever we encounter:          a question we cannot answer     something that does not seem right     or any problem we should look into       Record it on a purple post-it.    Sustainable pace      Finally, to keep the pace sustainable, we’ll take a 5 minutes break every 50 minutes.    Once this general speech is over, I usually quickly present every steps of the agenda.   To be continued   This only covers the first half of the workshop. In next post, we’ll go over the following steps.   This was the 5th post in a series about how to use Event Storming to kick start architecture on good tracks.   Continue Reading…  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming"],
        "tags": [],
        "url": "/detailed-agenda-of-a-ddd-big-picture-event-storming-part-1/",
        "teaser": "/imgs/2018-12-08-detailed-agenda-of-a-ddd-big-picture-event-storming-part-1/event-storming-beginning-teaser.jpeg"
      },{
        "title": "Detailed Agenda of a DDD Big Picture Event Storming - Part 2",
        "excerpt":"The DDD Big Picture Event Storming is a creative game. It sticks to the classic open-explore-close flow. Here is the second part of its detailed agenda.   ℹ️ NOTE: An updated version of this post has been published on the Event Storming Journal blog   This is the 6th post in a series about how to use Event Storming to kick start architecture on good tracks. If you want to learn how I got into Event Storming, I recommend you to start reading from the beginning.   This is also the 2nd part of a post detailing the agenda for a Domain Driven Design Big Picture Event Storming. The previous post contains the first part. Please make sure to read it first.   Let’s see what happens next in a DDD Big Picture Event Storming      4. Generation   This is when the workshop actually begins. Ask attendees to stick any Domain Event they can think of in the context of the use cases. To help them get started, be the first to stick the Domain Event you prepared in the middle of the design space.      💡 Alberto Brandolini’s trick: ignite the Event Storming by sticking a prepared domain event on the design board.    At some point, you’ll see that the rate of Domain Event generation will dwindle down. That’s the sign that it’s time to move on to the next phase. 25 minutes or so are usually enough for this first phase.   5. Sorting   In this second phase, we’ll ask them to sort the events chronologically. The idea is to represent the typical flow of work on the design board. During the generation phase, people were writing down any event they could think off. As a result, they were working alone. In this phase, they will need to speak with each other to sort the events.   This is where DDD Event Storming does its magic. Attendees all have a different point of view about the system. They materialized it on the design board with some domain events. They will need to discuss through these differences to sort the events.      💡 Event Storming does its magic when people try to sort all their domain events.    This phase should trigger intense discussions. Hopefully, the group will capture many domain definitions and problems to look into.      Feel free to organize the board as you wish to accommodate the flow. Typically, swim-lanes will emerge for alternatives, and vertical alignment will signify branching. You might also create duplicate post-its when some kind of looping occurs.      6. Actors and External Systems      You’ve started to write down the story of your system. All good stories have heroes though! This time, ask attendees to identify actors (people with a role) that trigger or respond to events. The convention is to use small yellow post-its for that. There is no need to add an actor to every event, sticking one at the beginning of a chain of events is enough.      Similarly, complex systems also interact with external systems. External systems are not humans, but they could be an online API for example. The convention is to use blue post-its for External Systems. Place some at place where the events interact with them.   We are almost there   At this point, attendees should have a better overall grasp of the domain. In the next post, I’ll go over the last steps of the DDD Big Picture Event Storming.   This was the 6th post in a series about how to use Event Storming to kick start architecture on good tracks.   Continue Reading…  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming"],
        "tags": [],
        "url": "/detailed-agenda-of-a-ddd-big-picture-event-storming-part-2/",
        "teaser": "/imgs/2018-12-16-detailed-agenda-of-a-ddd-big-picture-event-storming-part-2/event-storming-3-phases-game-teaser.jpeg"
      },{
        "title": "Detailed Agenda of a DDD Big Picture Event Storming - Part 3",
        "excerpt":"Thanks to massive knowledge sharing, a DDD Big Picture Event Storming is the 1st step to collaboratively draft a Rough Design Up Front. Here are its last steps.   ℹ️ NOTE: An updated version of this post has been published on the Event Storming Journal blog   This is the 7th post in a series about how to use Event Storming to kick start architecture on good tracks. If you want to learn how I got into Event Storming, I recommend you to start reading from the beginning.   This is also the 3rd part of a post detailing the agenda for a Domain Driven Design Big Picture Event Storming. Please make sure to read the previous articles first.      If you went through the previous steps, all the attendees should have a good grasp of the domain. Let’s test this understanding a bit.   7. Storytelling   Now is the time to check that the whole picture makes sense. Since the beginning of humanity, stories have been the vehicle of knowledge. Knowledge used to go from generation to generation through campfire stories. Our brains are hard-wired to listen, remember and make sense of stories.      Ask for a volunteer from the audience, or for a few if people are afraid to do it alone. Then ask the first volunteer to narrate the story of the system. He just needs to go through the events chronologically and explain what happens.   As the narrator speaks, the audience will raise questions and notice incoherences. This is again a time to add, remove or replace events to fix the story. A few extra definitions might emerge. If a problem seems too big to fix during the session, park it with a pink problem post-it.   Narrating the story can be pretty tiring, so ask a new narrator to take over at some point.   8. Reverse storytelling   For an even greater drill down in the domain, reverse storytelling is the way to go. Get a few more volunteers and ask them to tell the story from the end. Naming this step ‘storytelling’ is not accurate. It’s rather repeatedly asking “What might have triggered this event?”. This will generate or update events, actors or external systems.   The reason this works so well is that questions trigger the creative parts of our brains. We get to imagine all kinds of new possibilities. This phase is very productive and brings a lot of insights.   9. Closing      This is it, you’ve reach the end of the DDD Big Picture Event Storming. At this point, it’s a good idea to settle down and assess the outcomes.      💡 By far the biggest outcome of DDD Big Picture Event Storming is a better shared understanding of the domain.    You might actually be wondering what the deliverables are. At this point, most deliverables are intangibles :      By far the biggest is a better shared understanding of the domain. This will save tremendous time by improving collaboration. It will avoid specification bugs, and enable a better design.   All together, you have identified problems. Fixing these problems might be quick wins with high payoff.   The definitions are the first bricks of an Ubiquitous Language. Leveraging on it saves on-boarding time and maintains the system’s conceptual integrity.   Finally, this was also a mandatory step for collaborative architecture. A good shared understanding of the domain makes discussions about functional architecture possible.   Thus, the next steps can be:      To fix a major problem. In his book, Alberto Brandolini recalls such a situation. The Big Boss actually had every other work stopped until they fixed a ‘new’ problem.   To continue to grow the ubiquitous language, by adding and refining definitions   To do more workshop in order to draft a target architecture. I’ll explain how to do this in the next posts.   Depending on your session, the next steps might be more or less obvious. If they are not, it’s a good time to have an open discussion to get everyone’s opinion. Once the next steps are clear and taken care of by someone, call it a day and end the workshop.   Don’t forget to ask for feedback on the session itself before people leave. A ROTI is a quick way to do this.   The series continues      💡 Event Storming unlocks drafting an architecture, drawing teams and a sustainable refactoring path.    That’s it, after a few hours, you’ve reached the end of DDD Big Picture Event Storming. This is the massive knowledge-sharing foundation step. It will help us to draft an architecture, draw teams, find a sustainable refactoring path and more. In the next post I will give a few personal tips about running a DDD Event Storming.   This was the 7th post in a series about how to use Event Storming to kick start architecture on good tracks.   Continue Reading…  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming"],
        "tags": [],
        "url": "/detailed-agenda-of-a-ddd-big-picture-event-storming-part-3/",
        "teaser": "/imgs/2018-12-26-detailed-agenda-of-a-ddd-big-picture-event-storming-part-3/event-storming-finish-line-teaser.jpeg"
      },{
        "title": "4 tips that will make your DDD Big Picture Event Storming successful",
        "excerpt":"Although not rocket science, running a DDD Big Picture Event Storming can be tricky. Here are 4 hard won tips that will make your first workshop successful.   ℹ️ NOTE: Parts of this post has been republished and updated on the Event Storming Journal blog   This is the 8th post in a series about how to use Event Storming to kick start architecture on good tracks. Previous posts      explain why Domain Driven Design (DDD) Big Picture Event Storming matters   walkthrough a workshop in details   If you are not familiar with Event Storming, it might be a good idea to start reading from the beginning.   Here are the most important tips I discovered by running my own workshops.      Tip#1 Manage time with Pomodoro   A double Pomodoro is the most productive and sustainable time schedule. If you are not familiar with the Pomodoro technique, the Wikipedia page is a super short crash course.      💡 A double Pomodoro is the most productive and sustainable time schedule for a DDD Event Storming.                   Characteristic       Classic Pomodoro       Double Pomodoro                       Length       25 minutes       50 minutes                 Short break       5 minutes       5 minutes                 Long break every       4 pomodoros       2 pomodoros                 Long break       15 minutes       15 minutes           When I tried the classic Pomodoro, I was cutting interesting discussions all the time. In practice 50 minutes seem to work better.   People usually overflow the break, so most Pomodoros take a full hour instead of 55 minutes. Without enough breaks, people will get tired and the workshop will not be as productive.   Doing 2 hours before, and then 2 hours after lunch works better than 4 hours straight.              From Flickr By Marco Verch, under Attribution 2.0 Generic (CC BY 2.0) License        Tip#2 Assign roles for B2B for domain experts   Running an Event Storming workshop is especially challenging for Business to Business vendors. It is very difficult to find real end users or client domain experts to attend the workshop.   A trick is to do some role play, and assign business roles to in-house domain experts. This way, all aspects of the business get dedicated focus during.      💡 If you don’t have real domain experts, assign business roles for DDD Event Storming.    It’s very easy to prepare the roles in advance, with the help of your sponsor.      Tip#3 Don’t try to Event Storm your Legacy   If you are envisioning to refactor your legacy system, do as if you were starting from scratch.   It does not make a lot of sense to Event Storm what you currently have. Legacy systems were rarely designed with DDD in mind. You might have to dive into technical aspects to model them. I once let this happen. The workshop drifted into an unproductive mapping of current technical dependencies.      💡 For legacy refactoring, run DDD Event Storming as if you were starting from scratch.    On the other side though, it can be difficult for developers to “forget” the design of the current system. Especially as they start to see all the refactoring to do. Reassure them that you’ll discuss the transition road later on.      Tip#4 Get the Event Storming Book   My last advice is to buy Alberto Brandolini’s book about Event Storming. The last section contains tons of extra tips and advices.      To be continued   This was the 8th post in a series about how to use Event Storming to kick start architecture on good tracks.   In the next post I will explain how to use DDD Big Picture Event Storming to draft a target architecture.   Continue Reading….  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming"],
        "tags": [],
        "url": "/4-tips-that-will-make-your-ddd-big-picture-event-storming-successful/",
        "teaser": "/imgs/2018-12-26-4-tips-that-will-make-your-ddd-big-picture-event-storming-successful/4tips-event-storming-teaser.jpeg"
      },{
        "title": "Drafting a Functional Architecture Vision with Event Storming and DDD",
        "excerpt":"We did the 1st phase of DDD Big Picture Event Storming. It’s time to build on the shared knowledge to draft a functional architecture vision. All in just a few days.   ℹ️ NOTE: An updated version of this post has been published on the Event Storming Journal blog   Drafting an architecture vision is one of the most valuable outcomes of a DDD (Domain Driven Design) Big Picture Event Storming. Having an architecture vision is key to sustainable, successful and evolutionary design. Have a look at the first posts for full explanations.   This is the 9th post in a series about how to use Event Storming to kick start architecture on good tracks. It might be a good idea to start reading from the beginning.      Context   We can use this technique to draft a target architecture on top of a DDD Big Picture Event Storming. That is for 2 reasons:      It won’t work without the shared knowledge built with Event Storming   We’ll re-use the design board   A good way to run this architecture draft workshop is to continue on the next day! Check the previous chapters to know how to do this.   Here is how to do it in more details.   Bounded Contexts   The first step to draft a functional architecture vision is to draw the Bounded Context. If you prefer, you can use the term “Functional Area” instead, which are a jargon-free synonym to Bounded Context.              A sample map of Bounded Context (aka Context Map) from martinfowler.com           💡 Bounded Contexts are the most important part of Domain Driven Design. Maintaining a strong decoupling between different bounded contexts makes large systems more simple.    Walkthrough   At the end of the DDD Big Picture Event Storming, the design board should look something like that:              A DDD Big Picture Event Storming design board, with clumps of Domain Events        As you can see, Domain Events and other post-its gather together in groups. These groups are “proto bounded contexts”.   To materialize these further, try the following:      Ask for a volunteer   Grab:            some colored, thick, wool string       scissors       adhesive tape           With your volunteer, walk the board from left to right, identifying bounded contexts.   Discuss a bit. You will usually agree about bounded context boundaries   Ask the audience for a bounded context name. Tip: Look into names in “ing” for good ones (ex: accounting, ordering)   It might also be the occasion to capture a few domain definitions. Be sure to keep your definition post-its at hand      💡 Wool, scissors and adhesive tape is all you need to draw bounded contexts on an DDD Event Storming design board.    Outcomes   At this point, the outcomes are all about improved communication and collaboration.   Here is one key aspect of bounded contexts. The same concept might represent different thing in different contexts! (That’s why they are called Bounded Contexts.) Let’s think of the order entity in an e-commerce system for example. Orders have different responsibilities if we are talking about shopping, billing or delivering. Bounded contexts are an opportunity to specialize your domain definitions to different contexts. It’s a way to grow your ubiquitous language.   To be continued   This was only the first step in drafting a functional architecture vision. In the next post, I’ll go over how to identify core contexts. The 20% of your codebase that should get 80% of your efforts!   This was the 9th post in a series about how to use Event Storming to kick start architecture on good tracks.   Continue Reading.  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming"],
        "tags": [],
        "url": "/drafting-a-functional-architecture-vision-with-ddd-event-storming-part-1/",
        "teaser": "/imgs/2019-01-22-drafting-a-functional-architecture-vision-with-ddd-event-storming-part-1/functional-areas-teaser.jpeg"
      },{
        "title": "Build or Buy Software? Identify your core bounded contexts with Event Storming and DDD",
        "excerpt":"Event Storming and DDD are great at identifying bounded contexts. It’s even more valuable to identify the core contexts to focus on, and the generic ones to buy.   ℹ️ NOTE: An updated version of this post has been published on the Event Storming Journal blog   In the previous post, I explained how to draw the boundaries of bounded contexts. Not all bounded contexts are equal though. Some have tremendous value for you, whereas others just need to exist. Pareto’s Principle, also known as 80/20 rule goes like that:      Roughly 80% of the effects come from 20% of the causes. Wikipedia    The same goes with bounded contexts. A small part of your code base will generate much of its value. Event Storming and DDD (Domain Driven Design) can help us to identify where to focus, and what software to buy or build.      This is the 10th post in a series about how to use Event Storming to kick start architecture on good tracks. It might be a good idea to start reading from the beginning.   Bounded Contexts   Strategic DDD distinguishes 3 types of bounded contexts. Categorizing a bounded context is key to answering “Buy or Build Software” question.   Core contexts   Core contexts are your most important assets. These are the bounded contexts that make your competitive advantage. They are so specific and important to your business that you must build these yourself. You should strive to focus as much as you can on them. Focus goes through doing as little as possible of the rest…   Generic contexts   These are bounded contexts that have no specificities with your business. They are reusable across many industries. It’s not a good time investment to build your own version. You should rather re-use or buy an existing third party to provide this in your system.   Supportive contexts   Supportive contexts are the rest. Too specific to buy, but not differentiating enough to build serious competitive advantage. Here are typical supportive bounded contexts:      Custom libraries reused across many core domains. Technical in-house frameworks are a good example.   Features that are so basic in your industry that everyone takes them for granted. Configuration or administration often fall here.   You cannot reuse existing code for your supportive contexts. You don’t want to focus on them either! Here are some strategies experts recommend about supportive contexts:      outsource them   leave them to less experienced programmers   or apply looser quality rules in this code   Here is a post by Jonathan Olivier if you want to learn more about these 3 kinds of bounded contexts.   How to classify your bounded contexts   This activity goes after all the DDD Event Storming steps I presented in the previous posts. The design board should now look something like that:      Here is how to do this in group during the workshop.      Give a quick introduction to the 3 kinds of bounded contexts. It’s rather simple, and people usually get this quick.         Draw a few hearts ❤️, 🅖 and 🅢 post-its.   Pick one context, and ask how they would classify it. Discussion and, hopefully, agreement should follow. If you cannot reach a consensus, use a pink problem post-it and continue. You might also have a look at Nick Chamberlain’s technique of using both complexity and competitive advantage.   Hand the rest of the post-its to the audience, and ask them to classify the other contexts   It should not be very long for them to have classified all the contexts   The core should be as small as possible. So let’s try to highlight the core even further. For identified core contexts, ask the audience if they could extract non-core parts out. If yes, draw new supportive or generic bounded contexts.   Ask them if there is anything that they would like to comment      Outcomes   This small and simple activity can have paramount consequences. Especially for big topics like prioritization or buy vs build software decisions.   Here is an illustration. I gave this workshop to a team in Dublin a few month ago. They later told me they had unscheduled a large refactoring that was not tackling their core. By itself, this kind of decision pays back the whole workshop many times.   Another team I’ve worked with decided to replace a feature they had built in-house with a 3rd party. They had discovered it was ‘Generic’. This would allow them to re-focus on other core bounded contexts of their system.      💡 Use Event Storming and DDD to identify generic parts of your system. Then save time and maintenance by replacing them with third parties.    A less tangible outcome is that it focuses discussions and efforts on core contexts. At the end, there will be less work on non-core contexts, and more on core contexts. Work on core bounded contexts is also more valuable. All in all, it means less but more valuable work. This means a more profitable and sustainable pace.   The more we dive in Event Storming, and the more actionable the outcomes get!   To be continued   In the next post, I’ll go over decision power between bounded contexts. Now that we know our core bounded contexts, we want to make sure they have this power!   This was the 10th post in a series about how to use Event Storming to kick start architecture on good tracks.   Continue Reading….  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming"],
        "tags": [],
        "url": "/build-or-buy-software-identify-your-core-functional-areas-with-event-storming-and-ddd/",
        "teaser": "/imgs/2019-01-26-build-or-buy-software-identify-your-core-functional-areas-with-event-storming-and-ddd/gold-nugget-domain-teaser.jpeg"
      },{
        "title": "Ensure core contexts have the upper hand with Event Storming and DDD",
        "excerpt":"Event Storming and DDD can identify bounded contexts. In a bounded contexts relationship, one side will have the upper hand (be upstream). Core contexts should!   The previous post was about how to classify bounded contexts as core, generic or supportive.   For the system to work, bounded contexts must collaborate together. As an example, let’s see what the cart goes through in an e-commerce system. Once the shopping context did it’s work and filled the cart, the ordering context must receive its content. As long as domains need to interact in some way, there is a relationship.      Very often, when 2 contexts have a relationship, one will have the upper hand. Having the upper hand means having more decision power than the other. DDD has the (abstract) concept of upstream and downstream to identify who as the upper hand. Upstream has the power, Downstream complies.      💡 Upstream bounded contexts enjoy more focus, prioritization, quality and investment… at the cost of other contexts.    Obviously, we want our core bounded contexts to be upstream.   This is the 11th post in a series about how to use Event Storming to kick start architecture on good tracks. It might be a good idea to start reading from the beginning.   Where does decision power come from?   Many things can weight towards which bounded contexts is upstream or downstream. Unfortunately, not all are good reasons!      Because it is very old legacy code that is almost impossible to change   Because it’s an API that is directly used by some customers that don’t want to change it   Because it is a core bounded context and we constantly make sure it keeps the decision power.   Suppose the code is the big boss’s own baby, because he wrote it a long time ago. He might always grant more priority to this bounded context! See Alberto Brandolini’s post about the Dungeon Master for more details.   Suppose there is a big customer who weights 30% of yearly revenues. The features of the systems his business relies on might get more decision power!   Obviously, some of these reasons are good, while others are bad… If your core contexts does not have the power, you are in trouble.      An activity to identify upstream-downstream   To teach this notion, I use this simple activity. Start by quickly introducing the concept.  Present the poster, and ask attendees where post-it-X should be. I also ask where it often falls in their current system. It’s often enlightening for people to understand the flaws in their current system. I’ve seen people realize why they suffer so much from their legacy with this activity.      Developers often first misinterpret this with code dependencies. It’s a good idea to clarify that these 2 concepts don’t necessarily map to one-another. For example, your API might not depend on customer code. Unfortunately if your “Top Customer” does not want to migrate to a new version, you’re stuck!   The next step is to ask the group to find relationships between contexts. Every time they spot a relationship, ask them which bounded context should be upstream. It’s usually obvious. We want core contexts to be upstream. Deduce which contexts should be upstream from the core / supportive / generic classification.      Here is how to materialize the relationship. Stick a post-it on the boundary between contexts. Find a way to represent what is upstream or downstream. If contexts are far away, use some wool string and stick the relationship post-it on it.   Outcomes      First of all, this step introduces a new concept to attendees. This notion is very often new and enlightening to participants. It will help them to think of code and organization dependencies in a new way. They should see problem in the current organization. They will also start to have refactoring ideas to put things in a better state.   If you are developer in a team that is downstream, it might be a good idea to try move to another team. Life in downstream environment is often painful.      💡 Moving to an upstream team might be the quickest way to find a more sustainable pace    It can also be a good time to introduce Hexagonal Architecture and its benefits for core areas. Making a core area free from any dependency keeps it more testable and evolutive.   To be continued   This is only a fraction of what DDD offers about smart dependency management. In the next post, we’ll see the patterns of relationships from the DDD blue book. I’ll also walk through an Event Storming follow-up activity to pick the best pattern.      This was the 11th post in a series about how to use Event Storming to kick start architecture on good tracks.   Continue Reading.  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming"],
        "tags": [],
        "url": "/check-that-core-areas-have-the-upper-hand-with-event-storming-and-ddd/",
        "teaser": "/imgs/2019-02-02-check-that-core-areas-have-the-upper-hand-with-event-storming-and-ddd/power-to-the-core-teaser.jpeg"
      },{
        "title": "Focus on the Core with Event Storming and DDD Domain Relationships - 1",
        "excerpt":"Bounded Contexts are a key aspect of DDD. Here is a DDD and Event Storming activity to find what kind of domain relationships will ensure focus on core contexts.   This is the 12th post in a series about how to use Event Storming to kick start architecture on good tracks. It might be a good idea to start reading from the beginning.   Following the previous posts, here is where we stand      We have identified bounded contexts   their relationships   and where should the decision power be      💡 Focusing on core domains is the key to long term profitable and sustainable pace       Core contexts contain our competitive advantage. Focusing on them is the key to long term profitable and sustainable pace. In practice, we want core bounded contexts to always keep the priority, the budget, the time… Let’s see how we can use Event Storming and DDD (Domain Driven Design) to make sure that core contexts always keep the upper-hand.   DDD Domain Relationship Patterns      DDD can do one more thing for us. In the blue book, Eric Evans lists some domain relationship patterns. These patterns are a bit like a relationship contract between 2 bounded contexts. They cover technical, human and team organization aspects.   The use of relationship patterns serves 2 purposes:      It will help us to pick the good relationship pattern as we design the system. This in turn makes sure core contexts keep the upper-hand and won’t get blocked.   With explicit relationships, everyone will understand better what is expected from them.   Unfortunately, this remains very abstract, and could grant us the astronaut architect badge. Nonetheless, using relationship patterns correctly is a great boost to architecture.   Let’s see how to make this more down to earth.   Walkthrough   This step is going to be critical but also difficult because you don’t want to lose people. A good idea is to start this in the morning, with everyone fresh. You’ll start discussing architecture and system organization. As a result, it’s going to be more technical than the rest of the Big Picture Event Storming. The presence of business people is still very useful because most relationships have organizational consequences. It will be easier to get their buy-in if they remain in the loop.      Here is a trick to keep things digestible.   Radars   In his presentation “Context Mapping in Action”, Alberto Brandolini characterized the relationships. Here is a slide from his presentation:              From Alberto Brandolini’s “Context Mapping in Action” presentation        I built on top of his work and mapped the others as well. (I did my best, feedback is welcome.) Here are the different dimensions and what they mean:      Ease of organization   Ease of coding (Alberto’s Skills)   Ease of maintenance   Decoupling (Alberto’s Flexibility)      I flipped some dimensions so that the wider the radar surface, the better, or simpler the pattern. I also adapted naming to make this more explicit.   We need the attendees to brainstorm the relationships they envision in their system. Unfortunately, these relationships are something new to them. If we dump the 10 relationship patterns at once on them, they won’t understand a word. Before asking them for relationships, we need a way to introduce them in a digestible way.   Here again, we’ll use stories.         💡 DDD can seem too abstract, use Event Storming to present it with storytelling.    Once upon a time, there was a college dropout startup…   We’ll use this made up story to present only the most simple relationships. By simple, I mean (arbitrarily) ease of organization ≥ 2 and ease of coding ≥ 3.      Big Ball of mud   Separate Ways   Conformist   Customer Supplier   Inner Source     DDD Domain Relationships Radars by Philippe Bourgau  The trick is to present the story as a tale. Next step is to present the relationship and stick the radars on the wall at the same time. Leave a few minutes for everyone to understand what this means.   Make sure everyone understood the kinds of relationships before moving on. It’s then time to ask attendees to pick one for every bounded context relationships. Just stick a post-it with the initials of the pattern on top of the relationship post-it.   The last thing to do is to ask their feedback about the design they came up with. Most likely, they won’t be very happy with it.      To be continued   In the next post, I’ll go the veteran startup scenario. I’ll also close this activity with the outcomes, as well as a few animation tricks.   Event Storming is a DDD accelerator. In less than a week, you can get your product started as if it had been going for months. This will save tremendous rework later on. Coupled with refactoring skills, it is key to a profitable and sustainable pace. This was the 12th post in a series about how to use Event Storming to kick start architecture on good tracks.   Continue Reading….  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming"],
        "tags": [],
        "url": "/focus-on-core-domain-with-relationships-from-ddd-and-event-storming/",
        "teaser": "/imgs/2019-02-10-focus-on-core-domain-with-relationships-from-ddd-and-event-storming/core-stronghold-teaser.jpeg"
      },{
        "title": "Focus on the Core with Event Storming and DDD Domain Relationships - 2",
        "excerpt":"The advanced DDD Domain Relationships shine as they keep the core upstream. Here is the end of an Event Storming activity to pick the best relationships.   In the previous post, I started to present an Event Storming activity to select DDD (Domain Driven Design Domain Relationships. If you haven’t start reading from the first part.   This is also the 13th post in a series about how to use Event Storming to kick start architecture on good tracks. It might be a good idea to start reading from the beginning.   I ended last post with the first scenario of a college dropout startup. They only had access to the easiest DDD Domain relationships. Without further ado, let’s continue our story.      Once upon a time, there was a veteran startup…   After a break, it’s time for another tale. This time, a team of veteran developers just quit their jobs to fund a startup. As experienced engineers, they have access to all the other kinds of relationships:      Partnership   Shared Kernel   Anti Corruption Layer   Open Host Service   Published Language     DDD Domain Relationships Radars by Philippe Bourgau   Same as before, explain the relationship and stick the radars on the wall. Then ask attendees to update their relationships with their new preferred pattern. Relationships can also stack-up, for example, you might have:      Open Host Service + Anti Corruption Layer   Customer Supplier + Inner Source   Most of the time, people are a lot happier with the new design this time.      Closing   That’s a good time to close the workshop. Don’t forget to do the following:      Ask for feedback (ex using ROTI) to make your next workshop better   Copy the architecture draft on a sheet of paper and save it or display it somewhere.         You can leave the design board on the wall for a few days, but you’ll end up removing it. The real value is in the shared knowledge.    Outcomes   The main outcome is the formal agreement of how the teams and part of the system should interact. Anytime people will collaborate, they can refer to this shared knowledge.      It will also provide a kind of compass for day to day collaborative refactoring. Whenever people need to mock some code for testing, they’ll know what’s the best place to do so. Whenever they’ll need to take a design decision, they can check the architecture draft for a big picture.      💡 Context Mapping with DDD and Event Storming helps to avoid a tangle of mocks.    Tips   Before I end this post, here are 3 tips to animate this functional architecture workshop.   Exclude tricky interaction patterns   Some relationship patterns are pretty complicated. Cyrille Martaire, suggested to exclude partnership and shared kernel. I have seen partnership work in the context of SAFe with PI plannings… But it might still be a good idea to stay away from these 2 kinds of relationships. It will make the workshop more digestible. Plus large synchronization events like PI plannings have their own costs 😰   Know DDD or get an ally   Relationship patterns phase is the most technical part of the workshop. You won’t be able to present the patterns if you don’t understand them. If you are not DDD-savvy, try to find an ally to help you out through this phase.   Don’t talk about DDD         💡 DDD is like the Fight Club. Do not talk about DDD.    DDD has a bad reputation because it uses convoluted names. Try not to mention DDD and use less scary words. For example      Use functional area instead of bounded contexts   Use kind of relationship instead of relationship patterns   etc…    The goal is to demystify DDD it as much as possible.   Next Steps   By itself, this architecture workshop is already very valuable because of:      the shared knowledge   the buy-in   the alignment   the improved collaboration   the better day to day decisions   On top of that, we can use this architecture draft to define actionable next steps. For example, there are activities to define:      A plan to get to this vision from scratch   What prototypes to build to validate NFRs early   A refactoring strategy to transform our existing system   Teams that will use Conway’s law to move us toward this vision   Event Storming is an effective way to shrink architecture from months to days. Done at the start of a project, it saves a lot of rework and keeps the pace sustainable. It’s also very flexible and can will serve many kinds of decisions. Run your own now!   This was the 13th post in a series about how to use Event Storming to kick start architecture on good tracks. In the next posts, I’ll be presenting workshop and activities to these next steps.   Continue reading  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming"],
        "tags": [],
        "url": "/focus-on-core-domain-with-relationships-from-ddd-and-event-storming-part-2/",
        "teaser": "/imgs/2019-02-12-focus-on-core-domain-with-relationships-from-ddd-and-event-storming-part-2/ddd-startup-veteran-cap-teaser.jpeg"
      },{
        "title": "How to use Event Storming and DDD for Evolutionary Architecture",
        "excerpt":"Big Picture Event Storming and DDD let us share an architecture vision. Here is a way to realize it through evolutionary architecture and emergent design.   This is the 14th post in a series about how to use Event Storming to kick start architecture on good tracks. This post builds on the previous ones, so it might be a good idea to start reading from the beginning. This post will interest you if you are starting a new product or feature.   Let’s suppose we went through the workshops from the previous posts. We should have a good idea of where we would like to eventually be. Here is the tricky part. We don’t want to lose our time building this vision right from the start. We need something out of the door fast. We also want a sustainable pace, so we must avoid quick and dirty solutions. Remember the beginning of this series about Event Storming? I said that it complements evolutionary architecture and emergent design. Let’s see how in more details.      How can we build something incrementally, without sacrificing our target vision?   Principles   The good news is that we won’t need another intense workshop to get going, we already have everything we need.   We’ll need some skills and practices though.    Technical Debt Leverage   What we’ll need is smart technical debt management. Eric Dietrich call this Technical Leverage.      We need to get features out fast. The trick is to make things as simple as possible at the beginning while keeping the ability to refactor. We should not skip steps that are mandatory to let us refactor to the target vision later down the road. For example, suppose we know we’ll need a separate process someday. If we currently only have a single team and the NFRs are fine, then modularity is enough for now. We don’t want to skip modularity though, as it would make future refactoring too difficult.      💡 The trick is to make things as simple as possible at the beginning while keeping the ability to refactor.    Another obvious step we cannot skip is automated testing. Even better, I’d say ‘fast’ tests. It does not matter if they are unit, integration or end to end. What matters is that they are fast enough to enable a fast refactoring feedback loop.   Other famous authors also wrote about this principle:      Kent Beck’s 3X explains why taking technical debt is the good strategy at the beginning         Dan North’s Spike and Stabilize technique contains more practical advices about how to do it.   Incremental Refactoring Techniques   Obviously, on this path, you’ll need to master incremental refactoring techniques. Without them, it will be very difficult to transform the system as you go. Martin Fowler’s Refactoring book is the perfect reference to learn these techniques.      If you can, start a coding dojo to practice and master these techniques.   A last advice is that whenever you need to refactor your code, do so in the direction of your vision. Let’s come back to our modularity example. We can make our modules have the same boundaries as our expected future services.   What’s the path then?   Remember the 2 scenarios from the previous workshop? We ended up with 2 functional architectures:      That of the veteran startup   That of the college dropout startup   The veteran architecture should be our end goal. Let’s use the college dropout architecture as our starting point. The 2 architectures follow the same boundaries, so it should be possible to migrate from one to the other.   Remember we constrained the DDD Domain relationships available to the college dropouts. This architecture will use customer supplier, ball of mud, and inner source relationships. This is the classic way by which monolith are born. While the team and the code are small, this monolith should remain manageable.   The Veteran-target architecture will contain services and Anti Corruption Layers. With tests and a modular monolith, we should be able to incrementally refactor to the vision. That’s the Event Storming, DDD, evolutionary architecture and emergent design synergy.      Don’t forget, the main point is to be able to deliver features early.   Tips   Here are some tips to get the most out of this practice      Repeat. Nothing prevents us from running a new Event Storming from time to time. Every time we do it, it will be faster, as more and more knowledge is shared in the team. By repeating it, the target architecture will evolve with the domain knowledge.   As you focus on a bounded context, you can also run finer grain, design level event storming sessions. This shorter workshop yields a more detailed target design for one bounded context.   I found the // TODO XXX comment ... technique great at taking technical debt leverage. You can read more about it here. Other interesting techniques are Architecture Decision Records and Living Documentation. By documenting past decisions, they help us to change the system later down the road.         💡 By documenting past decisions, Living Documentation let us change the system later down the road.    Summary   If you went through the Big Picture Event Storming, you have everything you need to get started today.   Are you lacking evolutionary architecture or emergent design skills? Start your coding dojo today to improve your refactoring skills.   Are you afraid that this strategy will lead to unsolvable NFR problems at the end? For example, who said that this target architecture is going to handle the load we’ll need? You are right, and that is what I am going to talk about in the next post.   This was the 14th post in a series about how to use Event Storming to kick start architecture on good tracks.   Continue reading…  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming"],
        "tags": [],
        "url": "/how-to-use-event-storming-and-ddd-for-evolutionary-architecture/",
        "teaser": "/imgs/2019-02-12-how-to-use-event-storming-and-ddd-for-evolutionary-architecture/event-storming-emergent-design-teaser.jpeg"
      },{
        "title": "Using Event Storming and DDD to prototype (micro)services and NFRs - 1",
        "excerpt":"DDD Big Picture Event Storming is a great support to draft (micro)services boundaries and NFR prototypes. Let’s see how to with this workshop activity.   Previous posts presented how to use Event Storming to draft a functional architecture. In the previous post, I suggested to realize this target vision incrementally. This means starting with something simpler.   Let’s illustrate this. It’s not because the vision has open host services APIs that we should create services right from day-1. This brings up a new challenge though. How to make sure we don’t fall in a performance NFR (Non Functional Requirement) dead-end down the road? This kind of late discovery is very difficult to deal with. They often lead to crunch hours and non-sustainable pace.      💡 Late discoveries of NFR problems leads to crunch hours and non-sustainable pace.    In this post, I’ll present how we can check our Event Storming design against expected NFRs.    This is the 15th post in a series about how to use Event Storming to kick start architecture on good tracks. To get the most out of this post, it’s a good idea to start reading from the beginning.      An activity to check performance NFRs   This activity can happen right after the Event Storming, with the full audience. But you can also do it the day after, with a smaller group of volunteers.   Ask the audience for 3 performance critical use cases. For every use case, do the following:                  Ask someone to explain the use case                       Ask the audience to estimate the expected user performance requirements                       Walk the use case through the events on the board.                             The use case will go through some open host service APIs of the draft architecture. Discuss and estimate how the end user performance will translate through each. For example, you’d want to have an idea of the number, size and frequencies of messages.                       Discuss whether its going to be ok or not:                    If you reach a consensus that it’s going to be ok, then it seems you are safe enough with this design            If you quickly conclude that it’s not going to be ok, you’ll have to rethink your design and try to find something else           Finally, there is the situation where you cannot reach a consensus or where the consensus is that you don’t know. It’s a good time to schedule a prototype.                           Build prototypes to get your answers      Prototypes are about getting answers to specific questions with the minimal efforts. From the previous activity, we should have a clear question in mind. Let’s suppose the question is: “Can we handle N messages, of size S, at frequency F?”. We don’t need the full system to test this, just design a fake experiment, with fake messages. We don’t need to test every message passing framework out there to find the best one either. As long as we find one that fits the bill, we’re safe. We can defer the real choice.   The exact details of the prototype itself do not have to specified during the workshop. The only thing that needs to come out of the workshop is the question we need to answer. Let’s use a pink (problem) post-it for that. (Cf post about agenda of Event Storming for description of pink problem post-its.)   Materialize processes and services   We already had a lot of thoughts about relationships, APIs, messaging and NFRs. It’s only a small step further to draft processes and services boundaries.   Here is a 2 steps activity to do this:   1. Present service principles   Here is a short list that you might want to adapt according to your context      We can notice that bounded contexts typically fulfill the inside principles by design. That’s why they make pretty good service candidates.   To learn more about services principles, check this talk about evolutionary architecture. (By Ionut Balosin and my friend Xavier René-Corail)     2. Materialize your processes   To materialize your services or processes on the board, use wool, as you did for bounded contexts. (cf post Identify Bounded Contexts ) It’s better to use wool of a different color.      💡 Use wool, scissors and scotch tape to materialize services on your Event Storming board.    3. Optionally, draw an architecture schema   When done, you can draw your service architecture on a sheet of paper. It’s better to stick to back-of-a-napkin style diagrams. It’s only a draft, and you’ll need to update it throughout the life of the project. Even so, once we’ve striped out all the post-its, the diagram gets pretty simple. It becomes a lot easier to share and view on a screen. (I should write a post about how to curate knowledge out of an Event Storming workshop)      To Be Continued   I’m not done yet with this activity. This post was a walkthrough of this Event Storming follow-up activity. The next post will detail outcomes and tips about how to make this activity successful.   Continue Reading…   This was the 15th post in a series about how to use Event Storming to kick start architecture on good tracks. The next post will be about a follow-up workshop to get insights whether to rewrite or refactor.  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming"],
        "tags": [],
        "url": "/using-event-storming-and-ddd-to-prototype-microservices-and-nfrs-1/",
        "teaser": "/imgs/2019-02-15-using-event-storming-and-ddd-to-prototype-microservices-and-nfrs-1/event-storming-microservices-teaser.jpeg"
      },{
        "title": "Using Event Storming and DDD to prototype (micro)services and NFRs - 2",
        "excerpt":"With Event Storming and DDD, we can draft services boundaries and NFR prototypes. In this post sequel, we’ll see that microservices are no silver bullet.   The previous post presented an Event Storming follow-up activity to draft service boundaries. If you haven’t yet, read this post first.   This is also the 16th post in a series about how to use Event Storming to kick start architecture on good tracks. It all started with misadventures with Big Design Up Front.   By itself, the previous post presented all there is to know to run the workshop activity. Before going over tips and warnings, let’s first list the outcomes at this point.      Outcomes   With only 1 or 2 extra hours on top of Event Storming, we should have an idea of what our target services could be. Here are the benefits (quite like that of other Event Storming activities):      Collective buy-in in a target architecture. Everyone will take more coherent decisions in their day to day work. For example:            When developers will refactor, they’ll all do so in the same direction of the target architecture       Developers will keep code modules in line with the target services and boundaries           Short-term actions. The pink post-its: quick wins, show-stoppers and prototypes to reduce risks or fail fast   All this in less than a week. Traditional architecture discussions typically span over months. In comparison, Event Storming is a short, but focused, time investment.      💡 Traditional architecture discussions typically span over months. In comparison, Event Storming is a short, but focused, time investment.     Don’t jump into micro-services!              Sample Microservice Architecture by Paul Downey, available on Flickr, under Attribution 2.0 Generic (CC BY 2.0) License        A word of warning ⚠️: do not jump into micro-services. Even though everyone is talking about them, micro-services are no silver-bullet. They do solve tricky organization problems in large scale teams. They also bring their own set of problems:      They are a headache to operate   They tend to be more difficult to refactor    Maintaining consistency across services is incredibly more difficult   They create the risk for technology silos   Think of alternatives before jumping in micro-services:      Simple modular code   Start more instances of your monolith, and synchronize data through your databases    A few services or processes (not micro) might be all you need   Use the same code with different entry points. It’s an easy way to reuse the same business domain logic in different processes   Simple Publish-Subscribe event bus might be good enough right now   For french speakers, Arnaud Lemaire gave an interesting talk on the topic a Paris DDD meetup.      As always, software a matter of tradeoffs, and there is no one size-fits-all solution. You might very well start with modular code and keep your code “micro-service ready”. The day you really need micro-services, you’ll be ready to jump in!   Tips   Here are 3 tips to make this more effective.   Repeat   This is a general Event Storming tip. It’s good to regularly re-do the exercise to update the picture and vision. It’s faster every time a team repeats it.   Design Level Event Storming   We could also try to do Design-Level Event Storming. Going into finer-grained design will clarify the messages exchanged between services. Here is a follow-up series about Design-Level Event Storming.   Evolutionary design   Use evolutionary design and architecture techniques to transform these designs into real software. Building the target architecture up-front is a waste of time. Spend some time practicing and mastering these techniques if you don’t yet do. It might be time to start a coding dojo!      💡 Start a coding dojo to transform Event Storming and DDD into evolutionary architecture and emergent design    Just do it   Event Storming is a bit like paper prototyping for software design and architecture. It makes it possible to do, in a few days of workshops, what is usually done in months of architecture and planning. Learn more about how to get started here. It’s a lot easier to try than you might think.   This was the 16th post in a series about how to use Event Storming to kick start architecture on good tracks. The next post will be about a follow-up workshop to get insights whether to rewrite or refactor.   Continue Reading…  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming"],
        "tags": [],
        "url": "/using-event-storming-and-ddd-to-prototype-microservices-and-nfrs-2/",
        "teaser": "/imgs/2019-02-15-using-event-storming-and-ddd-to-prototype-microservices-and-nfrs-2/no-silver-bullet-teaser.jpeg"
      },{
        "title": "Rewrite vs Refactor? Get Insights from Event Storming and DDD",
        "excerpt":"With legacy code, rewrite vs refactor is a valuable but tricky question. Here is an Event Storming activity that shows how far we are from the target.   You already have an existing system. It’s in production, it’s providing value to users every day. This is what brings the money to the company and pays everyone’s salary. Unfortunately, the state of this software will not cope with the market demand trends. You need to transform your system, without loosing your customers!   We saw how Event Storming helps to agree on a shared target architecture vision. When you have an existing system (and legacy code), the gap between reality and vision can be daunting.      Here is an Event Storming follow-up activity to get a feeling of how much work we’ll need to get there.   This is also the 17th post in a series about how to use Event Storming to kick start architecture on good tracks. It all started with misadventures with Big Design Up Front.   What’s the point?   The idea is that with a few extra workshop steps, we should be able to envision a good strategy to get to our target. Here are some example strategies:   Refactor   This consists of changing the code one step at a time, while always keeping it working. This is the safest and most sustainable strategy if the reality and vision are not too different.   Rewrite      💡 Full, drop-in, bing-bang rewrite is often a poor strategy. It’s very risky, late, and non-sustainable for the people.    If the target look pretty different, rewriting might be a better strategy. Here again, there are different ways to rewrite:      If you system is small enough, and you have new ideas to put in, then create a new product! DHH gave a great talk about how they rewrote Basecamp 2 and 3 as new products.   If your system is large, full rewrite is usually not be a realistic option. You’ll need to find a way to do incremental rewrite. This is usually a mixed rewrite AND refactor approach. Here is an example. First: refactoring a sub-systems behind an API and building an anti corruption layer. Then applying rewrite techniques like canary releasing and Strangler app on the sub-system. For more ideas, have a look at this talk from Eric Evans.     This workshop will give a visual assessment of the refactoring effort involved. It will serve a base for discussion and might help you to identify obvious migration first steps.      💡 Refactor vs Rewrite? The best strategy is usually a hybrid between both.    The Workshop Activity   Mapping the existing structure   Suppose your existing code structure is something like that      Use this to create a large visual 2D map with colored post-its representing every code file. If they are too many files, try with directories:      This is where the workshop starts. Book a room and stick this map on the wall.   Mapping files to the vision   During the Event Storming workshop, you should have identified your target bounded contexts.   Create another large, but empty map for your target architecture. Stick it on the wall, next to the map of your existing system:      That’s when attendees will work. They need move the existing files post-its to where they would fit in the target architecture. The colors will give visual clues about the ‘distance’ between the reality and the vision. We should have a first gut feeling of how complicated the migration will be.   Good news   If we are lucky, the map might look something like that :      Target bounded contexts and current modules seem a good overlap. Files that are currently close to each other will mostly remain close in the vision. That’s good news, and the migration should not be too complicated. Incremental refactoring is probably the best strategy in this situation.   Bad news   If we are not so lucky, we could end up with a map like this one:    The mosaic of colors tells us that the current and target architectures don’t have a lot in common. The path to get there will be long. Rewriting, or hybrid approaches might be better strategies here.   Agree on a strategy   All attendees can visualize the distance between reality and vision. It’s a good time for a discussion. Try to reach a consensus about the general migration strategy. Keep in mind that we have a bias towards rewrite. As Joel taught us, it’s always more risky in practice than in theory. Try to push for refactoring most of the time. If you are really hesitating, check this post that explains how to quantify a business case for a rewrite. For large systems, the solution will almost always imply both refactoring and rewriting.   Identify the first steps   Whether you reach a general consensus or not, it might be easier to agree on first steps. Some steps would be beneficial whatever the strategy. For example:      Replacing an in-house generic library with an off-the-shelf third party. It’s an obvious part of a refactoring strategy. It would also simplify a rewrite down the road.   Domain concepts are usually pretty stable through time. So gathering some core aspects of the business in a single place is a good move. It’s part of refactoring, but it will make rewrite easier if we can reuse this core business logic.   Some parts of both current and target designs might already match. Incrementally rewriting or refactoring these small parts might be easy quick wins.   etc   As usual, the usual Event Storming tips apply here:      Redo the exercise regularly   Use large-scale incremental refactoring techniques. I guess I’ll post some live coding videos about these techniques someday … so stay tuned.   What else can we do?   It’s incredible how much answers we can get out of a few days of Event Storming. Running your own is not as difficult as it sounds. Go ahead, and start your own!   Through this series of posts, I went through a lot of activities that we can run around Event Storming. But guess what, I’m not finished yet. In the next post, I’ll explain how we can create teams that support our target architecture.   Continue Reading…   This was the 17th post in a series about how to use Event Storming to kick start architecture on good tracks.  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming"],
        "tags": [],
        "url": "/rewrite-vs-refactor-get-insights-from-event-storming-and-ddd/",
        "teaser": "/imgs/2019-02-15-rewrite-vs-refactor-get-insights-from-event-storming-and-ddd/event-storming-rewrite-vs-refactor-teaser.jpeg"
      },{
        "title": "Feature Teams vs Component Teams? Decide with Event Storming and DDD",
        "excerpt":"Team organization is tough. Event Storming builds enough shared knowledge and architecture for successful team re-organization workshops.   Here is Conway’s Law:      “Organizations which design systems … are constrained to produce designs which are copies of the communication structures of these organizations.”     M. Conway[2]    If you have an existing team and a product you are trying to transform, this law is of paramount importance to you. If you did the Event Storming workshop, you should have a shared architecture vision. Let’s see how you can make Conway’s law help you to refactor!              Credits to Manu Cornet from http://bonkersworld.net/organizational-charts           💡 Together, Event Storming and Conway’s Law help long term refactoring.    ⚠️ Disclaimer: You might be few enough to organize well as a single team. In this case, you don’t need all this… until you grow, hopefully 😉      This is also the 18th post in a series about how to use Event Storming to kick start architecture on good tracks. It all started with misadventures with Big Design Up Front.   Principles of Teams   Teams that follow the boundaries of the architecture vision will more easily realize it. We should not map teams to our existing system, but rather to our target system!   The idea is to conduct a team re-organization workshop as a follow-up to Event Storming. It’s best to run this activity just after we draft the target architecture, when it is still fresh in everyone’s mind.              Video of a team self-selection workshop at KPN iTV        The principles are pretty simple:         Display the architecture vision and team principles   Let people self organize into new teams!      💡 Event Storming helps people to dynamically self-organize into teams.    Workshop steps   Here are the detailed steps for this activity:      Do this in front of the Event Storming board. It’s better to have the bounded contexts and relationship patterns. They make good starting points for team boundaries   Present the team principles   Let people brainstorm ideal team boundaries. Again, use wool, tape to mark this on the board   Ask attendees to identify the skills needed for each team. They can mark this on the board with post-its   If it was not the case already, bring everyone in!         Present the current team boundaries.         Let people self-organize into new teams according to all the constraints. Depending on your situation, this step might take more or less time. If you envision a rough time, it’s a good idea to have a look at this book for how to run this activity.         Draw a quick diagram of the agreed on team structure.      Tips   Here are a few simple tips to make this workshop more successful:   Get Everyone In   It’s best to involve everyone in Event Storming workshops if we can. When we follow up with this team re-organization workshop, we’ll get a natural buy-in. This will make the change a lot more sustainable.   Sometimes thought, if the team is too large for example, it will not be possible to include everyone. In this case, we can send a call for volunteers before the workshop. Everyone will get a chance to take part. We should make sure to have representatives from all teams and jobs. We’ll need to invite everyone in the final re-teaming activity though.   Repeat   Agile teams will adapt! Once we’ve done this workshop once, we should not be afraid to let teams re-organize from time to time. That’s the whole point of Dynamic Re-teaming. Practice shows that only 20% of people change teams every time. (As Laurens Bonnema told us in his presentation at XPDays Benelux 2018) Running this kind of re-teaming once or twice per year seems like a good frequency.   Repeating this team re-organization workshop should also improve people’s motivation. They’ll feel they have control over what they do, and what they can learn. They’ll also be more likely to build compromise on team structure if they feel they have the chance to move again in 6 months.   Try it yourself   Combined with this workshop, the Event Storming high participation and natural buy-in shines. Event Storming is an easy way to speed up DDD (Domain Driven Design) adoption. It’s an easy and flexible workshop. This was the 18th post in a series about how to use Event Storming to kick start architecture on good tracks. Get started here.   In the next post, I’ll list other workshops that we can run in conjunction to Event Storming.   Continue Reading…  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming"],
        "tags": [],
        "url": "/feature-teams-vs-component-teams-decide-with-event-storming-and-ddd/",
        "teaser": "/imgs/2019-02-19-feature-teams-vs-component-teams-decide-with-event-storming-and-ddd/cut-teams-with-event-storming-teaser.jpeg"
      },{
        "title": "How to max out DDD Big Picture Event Storming with other Workshops",
        "excerpt":"DDD Big Picture Event Storming is a prelude and catalyst to other workshops. Ex: Design Level Event Storming, Example Mapping, Story Mapping &amp; Impact Mapping   I said many times throughout this series of posts:      💡 Event Storming creates the shared knowledge on which to build much more.    Once you master Event Storming, a whole new world of collaboration opens up to your team.   As examples, here are a few common workshops that you can run after a Big Picture Event Storming. They will all be more productive because of the shared knowledge.      This is also the 19th, and last, post in a series about how to use Event Storming to kick start architecture on good tracks. It all started with misadventures with Big Design Up Front.   Design at a finer grain with Design Level Event Storming              From Alberto Brandolini’s “Introducing Event Storming” book on LeanPub        You’ll reach a point where the Big Picture Event Storming is not detailed enough for design. That’s when Design Level Event Storming kicks in. This finer grain design activity is more focused and technical. It helps development teams to collaboratively design the inside of a bounded context.              A sample Design Level Event Storming board from boldare.com’s event storming guide        Be warned though that this kind of design leans heavily towards CQRS, Event Sourcing and DDD (Domain Driven Design). For more details, here is a a follow-up series on Design-Level Event Storming.   Eradicate specification bugs with Example Mapping      It helps developers, testers and domain experts to agree on a story’s details. It’s a very quick (30 minutes) workshop. It actually looks more like a codified conversation than a workshop. Done well, it removes almost all specification bugs!              Sample Example Mapping cards on the introductory post about Example Mapping, by Matt Wynne        It yields even more insights when paired with Event Storming. Kenny Baas gave a talk precisely about how to combine Design Level Event Storming and Example Mapping.   Map a visual 2D backlog with User Story Mapping         💡 DDD Big Picture Event Storming vs Story Mapping? Do both!    User Story mapping is a workshop to build 2D representation of the upcoming work. This representation is a lot clearer than the 1D Scrum Product Backlog. This unlocks better prioritization and communication.   User Story Mapping shares many similarities with Event Storming. It is an exploratory workshop as well. It also starts with the analysis of the users workflow. My experience is that User Story Mapping is a lot faster to run when done after an Event Storming.              A sample user story map from Barry Overeem’s walkthrough blog post        The User Story Mapping book contains many details about how to run this workshop. It event contains a section about how SAP created a variant for really large teams.   Rank hypothesis and features with Impact Mapping      By getting back to our goal, the actors and their potential impacts, impact mapping helps us to take a step back. It is particularly useful to come up with alternate ways to achieve our goals.   It’s also a great tool to explicit our current hypothesis and what experiments we should run. Again, thanks to the shared knowledge, Big Picture Event Storming makes this workshop both faster and more effective.              An example Impact Map from impactmapping.org, licensed under a Creative Commons Attribution 4.0 International License.        I cannot understate the importance of this workshop. Prioritization is the only key to sustainable pace. Maximizing the work not done is the only way to have more impact with less efforts.      Simplicity–the art of maximizing the amount of work not done–is essential. - The Agile Manifesto principles    Combine, Improvise, and Repeat   Maintaining enough shared knowledge within a team keeps it in a hyper-productive state. Regularly running these workshops maintains the momentum and focus. The more you run them, and the faster they’ll be.   After a while, you might get so good at them that you’ll define your own workshops for your specific needs! If you are lacking inspiration, Gamestorming is a good book to look into 😉      Try it yourself   This was the 19th and last post in a series about how to use Event Storming to kick start architecture on good tracks. It all started here.  ","categories": ["architecture","squash-BDUF-with-event-storming-series","event storming","example mapping"],
        "tags": [],
        "url": "/how-to-max-out-ddd-big-picture-event-storming-with-other-workshops/",
        "teaser": "/imgs/2019-02-25-how-to-max-out-ddd-big-picture-event-storming-with-other-workshops/event-storming-follow-ups-teaser.jpeg"
      },{
        "title": "A serious game for learning 'built-in quality at the source'",
        "excerpt":"Skip studying queuing theory! Play this 1h15 serious game for learning why building-in quality at the source leads to a productive and sustainable pace.      The company I work for, Murex, is currently going through a large transformation. This transformation involves training teams to new skills and practices. One such training is the lean concept of “Built-in quality at the source”.   Built-in quality at the source is first of all a set of practices (Software Crafting, Test Driven Development, Behavior Driven Development, Continuous Integration…). Unfortunately, the training was too theoretical. Attendees reported that it was painful and, in the end, useless.      💡 Stop of boring people out with theory of queues. Play the built-in quality serious game for learning why software best practices matter!    I had played the Dice of Debt game with some other Murex agile coaches. They asked me if I knew of a serious game for learning Built-in quality at the source. I did not, and that’s why we decided we could build our own!   Before finishing this story, let’s first see why this subject is so important.   Why built-in quality at the source matters   When we postpone quality, we discover problems late and they come back messing up the value flow. It can create so much perturbation that nothing gets done without tremendous efforts.    Imagine you are working on user story X when a bug appears. This creates at least 3 problems:      You’ll receive useless stress   You’ll lose some time switching from one task to another   People who might be waiting for story X will have to find something else to do…   Things get even worse at a larger scale. Suppose your team has been building a new feature for 6 months before it goes live and … users don’t like it! You might need to throw away everything and start with a new feature… Leaving a code architecture that is not adapted to what you now need.   This is not a sustainable pace.      In summary, without built-in quality at the source, people get stressed, which makes them take shortcuts and neglect refactoring, which degrades the code quality, which makes building-in quality at the source even harder…      💡 Build-in quality at the source to enable a sustainable pace and continuous refactoring.    The first test with other coaches              From the Built-in Quality Game under Creative Commons Attribution-ShareAlike 4.0 International License        As I said earlier, the first inspiration came from the Dice of Debt. It’s another serious game for learning the perils of technical debt. From the Dice of Debt, we kept:      the dice   simulating a software development team   playing over a few iterations   tracking what we were are building   We wanted to show the value of a few built-in quality practices, on the flow. We naturally thought of a Kanban board.   I created just enough material and rules to test the game. We had a test session with volunteer agile coaches at Murex.   This first session was encouraging. The feedback was positive and we got plenty of ideas to improve the game.    For example, we decided:      to simplify the Kanban flow   to write the mechanics of the game on the material as much as possible   to assign roles to players to start more quickly   Special thanks to Damien Menanteau, Hicham Ghorayeb, Joseph Soares, Julie Jeru and Matthieu Tournemire   The training day   Once we finalized the material, we were ready for our first real-life training.   They were about 40 people attending the training. We had 6 teams of about 7 people play the game at the same time.              From the Built-in Quality Game under Creative Commons Attribution-ShareAlike 4.0 International License        As part of a day-long training, we had agenda constraints for our game. We decided to use a faster “real-time” version, where all roles can play at the same time. Indeed, it is faster, but at the cost of lower learning. I definitely recommend to use the turn based version if you can.   Even with this small glitch, the feedback and engagement were great. For example, teams understood that continuous delivery is better than big batch deployment.   The coaches gave great feedback too. They said they would love to use this serious-game for teaching “Built-in Quality at the source”. That’s why I decided to open source it.    Try it yourself!   The game is only 1h15, it’s not too hard to find the time to play it.    If you are a team-member, try it with your team. Next time you do an end-of-sprint celebration, or when you have a bit of Slack Time. You could also organize a team lunch and play the game at the same time.   If you are a coach, the game is a good substitute for a training. People are usually happy to play instead of attending a classroom training.      All material is available through Github: the boards, the cards, the rules and an animation guide.              Main board from the the Built-in Quality Game under Creative Commons Attribution-ShareAlike 4.0 International License        If you try this game, I’d love to have your feedback so that we can improve it.   It’s all under Creative Commons, you are welcome to contribute improvements.  ","categories": ["lean","gamification","agile","team building","open source","infographic"],
        "tags": [],
        "url": "/a-serious-game-for-learning-built-in-quality-at-the-source/",
        "teaser": "/imgs/2019-03-08-a-serious-game-for-learning-built-in-quality-at-the-source/built-in-quality-game-box-teaser.jpeg"
      },{
        "title": "You should refuse to develop what you don't understand",
        "excerpt":"Understanding is key to building quality software. Here are 3 levels of understanding we should go through before we start coding.      Since the beginning of the year, I’ve had the position of a team lead/manager/dev lead, call this what you want, but I’m essentially in charge of a module as well as the team of developers that work on it.   This position is giving me a new point of view on software development projects as I’m involved in more projects and take more part to the decision processes. Recently a new insight has clicked in my mind thanks to this, about how much understanding shapes the code and the application.   My point is that the quality of the code (and of the software) is closely related to how much the people that write and design it understand what they’re doing.   Okay, you may think: “Duh! Of course we’d better understand the code we’re writing, otherwise we might as well hire monkeys to type away some random code and be done with it!”.      And you’d be right. Therefore, I’ll assume that you already realize that understanding what you’re doing is important. This is like Understanding level #0, and we won’t dig into that. What we’ll get into is what is necessary to understand, and how this affects the decisions you take on a daily basis. Knowing this beforehand would have saved me a huge amount of time and dubious code.   Even if we won’t see one line of code today, I consider what follows to have a big impact of the quality of the code and on how expressive it is.   Understanding level #1: Understand why something doesn’t work   This first level of understanding comes fairly early in a developer’s career, sometimes even without being taught (or at least that’s what I’ve observed around me).   Imagine you receive a bug report: a feature is broken in the application, and you need to fix it. How do you go about it?   The classical pattern is:   1) you locate the guilty code (how you do that is a separate topic which I talk about in my book about legacy code),   2) you make a change in that code,   3) you check that the bug is fixed and that you didn’t cause regressions in other use cases.   Let’s focus on part 2): “you make a change in that code”. There are two approaches to do this: the first one is to understand exactly what is going on with the current code, locate the error and correct it.  The second one is to grope around: maybe add a +1 to the stopping condition of a for loop, relaunch the use case to see if it worked, if not try something else, and so on.   The right approach is the first one. As Steve McConnell explains in its fundamental book Code Complete (highly recommended reading!), whenever you change the code, you must be able to predict with confidence the effect of your change on the application. If a fix doesn’t affect the application the way you thought, this must be a troubling feeling for you, that almost requires you to put yourself in question (my rewording, can’t put my finger on the quote in the book).      In short, to produce a robust fix that won’t damage the quality of the code, you must be sure to have a good understanding of the code and the source of the problem.   Understanding level #2: Understand why something works   The second level of understanding is less intuitive that the previous one. My manager had to teach it to me back when I started as a professional developer, and I’ve had to teach it to younger developers over the years.   This time, say that you receive two bug reports at the same time: the first one describes use case A, and the second one describes use case B. Both don’t behave the way they should.   So you go on to fix the first the bug in the first use case. Using understanding level #1, you take the time to get a detailed understanding of the involved code, realize why that code makes use case A behave the way it does, and make a sane fix that corrects use case A exactly the way you predicted. All good.   Now you move on to use case B. You test to check that you reproduce use case B and, surprise, use case B now works correctly! To make sure, you revert the fix you’ve made for use case A, and the bug for B reappears. Your fix has corrected both issues. Lucky you!   You weren’t expecting it. You designed your fix for use case A, and you’re not sure why it worked for use case B too.              Wikipedia: A cargo cult is a belief system among members of a relatively undeveloped society in which adherents practice superstitious rituals hoping to bring modern goods supplied by a more technologically advanced society.        At this moment, it is very tempting to consider the job done for both bug reports. This is the natural thing to do: after all, the point was to correct the two use cases, wasn’t it?   But the work is not finished: you still need to understand why your fix corrected use case B. Why? Because it may have corrected it for a bad reason, and you do need to make another fix. Bad reasons include:           by not making the right fix for use case B, you could have broken use case C without realizing it,            maybe there is another bug related to the feature, and your fix somehow corrects B by relying on it. Use case B looks good now, but the day that other bug gets fixed, that will break use case B again and perhaps more.       All those reasons make the code inconsistent, and this will come back to bite you later on, probably when it’s not a good time. It takes some will to keep spending time understanding why everything seems to work correctly, but that understanding pays off.   Understanding level #3: Understanding why you make a development project   This is the understanding level that clicked for me recently, and probably the one that could have had the more impact if I had fully realized it earlier.   To illustrate this, consider the following example: your module needs to be compatible with feature X. You don’t know much about feature X, but you’re told that to be compatible with feature X, you must implement the framework F. Other modules compatible with feature X also implement framework F.   Your code has lived all its life away from framework F, and implementing framework F is not an easy thing to do. It has profound implications on some of the features of your module. But you dive into the development regardless, spend weeks developing, testing, demoing, getting feedback, correcting regressions, discovering unpredicted issues, going over the initial estimates, developing some more, testing, getting feedback, correcting regressions, all this striving to implement framework F.   And at some point, you realize, or someone tells you, that maybe implementing framework F wouldn’t make your module compatible with feature X after all. Maybe all that time, sweat and code weren’t done the right direction.   Something like this happened to one of the projects I was responsible for. How did that happen? Because I wasn’t familiar with feature X and with its link to framework F. What should I have done? Understand the exact reason to implement framework F. Ask whoever was requesting the development the exact link between the project and the desired outcome, and not do it just because other modules do, or trust someone that it would make feature X work.   The same issues can happen with user facing tasks. Sometimes we are asked to code something without being told what’s the user need behind. If we knew the user need, we might be able to come up with better alternatives. We might even find a solution that does not require to code anything!              By Brooke Lark on Unsplash        What I learned in that project is this: we should refuse to start a development if we don’t have a clear understanding of why we are requested to do it. Plainly refuse it. It is tempting to start a project as soon as possible in order not to waste time. But blocking its start until we understand its implications can save time on another order of magnitude.   Even if someone pressures you to start a development and you don’t understand the detailed rationale, resist. Understand why they want you to do it first, and then decide if this is the right development to do.   I’ve learned it the hard way, I hope reading this can benefit you too.   Understanding level #4: ?   There is a lot to learn in software engineering, and surely I’m not so far past scratching the surface on the topic of understanding.   What other levels of understanding have you come to realize along your years of experience? What decisions had a positive result on the quality of your codebase and your application? What decisions had a negative impact and were useful to learn from?   Share your experience in the comment sections below.  ","categories": ["programming","bdd","badass-developer"],
        "tags": [],
        "url": "/you-should-refuse-to-develop-what-you-dont-understand/",
        "teaser": "/imgs/2019-03-11-you-should-refuse-to-develop-what-you-dont-understand/why-why-why-teaser.jpeg"
      },{
        "title": "10 Pair Programming Best Practices Questions & Answers",
        "excerpt":"Pair programming is not just sitting together in front of an IDE. Here are battle tested answers to common questions that will make pairing work for you.      Philippe and I are well known to be advocates for Extreme Programming and its practices (well Philippe more than me, but I will share some of the credits 😀). Because of that, we were asked lately to participate in a Q&amp;A session to share with our experience and recommendations on applying pair programming within a dev team.   Question I: When to do Pair Programming?   All teams do pair programming in one way or another, even if they don’t like to admit it!   Almost all developers have the tendency to ask a colleague for help when stuck on a complex piece of code or a bug, this is a form of pair programming! If we acknowledge that the four eyes principle can help solve complex problems faster, then why don’t we apply it all the time?   Going back to the question, some teams love pair programming all the time! But, it might be better and easier to start slowly. For example, in the next sprint, try to pair 50% of your time. Another idea is to try Mob programming on a specific story. Try that for one sprint, then in your next retrospective, evaluate your experience. If you think the team is ready to do full-time pair programming then go for it!    Question II: How should we start pair programming in our team? Will it impact the team’s capacity and velocity?   If the team believes that pair programming can bring them benefits, then we advise you try it in your next Sprint!   The first thing to do is allocating the next retrospective to define the pairing agreement within the team. This includes:        How long to pair?    How to organize the pairs?   How to distribute the stories?    When to switch between the driver and the navigator?    Philippe and I used to work together in the same team when we switched to full-time pair programming.    At first, the velocity did not change, our records showed that we were able to close the same number of story points before and after pairing. But, after a few sprints, we started to become more concerned about improving the code quality. As we focused more on that, the velocity dropped for a while, but we also managed to drastically reduce the defects and the time allocated for code review. On the long run, we can say that our productivity increased!    Finally, another important observation to be mentioned here is that thanks to pair programming and knowledge sharing, our team’s productivity was not impacted when any of the team members left the team, even senior ones!    Question III: How long do we pair?   Some teams tend to switch pairs every day or half a day. Although it might be a good approach to spread knowledge across team members, that doesn’t always work well, as it might take some time for the story lead to onboard their pair on the story’s tasks and objectives. Thus, it might take more time to finish the story.    In our team, we used to switch pairs either on a weekly basis or a story basis.    We used to organize the pairs and stories on a weekly basis during the sprint and mid-sprint planning meetings. We used to re-shuffle the during those meetings, even if the stories were not closed yet. By doing so, we ensured that no pair worked together for more than one week.    The second option was switching pairs after closing a story. During the daily standup, a pair used to announce that they have closed their story and ask if any pair is interested in switching the partners.    It is crucial to change pairs frequently for several reasons. Amongst those reasons is creating a stronger bond between team members and reducing any conflicts that might arise if two developers spend a long time working together.    Question IV: When do we plan the pairs?   The ‘sprint planning’ and ‘mid-sprint review’ meetings are good candidates for pair-planning. After going over all the pending stories (Not Started &amp; InProgress) in the sprint, we used to agree on the pairs and the story each pair will work on next. Our sprint was 2 weeks long, thus, we had one of those meetings every week which made them ideal for pair-planning.    Question V: How will we split the stories?      Each pair should work on a single story, that means working together on all of its tasks. There are two things that need to be considered when distributing the stories over the pairs.    First, ensure that all team members are contributing to the different modules of the code. By doing so, you foster collective code ownership between the team members.    Second, one of the pair should be familiar and knowledgeable of the respective code and the tasks needed to close the story. Pairing will definitely help to spread the knowledge across the team members, thus with time, this will be less of an issue!    Question VI: What are the best practices you applied when pair programming?   We built our best practices through trial and collecting feedback during retrospectives. But obviously, the best practices will vary from one team to another.    Here are four of the best practices we adopted in our team:       Frequently switch between the driver and the navigator. Failing to do so increases the risk of losing the navigator’s focus and engagement on the story.    Submit the code as frequent and possible. This practice should be applied always not just when pair programming. But, you can use this technique as an incentive to switch between the driver and the navigator frequently, especially when pairing remotely.    Keep an up-to-date shared document of the story’s tasks. This could be in the form of a to-do-list, word document on Sharepoint or Google Drive, or even a mind-map.     Do not be afraid of asking for clarification when there is something that you do not understand.   Do not be afraid of asking for a break when you are tired.    Question VII: Do we pair on complex stories? or all stories?   In our opinion, pairing should be applied to all stories. It was in very rare cases where we decided not to pair. Even for what might be considered ‘Silly’ stories, a pair can find a better solution than a solo developer, this could be like automating some tasks, proposing a new tool, factorization, etc. Pairing on all production stories is a good way to spot and perform refactoring.   If pairing all the time scares you, leave enough solo-slack to keep the pace sustainable.   Question VIII: What do we do if we had to wait for a build that takes one hour?      In extreme programming, the rule is to have a build that is at most 10 minutes long. But in reality, some projects take much more to build due to compiling different modules, legacy code, etc.  Rather than having a long build blocking you from pairing, think of how can pair programming help you improve the build time! Here are some ideas:       Due to the four-eyes principle, it is easier and faster to detect issues while writing the code and before pushing to build pipeline! With this, you will have higher chances for your build to pass    While waiting for the long build time to finish, think together how the build can be improved. In our team, we managed to drop the build time from 9 to 3 minutes   Question IX: How do we handle knowledge difference between the pair?   One of the key benefits of pair programming is knowledge sharing amongst the team members. Still, some developers will remain more knowledgeable on specific modules of the code especially senior members. When there is a knowledge difference between the pair, the driver should play the role of a coach for the navigator. Doing so might delay the story closure, but it is definitely much faster for the less experienced developer to become acquainted of the code.   Philippe wrote an interesting blog on this topic, you can check it here!    Question X: What do we do with the time difference between the 2 cities      That is an interesting question for us because it is linked to applying pairing programming in remote mode! In the book Extreme Programming Explained, Kent Beck &amp; Cynthia Andres recommends having the whole team sitting together in an open space. We could not do that since we were distributed between Paris and Beirut. Instead of quitting XP, we decided to try pair programming in remote mode! And we succeeded for 4 years!     To be honest, it wasn’t easy at first! But we managed to make it work through determination, feedback collection during retrospectives, and trial and tuning.    Time difference between Paris and Beirut was one of the issues we faced when remote pairing. Here are some of the agreements we reached that helped us overcome this problem:       Have an up-to-date shared document of all the story’s tasks. Any of the pair should be quickly updated on the story’s status by just having a quick look at the document.    Don’t leave un-committed code when you leave your desk! If you are using Git, create a branch for your un-committed code, if you are using perforce use the shelve feature.    Every morning, the pair share their calendars to be aware when they have separate meetings   Use the time difference to finish any paperwork you need to do alone.    Philippe and I gave the talk ‘Extreme Practices’ at Agile Tour Beirut in 2016 and the SPA Conference London in 2017. The talk covered our experience in adopting the XP practices in remote mode. You can check the talk here.    Finally…   Even a slight dose of pairing will have benefits on your team’s work. There is no one-size-fits-all with pair programming. Try it a little, see how it works, and improve! Refactoring the way you work is the only way to achieve long term productivity and a sustainable pace.  ","categories": ["pair programming","remote","extreme programming","agile"],
        "tags": [],
        "url": "/10-pair-programming-questions-answers/",
        "teaser": "/imgs/2019-03-18-10-pair-programming-questions-answers/how-do-we-pair-teaser.jpeg"
      },{
        "title": "How to use Event Storming to introduce Domain Driven Design",
        "excerpt":"Although incredibly effective, DDD is overlooked by developers because of its abstractness. Event Storming is a great way to introduce DDD without naming it!      State of DDD   I first read about Domain Driven Design about 15 years ago. It’s after reading the blue book that I decided to move to the rich domain of Corporate Finance… for the better and the worse!   I still remember the first project where we tried to follow the DDD principles. We were working in a bank, and writing a connector to an electronic broker. For 2 days following the deployment of the first version of our system, we heard nothing about it. This was quite unusual. The broker finally called us to say there was a bug in the way we were handling one message. We did not open the debugger. We only checked our logs. This was enough to explain which business rules where used, and that we thought we were right. 2 more days later, they admitted the bug was on their side.   Granted, this success might also have resulted from all our other eXtreme Programming practices. Still, putting the business logic before technology proved both very effective and sustainable.   15 years later, we’re only starting to hear a bit more about DDD. With the microservices trend, people are wondering “What boundaries should our services have?” DDD’s Context Map is one of the few serious answers available.   Why isn’t an effective concept like DDD more widely adopted?   The barriers to DDD      There are many reasons why DDD is not ubiquitous 😉. First of all, the complexity it brings is not justified for every project. Second, it explicitly builds on top of XP. It almost states merciless refactoring, unit testing and ‘whole team’ as pre-requisites. XP’s adoption is already small, which does not help DDD’s adoption either!      💡 Its awkward vocabulary is one of the reasons why DDD is not more widely adapted.    There’s a third reason why DDD still does not have the place it deserves: its awkward vocabulary! Here are a few examples:      Ubiquitous language   Bounded contexts   Upstream / Downstream   Aggregates   Relationships patterns (Shared Kernel, Conformist, Open Host API, Published Language …)   Hexagonal architecture (Even if not directly DDD)   …   All these contribute to make DDD look very abstract, and not practical. This is where Event Storming can help. Let’s see how.   Event Storming in a nutshell   Event Storming is a collaborative design workshop. Using a big wall and post-its, a group of people can draft a software system design in a few hours.   Alberto Brandolini invented it as a way to design Event Sourced systems. The workshop itself is pretty intuitive. The rules are straightforward and rely on natural human metaphors. After 1 hour in, people feel at home with Event Storming.   If you want to learn more about Event Storming, check my series of post on the topic.   How to use Event Storming to get people to do DDD without knowing?      💡 1st rule of DDD, don’t talk about DDD    Alberto was an early member of the DDD community. Event Storming has huge bias towards analyzing the system from the point of view of the domain. During the Event Storming workshop, many DDD concepts will emerge. For example: Business Events, Bounded Contexts, Actors, Aggregates…   This natural emergence of concepts is a great opportunity to explain to people what they are. Here are a few Event Storming tricks I use to make DDD more accessible.   Functional Area   Although nobody understands Bounded Context from the start, everyone gets ‘Functional Area’. Granted, Bounded Context is a more precise name. It’s also a lot easier to forget it and only mention ‘Functional Areas’ during the Event Storming. People will understand it straightaway.      💡 Talk about Functional Area instead of Bounded Context.    Vocabulary   Ubiquitous Language is a great name… once you get it! It’s easier not to scare everyone out by using ‘shared vocabulary’ instead.      💡 Talk about Shared Vocabulary instead of Ubiquitous Language.    Having the upper hand   The blue book introduces the notion of upstream/downstream relationship between bounded contexts. This concept is very valuable in practice. Unfortunately, it’s not so easy to explain during a workshop. The best introduction I found is “Who has the upper hand in the relationship?”   I also use a special activity to explain it further. I ask people to place a few typical situations upstream or downstream. If you want more details, I wrote a whole post about this activity.      Relationship patterns   The blue book also contains a full pattern language about bounded context relationships. This is of tremendous value to build a conscious functional architecture. Unfortunately, this part of the book is also pretty abstract and difficult to read.   Hopefully for us, Event Storming can help here too. After a while in the workshop, the system should feel more concrete. That allows to introduce the patterns in a less abstract way.      💡 Use radars and storytelling to introduce Bounded Context relationship patterns.       I use a special activity for that. It relies on storytelling and visual radars to compare the patterns. With this, I have had success to introduce the patterns smoothly to attendees.      Blank Aggregates   Aggregate is another concept introduced in the blue book that is not easy to grasp. Explanations involving invariant and consistency don’t make it easier to understand.      Alberto Brandolini himself suggests using a technique he calls ‘Blank aggregates’. At the design level phase, don’t ask people to come up with Aggregates. Instead, put empty post-its and ask people to write business rules. Don’t mention aggregates yet.   Only later ask them to group the business rules ‘as-they-would-with-code’. Developers usually get this well. They’ll naturally find good names for these Aggregates (of business rules).   Start now!   Don’t be a Smug DDD Weenie. You’ll have difficulties to persuade people to jump into DDD. It’s a lot easier to setup a workshop, and get started. For your next feature, product or refactoring effort, start with Event Storming!   For more details about how to run an Event Storming workshop, here is a full walkthrough.  ","categories": ["event storming","architecture","change management","collaborative work","ddd"],
        "tags": [],
        "url": "/how-to-use-event-storming-to-introduce-domain-driven-design/",
        "teaser": "/imgs/2019-03-28-how-to-use-event-storming-to-introduce-domain-driven-design/event-storming-to-ddd-teaser.jpeg"
      },{
        "title": "Event Storming lessons from Post-It haters",
        "excerpt":"After going through why Event Storming uses post-its, we’ll go over alternatives. We’ll then see how to use these techniques to improve Event Storming!      We regularly launch new ambitious projects at work. Event Storming is becoming the de-facto standard to get everyone on board quickly. A few months ago though, I was asked to animate an Event Storming… Without post-its! Attendees were not at ease with walking around and discussing in front of a wall of post-its.   Indeed, Event Storming uses a lot of (orange) post-its. If you hate those, it might be difficult to swallow…   Why Event Storming uses Post-Its ?   Event Storming is not alone here: User Story mapping, Example mapping, retrospectives… most of these use post its. Gamestorming contains many creative activities relying on post-its.      Here are a few reasons why post-its work well for creative workshops:      They are not set in stone. It’s easy to move them around, and to throw them away. This unlocks a quick feedback and improvement dynamic.   They are introvert friendly. Speaking up in front of everyone can be a challenge for introverts. They will find it a lot easier to write something down and stick it on the wall.   They work well around HiPPOs, who risk silencing down any other idea at the time they enter the room   People need to stand up and walk to stick their post-its, which creates engagement   Humans are visual animals. We are well equipped to grasp complex topics at a glance at a wall of post-its. We were never built to learn through long documents or emails              By Prof saxx on Wikipedia under Creative Commons Attribution-Share Alike 3.0 Unported           Finally, such visual radiators are an invitation and support for discussion   It’s not by accident that designers were the first to use post-its in their creative games. (Check Why are designers obsessed with post-it notes ?)   Traditional (doc and email based) architecture approach take months to get to something.      💡 Thanks to post-its, Event Storming speeds up drafting an architecture from months to days !    What alternatives do we have ?   Ok, got it, you still don’t want to use post-its.   In Want To Be A Great Designer? Ban Post-It Notes. The author suggests we should drop Design Thinking and do Design Building ! The idea is to skip the ‘post-its’ phase completely. Start building something and get feedback. He suggests 2 flavors of that.   Homework and Feedback   Every participant starts by designing something on their own. Next step is to share it with others to get feedback. They then rework their design and share again … and so on until they reach a consensus.   Suppose we want to get out of the workshop with a target functional architecture. People could draft one on their own, and then all share their design in a group session. Then repeat …   The more realistic the design, the more interesting the discussions will be. Drawing ideas from Event Storming and DDD, participants could:      Identify core, supportive and generic bounded contexts   Highlight upstream / downstream bounded context relationships   Define interaction patterns between bounded context   …   Event storming creates high value secondary outcomes, like problem detection &amp; knowledge transfer. I don’t know how this format would perform according to these.   Walking Skeleton   As I said, the more realistic the design, the better. Pushing this logical to its conclusion, we could build a walking skeleton.    Here is how to use the Walking Skeleton to draft an architecture. Setup a single small end to end and high skill team. This team can start by building a first end-to-end feature. They’ll collect feedback and improve the architecture with every features they’ll add.   Here are the benefits of the Walking Skeleton      All design and architecture is done with code, by the team working on real problems. Not in big upfront design sessions   The team delivers end to end features from sprint 1   We can test Non Functional Requirements can also from day 1   Finally, the team structure can grow and emerge from the initial team’s work.   Alberto Brandolini (father of Event Storming) suggests the team can maintain a functional architecture as it builds the system. Check Strategic Domain Driven Design with Context Mapping for more details. In Living Documentation, Cyrille Martraire suggests to go even further. The team can actually generate the diagram from the code. That’s as realistic as diagram can be!      Apart from that ?   Surprisingly little! Searching Google with many post-its hater keywords I only found these :      The Dark Side of Sticky Notes Which highlights the dangers of the glue for people   Wait, We’ve All Been Using Post-It Notes Wrong This Entire Time?! Which explains that we should be unfolding post its from the side rather than from the bottom !   In Defense of Post-its explains that post-its are not a silver bullet and can be miss-used. Fortunately, Event Storming got it right 😀.   Mixing the best of all Worlds   The interesting conclusion is that none of these approach exclude Event Storming! Mixing all strategies, here might be the best way to use Event Storming:      Setup a tight team of domain experts and skilled developers   Do an Event Storming all together   Build a walking skeleton   Get feedback and learn   Re-do Event Storming (or others)   Refactor and improve the walking skeleton   etc…   The true value of Event Storming is in fast communication within a whole team. By removing middle-men, we save time, money and stress.      💡 Don’t forget that Event Storming builds on a cross-disciplinary team and incremental architecture!    &quot;It&#39;s not stakeholder knowledge but developers&#39; ignorance that gets deployed into production&quot; #QuoteServer &mdash; Alberto Brandolini (@ziobrando) 27 mai 2016   ","categories": ["event storming","architecture","collaborative work","ddd","incremental-software-development","example mapping"],
        "tags": [],
        "url": "/event-storming-lessons-from-post-it-haters/",
        "teaser": "/imgs/2019-04-07-event-storming-lessons-from-post-it-haters/event-storming-no-more-post-its-teaser.jpeg"
      },{
        "title": "How to Capture the Outputs of an Event Storming Workshop?",
        "excerpt":"Event Storming builds ‘shared understanding’. This is often not enough to bring key action-oriented people in! Here are concrete techniques to convince them.      When I started to run Event Storming at work, I would often receive puzzled looks. Action oriented people would ask: “What are the outputs of this expensive Event Storming workshop you want us to do?”. Here was my typical answer. “We’ll get shared understanding, identify quick fixes and hotspots. We might also be able to draft a functional architecture…”   Putting shared understanding as the main output did not convince them.   Since then, thanks to a few early adopters, we’ve been able to make the value of Event Storming more obvious. It is becoming a de-facto standard for early architecture of new features. To achieve this, we had to better explicit what the outputs of Event Storming are.   But first, let’s go through the usual answer.   Shared Understanding   Wherever you’ll look on the web, you’ll get the same answer.      💡 The main output of an Event Storming workshop is the shared understanding.    This is true. This is the main output of Event Storming.   Let’s see what this answer means. Event Storming can kick-start the team in a state of high bandwidth communication. The team will remain hyper-productive as long as it stays in this state.   This ‘jelled’ state opens up many improvements:      it reduces the need for processes as people know what’s the good thing to do   it simplifies code as people understand the problem better   Finally it also leads to a sustainable rhythm of steady refactoring:      Event Storm   Start building something   Learn and get feedback   Event Storm (or other) again, with the benefit of this new knowledge   Incrementally refactor and add new features   Learn and get more feedback   …   This is the main output of Event Storming. Let’s now see how to capture others in a concrete manner.   Capturing decisions      Many discussions happen during an Event Storming sessions. Some of these discussions will lead to decisions. It’s a good idea to keep track of these decisions somewhere to avoid forgetting them.   I tried to use special post-its to keep track of decisions, to no avail. The Event Storming post-it bestiary is already complicated enough. In the end, Alberto gave us the right answer. The simplest thing to do is to keep a flip-chart close to the design board and record decisions there.   Depends on the decisions, and on the context. Usually putting decisions on a flip chart and sharing pictures might do the trick. &mdash; Alberto Brandolini (@ziobrando) 15 mars 2019   A photo of this decision board can serve as a minimal log of what happened during a session.   Capture outputs through curation and views   While you are in an Event Storming session, the design board remains crystal clear. Unfortunately, if you get back to it after a few weeks, you’ll have a hard time making any sense out of it. That’s why keeping the full board, detailed photos of it, or a digital re-transcription of does not work very well.   There is something we can do though. We can capture filtered and single focus views, making the information digestible. A context map for example, is easy to draw after an Event Storming session. Next week’s post will go into more details about different views.      💡 Capturing filtered and single focus views of the Event Storming board, makes the information more digestible and persistent.       Share all this with a Safari   If we need to share the outcome of an Event Storming session, we’ll be better equipped with these views. We can invite people to a Safari following these steps:      With the help of one or more attendees, go through the full narrative   Then go over and explain all the views   This is good way to share as much information as possible in the shortest time possible!   To be continued   This was the first half in a mini-series about capturing the outputs of an Event Storming workshop. Next week’s post will dive in more details in the different kind of ‘views’ we can capture out of Event Storming.   Continue reading…  ","categories": ["event storming","collaborative work","ddd","event-storming-outputs-series"],
        "tags": [],
        "url": "/how-to-capture-the-outputs-of-an-event-storming-workshop/",
        "teaser": "/imgs/2019-04-18-how-to-capture-the-outputs-of-an-event-storming-workshop/share-understanding-so-what-teaser.jpeg"
      },{
        "title": "5 Views to Capture the Outputs of an Event Storming workshop",
        "excerpt":"Recording the full Event Storming board is a waste of time. Here are examples of quick, focused, and digestible views that capture the board’s knowledge.      This is the second half of a mini-series about how to capture the outputs of an Event Storming workshop. If you did not yet, I recommend you to read the first half first.   The previous post suggested capturing single focus views of an Event Storming learnings. Here are some examples of such views.      💡 Do pattern matching on Event Storming board to ‘grep’ out focused views to take out of a session.    Capture Next Actions   During the workshop, we use pink post-its to capture subjects we cannot solve on the spot. These might be questions, quick fixes or problems.      Before closing the session, it’s a good idea to go through all these pink post-its and identify the next steps. These might be homework to do before the next session, tasks to add to the team’s backlog or refactorings to do.   Capture Domain Definitions   Open discussions between technical and business experts will clarify a lot of terms. This is the essence of Domain Driven Design. The practice is to write these down on definition post-its: usually large and yellow.      It’s a good idea to capture these definitions somewhere. One way is to stick them on the team’s workspace walls. Another is to add them in the code with Living Documentation. How to do it is up to the team!   Capture the Context Map      💡 Event Storming is a very effective way to build a DDD context map.    I’ve mentioned it before. During big picture event storming, we can identify the DDD bounded contexts. The context map is already on the Event Storming design board. It’s only lost within all the other post-its!      It’s very fast to sketch a context map on a piece of paper or a flip chart. It will also be a lot clearer to any later reader.      Capture user stories              The picture that explains everything by Alberto Brandolini from his Introducing Event Storming book        During design-level event storming, we will start to identify Aggregates and business rules. They always appear inside this post-it pattern:      We have discussed the context of the business rule during the session and add a few comments. This is a good candidate for a user story. “When user X does Y, he wants to see Z, in order to …”. As I wrote about before, it’s also the perfect situation to do a bit of Example Mapping.   Capture Input &amp; Output Messages   At some point, most systems need to interact with external systems to be of any use. At the end of the Event Storming session, External Systems should be visible as blue post-its.   Again the Picture that Explains Everything shows us that we can spot interactions in 3 ways:      Input messages:         Output messages triggered by an actor:         Output messages triggered by an event:      We can take some time to go through the board and spot these patterns. Every match is an input or output message for the system. We get even more benefits if we are zooming on a single bounded context in a Design-Level Event Storming. In this case, we should have modeled other contexts as external systems. We can also use any Read Model information to detail the content of messages.   We can then record this ‘API’ view of the board on a flip-chart. This can be very useful, especially if we are building a service oriented system.   Keep Maintenance in mind   Whatever the output, we should use cheap ways to record information. Photos, information walls or sketches fit the bill. We could also decide to invest in always up-to-date living documentation.      More expensive solutions will almost always be a bad use of the team’s time. Things will change, it’s a cheaper to build the system and keep the knowledge live in everyone’s heads. Finding out what not to do is key in maintaining a sustainable pace.   You have the answer!   The exploratory and intangible nature of Event Storming makes people uneasy. As a result, you might have faced puzzled faces when you first suggested to run an Event Storming.   Maybe you are only planning to run your first Event Storming. Whatever your situation, you now have concrete answers to convince action oriented people.   Give it a try! If you have other questions, check my full Event Storming walkthrough.  ","categories": ["event storming","collaborative work","ddd","documentation","example mapping","event-storming-outputs-series"],
        "tags": [],
        "url": "/5-views-to-capture-the-outputs-of-an-event-storming-workshop/",
        "teaser": "/imgs/2019-04-25-5-views-to-capture-the-outputs-of-an-event-storming-workshop/recording-event-storming-teaser.jpeg"
      },{
        "title": "The Similarities between Machine Learning and DDD",
        "excerpt":"We discovered that DDD concepts like Event Storming, Bounded Contexts, Ubiquitous Language, Entities and Values are useful in machine learning projects.      Story of Ismail’s internship   A few months ago, had the chance to welcome Ismail  as a Machine Learning intern. He worked on finding end-to-end tests that are most likely to fail given a commit. (The results were encouraging, you can read the full story in Why Machine Learning in Software Engineering)   Even if I had already played a bit with machine learning, I’m still a newbie on the matter. As I watched Ismail work, connections between DDD (Domain Driven Design) and machine learning became obvious.   Flow of Machine Learning   Before I dig into the similarities with DDD, let me illustrate the flow of machine learning.      Similarities with DDD   From the schema above, we can see that once we have decided on our goal, the next step is to understand the domain. But any further improvement also starts by digging deeper into the domain… Exactly what DDD advocates to build software systems.      💡 Like DDD, Machine Learning relies on understanding the domain.    Here are selection fo DDD concepts that should be useful to machine learning.   Ubiquitous Language   The ubiquitous language is a dictionary of all the key concepts in the domain. It should improve communication between the data scientists and the domain experts. It should become easier for the data scientist to understand what the domain expert says. In return, it should be easier for the later to understand and give feedback about the models.   Bounded Contexts   Large domains are too big to be manageable. It’s a good practice to identify sub-domains that we call bounded contexts.   DDD taught us that factorizing code between bounded context can be dangerous. Here is an example. An order in the shipping context will not share much with the same order in the shopping cart context. Handling both of these in the same code creates maintenance problems.      Data Science involves some code too, especially some data extraction code. Following the DDD principles, we should not try to factorize data extraction code that target different contexts… even if they start from the same source!   The main benefit is to make the extraction code easier to evolve and refactor.   Entities and Values   A key phase of machine learning is feature engineering. This is modeling the data on which the model will run.   DDD suggests modeling data within a bounded context using Entities and Values that map the underlying sub-domain. The code gains independence from any other concern, in particular technical infrastructure. As a result, it should be more stable and easier to evolve over time.   Suppose we model features like we would model Entities and Values. We can expect features to be more stable and easier to evolve over time as well. In particular, it should be easier to completely substitute the data source for a new one.   Start with Event Storming!              Photo from Mathias Verraes’s blog        Next time I start something similar, I’ll start by an Event Storming. It’s a collaborative and exploratory design workshop that emerged from the DDD community.      💡 I’ll start my next machine learning project by an Event Storming workshop.    It’s great at creating shared understanding on the domain in record time. This should get data scientists started in 1 or 2 days instead of weeks. A better start means a more sustainable pace throughout all the project.   With this shared understanding, Event Storming helps to build the Ubiquitous Language.   Finally, we can use Event Storming to identify bounded contexts. Within bounded contexts, it should be easy to model features as entities.   Running an Event Storming workshop is no rocket science. I wrote a series of blog post where I explain how to use Event Storming to identify the above artifacts.   Open question   In our case, we had to extract the data from systems that had not been built with DDD in mind. This resulted in very involved extraction code.   In contrast, the data stored by systems built with DDD in mind is more aligned with the domain. Would it be simpler to do machine learning on such systems?  ","categories": ["ddd","machine learning","event storming","data driven"],
        "tags": [],
        "url": "/the-similarities-between-machine-learning-and-ddd/",
        "teaser": "/imgs/2019-05-10-the-similarities-between-machine-learning-and-ddd/machine-learning-ddd-teaser.jpeg"
      },{
        "title": "How we scripted everyone to code review in the team",
        "excerpt":"Getting all the team to code review can be a real challenge. Here is the story of how a simple random review assigning tool nudged everyone to code review.      This is an old story, I wrote about it a long time ago, but I thought it deserved an overhaul.   It happened a few years ago, as I joined a team as an experienced developer. The team was mostly junior except one developer. They were trying to write tests and to do code reviews. Unfortunately, juniors did not feel they had the right to review other team members code. As a result, the only senior dev was crawling under code review, and had become a kind of ‘clean code enforcer’.   It was not sustainable for him. He could not review all the code, and quality of reviews was suffering. As a consequence, the expected learning was not happening through reviews.   The project was ambitious, and we needed all the team to skill up.   Championing code reviews   I remember reading Karl Fogel’s, Producing Free Open Source Software at the time. He describes public reviews as:      There should be at least one review for every commit   Anyone can review anyone’s code   That looked like a way to encourage sharing and learning.   To walk my talk, I started to code review myself every day. I would dedicate around 1 hour to code reviews every afternoon before I left.      To make juniors code review code as well, I started to ask them for to pair-review my code. That made them uneasy at first, but they soon saw the value of code reviews.   This was working better, but we were spending a lot of energy in the process. Some code was also still not reviewed. We raised the point in retrospectives.   A simple script      The company was using an online code review tool called Code Collaborator. The process was ok, but still not great. One Friday afternoon, I wrote a quick script to randomly pick a reviewer for every new commit.   (As it uses outdated tools, the script itself does not have much value today. Ask me if you want it though).   I demoed the tool to the team. As we had raised the issue in retrospective and the team agreed to give it a try.      💡 Maybe there’s a simple script you could write to smooth out the code review process to make it catch on?    The result   The tool caught on almost immediately. All the team was reviewing code, and all the code was reviewed. As a consequence, more interaction, more learning and more continuous refactoring was happening. We were writing more quality code, we had less bugs, less rework and the pace got more sustainable!   Second order effect   The improvement mindset is at the core of code review. Good reviews should follow the motto: “That’s great, and how can we make this even better?”   The team applied this improvement mindset to code reviews themselves. That’s what people said in the next retro. “Reviews are great, but they are stealing time away from programming. Could we make this even better?”   I had been suggesting pair programming for a few retros already, and they decided to give it a try… and it sticked! (If you want to read the end of the story, it’s here.)      💡 Losing time in code reviews made the team switch to pair programming.    I couldn’t have expected that a simple script would nudge the team into pair programming. But that’s how complex systems behave, don’t they?      DIY   If you are in the situation where only a few people review code, give this a try. The script only took me about 2 hours to write. Tooling is different now, so it might be even shorter today.   Why not be even bolder though and skip a few steps? Start every day with mob-code-reviews as Carlos Matias suggests!  ","categories": ["code reviews","change management","continuous improvement","team building","pair programming","mob programming"],
        "tags": [],
        "url": "/how-we-scripted-everyone-to-code-review-in-the-team/",
        "teaser": "/imgs/2019-05-17-how-we-scripted-everyone-to-code-review-in-the-team/code-reviews-for-all-teaser.jpeg"
      },{
        "title": "7 Remote pair programming best practices Q&A - Part 1",
        "excerpt":"Remote pair programming brings surprising benefits to a team! Daunting at first, it is quite manageable with the good practices. Here are the first 2 of 7!      A few years ago, Ahmad and I worked together in a distributed team. Ahmad and Mohammad were working from Beirut while the rest of the team was in Paris. As you might know, this setting is a recipe for a split brain team. Practicing almost full time pair programming was the thing that kept the team spirit.   For the better and the worse, remote work is here to stay. Remote pair programming offers other advantages on top of improved team spirit. It enables people to work from wherever they fancy. Finally the introvert in me found it more sustainable than in person pairing.   A few weeks ago, Ahmad wrote a guest post about pair programming best practices. Let’s build on this to address remote pair programming.      💡 Remote or not, pair programming remains pair programming!    Most pair programming best practices also apply to remote pairing. This is not enough though, remote pair programming has its own challenges. Here are questions we often get from teams envisioning remote pair programming.   Lag is preventing us from remote controlling the screen, what can we do?   We were working between Paris and Beirut, where local connection was not always very fast. Slow typing, letters appearing in different orders, shortcuts that don’t work, different keyboard layouts… We know the problem!   To stick to local code editing, we would send the code over when we switched between driver and navigator. As a result, we would not switch as often as when doing local pair programming. Though not ideal, this was the best workaround we found.   Pomodoro technique              By Tom Woodward, on Flickr, under CC BY-SA 2.0 license        To make sure we shared the keyboard between parties, we used the Pomodoro technique. Every 25 minutes, we would take a 5 minutes break, and send the code over.   If code was in a good enough state, then committing was our preferred way to pass over the code. Otherwise, we would send a patch or pull from each other’s local git repo. It might be possible to script this and make the switch a breeze.   An extra second order benefit of this technique is that it pushed us to do small commits, which is already by itself a good thing.   Screen sharing      Seeing your buddy’s screen is important to understand what’s going on. Dual screen was key to make this work: one screen for the work, one screen for your buddy’s face.   Some screen shares, like Slack and Microsoft Teams have multi cursors. This is a killer feature to highlight code. At first, we very often found ourselves pointing to a line on the screen with our finger. This is one of the most frustrating thing to do when remote pair programming. Before multi cursors, we would end up saying things like “at line 64…”. This was a pain for everyone.   Other tools     Some tools promise an improved pair programming experience: Floobits, Saros for Eclipse, Live Share for Visual Studio or the new ones like Use Together. They let you concurrently edit code, which fixes the lag problem.   Unfortunately for us, we did not manage to get any of those to work:      Saros was not a big enough reason for us to leave Intellij for Eclipse   We could not use Floobits for company policy reasons   As Java programmers, Visual Studio was not an option   Use Together did not exist at the time   That said, these tools only synchronize editor windows of the IDEs. They don’t save you from sharing your screen, to see the test execution for example.   Other, like Ham Vocke, have reported success with TMux. If your team is not using an IDE, it’s a great way to work collaboratively on the same terminal.      NB: I’ve since then written a more detailed post going over the tools for remote pair programming. If you want more, it’s here: Best open source tools for remote pair programming    Pairing is tiring, what about remote pairing?      When I first started intense pairing, I felt a wreck at the end of the week. When we switched to remote pairing… it got worse! The extra effort to accommodate for video and headphone adds up to the mental load.   If done without care, it can ruin the team out. It’s important to stick to a sustainable pace with pairing too.   Here are my advices:      Invest in the best material you can. A comfortable headphone will make a tremendous difference at the end of the day.   Accept that you cannot be productive for 8 hours of pairing a day. 6 hours of local pairing is plenty, 5 of remote is as well.   Don’t be afraid to take breaks as often as needed.   It’s important to leave some time out for solo work to let introverts re-charge batteries. A time difference will actually help you. Why not use these early morning or late afternoon hours for deliberate practice? In the long run, it should make the whole team more productive (see Nobody ever gets credits for fixing problems that never happened)      💡 When remote pair programming, use time difference at your advantage to deliberate practice.    To be continued   This was the first part of a mini series about Remote pair programming best practices. In the next post, we’ll go over 5 more tips.   Continue Reading…  ","categories": ["pair programming","remote","remote-pair-programming-best-practices-series"],
        "tags": [],
        "url": "/7-remote-pair-programming-best-practices-q-and-a-part-1/",
        "teaser": "/imgs/2019-06-03-7-remote-pair-programming-best-practices-q-and-a-part-1/remote-pairing-teaser.jpeg"
      },{
        "title": "7 Remote pair programming best practices Q&A - Part 2",
        "excerpt":"5 more best practices about remote pair programming. Let’s deal with a painful headphone, a todo list, time difference, eye contact and continuous improvement      This is the second post in a mini series about Remote pair programming best practices. In the previous post, I gave answers to 2 questions about remote pair programming:      How to deal with the connection lag?   How tiring is remote pair programming?   If that rings a bell, start with this first post and get the full story.   Here are 5 others best practices.   The headphone is hurting me!   I remember the headphone was hurting the top of my head. I had to hack it with a small cushion so that it would be more comfortable.      We were remote pair programming pioneers in the company. It was difficult to get budget for top notch headphones. Nonetheless, here again, my advice is to buy the best you can.   Another option is to use a loud speaker and the camera’s mic. This setup works great as long as you are the only one remote pairing in the office. If you are not, all conversations will blend together in an unintelligible mess!   We used to keep a todo list on paper, how can we do?      As Ahmad said in 10 Pair Programming Best Practices Q&amp;A, it’s a great practice for the navigator to maintain a TODO list of some sort. It reduces driver interruption while keeping note of improvements points. Usually, when the driver switches, the sheet switches hand too. This cannot be done when remote pairing…   Any online concurrent editor will do the trick here. We had success using Google Docs, but Office 365 or even a mind map would work.   As a side-note, this can be particularly useful for the Mikado Method. Regular readers will know about it. It’s an incremental refactoring technique that relies on a graph of small steps.   How can we deal with the time difference ?   We had 1 hour difference between Paris and Beirut. 1 hour of time difference is not a lot, but still needs to handled. Ahmad went over this topic in 10 Pair Programming Best Practices Q&amp;A.           Have an up-to-date shared document of all the story’s tasks. Any of the pair should be quickly updated on the story’s status by just having a quick look at the document.     Don’t leave un-committed code when you leave your desk! If you are using Git, create a branch for your un-committed code, if you are using perforce use the shelve feature.     Every morning, the pair share their calendars to be aware when they have separate meetings     Use the time difference to finish any paperwork you need to do alone.      I’d also add the following:      💡 A time difference while remote pair programming is a perfect opportunity for learning and deliberate practice.    I’ve already done some remote pair programming with people in India. We had 4 to 5 hours of time difference. We could only pair for 2 to 3 hours every day. Nonetheless, it was very useful.   Any other trick?   Eye contact   Facial expressions are super important when discussing code or design. Communication is a lot more fluid when we manage to keep eye contact as we work.   Unfortunately, we never managed to completely fix this issue. Ideally, when you look both at the code or at your buddy, he should feel you are watching him in the eyes… More easily said than done.   Here is a setup suggested by Justin Gordon on his blog:              By Justin Gordon from his blog post Remote Pair Programming Tips Using RubyMine and Screenhero        Technology might improve this in the future:      Cameras behind and at the center of our screens   Augmented reality glasses   Until then, this looks like the best setup I found.   Continuous Improvement   We did not get there from day 1, it took us many retrospective iterations.      💡 Our Do It Yourself culture helped a lot to get remote pair programming working.    Be ready to discuss and experiment with your remote pairing setup. At some point you should find something that works well for you.   You don’t have to wait for the sprint retrospective to improve. I recently attended Woody Zuill’s (great) workshop on mob programming. He used to do daily mini-retro at the end of every day.      If it hurts, do it more often. [eXtreme Programming moto]    Try it yourself!   Remote pairing might seem daunting at first. I hope our experience will help you to get started better than we did. Give it a few tries and see how it works for you. Who knows, you might decide to make it part of your daily practices!   We’d love to hear your own pair remote pair programming best practices. Comments are more than welcome!  ","categories": ["pair programming","remote","remote-pair-programming-best-practices-series"],
        "tags": [],
        "url": "/7-remote-pair-programming-best-practices-q-and-a-part-2/",
        "teaser": "/imgs/2019-06-06-7-remote-pair-programming-best-practices-q-and-a-part-2/remote-pair-programming-yagourt-phones-teaser.jpeg"
      },{
        "title": "How to use Mob Programming at the rescue of Pair Programming burnout",
        "excerpt":"Full-time pair programming burns introverts out. Regular mob programming sessions yield the same benefits but are more sustainable.      As many, I got into pair programming through eXtreme Programming. I started pairing at my first XP project, almost 15 years ago, in a large bank. Since then, I’ve tried my best to find XP projects to work on. I had to take a few ‘traditional’ programming jobs though. As a consequence, I’ve alternated periods of intense pairing and solo programming. The longest streak of full-time pair programming I have done was around 3 years.   What’s great about pairing      Given enough eyeballs, all bugs are shallow. Linus Law    With this experience under my belt, the advantages of pair programming are obvious to me.      2 pairs or eyes catch more bugs than 1. Pairing improves quality and reduces bugs.   Better skills. It’s natural to learn from each other when pairing. It just happens. The more senior or expert developers might have to slow down and get used to this new ‘mentor’ attitude. But it’s a habit worth taking.   Better team work. People skills will improve while you pair. You will also discuss coding conventions. It’s going to be easy to raise a point to the team for a pair than for a solo programmer. In the end, everyone starts working more like a team. Today, I cannot feel part of a team that does not pair program!   Smaller code base. In the end, all this knowledge exchange increases code re-use. When pairing, we’re cutting the risk of re-writing something because you did not know it existed by 2.   All in all, it’s just more productive! I know some studies say that it’s 15% slower. In the long run, though, all these improvements compound, and more than make up for these 15%. I’m not the only one to say so.   You might think that pair programming is ponies and unicorns…   The problem with pairing      💡 Full-time pair programming burns introverts out!    Given the choice, I’d always go for the pair programming job. It’s so much more productive that it annoys me to work any other way. I’m an introvert though, and after a few years, it wears me out. This might explain why I’ve been in and out of pair programming for 15 years!   I’m not alone to think this way. Erik Dietrich comes to the same conclusion in Should You Take a 100% Pair Programming Job?   What should we do then? How good is a practice if it’s not sustainable for half the people? There’s an agile soundbite that says “If it hurts, do it more often”.   More than full-time pair programming? The only thing I can think of is mob programming!   Mob programming in the morning      All the brilliant people working on the same thing, at the same time, in the same space, and on the same computer. Woody Zuill, mobprogramming.org    I’ve been working as a technical coach for a few years now. I started mobbing as a way to coach all the team at the same time. It’s a lot more time effective to coach through a mob than an individual pair.   I often have 3 to 4 mob sessions within a single week. Up to now, I never felt it was burning me out.   Other teams have also adopted this work schedule. For example, the cucumber pro team has been doing this for a few years now.              From Cucumber pro team Far from the Mobbing Crowd presentation at QCon London 2016, available on InfoQ        How is it more sustainable for introverts   Here are a few key points:      If you mob program for half day only, you get regular solo time to recharge batteries   There is less personal involvement in a mob than in a pair. When a pair disagrees, it can easily slip into a me versus you argument. This is lot less likely to happen in a mob. When a mob disagrees, it usually tries to identify tradeoffs or experiments.   Mob programming favors strong style pairing. It means that the driver only types what he understood from the rest of the mob. It makes the driver’s job the easiest! Unlike the driver in a pair, who can feel scrutinized by his buddy.   Finally, mobsters use a timer to switch the keyboard. It’s less likely for a control freak to keep the keyboard all the time.   What about effectiveness?   Even though we don’t all agree, we need a high dose of pairing to reap all the advantages. But even a small dose of mobbing brings huge advantages. I’m always astonished by the ton of knowledge sharing that goes through mob sessions. Even in teams where people have been working together for years! Samuel Fare explains it the science behind mob programming.   Suppose you are mob programming 10 hours per week. It’s quite realistic to find topics that will leverage on everyone being together.    Bonus: easier to sell to management   I haven’t seen many forbid morning mobbing:      They understand the value of knowledge sharing   They get that some complex tasks are better dealt with by all the team. For example: complex design or architecture, complex refactoring.   Need some social proof? Read Rebecca Campbell, a manager at New Relic explain what she loves in mob programming.   Remote mobbing   Even though we did remote pair programming, today, it remains very uncommon. Surprisingly, it does not seem so with remote mob programming! Many teams are embracing the practice:      The cucumber pro team again   There is a website dedicated to remote mob programming   I do it at Murex with teams I coach   I know a few teams enjoying remote mob programming   I’ve seen it myself, remote mob programming is a great way to keep team spirit within a distributed team.      💡 Remote mob programming works surprisingly well!    One more thing…      There is one topic where remote mob programming becomes magic!   It’s about inclusiveness and diversity!   People from anywhere can work together. People from different countries and cultures. Busy parents can work with startup-kids. People can setup their desk to what suits them best! The cucumber pro team (again) experienced and wrote about it themselves.   As remote work is the future, remote mob programming might become more and more relevant.   How to start?   Whatever experts say, nothing beats first-hand experience. Give it a try and you’ll know if it works for you! It’s also a lot easier to get people to try than to jump-in.   Here are a 4 steps to put regular mob sessions in place in your team:      Run regular team coding dojo sessions. Use Randori mode: all together on the same problem and computer. Stick to strong-style pairing.   Experiment a with few morning group reviews, like people at New Relic suggest   Run a few morning mob sessions   Retrospect and decide how you want to continue from there   These steps should get you through the technique, et let you make and informed decision.   However this turns out for you, I’d love to read your comments about it!  ","categories": ["pair programming","mob programming","remote","sustainable pace"],
        "tags": [],
        "url": "/how-to-use-mob-programming-at-the-rescue-of-pair-programming-burnout/",
        "teaser": "/imgs/2019-07-10-how-to-use-mob-programming-at-the-rescue-of-pair-programming-burnout/pairing-extrovert-introvert-teaser.jpeg"
      },{
        "title": "Is XP the middle child of Agile?",
        "excerpt":"In this podcast, you’ll learn about eXtreme Programming. Where did it come from? What are its differences between XP and Scrum or Kanban? How things like DDD build on it?      A few months ago, Rahul asked me if I would like to record a podcast for his Agile Atelier. Here it is! Aside differences between XP and Scrum or Kanban, it also addresses concepts like:      How to use Event Storming for complex decisions   How XP creates a sustainable pace   Why merciless refactoring is key to XP   Here it is:      Thanks a lot Rahul for your podcast. Listen to this podcast on Agile Atelier  ","categories": ["extreme programming","event storming","ddd","refactoring"],
        "tags": [],
        "url": "/is-xp-the-middle-child-of-agile/",
        "teaser": "/imgs/2019-07-10-is-xp-the-middle-child-of-agile/xp-middle-child-mug-teaser.jpeg"
      },{
        "title": "7 Books About Data Driven Software Engineering",
        "excerpt":"Software is mostly built according to experts opinions. These 7 data driven software engineering books show us another way to a productive and sustainable pace.      Here is a French saying “Le cordonnier est toujours le plus mal chaussé”. I found an English equivalent in “the shoemaker’s son always goes barefoot”. I believe this is nowhere more true than in the software industry!   “Software is eating the world!” Software is now able to do things that only humans used to be. Why the hell are we driving our projects as if we were a horde of amateur hitchhikers?   What is data driven software engineering?   Being data driven would allow us to answer questions such as :      How much is the feature we delivered last week contributing to the bottom line?   How much is the feature we are currently developing expected to contribute to the bottom line?   What are the estimated cost and value of increasing our test coverage by 1%?   What are the estimated interests and nominal amounts of our current technical debt?   What is the value of refactoring this file?   Which is the most valuable: improving our build system or building this new feature?   Most projects I’ve worked in have absolutely no clue about the answers to these questions. The decision is left to experts, to the one with most influence, or simply to the developer, who can do how he thinks is best…   Books   Hopefully, some people are thinking differently, they believe it is possible to quantify all this. They even explain how!   Running Lean: Iterate from Plan A to a Plan That Works by Ash Maurya      Details a very practical guide about the lean startup process. It’s a very good starting point to any kind of lean software development.   Kanban: Successful Evolutionary Change for Your Technology Business by David J. Anderson      This book explains with real world examples how to use Kanban board to control your work queues and improve your flow of work. A true classic for any lean product development.   No Estimates: How to measure project progress without estimating by Vasco Duarte      It’s all too easy to get caught up in numbers. It might sound counterintuitive, but numbers are not the main benefit of data driven software engineering. Its real value is in rational and collective decision making. (Think Big O() analysis for an analogy) The figures don’t have to be 100% exact to be useful. This book presents ways to measure the flow without loosing time in estimating.   The Principles of Product Development Flow: Second Generation Lean Product Development by Donald G. Reinertsen      This book is rather theoretical, but it links all subjects together: lean startup, risk management, Kanban, and economics. It’s a must-read on the subject.   How to Measure Anything: Finding the Value of Intangibles in Business by Douglas W. Hubbard      The flow book gives a big picture view of what we want to achieve. This book explains how to actually measure all the aspects of your product development in dollar value.   Competitive engineering by Tom Gilb      I won’t say this one is an easy read. It is really theoretical, and the format is not the most reader friendly. That said, the more I was through the book, and the more it all made sense. It presents formal ways to use measures to define what needs to be done. Defining these measures early simplifies prioritization and improves quality.   Note: the pdf book is available for free by subscribing to the mailing list.   Waltzing With Bears: Managing Risk on Software Projects by Tom DeMarco and Timothy Lister      Disclaimer: this book is getting old, and is a bit outdated when compared to agile development practices.   That said, it provides real-world examples of how scientific Monte-Carlo simulations can be applied to software product development.   Bonus: Making the Business Case for Best Practices, PluralSight course by Erik Dietrich   This one is not a book but an online course. It explains how to estimate the value of new development practices. It contains a lot of examples that we can use directly, but it also teaches how to adapt to new practices. Finally, it gets in the details of how to get real figures! 5 hours definitely well spent on the topic.   My future reading-list   Data driven software engineering is a wide topic. I definitely still have a lot to learn about it. Here is a selection of the books from my reading list:      The Leprechauns of Software Engineering by Laurent Bossavit   Software by numbers, low-risk, high-return developments by Mark Denne and Jane Cleland-Huang   An opportunity   Digging into this topic was a real eye opener for me. The software development world is plagued with cargo cult and supposed best practices. We follow advices, but most often without verifying if they actually work! I believe we should start applying the techniques in these books. We could create standard ways to measure the values of productivity, technical debt, quality, testing…   I see real opportunities for data driven software engineering:      Avoiding a lot of useless argument between proponents of A and B   Communicating better with stakeholders   Finally, achieving a sustainable pace   Read these books and give it a try!   PS July 2019   A few months ago, I had the chance to welcome Ismail as a Machine Learning intern. He worked on finding end-to-end tests that are most likely to fail given a commit. The results were encouraging, you can read the full story in Why Machine Learning in Software Engineering  ","categories": ["lean","agile","kanban","data driven","business value"],
        "tags": [],
        "url": "/5-books-about-data-driven-lean-software-development/",
        "teaser": "/imgs/2016-04-29-5-books-about-data-driven-lean-software-development/7-data-driven-software-engineering-books-teaser.jpeg"
      },{
        "title": "5 XP practices that will make your remote team more effective",
        "excerpt":"By focusing on small increments and collaboration, XP practices like CI, Pair or Mob programming, TDD, continuous refactoring and TCR make remote work easier.      A while ago, Ahmad and I worked in a remote team for about 2 years. When Ahmad joined the team, we were already on our way to eXtreme Programming. XP by the book recommends onsite teams. With a new remote worker, it was tempting to simply abandon XP. Instead, we decided to push it further than 10, and see how it could work for a remote team.   We’ve already wrote and talked about the tools and processes we used to be an effective remote team. It turns out remote programming teamwork is also about the way we write code…   Continue reading…   A few weeks ago, I wrote a guest post for my friend Ahmad. Here was the beginning.  ","categories": ["pair programming","remote","extreme programming","mob programming","tdd","tcr"],
        "tags": [],
        "url": "/5-xp-practices-that-will-make-your-remote-team-more-effective/",
        "teaser": "/imgs/2019-06-28-5-xp-practices-that-will-make-your-remote-team-more-effective/5xp-practices-4-remote-coding-teaser.jpeg"
      },{
        "title": "Too busy to prioritize?",
        "excerpt":"This infographic presents how the lack of prioritization triggers a vicious circle of bloated organizations, late deliveries, and a non-sustainable pace              By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        Takeaway      Don’t save on prioritization.    During this summer, I’ll post a few similar infographics. Next on is Scrum with component teams leads to Waterfall Agile!  ","categories": ["infographic","lean","business value"],
        "tags": [],
        "url": "/too-busy-to-prioritize/",
        "teaser": "/imgs/2019-07-19-too-busy-to-prioritize/lack-of-prioritization-teaser.jpeg"
      },{
        "title": "Scrum with component teams leads to Waterfall Agile!",
        "excerpt":"This infographic presents the perils of Scrum with component teams. It pushes organizations back in Waterfall Agile, late deliveries and non-sustainable work.              By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        Takeaway   A small ratio of component teams is not necessary bad. When features involve many component teams though, we’re in painful integration and Waterfall. If this is your case, you’d rather rethink your teams organization. Here are 2 strategies:      Big Picture Event Storming can help you to pick the right team organization   Continuous improvement and coaching can get your teams to reorganize step by step   During this summer, I’ll post a few similar infographics. Next one is How to play planning poker… Badass Mode!  ","categories": ["infographic","scrum","agile"],
        "tags": [],
        "url": "/scrum-with-component-teams-leads-to-waterfall-agile/",
        "teaser": "/imgs/2019-07-21-scrum-with-component-teams-leads-to-waterfall-agile/scrum-with-component-teams-teaser.jpeg"
      },{
        "title": "How to play planning poker... Badass Mode!",
        "excerpt":"Planning poker and story points often turn into a power struggle where everyone loses. Here are 2 winning strategies for how to play planning poker as dev teams              By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        The large scale trap!   We must be extra careful with any flavor of large-scale agile. It’s too tempting to compare or even display velocity figures of different teams. It can even create an artificial sense of competition that will make things even worse.   Velocity was invented for team internal planning, not to commit.      💡 If we need predictions, #NoEstimates and Lean management tools will work better.    Don’t forget who has the story point printing press!      💡 Keep story points inside the team, and you’ll have a lot more control over technical debt and refactoring    As developers, we are the only ones able to keep the code base in a sustainable state. Unless our company is on the brink of bankrupt, we should think for the long term! Whenever our estimates gets challenged, we have 2 strategies:   Say NO!   As a team (or as a team of teams) we should meet before the encounter and agree that we will refuse to reduce our estimates. When everyone refuses, business people will have no choice but to accept it.   Say YES!   The idea here is to reduce the estimates, but fail until business people listen.      First, you’ll need to explain why you think these estimates won’t work. Using metaphors and references will help here.   Second you’ll need to stick to clean code and refactoring. Even if this means that you will not meet the estimates, do it anyway.   Third, explain again to business people why estimates where not realistic. Compromising on clean code should not be an option.   Repeat…   During this summer, I’ll post a few similar infographics. Next one is Does Programming equal Refactoring?  ","categories": ["infographic","planning","badass-developer"],
        "tags": [],
        "url": "/how-to-play-planning-poker-badass-mode/",
        "teaser": "/imgs/2019-07-28-how-to-play-planning-poker-badass-mode/how-to-play-planning-poker-teaser.jpeg"
      },{
        "title": "Does Programming equal Refactoring?",
        "excerpt":"This infographics presents how TCR increases the share of refactoring in TDD. But Refactoring is already most of our daily work! We’d better learn it seriously.              By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        TCR stands for test &amp;&amp; commit || revert. It started as an experiment by Kent Beck. It’s a script that automatically commits or reverts whether the tests pass or fail.   TCR is the ‘natural’ evolution of TDD. Kent Beck has been arguing for a long time to      Make the change easy, then make the change    When you follow this mantra, even without TCR, most of your work becomes refactoring.   Why refactoring?   Programmers, especially juniors, often shun refactoring. We are afraid of Legacy code. We tend to look for greenfield projects, to avoid refactoring other people’s code.   Done well, I actually find Refactoring pretty rewarding. I know I’m making the lives of my teammates easier every time I improve a line of code. When all the team does it, it creates a sustainable pace, which dribbles down on all aspects of our lives.   Start now!   We can learn TDD refactoring techniques, like any skill. Reading a few books and starting a coding dojo will get you on good tracks.   During this summer, I’ve posted a few similar infographics. This was the last one, the next post is a longer read about maintaining a TO DO list or a Mind Map for programming.  ","categories": ["infographic","tdd","tcr","refactoring"],
        "tags": [],
        "url": "/does-programming-equal-refactoring/",
        "teaser": "/imgs/2019-08-02-does-programming-equal-refactoring/tdd-tcr-programming-equals-refactoring-teaser.jpeg"
      },{
        "title": "TO DO list or Mind Map for programming",
        "excerpt":"Maintaining a TO DO list or a Mind Map for programming task has many benefits. It improves collaboration, our decisions and our vision of what’s left to do.      Through my years of programming, I’ve been refining the notes I keep next to my keyboard. It went from nothing to an online mind map! Here are the steps I went through.   Nothing… and buggy code!   I always had my head in the clouds. At school, I did a lot of silly mistakes, but it ended up ok most of the time. When I started programming, I thought I could approach the job the same way.      💡 Don’t worry too much, it will work fine in the end…    That did not work! I soon got the feedback that my code contained a lot of bugs.   Hand notes for TDD   Knowing that I could not trust myself to be-more-careful, I looked for something else. That’s how I discovered Test Driven Development and eXtreme Programming.   TDD consists of 3 steps Fail the test, Pass the test and Refactor (aka Red, Green, Refactor). The catch is that we can always come up with more things to refactor than we have time. So, we often postpone refactoring, for good reasons:      “These 2 pieces of code look similar, but I am not sure they relate to the same business concept. Let’s wait until I catch Joe and ask him”   I could refactor this now, but it might be faster to wait until I have done X.   This is duplicated, but it’s a lot of work to factorize. More than I can afford given the customer constraints. We’d rather start a long term refactoring with the team   etc   That’s why I started to keep hand-written notes next to my keyboard. To be sure not to forget things before I closed my task. With time, I would add more reminders to these notes, like special test cases for example.      These little notes already brought quite some benefits:      A higher-level view of the full task.   They help to take a step back and make better decisions.   They help to skip parts of the work that are not necessary right now, which is key to a sustainable pace.      Simplicity–the art of maximizing the amount of work not done–is essential. The Agile Manifesto    A TODO-list for pair programming   A few years down the road, I had the chance to join an XP team. I discovered pair programming and the role of navigator. The hand-written notes proved even more useful there.   When navigating, we are at a unique place to think of the design, notice things to improve, test cases to add, etc. The navigator can spend his time updating the notes without interrupting the driver! That’s one of the main benefits of pairing.   The difficulty though, is that both buddies need to understand the notes. When we switch roles, the new navigator takes over the notes. This forced us to use a more structured format. That’s how we ended up with real TO DO list.      Plus, when starting to pair on a task, it’s useful to lay down the steps we’ll go through. Here again, the TO DO list is a perfect support to write this down.   To summarize, maintaining a TO DO list for programming boosts pair programming collaboration.   The story of my hand notes does not end there though…   An online doc for remote pairing   A while ago, I started to remote pair program with my friend Ahmad Atwi. Maintaining a TO DO list on paper did not work anymore. 😰   We switched to online collaborative editors like Google docs to keep this TO DO list up to date. This was an easy improvement that made remote pairing simpler for everyone!      As we improved the way we worked, we improved our TO DO list for programming as well!   A Mind Map for refactoring   As explained in 5 XP practices that will make your remote team more effective remote pairing is easier with small commits. We were trying to keep the tests passing all the time and were doing more and more baby-steps refactoring.      Make the change easy, then do the change. Kent Beck    To make this manageable, we started to split the TO DO list items in sub steps. Our TO DO list was looking more and more like a graph of baby steps changes. (Side Note: Graph of baby refactoring steps are also called Mikado Graphs)   At this stage, the TO DO list had become a full plan of the work to do. The root node represented the story or task itself. Its children were the main sub-tasks. Each sub-task could have sub-parts, etc. We did not plan everything in details from the start, but we updated it as we went.   The text todo list was showing its limits, that’s when we switched to a mind map for programming.      Works fine for mobbing as well   I now coach teams to mob programming. I also use mob programming to help them to refactor their own code.   We use a mind map for programming as a mob. Maintaining a mind map of the work to do ensures everyone understands the why, what and how of the work.   We’re not alone! Other teams, like the cucumber pro guys, are doing it as well:      We use the MindMup tool to draw and update a mind-map of our problem solving as we go. see    Check their post The Surprisingly Inclusive Benefits of Mob Programming for the full story.   Start yours today!      Whether you are solo, pair or mob programming, I encourage you to start your TO DO map now. It’s very easy to get started. Draw a mind map of your plan, and update it as you go. It’s a low effort, high return move.   You’ll be able to keep a high-level vision of the task at hand:      You will make better decisions   You will know what’s done and what’s left to do   You will be able to efficiently pair or mob program   If you are solo or colocated, a sheet of paper or a whiteboard will be enough. If you are remote, you’d better head to Google Docs, Coggle, or any online mind mapping tool.  ","categories": ["personal-productivity","mob programming","pair programming","remote","mikado-method"],
        "tags": [],
        "url": "/to-do-list-or-mind-map-for-programming/",
        "teaser": "/imgs/2019-08-16-to-do-list-or-mind-map-for-programming/todo-map-for-programming-teaser.jpeg"
      },{
        "title": "Why Machine Learning in Software Engineering #1: A World of Experts",
        "excerpt":"It’s the opinions of experts that drive Software Engineering! Models, like Big O, would lead to more fact-based discussions and better decisions.      This is the first part of a 3 posts story about how we tried to apply machine learning to software engineering. During a 6 months internship, we found a way to identify the most critical tests to run. We also confirmed that practices like small commits, modular code, and the open/closed principle apply to our codebase.   Fashion, fame and strong personalities drive the software engineering world! Think of .js frameworks, or of Uncle Bob’s Clean Code and Software Craftsmanship movements.   All these define the current best practices. Even though they are only the result of the experience of some people.      Our ‘best-practices’ are only the result of the experience of some people!    The same thing happens on a smaller scale. Many companies have a veteran-guru who defines the inside best practices!   The catch   Experts are humans too! Unfortunately, this also means that they have biases and make mistakes. Could we get out of experts’ rhetoric and in fact-based problem-solving?   Think of the following questions we get in many projects:      Our test-suite is too slow, which tests should we run first?   We have a lot of legacy code that is dragging us down, given the features ahead, what should we refactor?   We have too many bugs to fix, which ones should we focus on now?   The experts’ answers are of little help here 😞. Still, answering these questions could increase productivity and set a sustainable pace!   How a model would help   These challenges are contextual. There is not a single answer that is valid across all projects and teams.      In Thinking Fast and Slow, Daniel Kahneman explains that we, humans, are full of biases. He concludes that, in the face of high uncertainty, we should prefer algorithms for better decision making!   I had a demonstration about that a few weeks ago while I was coaching a team. We were doing the ‘Median of a list of list’ Randori kata to practice Test Driven Development with algorithms. The trick is that TDD won’t drive you towards a non-trivial algorithm (cf Ron Jefferies Sudoku). We can still leverage TDD with the following strategy:      Think an algorithm   Write tests to incrementally write to a slow version of this algorithm   Optimize this algorithm with the safety of the test harness   We had reached step 3 and the team was discussing which part to optimize. People could not agree, so we did a quick Big O() analysis. In no time, everyone agreed.      💡 Models are always wrong. But they get us out of rhetoric, gut feelings, biases and into fact-based discussions!    Data Science is all about building models.      As software engineers, we have a lot of data and we build data-science systems for others. It’s a pity we don’t use Data Science for ourselves!   To be continued   This was part 1 of a 3 posts story about how we tried to apply machine learning to software engineering.      A World of Experts   An Experiment   The Future   In the next post, I’ll go over how we applied machine learning to identify the tests that are most likely to fail.   Continue Reading…  ","categories": ["continuous improvement","machine learning","why-machine-learning-in-software-engineering-series"],
        "tags": [],
        "url": "/why-machine-learning-in-software-engineering-1-a-world-of-experts/",
        "teaser": "/imgs/2019-08-24-why-machine-learning-in-software-engineering-1-a-world-of-experts/a-world-of-experts-teaser.jpg"
      },{
        "title": "Why Machine Learning in Software Engineering #2: An Experiment",
        "excerpt":"A Machine Learning in Software Engineering intern identified critical tests to run. He also confirmed the open-closed principle, modular code, and small-commits!      This is part 2 of a 3 posts story about how we tried to apply machine learning to software engineering. In the previous post, I explained how data science could help us to make better decisions. If you haven’t, start reading from the beginning.   With all the technical coaching going on, I am already quite busy at work. I cannot dive into machine learning for software engineering myself. Fortunately, I found an intern to explore the topic.   Ismail’s internship   That’s how I had the chance to meet and welcome Ismail for 6 months. We had many topics to work on but here is the one we selected:   Given a commit and a list of slow-running end-to-end tests, which tests are most likely to fail?   Developers and testers used to pick the tests to run from experience. Automating this looked promising because:      Murex has already industrialized its QA, so we had log-data to rely on.   It could save developers from a lot of waiting-for-tests time.   The internship was not straightforward and Ismail had to go through many hoops. Still, it generated very interesting results.      Main outcome   Ismail succeeded in finding a subset of tests that are most likely to fail. The model is not accurate enough to avoid running the entire test harness though. 😞   Still, it is useful in different ways:      It can generate a fail-fast test-suite to run first. This can find errors earlier, or increase the confidence about a commit   It can also compute a quality-score on a commit in seconds, even before running any test   As of today, people in the QA department still need to see how to use this prototype.   Secondary findings   That was not all though. Ismail used the Gradient Boosting algorithm to train his models. Gradient Boosting is interpretable. A machine-learning algorithm is interpretable if it can explain its decisions. As Gradient Boosting relies on decision-trees, it’s easy to understand its conclusion.   That’s how we confirmed that software engineering best practices apply to our codebase!   Modular Code   The model relied on the position of files in the codebase. The model computes the average and standard-deviation of the positions of the committed files.   The decision-trees showed something surprising:      The average position only had a small impact on the test result!   Unlike the standard-deviation, which was everywhere in the decision-tree!              Graph of the main decision tree used in a test-result prediction model. The nodes checking the standard-deviation of the positions of the files in the commit are highlighted. We see that the first nodes in the tree, the almost half the nodes are about checking standard-deviation of the positions of the files. High Res image.        If a codebase is modular, it means that we can add new features with small and local changes. This will reduce a commit’s standard-deviation of positions. Which makes the commit more likely to pass the tests.   This confirmed that, in our codebase, modular code is easier to evolve.   Small commits   Ismail discovered that the number of files in a commit correlates with more test failures. In short, the smaller the commit, the safer.              Graph of the main decision tree used in a test-result prediction model. The node checking the number of files in the commit is highlighted. We see that this node is the most significative after the standard-deviation of the positions of the files. High Res image.        Nothing new here, but a statistical proof that this applies to a specific codebase has value. Next time a grumpy programmer tells us something like:      Small commits are fine in theory, but it does not work like that here!    We’ll have an extra argument in our pocket to get the point!   Open-Closed principle   Ismail also took into account the revisions of files in a commit. He found that the higher the standard-deviation of file revisions, the more tests fail.              Graph showing test results probability of a commit over number of files on the abscissa, and standard-deviation of revisions on the ordinate. Passing tests are in green, failing tests are in red; the darker the color, the more likely. We see that passed a particular standard-deviation, it is almost impossible for the commit to pass the test.        When code follows the open-closed principle, new features don’t impact the old code. This means that a commit’s standard-deviation of revisions will be small. Which, again, makes the commit more likely to pass the tests.   It’s also a case for splitting new features and refactoring in different commits:      Refactor the old code to make the change easy   Add the feature with new code only   Again this confirms a known best practice.   To be continued   This was part 2 of a 3 posts story about how we tried to apply machine learning to software engineering. In the next post, I’ll go over different opportunities for Machine Learning in Software Engineering.   Continue Reading…   PS   Ismail and I gave a talk at Agile France 2019 and a BBL talk at leboncoin.fr. The slides are online.  ","categories": ["testing","continuous improvement","machine learning","why-machine-learning-in-software-engineering-series"],
        "tags": [],
        "url": "/why-machine-learning-in-software-engineering-2-an-experiment/",
        "teaser": "/imgs/2019-08-24-why-machine-learning-in-software-engineering-2-an-experiment/machine-learning-teaser.jpg"
      },{
        "title": "Why Machine Learning in Software Engineering #3: The Future",
        "excerpt":"There are many opportunities for Machine Learning in Software Engineering: testing, product management, processes… It might even set a sustainable pace!      This is the last of a 3 posts story about how we tried to apply machine learning to software engineering. The previous post was about how a 6-month internship in Machine-Learning confirmed software-engineering best-practices. If you haven’t, start reading from the beginning.   This was only a 6 months internship. We only had a glimpse at machine learning and data science for software engineering.   Testing   Data science for software testing is becoming a hot topic. I’ve noticed an increasing number of talks about data science at testing conferences.      I’ve heard the most about fuzzing smart inputs and creating an AI for large test failure analysis.   The cucumber pro team also seems to be looking into the same problem as Ismail did.     Unit Testing   At the developer level, IDEs could embed an AI similar to Ismail’s. It could analyze every test run and identify the tests which are the most likely to fail. Running tests in a particular order could save a few seconds every time. This could particularly improve continuous testing tools like NCrunch or InfiniTest.      💡 A smart AI could record what’s done on the IDE to provide working improvement ideas.    Tools   A new breed of tools that leverage data science is appearing. Here are a few I’ve heard about:      Code Climate’s Velocity analyzes pull-requests to identify process bottlenecks   Code Scene analyzes git-history to pin-point high-ROI refactorings   LGTM analyzes quality and security violations to highlight the ones to fix now.      💡 LGTM team’s blog about data science and software engineering is pretty interesting!    Other usages   Could it help us to learn best practices in product management? It might also support continuous-improvement, similarly to what Velocity does by analyzing PRs. It seems a team at Content Bloom has been successfully applying a data-driven mindset to an Agile Framework.   Data Science will improve software engineering in ways we cannot even think of today. What could we do if we combined data from all sources: ticketing-systems, VCSs, IDEs, static analyzers, CI servers, production…?   Proving ‘slow’ practices   Finally, a lot of the industry’s ‘best practices’ are about taking the time to do good work… to go faster in the end. Unfortunately, this is a hard message to sell when we are under the stress of fire-fighting bugs.      In 21 Lessons for the 21st Century Yuval Noah Hariri says that we will resort to AIs to make better decisions for us. With more knowledge and fewer biases, these decisions should be smarter. Daniel Kahneman supports a similar idea in Thinking Fast and Slow.   This sounds frightening! But this could also achieve a sustainable-pace in a profession that too often hasn’t.   Give it a try!   Believe it or not, it is not as difficult as it seems to get started with Data Science and Machine Learning. Especially for software engineers who already know how to manipulate data. Improving our processes is the perfect occasion to get started! 20 hours is all you need.   Unfortunately, there is also another, darker, reason to get into the topic. As data science can increase productivity, it will become part of our daily work, sooner or later. We’d rather learn and own the topic before it becomes the Pointy-Haired Bosse’s Big Brother dream…  ","categories": ["machine learning","testing","continuous improvement","why-machine-learning-in-software-engineering-series"],
        "tags": [],
        "url": "/why-machine-learning-in-software-engineering-3-the-future/",
        "teaser": "/imgs/2019-08-24-why-machine-learning-in-software-engineering-3-the-future/dive-in-the-future-teaser.jpg"
      },{
        "title": "Let's give Remote-First Open-Space Technology Un-Conferences a try!",
        "excerpt":"Open-space technology un-conference is a powerful self-organized workshop. Here is the story of how we made it remote-first to grow a community of practice.      This post is the first half of a mini-series about remote-first open-space technology un-conferences.   Like most successful companies, Murex, the company I work at, has its share of legacy code. Legacy code is great because it’s bringing money in; it’s more problematic when we need to change it.   To help developers to do so, I spawned a Refactoring Community of Practice. As a coach, my goal is for this COP to survive me when I move to other activities. I want the COP to become self-organized and part of the culture.   Open Space Technology is an excellent un-conference format that fosters self-organization. People have organized un-conferences of hundreds of people with it. It’s even powerful enough to transform organizations!              Credit: NASA/GSFC/Debbie McCallum - https://www.flickr.com/photos/nasa_goddard/4465372408 - Under CC BY 2.0 - Visible on Open Space Technology Wikipedia page        The catch is that we have offices in Beirut, Dublin, and Paris. We tried satellite Open-Space Technology un-conferences in the Refactoring COP. Unfortunately, with mitigated results. Most developers are in Paris. It makes the workshop less interesting in Dublin and Beirut.      💡 The best way to go remote is to jump in completely and become remote-first.    Trying to make something remote by only adding a webcam seldom works. The best way to go remote is to jump in completely and become remote-first. I searched the web for remote-first open-space technology un-conferences. I found a few mentions that it should work, but not much:      In Building Open Space Online, Judy Rees explains that she managed to run a distributed un-conference using the open-space technology format.   This 2009 article Open Space-Online by John Folk-Williams explains how they used a tool called open-space-online to run a distributed text-based open-space technology un-conference.   Even though Open-Space-Online is closed until they add video chat to the system, these links were encouraging!   Let’s try it!   I like being at work in August. In France, almost everyone takes their yearly vacation in August. The office is calm, and it’s the best time of the year to concentrate and do great work. We took a bit of time to think about the topic. We were ready to try a remote-first Open-Space un-conference at the Refactoring COP.      💡 Remote work is here to stay! It’s time to be creative.    We relied on the company’s installed software: Microsoft Teams and Confluence Wiki. For every topic, we created a chat channel and a wiki page. Participants could move from one channel to another. Topic owners would take notes on the wiki.   It worked pretty well!   It went surprisingly smooth for the first time. People said      It was easy to switch from one channel to another.   They liked that they could ‘leave’ a discussion at any moment.   Remote-first brought its specific improvements:      People could share their screen, for example, to demo some code. Doing this is not always easy in a physical un-conference session.   It’s more inclusive of people working from Beirut and Dublin. They could enjoy all the discussions happening in Paris as if they were there.   By simplifying working from home, it contributes to a sustainable-pace and a sustainable living      Continue Reading   This post was the first half of a mini-series about remote-first open-space technology un-conferences.      Let’s give remote-first open-space technology un-conference a try!   How to run a Remote-First Open-Space Technology Un-Conference   The next post is a detailed step by step guide about how to run your own.   Continue Reading…  ","categories": ["selforganizing","remote","continuous improvement","collaborative work","coaching","remote-open-space-series"],
        "tags": [],
        "url": "/lets-give-remote-first-open-space-technology-un-conference-a-try/",
        "teaser": "/imgs/2019-09-10-lets-give-remote-first-open-space-technology-un-conference-a-try/remote-first-open-space-technology-un-conference-teaser.jpeg"
      },{
        "title": "How to run a Remote-First Open-Space Technology Un-Conference",
        "excerpt":"Open-space technology un-conference is a powerful self-organized workshop. Here is a guide to making it remote-first, and even more inclusive!              Based on original work from Agile Luxembourg Meetup        This post is the second half of a mini-series about remote-first open-space technology un-conferences. The previous post was our success story with it. If you haven’t, read it first!   What it looked like   It all started by sending a classic calendar invite. At the event date, people joined a video chat channel on our conferencing software. Being a remote-first event, everyone stayed at their desks.   After a quick introduction, I asked people to pitch topic ideas they’d like to discuss or work on. I played the moderator role. As people suggested topics, I created chat channels and corresponding wiki pages.      Once we had enough topics, people moved from ‘General’ to the topic channel of their choice. The topic owners took notes of what was going on their dedicated wiki page. As in the classic open-space un-conference, people could move around between discussions.   After a while, I asked everyone to come back to the General channel. We ended sharing a few takeaways from each discussion.   Principles              By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        First, even though it is remote-first, it is an open-space technology un-conference! It follows the same principles as the co-located variant.      4 principles          Whoever comes is the right people…     Whenever it starts is the right time…     Whatever happens is the only thing that could have…     When it’s over, it’s over…       The law of 2 feet     If at any time during our time together you find yourself in any situation where you are neither learning nor contributing, use your two feet, go someplace else     Source: Wikipedia    Check https://openspaceworld.org for a ton of resources on the topic.     Second, it’s remote-first! So everyone must attend from their device. No gathering in rooms! A half-remote setup always ends up in an asymmetry of information and hinders self-organization.   Tools   Our setup consisted of:      A webcam and headset for every participant   A video conferencing system. We use Microsoft teams in the company, but I guess Slack or Zoom who be perfect as well.   A wiki to gather notes   Roles   There were only 3 roles.      Participants join channels and take part in discussions.   Topic owners volunteer by pitching a topic. They are responsible for animating the discussion on this topic.   The moderator animates the general discussion. He also keeps track of time and creates the channels and wiki pages.   I thought there would be a lot of moderation work, but it ended up being pretty light!   Special channels   Before the event started, I had created a dedicated team in Microsoft Teams. I had sent a link to join the General channel with the invite. I had also prepared other channels:      A ‘no-topic’ channel, where people can hang around when no topic suits them   A ‘break’ channel. Large open-space un-conferences often have many rounds of discussions. For example, 40 minutes of discussion, 10 minutes of break before another 40 minutes of discussion. The break channel is where people could meet during breaks.   An ‘orga’ channel. For messages related to the organization. It ended being useless for my small open-space un-conference. It might be more useful for more significant events.   Formalized summary   One beautiful thing with the co-located open-space is that people can walk around during break time. It’s an occasion to see what people have noted on walls or whiteboards.   We can replicate this in the remote-first version. We could ask topic owners to share their takeaways in the General channel at the end. There is a risk that it takes too long and bores people out though. Here is what we ended up doing instead.   We prepare the wiki page from a template table looking like this.      The moderator could then go over the boards and quickly read through. Other attendees could also visit wiki pages to see the main takeaways. As in a real open-space, they could use the ‘break’ channel to start collaboration.   Give it a try   If your problem sounds like one of these:      You want to organize a remote un-conference, but don’t know how to do   You already use open-space technology, and your organization is moving to remote work.   You want to use open-space technology to transform your remote organization.   Like us, you are struggling to find the right format to make a remote meeting self-organized.   Try the remote-first open-space technology un-conference. Making it work was easy!    When you try, please leave a comment, I’d be very interested to know how it worked for you.  ","categories": ["selforganizing","remote","continuous improvement","collaborative work","coaching","remote-open-space-series"],
        "tags": [],
        "url": "/how-to-run-a-remote-first-open-space-technology-un-conference/",
        "teaser": "/imgs/2019-09-11-how-to-run-a-remote-first-open-space-technology-un-conference/remote-first-open-space-technology-un-conference-teaser.jpeg"
      },{
        "title": "Why should we use Design Level Event Storming for DDD?",
        "excerpt":"Design Level Event Storming is a collaborative design workshop. Use it to build and share the design of critical parts of systems using Event Sourcing and DDD.   ℹ️ NOTE: An updated version of this post has been published on the Event Storming Journal blog      Imagine you are starting a new product with your team. You had been struggling for a while about how to start.      What should the high-level design be?   What should we focus on first?   What are the main risks?   All these questions remained unanswered until you heard about Event Storming!   You decided to try Big Picture Event Storming. In a single (and intense) day, you drafted a functional architecture vision! That’s more progress than you had made in weeks. You managed to identify the primary contexts in your domain. You highlighted the topics you’ll focus on to create your competitive advantage. These are your core sub-domains.   Everyone in the team now understands where to focus. Developers also know what sub-parts of the system should remain independent. It looks like developers are ready to start coding!      Wait! Event Storming has more to offer!   Let’s dive into a core context with Design-Level Event Storming.       💡 Big Picture Event Storming was about exploring strategic DDD; Design-Level Event Storming is about tactical DDD inside a domain.    The goal   The Design Level flavor of Event Storming lets you dive into the details of a bounded context. Its primary outcome is a good enough design vision. Developers who attend the workshop should be able to start coding straight away.      💡 Developers who attend the workshop should be able to start coding right after a Design Level Event Storming.    Design Level Event Storming is particularly suited to design DDD event-Sourced systems. In any case, it can be useful even if you are not building an event-based system. It helps in identifying aggregates and other crucial domain concepts.              The picture that explains everything by Alberto Brandolini from his Introducing Event Storming book        Similarly to Big Picture Event Storming, the Design Level flavor is a time compressor! It shrinks weeks of up-front or emerging design into a few hours. It is intense and high bandwidth collaboration that makes this possible. It’s the perfect balance between Big Up-Front Design and Emerging Design. With a better start, it sets a sustainable pace for the team from the beginning!   Like Big Picture Event Storming, again, it helps us to spot critical topics. For example, the most pressing problems and the primary domain concept definitions.   Continues   This post was the first in a series about how to build event-based systems with Design Level Event Storming and DDD:      Why you should use Design Level Event Storming for DDD   Detailed agenda for a DDD Design Level Event Storming #1   Detailed agenda for a DDD Design Level Event Storming #2   7 tactics that will make your DDD Design Level Event Storming payoff   The next post will be a step by step guide to run a Design Level Event Storming.   Continue Reading…  ","categories": ["event storming","architecture","ddd","how-to-run-a-design-level-event-storming-series"],
        "tags": [],
        "url": "/why-should-we-use-design-level-event-storming-for-ddd/",
        "teaser": "/imgs/2019-10-06-why-should-we-use-design-level-event-storming-for-ddd/design-level-event-storming-zoom-teaser.jpeg"
      },{
        "title": "Detailed agenda for a DDD Design-Level Event Storming - part 1",
        "excerpt":"Running a DDD Design-Level Event Storming is not rocket science! It’s still crucial to start the right way: with domain events and the Picture that explains Everything!   ℹ️ NOTE: Part of this post has been republished and updated on the Event Storming Journal blog      This post is the second in a series about how to build event-based systems with Design-Level Event Storming and DDD. If you haven’t yet, start by the beginning.   Pre-requisite   Are you eager to start? Not so fast! There are a few things to take care of first.   Design-Level Event Storming is about focused design on a subpart of the system. This not possible without enough shared context among participants.   That’s why it’s very natural to do a Design-Level Event Storming after a Big Picture Event Storming. It’s a way to dive into the core subdomains that are complex enough to deserve Domain-Driven Design.   Design-Level Event Storming can also be a recurring workshop on a jelled product team. Such teams share enough context to be able to dive into the design very fast. Whenever the team members feel that their design vision is out of date, they can run one. With a bit of luck, they might find their next refactoring breakthrough!   Design-Level Event Storming is also a good fit for a team that is already building a DDD and event-based system. It’s easy to map their current events on the design board and improve from there.   A guided agenda      By its nature, Design-Level Event Storming is a lot less chaotic than its Big Picture cousin. Big Picture Event Storming is about exploring. Design-Level Event Storming is about designing and building.   As a consequence, some steps in the agenda are pretty straightforward, almost mechanic.              I’ll illustrate the agenda with elements from my training on Event Storming facilitation. Let’s imagine we are running a Design-Level Event Storming on the ‘live game’ subdomain.        1. Domain Events   Before you run the Design-Level Event Storming, you’ll need domain events!   You might be focusing on a core bounded context after a Big Picture Event Storming. In this case, copy these events to another blank design board.      You’ll have to find another way if you are starting from scratch. You can run the first steps of the Big Picture Event Storming on this bounded context alone. I would do at least:      Events Generation   Events Sorting   Actors and External Systems   Storytelling   You might check my post A detailed agenda of Event Storming to learn how to run these steps in detail.   2. The Picture that explains everything              The picture that explains everything by Alberto Brandolini from his Introducing Event Storming book        Now is the moment to show and explain this picture. Display the picture on the wall, and ask people what they understand about it. Usually, the audience gets it pretty well. You can then fill in the missing part.   Here is what I would say about it:      Alberto Brandolini calls this “The picture that explains everything”. It shows how DDD event-based systems work.     The goal of this Event Storming is to design our system with this pattern.     This pattern explains how Domain Events cascade during the life of the system.     From the left, we see that we can send commands either to external systems or to our aggregates. Either of these then raises a domain Event. Aggregates are ours to code; it’s where the business logic of our system is.     Some events will “automatically” trigger another command. We materialize this link through policies ‘Whenever event then raises command”. We use lilac post-its for policies, like the one at the bottom right.     Other events notify users. Users should be able to react to events by sending new commands. To be able to do so, they’ll need to see the correct information. That’s the green read-model post-it. We’ll also have to display this information in a good UX. We can mock-up this UX in the white.     By the end of the workshop, the board should contain patterns like that.     From now on, we should make everything explicit. Everything we say should appear on the board.    Continues   This post was the second in a series about how to build event-based systems with Design-Level Event Storming and DDD. The next post will detail the remaining steps of the Design-Level Event Storming.   Continue Reading…  ","categories": ["event storming","architecture","ddd","how-to-run-a-design-level-event-storming-series"],
        "tags": [],
        "url": "/detailed-agenda-for-a-ddd-design-level-event-storming-part-1/",
        "teaser": "/imgs/2019-10-11-detailed-agenda-for-a-ddd-design-level-event-storming-part-1/design-level-event-storming-good-start-teaser.jpeg"
      },{
        "title": "Detailed agenda for a DDD Design-Level Event Storming - part 2",
        "excerpt":"The most valuable activities in a DDD Design-Level Event Storming are UX and Aggregates explorations. Here is a step by step agenda that will lead us there.   ℹ️ NOTE: Part of this post has been republished and updated on the Event Storming Journal blog      This post is the third in a series about how to build event-based systems with Design-Level Event Storming and DDD. If you haven’t yet, start by the beginning.   3. Commands   The first step to making the pattern emerge is to prefix every domain event with a command. This step is pretty mechanic and straightforward. If you have an event called “Trade booked”, prefix it with a command “Book Trade”. Sometimes, the command names are a bit different, but you should manage to figure this out.      4. Actors or policies   Commands can either be sent by a human (an actor) or automatically by a policy.   Go through all the commands and prefix them with an actor or a policy. Having done a Big Picture Event Storming before might help. Look for actors you might already have identified.      Actors should have a title, and policies should follow the form “Whenever Event X, then Command Y”.   We sometimes automate policies with a Mechanical Turk. In this case, it’s simpler to keep modeling this as a policy rather than introducing a ‘dumb’ actor.      5. Empty read-models and UX mock-ups   Before actors can send a command, they need data from which to decide. That’s where UX and read-model come into play.   This step is, again, a straightforward step:      We add empty green and white post-its between domain events and actors.    The green post-its are for read-models.    The white post-its are for the UX mock-ups.      We just set up the stage for the next step.   6. Content of read-models and UX mock-ups   We just made explicit the points where actors react to new information.      💡 Design-Level Event Storming is the perfect workshop to discuss UX of domain events    It’s time to discuss what the actor needs to know to decide what to do next.   Write down the data you want to display on the green, read-model post-it. Draw a UX mock-up of the screen that should display this data.      7. External systems   We’ve been looking at what happens ‘after’ a domain event. Let’s dive into what happens between a command and an Event.   Let’s check the picture that explains everything again. It tells us that external systems raise domain events too.   If you did a Big Picture Event Storming before, you’d already have spotted a few external systems. Copy them to pink post-its, and stick them between Commands and Events where it makes sense.   In the scope of a bounded context, other contexts become external systems too! Go through all your commands. Add a pink post-it between the command and the event when they involve another context. Write the name of the other context on the pink post-it.      8. Empty aggregates   Here’s another mechanic step. If an external system did not raise an event, then an aggregate must have raised it.   Go through all commands and events that are not linked by an external system. Add an empty yellow post-it there.      9. Content of aggregates   Now is the time to fill in the aggregates.      💡 Discussing what Aggregates will do is the key moment of Design-Level Event Storming    Please don’t call them aggregates! Remember the first rule of DDD? Don’t speak about DDD. It’s going to work better if you call them Business Rules.   Ask participants to fill in these business rules with:      Preconditions: what must be true before   Postconditions: what is valid after   Invariants: important things that remain true all along   If you want to learn more about these concepts, check out this stackoverflow question. Some business rules are dead-simple, but others generate much discussion. This knowledge sharing between domain experts and developers is invaluable.      10. Name aggregates   Up to now, we’ve always kept the chronology in the design. Let’s break this!   This last step helps us to identify aggregates around which we’ll code the system. When you spot 2 business rules that deal with similar data, move them on top of one another.      💡 Finding good names for Aggregates is the last thing we do in a Design-Level Event Storming    Your board should start to look like that:      At this point, it should be easy to name your aggregates! Add an extra yellow post-it on top of aggregates to give the group a name.      Continues   Congratulation, you’ve reached the end of the workshop!      This post was the third in a series about how to build event-based systems with Design-Level Event Storming and DDD. The next, and last, post will provide tactics to reap the most value out of a Design-Level Event Storming.   Continue Reading…  ","categories": ["event storming","architecture","ddd","how-to-run-a-design-level-event-storming-series"],
        "tags": [],
        "url": "/detailed-agenda-for-a-ddd-design-level-event-storming-part-2/",
        "teaser": "/imgs/2019-10-13-detailed-agenda-for-a-ddd-design-level-event-storming-part-2/design-level-event-storming-last-piece-teaser.jpeg"
      },{
        "title": "7 tactics that will make your DDD Design-Level Event Storming pay-off",
        "excerpt":"What should we do after a DDD Design-Level Event Storming? How do we capture shared knowledge? How do we make the workshop more effective? Find answers here!   ℹ️ NOTE: Parts of this post has been republished and updated on the Event Storming Journal blog      This post is the fourth, and last, in a series about how to build event-based systems with Design-Level Event Storming and DDD. If you haven’t yet, start by the beginning.   What can we do after?   Design-Level Event Storming creates enough shared understanding to open up many opportunities.   Tactic 1: Highlight the Core   A first thing you might do, even before you leave the workshop, is to shrink the core. DDD calls this a Highlighted Core. The smaller the core subdomain, the clearer priorities are.   Aggregates already group events together. Aggregates make it easy to draw subdomains.      Tactic 2: Curate views   Depends on the decisions, and on the context. Usually putting decisions on a flip chart and sharing pictures might do the trick. &mdash; Alberto Brandolini (@ziobrando) March 15, 2019   As Alberto said, the real outcome is in the developers’ heads. You might extend their memories a bit by taking a picture of the design and sticking the board in the team’s space. Yet, don’t expect these memories to last for months.   There is a better way. You can also capture focused views. For example: collect domain definitions, problems, and decisions through specific documents. Check my post How to Capture the Outputs of an Event Storming Workshop for all the details.   Tactic 3: Run Example mapping   All the business rules are a significant first step to defining precise user-stories. It’s an opportunity to do Example Mapping workshops and detail these stories further.   Tactic 4: Build it!      💡 The best way to get more feedback after a Design-Level Event Storming is to give try to build the design!    Finally, the best way to get more feedback is to give all this a try! If you are starting, building a walking skeleton creates the most learning. Event Storming is pretty useful at scope-selection.      If you already have a running system, the question might be a bit more complicated. Before people leave the workshop, have a discussion to agree on the next priorities. You might need to mix learning experiments with money-generating features.   Facilitation tricks   I’ve now run the Design-Level Event Storming quite a few times, and I’ve collected some best practices.   Many general Event Storming best practices also work here. Read 4 tips that will make your DDD Big Picture Event Storming successful for a few. (NOTE: Since then, I’ve written 21 More Event Storming Tips)   In any case, Design-Level Event Storming also has its specificities.   Tactic 5: Don’t talk about DDD   First, it goes deeper into the DDD folklore. That might be too big a step for people who don’t know DDD. I’d first go through the Big Picture Event Storming. It lets people discover DDD and appreciate all the benefits it has to offer. To say it another way      Use Big Picture Event Storming to sell DDD   Only later dive deeper with Design-Level Event Storming   Tactic 6: Make the Design Space Really Infinite      💡 You’ll need even more infinite design space for Design-Level Event Storming 😉    Second, post-its are great because we can move them around many times. As you might have noticed on the agenda, we are going to insert new post-its between existing ones very often. Every time we do this, we’ll need a bit more design place. That’s when ‘Infinite Design Space’ takes all its meaning. As the facilitator, you need to be proactive:      Add more design space   Move the post-its to keep the chronology or design clear   Shared understanding relies on common metaphors. If post-its get aligned vertically instead of horizontally, add space NOW!              To Infinity and Beyond! By Michele M. F., under Attribution-ShareAlike 2.0 Generic (CC BY-SA 2.0), original on Flickr        Tactic 7: Optimize the time of Domain and UX experts   Many steps are pretty straightforward, almost mechanic. There are the pre-requisites to the critical steps: business rules and UX exploration.   These mechanic steps don’t need the experts. Also, we don’t want to waste their precious time! The simplest thing to do is to go fast on these steps. Explain what to do and ask everyone to take part.   Finally, exploring the UX of all screens and Business Rules takes some time. If all the group is waiting for 2 specific people to discuss all the topics, the workshop will take ages! Event Storming makes it easy to work in parallel! UX experts can work on UX, while domain experts discuss business rules. With enough experts, you might even have many groups working on UX or business rules at the same time!   Give it a try!      Design-Level Event-Storming is the most effective way to design an event-based DDD system      It’s collaborative.   It will get you pretty close to design in code.   It will speed-up the building or improvement of your system.   Also, running a Design-Level Event Storming is pretty straightforward.   It’s more structured than the Big Picture variant. If you have already done the later, diving in the Design-Level will be easy. If you haven’t yet, start with Big-Picture Event Storming!   Are you already building an event-sourced system? In this case, mapping it with Event Storming will be simple too!   Whatever your situation, give it a try!  ","categories": ["event storming","architecture","ddd","example mapping","how-to-run-a-design-level-event-storming-series"],
        "tags": [],
        "url": "/7-tactics-that-will-make-your-ddd-design-level-event-storming-pay-off/",
        "teaser": "/imgs/2019-10-15-7-tactics-that-will-make-your-ddd-design-level-event-storming-pay-off/7-design-level-event-storming-tactics-teaser.jpeg"
      },{
        "title": "Sustain Collective Intelligence with Event Storming",
        "excerpt":"Event Storming builds collective intelligence. We can keep it alive if we continue to work closely together and regularly repeat Event Storming.              By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        Don’t use Event Storming to do upfront design!   Event Storming is an iterative process: design, prototype, test, learn, repeat, etc.   It would  be a shame to stop the momentum once the Event Storming is over. As soon as you leave the workshop, start one of the these:      Build a walking skeleton if you haven’t already   Spike a burning question   Build Proofs of Concepts for critical NFRs   Refactor core concepts   Add this $$$ feature   And then return to the Event Storming design board with more knowledge. Focus and risk reduction are the keys to a sustainable pace.      💡 Event Storming builds collective intelligence, work as a mob to keep it alive!    I’ve since written about introducing Event Storming and DDD in your organization: Organization refactoring: Event Storming and DDD injection.  ","categories": ["event storming","architecture","ddd","infographic"],
        "tags": [],
        "url": "/sustain-collective-intelligence-with-event-storming/",
        "teaser": "/imgs/2019-10-20-sustain-collective-intelligence-with-event-storming/event-storming-iterative-process-teaser.jpeg"
      },{
        "title": "How to fight priority paralysis with Event Storming and DDD",
        "excerpt":"Where should we start? It’s easy to fall into priority paralysis as you envision an ambitious product. Let’s use DDD Event Storming to scope your next step!      I’ve said it earlier: Event Storming helps you to make complex decisions in a short time!   Story of a new feature   A few months ago, at work, I trained some people to run Event Storming workshops. Some of them tried their new knowledge on a significant feature involving many teams.   They had been wondering how to start this topic for weeks when they ran the Event Storming.   Two of the outputs of these 8 hours of Event Storming were:      They agreed on an end-to-end Walking-Skeleton to test and build first   They had a first idea of the end to end NFRs (Non-Functional Requirements) that the business needed   Everyone went out very satisfied with these results.   The problem: What’s the next target      When you don’t all agree on the next target, it is very difficult to be effective.      Prioritization is tricky   Progress is slow as there is not enough focus   You won’t reach a sustainable pace. Running after too many rabbits generates stress.   It gets even worse with different stakeholders who have different topics in mind.      Domain experts care about features or business risks   Technical people have NFR and maintainability risks in mind.   Getting all these people to agree on the next step is a real challenge!   An easy ES trick!   Again, the shared understanding generated from Event Storming makes this step simple. Here is what you can do to get people to agree on the next target:         Hand them small dot stickers   Ask them to mark what they want in the next target   Let them negotiate   The result is a mix of features and de-risking. Stickers work great because they are easy to remove. From my experience, this scoping step takes about 30 minutes.   The key to making this simple activity work is to have enough shared understanding. You’ll build this with the beginning of the Big-Picture Event Storming. Go at least as far as the storytelling. It works even better if you identified the contexts and subdomains.   The more shared understanding you built, the easier the negotiation is.   Why does it work?   Shared understanding is the first enabler. Here are other aspects.   By working together in the Event Storming, people get to know and trust each other more. As a result, it also improves collaboration after the workshop!   Best practices against priority paralysis provide another explanation. DB Hurley presents a 3 steps process in First Things First: How to Handle Priority Paralysis:      Be specific and gather details. Event Storming should fill this first step.   Identify both short term and long term goals. Here is the negotiation part.   Start something on the list. As soon as you leave the Event Storming workshop, you should start something.      Where’s the catch?   OK… there’s an implicit assumption here. I did not mention system design constraints here.      How can we address critical features and risks first? Without bothering about the code?    To do this, you’ll have to code by refactoring and change the code many times. You’ll need to master incremental development techniques. Without them, you’d have to build full components in large work batches.   If you want to learn ‘code by refactoring’ techniques, the simplest way is to start a coding dojo.   Don’t let priority paralysis kill your product before you even start!   Watch out for projects or products that are taking too long to start. If you hear something like:      We just need to figure out how to cut the elephant…    Suggest an Event Storming!   In 2 days of effective collaboration, you’ll agree on the next target. As a bonus, you’ll get all the other outcomes of ES:      Shared understanding   Growing tech-domain collaboration   The beginning of a ubiquitous language   High impact quick fixes   …   Running and Event Storming is not rocket science. If you are wondering how to start, follow the guide!  ","categories": ["event storming","architecture","ddd","planning"],
        "tags": [],
        "url": "/how-to-fight-priority-paralysis-with-event-storming-and-ddd/",
        "teaser": "/imgs/2019-11-09-how-to-fight-priority-paralysis-with-event-storming-and-ddd/event-storming-walking-skeleton-teaser.jpeg"
      },{
        "title": "Organization refactoring: Event Storming and DDD injection - part 1",
        "excerpt":"Injecting DDD or Event Storming in your organization can be tricky. Here are techniques to onboard your domain experts, one step at a time!      A few months ago, I received an email from my friend Bastien asking for help around Event Storming. He had injected some tactical DDD (Domain Driven Design) in his development team:      They were now using Value Objects   They had separated the domain from the infra code   They were already moving on to other DDD patterns   To continue his DDD injection, he needed to onboard domain experts. He also wanted to draw a better context-map of his system. His question was, “How to get domain experts to take part in an Event Storming?”   As we discussed, we found 3 steps I had used to inject DDD and Event Storming at Murex.   1. Practice yourself first   I wrote it before: running an Event-Storming workshop is not rocket science. Still, it requires a bit of practice to get right.      💡 Don’t waste your chances with not enough preparation!    Imagine you persuaded your domain experts to spend 8 hours in an Event-Storming workshop. Now imagine the workshop is a failure because of lousy facilitation. You’ll have no second chance!      Practice short sessions first!      Read a guide   Try it with a few colleagues on a sample startup business   Use it on your development process to find improvements   You can use it as onboarding for newcomers. Do it within your team, playing roles.   This last trick can raise many questions to ask your domain expert. You could even use the onboarding excuse to lure a domain expert in the workshop!   2. Start with your current tasks   It’s challenging to get domain experts to sign in for a 2 days workshop. It’s a lot easier to have a 15 minutes chat with them!      Next time you have a domain question about your current task, ask the experts! You’ll learn about the domain, and you’ll start to build a relationship with them.   You can even use this technique to inject a bit of Example Mapping! Example mapping is a simple conversation format. Practice it a few time with your teammates, and you’ll be ready to use it with your domain experts.              Sample Example Mapping cards on the introductory post about Example Mapping, by Matt Wynne        Example mapping is handy in understanding domain rules. Domain savvy developers become trustworthy in the eyes of the domain experts.      💡 Hijack a conversation with a domain expert into an Example Mapping.    The key is to have these conversations as often as possible. With time, a trustful relationship should settle. That’s the perfect context to shift-up to next gear with Event Storming.   Continue Reading   This post was only the first half of the story. The next post contains the third and last step to inject Event Storming. As a bonus, it also provides a few organization hacks to get experts on board.   Continue reading…  ","categories": ["event storming","ddd","bdd","change management","psychology","example mapping","event-storming-and-ddd-injection-series"],
        "tags": [],
        "url": "/organization-refactoring-event-storming-and-ddd-injection-part-1/",
        "teaser": "/imgs/2019-11-22-organization-refactoring-event-storming-and-ddd-injection-part-1/ddd-injection-teaser.jpeg"
      },{
        "title": "Organization refactoring: Event Storming and DDD injection - part 2",
        "excerpt":"The key to successful DDD and Event Storming injection is to wait for the perfect challenge. Otherwise, we’ll have no choice but to hack the organization!      This post is the second half of the story. If you haven’t already, start by the beginning!   3. Look out for the next business challenge   To convince domain experts to join an Event Storming, you’ll need to highlight what’s in it for them!      💡 Remember 1st rule of DDD: Don’t speak of DDD. Wait for the good business challenge to suggest Event Storming.    Don’t try to sell the technical benefits of DDD. Instead, wait until you spot something that Event-Storming could help domain experts. That’s the best moment to suggest a workshop to fix their problem. Here is a non-exhaustive list of problems Event Storming is very good at:      What will our next increment be?   What prototype should we build to de-risk Non Functional Requirements?   Should we organize as feature teams or component teams?   What are the bottlenecks in our way of working?   What architecture should we use to maintain our competitive advantage?   Should we build or buy this component?   Should we rewrite or refactor this component?      The trick is to highlight that it is only 1 or 2 intense days to get the great answer they need. The alternative is to go through many email exchanges and one to one meetings. It usually takes weeks, if not months, to make a very average decision. Compare the 2 alternatives to make the value of Event Storming obvious. Don’t let availability be an issue; be open to accommodate people’s schedules. For example, you can split the workshop into 2 hours slots over a few days.   You might not convince them the first time 😞      Sometimes, by losing a battle, you find a new way to win the war.    If they chose the emails and meetings path, keep notes of what happens. Try to keep track of all the meetings and email exchanges that went into the decision. Measure how many days it took to reach a consensus. If you can, measure how valid the decision was (or not). At the next business challenge, suggest Event Storming again. Show what happened last time. Be patient; time plays on your side.   Other strategies   That’s how we brought Event Storming at Murex. From hesitant first tentatives, it has become one of the architects’ main tools.   Barry Sullivan suggests a similar strategy in Introducing DDD to your Company.      💡 Event Storming is viral. Yesterday’s attendees are tomorrow’s organizers :-)    Unfortunately, sometimes, domain experts remain out of reach. The Paris DDD Meetup went over this question in DDD From the trenches. People came up with corporate hacks:   Seduce an expert      With a burning question   Bribe them with a free lunch or their favorite chocolate   Fake it till you make it      Bring in another expert somewhere   Buy a book and play the expert yourself      The Trojan horse      Do you remember Bastien, who asked the question in the first place? You might be wondering how he managed in the end?   The latest news I had was that he used a hack of his own too! We could call it the Trojan Horse hack. A developer moved position from dev to product. He became the perfect entry point to the domain experts 😉   Get started!   DDD has a lot to offer:      happy users   more maintainable systems   fewer bugs   faster evolution   a more sustainable pace   Event Storming is excellent at injecting DDD in your organization. Unfortunately, asking domain experts to spend 2 days in a workshop won’t work. Still, with some practice, preparation, and patience, you can get your domain experts on board!   Start practicing today! Here is the guide.  ","categories": ["event storming","ddd","change management","psychology","event-storming-and-ddd-injection-series"],
        "tags": [],
        "url": "/organization-refactoring-event-storming-and-ddd-injection-part-2/",
        "teaser": "/imgs/2019-11-27-organization-refactoring-event-storming-and-ddd-injection-part-2/event-storming-detour-teaser.jpeg"
      },{
        "title": "First rule of DDD is: let's not talk about DDD",
        "excerpt":"DDD jargon is a domain expert repellent! Let’s not talk about DDD, but instead, start engaging domain experts in doing DDD workshops!   Let me share a short story. It happened a long time ago when I was a junior developer at a bank. I had the opportunity to meet one of our business sponsors. I had built a rate curve server for his trading activity. Throughout the discussion, I mentioned that I had used Value Object for thread safety. The look on his face was frightening. It taught me never to mention this kind of technical detail with domain experts again.   Let’s not talk about DDD, let’s DO DDD instead!              By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        How to DO DDD?   DDD promises a lot: better software, a more sustainable pace, and continuous refactoring breakthroughs…   But how do we get started in practice? Here are 2 posts I wrote on the topic:      Organization refactoring: Event Storming and DDD injection   How to use Event Storming to introduce Domain-Driven Design   The main idea is to run workshops like Example Mapping, Event Storming and others…   Is facilitation a new aspect of the developer job?   With complex domains, domain experts’ collaboration is mandatory for good software. Having them in workshops is the most effective way to collaborate.   To build better systems, we’ll need to master these collaboration techniques!   That’s a big step away from the traditional developer stereotype. It is not a surprise: DDD has always been about collaboration!   Example Mapping is straightforward. Reading the description should be enough to master it. Event Storming is a bit more involved, but still not rocket science. You can start with my detailed guide about how to get started with Event Storming.  ","categories": ["event storming","ddd","change management","example mapping","infographic"],
        "tags": [],
        "url": "/first-rule-of-ddd-is-lets-not-talk-about-ddd/",
        "teaser": "/imgs/2019-11-28-first-rule-of-ddd-is-lets-not-talk-about-ddd/1st-rule-of-DDD-teaser.jpeg"
      },{
        "title": "21 More Event Storming Tips - Part 1 - Understanding and Rhythm",
        "excerpt":"You’ve just invited 10 people to join your 8h Event Storming. Here are 9 Event Storming tips to maximize knowledge sharing and the use of everyone’s time.   ℹ️ NOTE: Parts of this post has been republished and updated on the Event Storming Journal blog      When we ran the first Event Storming workshop at Murex, it was an all-new practice. Fast forward a few years, it has become a standard. As the method is spreading, we are also discovering new best practices.   Learning and Sharing   The main output of Event Storming is shared understanding. Here are a few tips to maximize this.   1. Distribute domain-specific learning material   If you work in a complex domain, there is a lot to learn for developers. Think of newcomers, for example. They will have a hard time understanding all the discussions during the workshop. Fortunately, most of these complex domains have read-made reference learning material.      For example, financial software has “The Hull”. It’s like the bible of finance. Sharing snippets before the workshop will help people to hit the ground running.   Domain experts can have a crucial role here. Compiling some specific material for a particular workshop will be even more useful!   2. State the goal ‘sharing over learning’   The feedback I often get from domain experts is: “I did not learn that much.” Here is something I now always say at the beginning of an Event Storming. “The goal is 1st knowledge sharing, 2nd learning”. You can even cite, or display Alberto Brandolini’s tweet:   @tpierrain @mathiasverraes &quot;it&#39;s developer&#39;s understanding, not your knowledge that becomes software&quot; I once said. Guys seemed to understand &mdash; Alberto Brandolini (@ziobrando) August 21, 2015   3. Playing the silliest person in the room   One of Alberto’s tips is to play the silliest person in the room. It turns out it’s more complicated than it seems!   Playing the silliest person means asking candid questions. You’ll have to follow the content of the workshop to ask these questions. Following the content at the same time as facilitating is tricky. If you’re a beginning facilitator, ask someone else to play the silliest person in the room 😉   4. Ask a new-joiner to collect definitions   It turns out newcomers are perfect for playing this candid role! If you can, ask a newcomer to write down definitions during the Event Storming.   Newcomers don’t know much about the domain or the company, so they will detect all buzzwords. By being active, they should also learn better!   Rhythm   Let’s admit it, Event Storming is pretty intense. Here are a few best practices to make it sustainable and as useful as possible.   5. Twice 2 hours per day   It can be tricky to get people to dedicate 1 or 2 full days to a workshop. After 4 hours of Event Storming, people also get too tired to be effective.   The best schedule we found around these problems is to do two times 2 hours per day.      2 hours in the morning   lunch   2 hours in the afternoon   repeat for as many days as needed   People will have time to rest. The breaks will also trigger their System 2 brains. They might come back with new smart ideas!   6. Use breaks   Breaks are crucial for the Event Storming to go well. People will need them to maintain enough energy, but we can also use them for facilitation. I usually use a Double-Pomodoro technique and break every 50 minutes.      After the narrative storytelling, the agenda of an Event Storming is very flexible. Breaks are the best opportunity we have to decide what to do next. Take the time to think about how the workshop is going. Try to discuss with some participants, get their feeling, and adapt the agenda.   You can skip some activities to leave more time for others. Or you can decide to dive in a Design-Level Event Storming on a particular subdomain. There is an infinity of options depending on your challenge.   7. Adapt with ROTIs   With the large audience of an Event Storming, it’s easy to lose track of how things are going. We found that having a quick ROTI (Return On Time Invested) check every 2 hours helps.      It’s straightforward if you follow the 2 hours chunks. Take 5 minutes before long breaks to get their feedback about the session. Use this feedback to decide on what aspect of the problem to work on next.   8. Re-narrate for new-joiners   It’s a fact: some people will miss parts of the workshop. Many of us suffer from overbooked agendas. Finding 8 hours where everyone is available is not easy. Some people will miss the beginning, the end, or a part in the middle.   New-joiners will have a hard time catching-up when collective intelligence is already growing. A good workaround is to re-narrate whenever someone joins.   The 2 hours chunks schedule also makes this easy. Plan 10 minutes at the beginning of every chunk for this storytelling.   9. Leave with actions   Here is one thing you don’t want:      have your Event Storming   build some shared understanding…   …and do nothing with it!   To avoid this, take 30 minutes at the end of the workshop to agree on the next steps. It’s crucial to define actionable tasks with an accountable person for each.   You can read more about the next actions in “5 Views to Capture the Outputs of an Event Storming workshop”.   More tips in next posts   This was the first of a 3 posts series compiling Event Storming best practices. Next post will contain tips about facilitation and dealing with existing code.   Continue Reading…  ","categories": ["event storming","ddd","more-event-storming-tips-series"],
        "tags": [],
        "url": "/21-more-event-storming-tips-part-1-understanding-and-rhythm/",
        "teaser": "/imgs/2019-12-21-21-more-event-storming-tips-part-1-understanding-and-rhythm/more-event-storming-tips-understanding-rhythm-teaser.jpeg"
      },{
        "title": "21 More Event Storming Tips - Part 2 - Facilitation and Existing Code",
        "excerpt":"More Event Storming tips! 4 tips to delight attendees with fantastic facilitation. 3 tips about animating an Event Storming when there is existing code.   ℹ️ NOTE: Parts of this post has been republished and updated on the Event Storming Journal blog      This was the second of a 3 posts series compiling Event Storming best practices. The previous post contained tips about understanding and rhythm. If you haven’t you might start by the beginning!   Facilitation   Event Storming facilitation is not rocket science. With a bit of practice, anyone can do it. Yet, lousy facilitation can damage your Event Storming initiative. Here are a few tricks that will make facilitation easier.   10. If things are slow to start   The workshop can be slow to start. People can get lost in the chaotic nature of Event Storming.   For example, people discuss in small groups instead of sticking domain events.   Go and discuss it with people or groups. Make sure they understood the instructions. Kindly ask them to stick events as soon as possible. Also, notify them that we’ll be doing a first event review in 10 minutes.   11. If there is a single discussion bottleneck   Sometimes things are slow because all people are spectators of a single discussion.      It can be because only one person knows some crucial information. If this is the case, then we have no choice but to wait.   Other times though, it happens with no particular reason. Suggest to split the group in 2 to work on 2 halves of the board should speed up the process.   It involves merging and synchronizing at the end, but it should still be faster.   12. Post-it twist   Here’s one from Alberto Brandolini. Post-its, domain events, in particular, are usually not top-quality from the beginning. People need a bit of time to distinguish an event from a command.   We cannot harass them to rewrite their post-its.   Here is Alberto’s trick: whenever you notice a ‘non-standard’ post-it, flip it 45º. When people ask why it is flipped, repeat event-writing best practices. You can also suggest a rewrite for the faulty one. With time, more and more people will ‘get’ what a domain event is. The quality of what’s on the board will improve by itself.   13. Explicit decisions   Here’s another one from Alberto:   Depends on the decisions, and on the context. Usually putting decisions on a flip chart and sharing pictures might do the trick. &mdash; Alberto Brandolini (@ziobrando) March 15, 2019   You’ll make many decisions during the workshop. Given the large audience, some people will ‘miss’ some decisions.   Indeed, making these decisions explicit on a dedicated side-board works best. At the end of the workshop, you can go through this board and:      take a photo   save it somewhere   or share it with a broader audience      Dealing with existing code   Event Storming works like a charm on greenfield projects. Though it also brings tremendous value to existing projects. Let’s see a few best practices when some code already exists.   14. Start with 30 minutes brief   Starting with a 30 minutes brief gives everyone an idea of what already exists. This brief can go over aspects of business, domain, architecture, and target scope.   Event Storming would still work without this briefing. People would talk during the workshop, and eventually reach the same knowledge. We just observed it was more efficient to state what we already know.   15. Adapt the schedule even more!   The typical agenda of Event Storming fits new products entirely.   Here is what changes when you’ve already written some code.       Some areas will already be clear to everyone   Unlike other parts of the system, about which people will have contradicting views.   I already said it’s a good practice to use breaks to adapt the schedule. When dealing with existing code, it’s not only best practice, it’s crucial! Use pauses to discuss with critical people, and agree on where to dig next.   16. Make stress explicit   One of the first Big-Picture Event-Storming I did was with a team working on a legacy system. The developers wanted to do some refactoring. They needed a target architecture to guide them.   As we went through the workshop, I noticed something: stress from the developers. I had not expected this, but Alberto had predicted it. He mentions this in the Legacy Code Rocks podcast and even calls it fear.      The legacy system had not been built with DDD in mind. It looked very different from the system we were designing with Event Storming. The more concrete our design became, the more the developers anguished. I could almost hear them thinking:      How the heck are we going to go from here to there⁉️    Trying to mute this stress does not work. People will stop cooperating. The best thing to do is to acknowledge it.   One option is to say that we will deal with the refactoring path in a later activity or workshop.   Another option is to have a refactoring-challenges side-board. Developers can stick what they foresee there. Explicit and park their stress should help them to remain in a positive mood.   More tips in next post   This was the second of a 3 posts series compiling Event Storming best practices. Next post will contain anti and meta patterns.   Continue reading…  ","categories": ["event storming","ddd","refactoring","more-event-storming-tips-series"],
        "tags": [],
        "url": "/21-more-event-storming-tips-part-2-facilitation-and-existing-code/",
        "teaser": "/imgs/2019-12-24-21-more-event-storming-tips-part-2-facilitation-and-existing-code/more-event-storming-tips-facilitation-and-existing-code-teaser.jpg"
      },{
        "title": "21 More Event Storming Tips - part 3 - Anti and Meta Patterns",
        "excerpt":"Even more Event Storming tips! 3 will help you to avoid the mistakes we did. The last 2 are the recipe to learn more tips by yourself!   ℹ️ NOTE: Parts of this post has been republished and updated on the Event Storming Journal blog      This was the last of a 3 posts series compiling Event Storming best practices. The previous post contained tips about facilitation and dealing with existing code. If you haven’t you might also start by the beginning!   Anti-patterns   As we tried variations around Event Storming, we also learned from our mistakes. Here are a few:   17. Adding special post-its   Before getting Alberto’s advice about logging decisions, we tried to use custom post-its. That did not work well. Event Storming already has a pretty large post-it bestiary, adding more is too much. Plus, it’s almost impossible to find a post-it color that is not already used!      When we tried this, people ignored these post-its. They made decisions but did not record them.   18. Remote participant   Our company is over 3 main offices: Beirut, Dublin, and Paris. With this constraint, organizing an Event Storming is often a mess. We have to bring some key people from different cities to the same office. Sometimes, we have to delay a workshop for a few weeks because a single person is missing!   I heard good feedback from remote-robots at a conference. We thought we could give it a try for Event Storming. The idea was:      Pair up the remote participant with someone in the room   Let the remote attendee contribute through his robot-avatar   We could not pay for a robot without being sure it would work. We tried to hack one with a moving speaker desk and a laptop… This was a disaster! The only outcome was that the poor remote-guy ended-up with a headache…      Up to now, we did not find a satisfying way to run an Event Storming with remote participants.   19. Big Design Up Front   Event Storming is a design activity. Like any design activity, we risk pushing it too far. I wrote that it works best as a tool to do a Rough Design Up Front.   You can always add more refinement to your design. I’m sure you could spend a full week doing detailed Design-Level Event Storming. Though that’s not how it’s meant to be.   You’d have better results by:      Drafting just enough to get started   Building something   Learning from it   Repeating              By Philippe Bourgau, under CC BY-SA 4.0, original and high resolution image        Meta   Here are 2 extra ‘meta’ best practices about Event Storming.   20. Create an Event Storming community of practice   Spreading Event Storming in a small organization is straightforward:      Get everyone in   Make it a success   You’re done!   It’s a lot more complicated in large organizations. Building a community of practice helped us a lot.   We started ours as I animated a training about Event Storming to other people in the company. We ended this training with time for open discussions. This was the first ‘community’ meetup.   We then created a wiki space and a chat channel. We used them to exchange best practices and experiences around Event Storming. It’s a great way to learn from each other. It’s also very encouraging to hear other’s success stories.   21. Ask Alberto!   You might have noticed that some of these best practices come from Alberto Brandolini. I don’t know Alberto in person, but he’s kind enough to answer my naïve questions on Twitter. So I guess he would answer your questions too!   Run your own!   Event Storming is a magic workshop that unlocks complex decision making in a short time! It can help you with:      Agree on a target functional architecture   Decide what to buy or build   Decide what to rewrite or refactor   Self-organize the best team structure for your architecture   and a lot more   Running an Event Storming is rather simple once you get the hang of it. The trick is to get started! Find a forgiving audience, and follow the guide.   I hope these tips will help you along your Event Storming journey. Whenever you discover your own tips or anti-patterns, take a minute or so to share them with a comment!  ","categories": ["event storming","ddd","antipattern","remote","more-event-storming-tips-series"],
        "tags": [],
        "url": "/21-more-event-storming-tips-part-3-anti-and-meta-patterns/",
        "teaser": "/imgs/2019-12-25-21-more-event-storming-tips-part-3-anti-and-meta-patterns/more-event-storming-tips-anti-and-meta-patterns-teaser.jpg"
      },{
        "title": "Make Testing Legacy Code Viral: Mikado Method and Test Data Builders",
        "excerpt":"When testing legacy code, combine the test data builder pattern with the mikado method. This snowballs into more and more objects that are easy to set up in tests.      Your Mission… Impossible!   I’m sure you’ve heard of this recommendation:      Whenever you fix a bug, add a test before. [stackexchange.com]    Unfortunately, when working in legacy code, this is more depressing than empowering. I’ve seen codebases in which adding a unit test can take weeks! Spending 3 weeks to add a test when fixing the bug will take 1 hour is often unjustifiable. It’s not easy to swallow for product managers or customers.      This situation can get pretty stressful. Worst of all, it can only get worse!   The False Promise: Mocks   When testing legacy code, most of the time is usually spent with the data setup. Legacy code is often a tangle of dependent classes. For every object we need to instantiate, we’ll have 5 others to set up before.   The default solution to this problem is to use mocks to shortcut all this data setup.   Pros      It gets the test running in a reasonable time   Cons      Every test will have its own mocks of specific classes. Unfortunately, legacy code mocks duplicate the behavior of the real system. After a while, this creates rigidity. It will backfire and make refactoring more difficult. The exact opposite of what we wanted! That’s what I call Mock Hell.   The flip side is that these tests won’t be very robust. When we have to refactor the code, we’ll have to refactor the tests. Worse, if the real code behavior changes, tests will silently continue to pass!   Finally, we often need to refactor the code before we add a mock.   Mocks have serious downsides. As I wrote before, using mocks to speed up test data setup is an anti-pattern.   Mikado to the rescue      The mikado method is the perfect workaround against long technical endeavors. It lets you split them down in small steps that we can deliver along with other business-as-usual tasks.   Let’s update the initial recommendation:      💡 Whenever you fix a bug, start a mikado graph to add a test around it.    You can now write your test at your own sustainable pace. It might take 1 month, but it’s not an issue because you are not blocking features. There is no tunnel effect. You won’t need to use mocks.   What does it mean to start a mikado graph?              Sample mikado method graph as we can find on mikadomethod.info/ by Daniel Brolund and Ola Ellnestam        You want to make the plan and the progress visible for everyone in the team.   If you are all in the same room, stick it on the wall! If you’re working as a mob, remind it to your buddies.   Otherwise, you’ll need valid working agreements! For example, we used to generate mikado graphs from Jira tickets. Your team will have to find its own ways. Retrospectives help there!   The Test Data Builder Pattern to make it Viral   The flip side of using the mikado method out of the box is that many tests will end up setting up similar data. The typical fix to this problem is the test data builder pattern. This is where it becomes magic.   Let’s update the initial recommendation again:      💡 Whenever you fix a bug, start a mikado graph of test data builders to add a test around the bug.    Most nodes of the mikado graph will be about writing a test data builder for some data of the system. Every time you finish a step, you’ll have a new reusable test data builder.      At every step, you increase the testability of the system. As other developers try to test their own code, they are likely to:      Re-use these new builders to add their tests faster.   Improve the builders along the way.   Make others even more likely to re-use these builders!              By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        It’s viral!   Try it yourself   Ahmad and I published a kata to spread the idea. It’s under MIT license on Github. We made it as self-service as we could. Give it a try! We’d love to get your feedback!   It’s between 2 and 4 hours long. For the moment, code is available in Java and C++.   We have also submitted this session at Devoxx and NewCrafts conferences in Paris. Fingers crossed!  ","categories": ["mikado-method","testing","refactoring","mocking"],
        "tags": [],
        "url": "/make-testing-legacy-code-viral-mikado-method-and-test-data-builders/",
        "teaser": "/imgs/2020-01-12-make-testing-legacy-code-viral-mikado-method-and-test-data-builders/mikado-method-test-data-builder-pattern-squid-teaser.jpeg"
      },{
        "title": "The story about how we do Agile Technical Coaching",
        "excerpt":"You will not find an Agile Technical Coach Job Description. This job is more invented than found. Here is the story of how we came to practice it at Murex.      People often wonder what Agile Technical Coaching is about. Unfortunately, you’ll have a hard time finding an Agile Technical Coach Job Description. In the days of Scrum and SAFe, this way of coaching teams has become pretty rare. Instead of a formal description, here is our story. The story of how we grew a team of Agile Technical Coaches at Murex.   It started with Deadpool   Somewhere in 2017, Murex undertook an agile transformation. Agile coaches where helping teams to get used to this new way of working. Being the only former agile developer, I was trying to find my place among the other coaches.      One day, another agile coach asked me to help the Deadpool team. The team was going through a lot of difficulties, and stress was increasing for everyone. The team had inherited a lot of problems:      High turnover   Experts working in silos   A lot of poorly designed legacy code   Slow end-to-end tests   As if all this was not enough, the team:      had a lot of maintenance work to do   and was under a lot of pressure to deliver new features   Unsurprisingly, this mix was resulting in growing tensions among the team members.   What could I do?   All coaching I had done before consisted of embedding full time within a team. I would:      spend a lot of time pairing, with everyone   facilitate weekly coding dojos to share development practices and learnings   facilitate retrospectives and put real continuous improvement in place   I knew this worked great.      💡 Full time pairing within a team is slow but effective agile technical coaching    Unfortunately, it also takes a lot of time. It requires a few months to get real momentum, and a few more to see concrete outcomes. It also compounds, and once the wheel starts spinning, results pile up.      I was the only technical coach in the place. There are more than 60 development teams in Murex. If I was to replicate this method with only half of them, this would still take ages!   v0.1, part-time embedding   My first try was a light version of the full-time embedding. I would      pair for 2 days per week   facilitate coding dojo per sprint   help the Scrum master with the Scrum events   Scrum events got better, and I could sometimes spot useful things during pairing sessions. Unfortunately, people did not yet see the benefits of katas, and I could only pair with 1 person at a time. As a result, the progress remained very slow, and the team’s feedback was average.   I had to find another way.   v0.2, katas and mobs   I decided to spend less time on Scrum events and to trade pairing for mobbing. We were doing 2 sessions of 2 hours of mob per week.      💡 Katas + Mobs is a time efficient agile technical coaching scheme    I continued to do one 2-hour kata per sprint. As the team was struggling with legacy code, we focused on refactoring katas.      At some point, another team member left. He was the only expert on an essential area of the code. I had been highlighting the importance of reducing the WIP and increasing the bus factor. This new event convinced the team. Facing this new challenge, the team jelled. They met their management as a single unit, and insisted on taking the time to work differently:      documenting critical knowledge   refactoring code areas that were too tricky to work with   Managers had no choice but to accept. That’s how Deadpool resurrected.      Epilogue   2 years down the road, the team’s day to day life has dramatically improved:      A lot less stress   30-40% test coverage and increasing. This is a great achievement given the state of the initial legacy code.   The team is taking time for refactoring and testing at every sprint   The team now follows a sustainable pace   There is a friendly atmosphere in the team   The situation, especially turnover, pushed the team into trying something new. The final jelling unlocked all the rest.   To which extend did my coaching contribute to these changes? Most of these results come from the team’s work. To know better, I had a chat with the team members. Here is what they say:      Seemingly simplistic katas made them understand the value of baby steps. They discovered that it unlocks better design and smoother deliveries   Mentioning WIP reduction made them envision ‘another way’ when the experts left   Randoris and mobs taught them better collaboration   They now see the speed-up effect of tests, even though they did not understand the point at the beginning   It looks like this coaching nudged the team to change how they work.      But could we repeat this?   Scaling to other teams   A new technical agile coach joined me at that point. We were now a team of 2.   v1.0, stable   We extracted the strong points into a recipe we could re-use. Here are the key elements:      Focus on technical agile coaching only. There were enough process/people coaches in the company. We decided to let them do this part of the work! Anyway, once developers get TDD and pairing, they get the agile mindset. (Note: This might make a good post, so stay tuned!)   We would use a kata plan   We would start with TDD katas   And continue with refactoring katas. (I’ll write a more detailed post about the katas we are using, stay tuned!)   We would mob with the teams on selected stories   We would stick with the teams for a few months   I’ve since discovered that other agile technical coaches are using similar techniques. Emily Bache gave a talk at NewCrafts, where she described something pretty similar.               She is now writing a book Technical Agile Coaching about it.      A tiny glitch remained: we now needed to find teams to coach!   Marketing and sales…   There are 2 ways to coach teams.      Push-coaching: management sends the team to coaches   Pull-coaching: the team gets coaching when it asks for it   From the beginning, we decided to avoid the former at all costs, for 2 reasons      It does not work very well, as people resist to ‘being changed’   In the end, it’s painful for everyone, the team and the coaches   I have a friend who says that:      💡 Push-coaching is like inflicting help.    The problem was that we now had to attract teams to work with us! We were now equipped with excellent feedback from Deadpool and a recipe. We started to meet teams and test our proposal. Every time we discussed it with a new team, the pitch got better. Here are the final selling points of our coaching:      Use refactoring and legacy as the base motivation. We highlight that they will learn useful refactoring techniques.   We don’t try to sell TDD. We tell them it remains their choice to use it or not in their daily work.   We explain we’ll enforce TDD during the coaching. It provides the fast feedback loop required for deliberate practice.   We illustrate the power of deliberate practice with how athletes use it.   We present mobbing, pairing, and remote collaboration techniques as bonus learning.   We adapt to the team’s schedule as best as we can.   Word of mouth and this improved pitch did the trick! We soon had new teams to coach.   Intrapreneurship   It turns out there is a third and even more important reason to stick to pull-coaching. If your product is not good enough, you won’t find teams to work with… In the end, it’s a guarantee to have a valuable offer.   Actually, it’s like a startup!      Where do we stand now?   Successes   We have now coached 6 teams. Their feedback has been great. Depending on their situation, they highlight different aspects of coaching:      Collaboration   Testing   Refactoring   …   We are now 2 full time agile technical coaches. We’re also onboarding Ahmad, and Patrice, a developer from Deadpool, also joined us as a part-time coach!   We also compiled our katas into a kata repository, with coaching instructions. Some teams managed to learn refactoring techniques with them, without requiring our presence. We’re hoping to take the time to open source this repo at some point.   Our challenges   There are still a lot of things in our way, though.   Asking for coaching is shameful   Many teams still see coaching as ‘shameful’. Asking for help is often seen as a weakness in our ego-driven industry… We’ll have to work on our marketing to overcome this.   Sustainable communities   We are struggling to build communities of practice around refactoring and Continuous Integration. Unfortunately, these communities are far from being autonomous at the moment. They still require a lot of effort from our side…    Again, we’ll have to find the recipe to install communities in the company culture.   Finding the time for self-development   We’re having a hard time to continue to learn, explore, and experiment with new techniques. This is super important for agile technical coaches, and again, we’ll need to make more time for that!   Startup metrics   I said we’re thinking of us as an internal startup. Startups use metrics to know they are on the right track. This looked especially important to us. As we started the activity by ourselves, we feared we had to ‘prove’ that we were productive.   We tried standard startup metrics, like metrics of the pirate, and others for a while. Unfortunately, it did not work great for us because:      Without any tooling or available figures, it generated some manual work overhead   With 500 developers in the organization, our sample size is small   We also realized that fear is not a good motivation. We decided to stop tracking these metrics systematically. We are now experimenting ‘just-in-time metrics’. Whenever we do something, we put metrics in place to make sure it’s doing what we expected… If not, we review and adapt.   It’s now close to a lean way of working. We are currently using the lean story template.   Crossing the chasm?   Lately, the demand for new coaching has been slowing down. If we’re growing like a startup, could it be that we are crossing the chasm? Did we only tackle the innovators and early adopters?      If this is the case, it means we need to put all we can in the next early-majority team we get the chance to work with!   v2.0, leadership   New topics   Up to now, we’ve helped teams on refactoring. Lately, a team asked for help around agile testing. We are trying to adapt our recipe to testing instead of refactoring.   The plan was to:      Start with TDD katas   Start mobbing on their stories   Continue with new testing katas   This would work, but       We would need to work hard to prepare all these new testing katas   These katas might even not be well adapted to the team’s need!   We decided to go one step further.   Growing leaders   We are not going to run test-related katas directly with the team! Instead, we’ll first work with 1 or 2 wanna-be-experts from the team:      We’ll share references to let them think what’s best for their team   We’ll present some kata ‘base’.   We’ll fine-tune the katas for the team   We’ll run the kata with the whole team   The idea is to      build something even more practical and tuned to the team   but also grow subject-matter experts   increase buy-in from the rest of the team   We also have other long term hopes about this approach:      Have subject matter experts that the company can rely on   Have these experts share what they did with other teams   Make these experts a bit more ‘badass’. This will increase the devs negotiation power with product people   If they feel like it, share their experience at conferences!      💡 Let’s grow leadership among developers by involving experts in kata preparation.    Advice to Technical Agile Coaches   Try katas + mobs      If you are an agile technical coach, I recommend giving this kata and mob format a try. So far, it’s the most effective recipe I’ve tried or heard about!   Not only does it train teams to software craftsmanship techniques. It also makes them more agile. TDD, pairing and mobbing are agile “in practice”. In the end, this can have profound consequences on all their work.   Amitai Schleier gave a podcast about the synergy of mobbing and coaching.      Build your own job description   Let’s look back at this story.       Someone asked me for help in the first place   A change in management left me to do what I wanted for a while. I took the opportunity to offer this coaching.   By the time my new manager asked what I was doing, I could show results. We were working with teams and had great feedback.   He saw the value of it and let me continue.   You will not find a clear agile-technical-coach job description. All organizations are different, and the role is pretty new. The best way to become one is to start and invent your own job!   Become a leader badass!              By Brooke Lark on Unsplash        These pieces of advice apply even more if you are not a coach. If you are a team member, starting to act as an agile technical coach will help you to:      become a leader   make your team great   All this is not rocket science!      Start a coding dojo   Then try a few randoris in Strong-Style Pairing   You’ll be ready to start mobbing for a few hours a week   Sticking to this for a few months will transform you and your team. On top of building a great team, it will grow your natural leadership.   What’s the point of becoming a leader? Here are some benefits of growing your developer leadership:      You will make a better product thanks to more negotiation power with product people   All the team will enjoy less stress and smarter work, as you negotiate a more sustainable pace   Eventually, you’ll have a better career, as you will feel safer in your professional life. Acting as a leader builds what I call “intrinsic psychological safety”.  ","categories": ["coaching","coding dojo","mob programming","badass-developer"],
        "tags": [],
        "url": "/the-story-about-how-we-do-agile-technical-coaching/",
        "teaser": "/imgs/2020-01-31-the-story-about-how-we-do-agile-technical-coaching/technical-agile-coaching-recipe-teaser.jpeg"
      },{
        "title": "3 Good and Bad Ways to Write Team Coding Standards and Conventions",
        "excerpt":"Writing team coding standards and conventions is hard but critical. Brainstorming doesn’t work, so let’s use asynchronous decision making or mob programming!      Let’s start with a story about coding standards and conventions. About 10 years ago, I was working in an XP team at a bank. The team was working quite well:       We were pushing a new version every week   We were sitting 10 meters away from our users   They were happy 😀   We were doing TDD and pair programming almost 100% of the time   We were also trying to do collective code ownership. Unfortunately, the programmers were using different coding styles, and it caused friction.   At some point, we decided we needed to agree on coding standards for the team. I was playing the ScrumMaster role at the time. I set up a meeting for the whole team to write down coding standards.   This meeting was a failure. In 1 hour, we more or less agreed on Microsoft’s recommendations on writing C#. The topic was too broad. Without some code to make the debate more concrete, we ended up in abstract discussions.   My conclusion is that brainstormings are not the way to write coding standards.      #1. Team Coding Standards Anti-Pattern: Brainstorming    A few years later, after I came back to work at Murex, my new team was facing similar issues. I knew a meeting would not help us to write coding standards. Instead, I set up a wiki page for suggesting, discussing, and voting on conventions. We used code snippets to start the discussion. It had the advantage of documenting new agreements by design. It worked surprisingly well!      #2. Team Coding Standards Best Practice: Asynchronous decision making    The flip side is that it was slow. We would only agree on 1 or 2 conventions per month.   Fortunately, the team was also doing coding dojos. At some point, Thomas suggested doing a randori on a piece of code he wanted to refactor. In 2 hours, the whole team had discussed and agreed on 3 rules! That’s how we discovered the use of mobbing to write coding standards and conventions!      #3. Team Coding Standards Best Practice: Mob sessions    Before I dive into these techniques, let’s see what’s at stake.   The challenges of team coding conventions              By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        They have many names: working agreements, coding standards, or coding conventions. Whatever we call them, they have a massive impact on a team’s work. Here’s a non-exhaustive list of the problems that occur when they are lacking:      Recurring topics will pollute collaboration around code. For example, imagine your team does not agree about ‘How to do proper dependency injection.’ It will trigger many long discussions during code reviews, pair, or mob sessions.   There will be an inconsistent style throughout the code, depending on who wrote it. This adds a bit of mental load when reading the code, as we’ll have to adapt to different styles. In the end, it makes us less productive, especially since reading is where we spend most of our time!   It also increases the cost of collective code ownership. In extreme cases, it can make pieces of the code the exclusive property of a single developer. That’s not good for the Bus Factor!   Inconsistent and undocumented styles make open and/or inner source more difficult. It will be more difficult for people to contribute valuable pull requests.   The lack of coding standards and conventions impacts teamwork, productivity, and quality. If you are a regular reader, you know how much I care for a sustainable pace. It turns out the lack of coding standards also makes work non-sustainable:      It wastes some time   It makes collaboration more difficult   It harms the quality of the code   It makes work less fun for everyone   If team coding standards and conventions are so critical, why do we so often lack some? The truth is that they are not easy to agree on!      Opening the topic of coding standards can start an endless ping-pong discussion. Coding standards remain a never-ending work in progress until everyone gives up.   Even getting a coding-standard initiative to its conclusion is not enough! It often turns into documents that everyone forgets when facing day to day urgencies.   Some opinionated people won’t change their habits for the sake of the team.   I’ve seen team members agree on the conventions in public, but continue to use their own style. This generates passive-aggressive fights, where the most stubborn wins, but the team loses.   Let’s see go over different strategies to write team coding standards and conventions.   Why brainstorming team coding standards does not work?      The first time I tried to write team coding standards, I tried the classic brainstorming. It did not work. I now understand why:      In recent years, neurosciences have explained why brainstormings are not very useful. “It turns out that the brain needs time away for background processing to have insights”. Brainstorming might be great to generate initial seeds of ideas. But we cannot expect smart ideas at the end of a 1-hour brainstorming.    This effect is even worse for introverts, who make up for about half of the population. Meetings burn their energy.   Finally, time pressure and the brainstorming format generates abstract discussions. The goal of brainstorming is Quantity-over-Quality. This means that we throw many ideas in, but never analyze them according to a real situation.   Ok, we now know why we should not try to brainstorm team coding standards and conventions. Let’s see what works!   How to write coding standards in a team working asynchronously   When I went back to Murex, we managed to write team coding standards through the wiki. As I said, it was slow but effective.   A few years ago, I attended a talk by Bertrand Delacretaz at Devoxx FR. He presented a structure for asynchronous decision making used in open source projects.       Asynchronous Decision Making - FOSS Backstage 2017  de Bertrand Delacretaz   It turns out we had been re-discovering a well-known collaboration technique. Here is how we could map its steps to writing team coding standards:      0. Emergence: During a code review, for example, you notice a pattern that could be worth a coding standard.   1. Brainstorm: Through the team’s chat (or wiki), start a discussion on a specific rule. The goal at this point is to get as many points of view as possible and trigger conversations.   2. Options: Continue the discussions until a consensus starts to emerge. As Bertrand explains, a consensus is not unanimity.   3. Consensus: Once a large enough consensus is reached, propose a coding-standard rule. Bertrand explains that we need a tool with reliable traceability here. For coding standards, our VCS seems to be the obvious choice.            Create a markdown file inside your codebase with all the details of the coding-standard rule.       Explain the coding-standard meaning, pros, and cons.       Add a few code samples and a link to the chat (or wiki) discussion that led to this proposal.       Create a pull request and link it from your chat discussion.       Update the pull request based on feedback.       Or cancel it and move back to the previous step if needed.           4. Decision: Once the team approves the pull request, merge it!   As Bertrand explains in his talk, this is not a linear process, you might get back to previous steps if needed.      💡 Asynchronous Decision Making has Living Documentation built-in!       There are many advantages to this technique:      It’s documented by design. This will be useful for future team members and contributors.   Similarly to Architecture Decision Record, the coding-standard decision is traceable. In the future, the team will be able to review how and why it agreed to this coding-standard. This will help future team members to review, update, or delete this rule.   As it is meeting-free, it’s an introvert-friendly technique! Did you know that inclusive teams make better decisions?   Finally, it’s the only option for teams that span many time zones   The main drawback, as we experienced and as Bertrand confirmed:      Asynchronous decision making takes more time… but also yields to better decisions!    How to write coding standards in a team working synchronously   I currently do a lot of mob sessions with the teams I coach. I knew that team coding-dojos are great to discuss coding standards. Mob sessions are even better!   Team coding standards emerge during mob-programming               Here is how:      Real situation. What happens in a mob session is like what happens in a coding dojo randori. As everyone sees the code, people will ask why it is being written this way. The advantage is that the team can discuss real production code, not only kata examples. This ensures that the coding standards remain grounded in the actual team context.   Start small and improve. Discussions happen when the problem is fresh in everyone’s mind. The standard does not have to deal with every possible case. It’s more effective to stick to the current problem’s specificities. It will grow and generalize the next time the mob faces a related design question.   Fast. A side effect of not trying to deal with every case is that it can be quick. The team can agree in a few minutes, not weeks!   Understood by design. As everybody is there to discuss the standard, everybody has a chance to question it. This ensures that all the team will understand the rationale, the pros, and the cons.   Here is an example. A few weeks ago, we started to coach a team that wanted to improve its testing practices. As we always do, we started the coaching with an intensive week of TDD coding dojos. After this initial phase, the team wanted to test this new way of writing code by re-doing a small feature. This was the first mob topic we did with the team. One of their classes was challenging to test because it had too many constructor arguments. The mob decided to start using observers for dependencies that only receive notifications.   That’s the typical ‘lightweight’ coding-standard a mob will add to the team’s oral tradition.              From New Relic’s blog post Taming the Mob, Part 2: Mob Programming Is Like Tending a Campfire        What makes it work so well?      No Need for documents. As this example shows, there is no need to document. When a team regularly works as a mob, it will grow a body of oral tradition. Do not doubt the effectiveness of oral tradition! We have shared knowledge this way for most of our history! Homer’s Iliad and Odyssey came to us through Oral Tradition. In comparison, the oldest known printed book is only about 1200 years old!   Resilient. A single team member is enough to remind the mob about a coding standard and to make it stick! It can be a different person for every coding convention! This is the critical point that makes it work so well. When solo programming, everyone needs to remember and be disciplined. With pairing or code reviews, we need roughly half the team. With mobbing, 1 person is enough!   Mini-retrospectives. Mobbing makes it possible to do frequent quick-retrospectives. Many successful mobs do them:            Woody mentioned them when he presented Mob Programming.       Here is the format we borrowed from the Cucumber Pro team:               Far from the Mobbing Crowd  de C4Media   Quick-retrospectives are a way to think again about coding standards and conventions. The “decide” question, in particular, is when the team commits to a new agreement.      💡 As long as at least 1 person remembers the coding standard, it will stick in the mob!    If you are still with me, you must be interested in trying Mob programming for coding standards. Unfortunately, mob programming can look daunting! So let’s see how to start.   How to inject mob in your team   Maybe you tried to jump in mob programming without any preparation. Or perhaps the first sessions were a bit rough! Here is what often happens:      It feels unnatural because it’s different   It’s slow because people don’t know what they should do.   On top of that, all kinds of debt become obvious and painful!   Lousy code and technical debt are suddenly discussed   Conversation debt kicks in. People have the essential conversations they never had. For example: about coding standards and conventions.   Tooling debt, like slow builds, embarrasses everyone. The team might have been waiting for a slow build for years, individually. It’s only when everyone is waiting together that it becomes unbearable!   If your first try looked like that, you might have been afraid to continue. Don’t panic, you might just have gone too fast.   Starting mob programming is not as difficult as it seems. People need a bit of support, though. It’s like boiling frogs:      Start a regular team coding dojo (weekly or every sprint). Sell dojos as a way to learn together.   Once you are used to randoris, try a few with strong-style pairing mode. This is already mob programming, albeit on a kata. This is how you’ll learn the discipline to make mobs work.   Then wait for the next time you face a challenging task. For example: a design decision, some tricky legacy refactoring, or a nasty bug. Then ask the team to help you in a randori-like session.   Repeat this a few times. End every session with a quick-retrospective. Chances are mobs will become part of your regular practices.   Let the team master mob programming at its own pace. Persevere as the benefits of mob programming become more and more evident.   If you are still struggling, find a technical agile coach to help you!   How can a technical agile coach help?   I mob with teams as a way to apply what we practice in katas. A coach will help you to do effective mob programming. He will also use mobbing to help you build coding conventions:      By asking the right questions. The team will come up with good answers.   By reminding the team with “Is this a convention?”   By suggesting to dig further. Sometimes, the team will have more questions than answers. A coach will make the team aware of its indecision. He can then either            provide guidance       suggest digging into this before next session       or prepare a kata           And guess what? Someone from the team can play this role too!   Start to write team coding standards and conventions today!      If you don’t have coding standards or conventions, you can start to write one today!   If you do more asynchronous work, start a chat discussion about a particular point.   Otherwise, start a coding dojo randori. You might wonder about the cost of mob programming, though. In 3 long-term benefits of mob programming that make it cost-effective, I have my take at why mobbing is cost-effective in the long run!   Here are my last bits of advice, for you, self-declared change-agent.      Keep your goal in mind and lead by example.   Celebrate when you and your team have agreed on your first coding standard.   Motivate your teammates to start similar discussions.   As you agree on more and more conventions, the team will become more and more effective!  ","categories": ["remote","coaching","mob programming","coding-dojo","team building"],
        "tags": [],
        "url": "/3-good-and-bad-ways-to-write-team-coding-standards-and-conventions/",
        "teaser": "/imgs/2020-03-16-3-good-and-bad-ways-to-write-team-coding-standards-and-conventions/3-good-bad-ways-to-write-coding-standards-and-conventions-teaser.jpg"
      },{
        "title": "Best open source tools for remote pair programming",
        "excerpt":"COVID-19 has thrown us into remote work. Here are 3 world-class open-source tools for remote pair programming that you can set up today.      As I write this today, the COVID-19 pandemic is violently attacking us. More than 2 billions people are confined at home! It’s the first time in mankind history that so many people are working from home.   In the last decades, more and more people have started to remote-work. I’ve been doing more and more remote work. In the past few months, I’ve enjoyed 60% of my time working from home. Unfortunately, choosing remote work and being suddenly forced into it is entirely different.   Did you know that remote pair or mob programming can make your remote team more effective? Even if you have been doing local pairing or mobbing, adjusting to remote work might be a challenge, though. Tools exist, but most of them are proprietary. In today’s situation, going through a company’s full purchase process is too long. Your company’s IP policy might also prohibit you from using these tools!   Fortunately, they are some open source alternatives! They are different, yet as effective, if not more, as proprietary solutions! You keep the data, you don’t need to ask permission, and you can set them up right now. With them, you will be able to get a world-class remote pairing experience before the end of the day!   Before I go over the solutions, let’s first talk about screen-sharing…   Why giving control over screen sharing sucks      I first started to remote pair program at Murex in 2014. The team had been doing local pair programming for a few months when Ahmad joined us from Beirut. We did not want to abandon pair programming. We resorted to screen sharing with Skype for Business (Lync at the time). Even after we tweaked it as much as we could, it remained a painful experience. To make the case clear, here are the pros and cons of screen-sharing and remote-control:                  The pros       The Cons                       Most of the time, you will already have the tools       You’ll suffer a substantial lag when typing on the remote machine                 It does the job       It’s painful to adapt to different keyboard layouts when typing remotely                         It’s not easy to point to a line of code. We often resorted to giving the line number or full control over the screen.                         Finally, it does not work if you have a bad connection. The full-screen video goes over the wires which makes it pretty heavy on bandwidth           The main tweak we did to make it less painful was to adopt the Pomodoro technique to:      take regular breaks   use the break to send the code over to your buddy   switch driver   My conclusion is that we should avoid default screen-sharing for pairing. There are open-source alternatives that are very easy to setup. You have no reason not to use them!   Open-source alternatives are few but world-class!   By combining open source bricks, you can set up an effective remote pair programming tool. Today, I know of 3 such combinations:      TMux + terminal + editor   Git pull-push loop script + screen sharing   Saros + screen sharing   Let’s go through them in more details   TMux + Terminal + Editor   TMux is a command-line terminal-multiplexer. It lets you set up a terminal session on a server and have ‘client terminals’ share and interact with it. You can even split the terminal in panes, and see simultaneously:      a terminal   a command-line editor like Vim or Emacs   a test runner   a log, from your server for example   Here is what it looks like:              Screenshot from a video by Ham Vocke. You can find the full video on his blog.        Everyone sees the same terminal. Anyone can type in at any moment!                  The pros       The Cons                       Collaborative, real-time edition       No IDE                 It’s easy to point at a line       All the team needs to use the same editor                 No lag thanks to extra light network usage       Anything that happens out of the terminal is not visible. This can be an issue for manual tests, for example.                 Everyone can see everything that happens on the shared terminal                         It works from any platform, thanks to the use of core tools                         Any bare server can host the TMux session                   Most of the time, you’ll be able to work around the cons with TDD and automated testing. With a good test harness, running the full app, or starting the debugger becomes very rare. To see what’s happening on someone’s screen, you’ll have to resort to a short session of screen sharing.   Here is what my friend Arnaud Bailly, who’s been remote pairing for 10 years says:      It’s for me the best solution. It’s the fastest interaction, both with the machine and with the people. It’s easy to switch between driver and navigator roles. The keystroke quality is optimal (there is no lag or visual artifacts). The whole team shares a single environment (enforces XP coding standards by design). It’s also easy to automate the VM setup, which makes it very smooth to create new ‘Rooms’ when needed and ensures that the dev environment is always up to date with the code…       The only glitch is that you need to get used to working with one editor in text mode. At Symbiont, we were using Spacemacs (emacs + vim), and everyone was ok with it. I’m more of an Emacs person, but it was still working ok.    If you want to learn more about how to set TMux up for remote pair or mob programming, here are a few useful links:      Remote Pair Programming Made Easy with SSH and TMux by Ham Vocke   Remote Pairing: Collaborative Tools for Distributed Development by Joe Kutner has a chapter on TMux   Here’s an extra tip we owe to Emmanuel Gaillot:      Add transparency to your terminal to see the other mobsters behind it!               I built this illustration by superposing a screenshot from Swapnil Singh’s dotfiles repo on top of a zoom gallery view from the official Zoom documentation. Here is a higher res picture.        Open-source alternative 2: git pull-push loop   As you might know, if you are a regular reader, I am currently an Agile Technical Coach at Murex. I spend a lot of my time mob programming with teams on katas or their daily tasks. Most teams at Murex are somewhat distributed. As a result, we’ve been experimenting with remote mobbing a lot.   A few months ago, Ahmad and I decided to give TCR (Test &amp;&amp; Commit || Revert) a try. As Ahmad lives in Beirut, we needed a remote setup from the beginning. I found Thomas Deniffel’s posts, and we started from his TCR Variants scripts.   These sessions proved very interesting. Trying TCR was great, but the scripts themselves are too! They watch the file system for changes to automatically test, commit, revert, pull, and/or push.   We started to play with a ‘gentler’ version of these scripts that      did not run the tests   did not revert   but continuously pulled and pushed to the current branch   When all mobsters run this script on the same branch, all their local repos are synced in almost real-time.   The flow      Here is how to use this setup when remote pair or mob programming:   When you start on a new task      The first driver shares his screen   He pulls a new (short-lived) work branch from ‘master’ if you are doing trunk-based development   He starts the watch.sh script   Other mobsters pull the new branch and check it out   They start the watch.sh script themselves   As in a local mob, they don’t touch the code when they are not driving   When you switch driver      The previous driver stops typing and sharing his screen   In the background, the watch.sh scripts sync the latest code changes to the new driver’s repo   The new driver starts sharing his screen   He compiles and runs the tests to make sure everything works as it used to work on the previous driver’s env   When you want to commit      The driver stops the watch.sh script   He rebases and squashes all the changes on master with a clear commit message   Once he’s pushed on the master branch, he goes back to his work branch and starts the watch.sh script again   The pros and cons   Here are the pros and cons of this technique:                  The pros       The Cons                       Everyone is working in his own IDE and preferred personal environment       There is no collaborative code edition                 There is no lag as you’ll be typing on your own machine       You cannot quickly point to a line of code                 You don’t need an extra server outside your current git team repo       You will need a bit of discipline to make sure not to change code while you are not the driver                 It works from any platform as it only relies on git and the shell                   A screen-sharing tool is mandatory to see what happens on the driver’s machine. Proper screen-sharing tools will also workaround some of the cons:      They will let you have multiple cursors   You can use them for extra short ‘take control’ sessions   Some screen sharing tools even allow you to collaboratively sketch design diagrams on a virtual canvas!   The scripts   Here are the scripts we currently use:   watch.sh   #!/bin/sh  # Loops on git pull-push in a remote mob  while true do   ./pull-push.sh done   pull-push.sh   #!/bin/sh   # Pulls and pushes the current git branch. # Use it to quickly switch driver in a remote mob or randori  export CURRENT_BRANCH=$(git symbolic-ref --short -q HEAD)   git pull origin \"$CURRENT_BRANCH\" git add . git commit -m 'Working...' git push origin \"$CURRENT_BRANCH\"    We’ve tested this setup with a few teams now, and the feedback is unanimous: people love it!   Open source alternative#3: Saros   The last open-source alternative I know of is Saros. Saros is an IDE plugin that lets you do distributed pair or mob programming. It gives you multiple edition cursors in the IDE editor.                  The pros       The Cons                       Real-time collaborative code edition       It only syncs the code editor.This means that you cannot see the rest of the IDE and the screen. In particular, it does not display execution of automated or manual tests                 It’s easy to point to a piece of code       It currently only supports IntelliJ and Eclipse                 It has light network usage compared to a screen sharing system       You’ll need to install an XMPP chat server to make it work           In the end, you’re most likely to also need a screen-sharing tool to get your Saros solution to work!              Animation by the Saros team for the Eclipse marketplace plugin.        Open source alternative#4: conferencing software.   You will always need an excellent conferencing and screen-sharing system. If you don’t have one already, here is a list of open-source alternatives to skype.   Proprietary tools   They are now proper proprietary tools for remote pair programming and mobbing. This is not the main topic of this post, but I felt it would not be complete without them. The ones I know of fall into 2 families:      Real-time collaborative code edition tools (like Saros)   Improved screen-sharing software, optimized with pair programming specific features   These commercial tools cost money but offer professional support and easy setup. Let’s see the pros and cons in a bit more detail.   Real-Time Collaborative Code Editors   For example: VS Live Share, Floobits, Code Together                  The pros       The Cons                       Real-time collaborative code edition       It only syncs the code editor. Which means that you cannot see the rest of the IDE or the screen. In particular, you don’t see test execution or a manual test                 It’s easy to point to a piece of code       These tools are often platform-specific                 It has light network usage compared to a screen sharing system       The code is sent through the network           Improved Screen-Sharing   For Example: Tuple, Use Together                  The pros       The Cons                       You can have multiple cursors on the same screen       You might still suffer from typing lag if you have a low bandwidth                 It’s easy to point to a piece of code       These tools are often platform-specific                 These tools have extra pair-programming-specific features like drawing on the screen                         Compared to traditional screen sharing, network usage is optimized for pair programming                   I had particularly good feedback about Tuple from different people (NB: this post is not sponsored):   our team is experiencing with two tools for a true remote pairing session: @PairWithTuple (pro: very low latency, still the best tool on OSX, cons: only for OSX) and @USE_Together (pro: available for OSX, Windows and Linux (alpha), with a steady roadmap of upcoming improvements) &mdash; Pietro Di Bello (@pierodibello) March 22, 2020   Or from a former colleague:      It just works!    What should you do?   COVID-19 suddenly threw you in remote work?      Don’t wait for a corporate decision about the best remote pair programming tool to use. The situation we are in is chaotic, and we must act now! It’s the perfect occasion to refactor your organization and inject new practices.    Keep in mind that there is no one-size-fits-all tool for remote pair or mob programming. That said, here are my own recommendations. You should be able to set up a best-of-breed remote pairing and mobbing solution before the end of the day:      Use TMux if you can. If your team is not using an IDE, or if you can be as effective without one, then TMux is a no-brainer.   If you need an IDE, set up a git pull-push loop in 5 minutes. You’ll get something that works well, that will not change your habits, and with no extra tooling. This the solution we are currently using at work.   You might also try Saros if it supports your IDE. Test it and see if it works for you.   Finally, there might also be a proprietary solution that will work for you! If you have the bandwidth, the money, a quick process to buy a tool, and need an IDE, here is the way to go:            Short-list them based on your platform and preferred features,       Try them       Buy what works best for you!           I hope this little guide can help those of you who suddenly have to pair or mob program remotely. Remote pairing or mobbing can be a pretty enjoyable and sustainable experience. If you want you can learn more, here are 7 Remote pair programming best practices Q&amp;A.   Keep hope, persevere, and you’ll soon be as effective as you used to be!  ","categories": ["remote","pair programming","mob programming","extreme programming"],
        "tags": [],
        "url": "/best-open-source-tools-for-remote-pair-programming/",
        "teaser": "/imgs/2020-03-28-best-open-source-tools-for-remote-pair-programming/covid19-thrown-in-pool-of-remote-work-teaser.jpeg"
      },{
        "title": "3 compounding benefits of mob programming that make it cost-effective",
        "excerpt":"Mob programming requires time to get used to it. With practice, though, you’ll discover unexpected and compounding long-term benefits of mob programming.      As a technical agile coach, I practice mob programming in 2 contexts.   First, I coach teams through mobbing.   Second, I mob with the other coaches to practice and prepare katas. We are also mobbing on different topics:      To increase skill sharing.   To avoid delaying work because of our individual constraints.   Through experience, we discovered some unexpected long-term benefits of mob programming:      Mob programming reduces the amount of work to do   Mob programming makes continuous improvement more effective.   Mob programming improves design discussions.   Let’s go through these in more detail.   1. Less work to do   I attended his talk about #NoEstimates at NewCrafts Paris. He asked us to estimate the time it would take to resolve a small math puzzle. He then asked us to solve it and compared the time it took us with the estimate. He did a few iterations of this with variations of the puzzle.               The conclusion was that the amount of work does not depend on how complex the task looks, but more on:      our knowledge about the domain   our understanding of previous solutions   Any developer who has worked on legacy code knows that even simple features can take months to build. This is for two reasons:      We are often not familiar with a legacy system’s design   The legacy system’s design is often unnecessarily complicated   As a result, tiny changes can have significant undesired consequences. In fact, these 2 dimensions are what drives the long-term productivity of a team.              I found this game ‘Decrocher la lune’ (which translates to ‘Reaching the moon’ in English) to be a great metaphor for working with legacy code. The goal of the game is to stack small wooden ladders as high as possible, but without making them fall. I once setup the game in the office cafeteria, with a poster explaining the legacy code metaphor.        Here’s the thing: Mob Programming both builds knowledge and simplifies the current system. Let’s see how…   Mob programming makes the code base smaller.   20 years ago, Alistair Cockburn did a study on pair programming. It concludes that pair programming makes the codebase at least 10% smaller. My personal experience backs up this study. I would even have estimated the gain to be more than 10%.   This results from more knowledge about existing code in a pair of brains than in a single one. Two developers working together spot more opportunities for reuse. In the end, these small reuses compound and make a significant impact on the codebase size.   Mobbing has an even more significant impact, as there is more knowledge in a mob than in a pair.   In the end, a smaller code base is easier to work with and makes adding new features faster.   The mob is allergic to debt.   Another benefit of mob programming is that it makes debt evident and unbearable. Debt slows you down for stupid things. Debt is always painful, but it becomes intolerable when the full team works together.   Suppose your build is slow. Every time you build, all the mob is going to wait, doing almost nothing. It won’t take a lot of these group ‘breaks’ before the team decides to tackle this slow build.   As a result, the system remains simple and easy to extend.   More knowledge sharing happens in the mob.   There’s not much to explain here. Working together on the same task will share knowledge about the current system.   When a new feature request comes in, the mob will have as much knowledge as possible to build it. As a result, it will be ready sooner.   It compounds!   Here’s the most exciting thing: these benefits compound over time! In fact, they even support each other.              By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        Over time, this makes a massive difference in both       the system’s complexity   the team’s knowledge about it   As a result, features that would take days to build in a traditional team only take a few hours.   2. Continuous Continuous-Improvement   I wrote before about how retrospectives only take us as far as known best practices. As an alternative, I’ve had great success using the improvement kata.   Since then, I’ve also used the improvement kata on my personal tasks. It has proven even more useful. It has helped me to design a productive and sustainable pace. Currently, in these times of lock-down, it’s also helping me to get a grip on the new situation.   Finding data   Unfortunately, gathering data can be a real challenge. The improvement kata is a data-driven technique: no data means no improvement!   Sometimes, you will manage to extract the data out of your existing tools. In the past, we managed by digging data out of JIRA. That’s also what many data-driven tools are doing (Code Velocity, IdeaFlow…). (BTW, these tools are, in fact, packaging the improvement kata around a particular problem. This might be an interesting post!)   When you have a specific problem and no data, you’re kind of stuck. Collecting data should not be that difficult! Gathering data on my personal tasks is very easy. I just make sure to log what I need every day for a while. A simple spreadsheet is enough. After a few weeks, I’m able to accurately understand what is going on. I can then try and measure the impacts of experiments.   The teamwork challenge   In a team, daily data tracking is a lot more complicated. I’ve always had a hard time getting the whole team to log data. Team members might not all agree on the importance of the problem. Some others might have something else in mind. And some might be afraid of the big-brother potential of this practice.   One solution is gamification. For example, Joe Wright used stacks of lego bricks to represent time. That’s a friendly solution when it works. Unfortunately, sometimes it’s not possible:      Lego bricks won’t help you if your team is remote.   If you need small-grain tracking, you might never have enough bricks.      It’s a lot easier in a mob.   In a mob, this problem disappears. A single team member caring about data tracking is enough! The privacy concern goes away too. We are logging the mob’s activity, not that of individual developers.   We might also wonder if this is not a return to old-style command and control process improvement. I’ve seen this in practice. It does work, but at the cost of high turnover and disengaged programmers.       Disengaged employees are 34% less productive [Forbes].    Mob-style continuous improvement has nothing to do with this. A mob is empowered and autonomous by design. This actually increases engagement! Mob-style continuous improvement will just blow the old-style process improvement away.   It compounds!   Even a modest 1% improvement per month will become massive in the long term. The Coronavirus epidemic should have taught us the power of exponential growth!              Exponential Growth curve, from Wikipedia. The continuous continuous-improvement benefit that comes with mob programming yields to the productivity increase of the green curve.        3. Higher quality design discussions   The smart keyboard   Here is the central revelation I got from Woody Zuill’s mob programming masterclass:      The developer acting as the driver is a ‘smart input device.’    Like a genius keyboard! He takes care of compilation issues, semicolons, formatting, and a lot of other stuff.      Let’s go through an example. A few weeks ago, we were practicing the roman addition kata. I was driving when the mob asked me to rename the tests using some uppercase letters for roman numbers. We had names all our tests xxx_plus_yyy_is_zzz. As a smart keyboard, this renaming fell in my area of responsibility. I checked the internet to know if Intellij could do some uppercase regexp / replace. It turned out it could! I worked in parallel with the rest of the mob. My colleagues were following some higher-level discussions. I worked out my regexp and did a global file replace.   Now that I master uppercase regexp replacements in IntelliJ, I can say that I’m a smarter keyboard. Not only this, but the whole mob knows that the driver can deal with similar queries in the future.   Higher-level programming   Text replacement is a straightforward example. The same phenomenon happens around coding standards, conventions, work habits, and even language.      The more the mob works together   The more it grows its own folklore   The more the driver understands high-level queries   The more the rest of the mob can have high-level discussions   Eventually, the more productive the mob is.              By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        It’s a bit like if the driver was raising the level of abstraction of the programming language. Similarly to a higher-level language.   Here is something Fred Brooks states in The Mythical Man-Month:      Developer productivity, in lines of code, is independent of the programming language.       This means that if you can write 4 lines of assembly in 1 line of C, switching to C will make you 4 times more productive. Similarly, if the mob driver lets you write 4 lines of code in 1 instruction, then the mob is 4 times more productive.   Granted, measuring productivity in lines of code is not a good idea. Still, the logic explains why Mob Programming is not as slow as we could fear.   It compounds!   Don’t forget: the more the team mobs, the smarter the driver becomes!   Seriously… how can mob programming be productive?   We’ve seen 3 unexpected benefits of Mob Programming. Still, mob programming remains very counterintuitive! Selling mobbing to your boss or colleague will always be a hard bargain. I’m not the first one to try to explain this, for a larger perspective, have a look at these posts:      Mob Programming - the Good, the Bad and the Great.   Here’s the science behind mob programming.   There are so many variables in software that measuring productivity is almost impossible. What we can do, though, is to check how the unexpected benefits I mentioned earlier make a better case for mob programming.   Long term impact   The first thing we notice with these unexpected benefits is that they compound!      If you ever read a personal finance book, you’ll know the importance of compound interest. The same goes for mob programming.   The glitch is that there is an initial cost to mob programming. A good strategy is to start with only a few hours of mobbing every week until the team gets good at it. You can then slowly raise the ratio. You could even start with a coding dojo before production work mobs.   As the long-term benefits compound, the mob will become more and more productive. More productivity means more features for the users and a sustainable pace for the team.   Be careful, though. Full-time mobbing might not be suitable for everyone in the team. Some people, especially introverts, will need some solo time to recharge batteries. You can get the perfect ratio for your time trough trial and error…   But you can also have a structured conversation!   A business case for mob programming   At Murex, we tried a business case for pair programming with a team. We went through this course, Making the Business Case for Best Practices. We extended the pair programming case to take a ‘ratio’ of pairing time. This allowed us to take into account a team member who was very reluctant to pair program.   We found out that the optimal ratio of pair programming was around 20%.      Between 0% and 20%, the benefits of pairing grew. Past 20%, the moral tax would be too heavy on the team!   A business case makes it a lot easier to discuss this kind of issue a priori. It’s challenging to voice your burnout to the team when all the team in enjoying pairing.   We can do something similar with mob programming. The Pluralsight course does not include a section on Mob Programming. With fellow technical coaches, we brainstormed the costs and benefits of mob programming. Here is what we found at the time:   Benefits      Pair programming Benefits +   Reduced time spent in daily meetings   Reduced time spent in sprint planning/backlog grooming meetings   Costs      Adapting management practices   Whole team doing the same task   Mobbing setup   Morale dip   We could now also add the long term compounding speed up. Unfortunately, I don’t know how to measure it. My best shot would be to brainstorm a speedup of X% per month when working as a mob. A human-friendly way to think of exponential growth is ‘How many months for it to double?’.   For example:      If a team did work 4 hours per day as a mob, I could expect its productivity to double every year. You can then use the Rule of 70 to deduce your monthly speed up. In our example:     70/12 = 5.83%     This means 5.83% more features every month.    We could then complement the business case with this figure. As Erik Dietrich explains in his course, be ready to play with this number until everyone is ok with it.   This exercise will not only help you to sell mob programming to your boss or team-mates. It will also show you what you are trading off between solo and mob programming. It will help every team member to take a step back from their personal gut feeling. Eventually, this will trigger more in-depth discussions. It’s a bit like a Big-O analysis for best practices.   A new way to split the work?   If the compound effect and the business case still did not convince you, let’s try another story!   Here is yet another way to think of mob programming. Mob programming makes up for work parallelization by reducing mental task switching.   Let’s go over all the different tasks we do when we program:      Type in some code   Run compilation   Fix compilation errors   Reorganize code for readability   Think of design (at function and at larger scales)   Apply automated or semi-automated refactorings   Think of performance (at function and at larger scales)   Write tests   Think of edge test cases   Keep track of where you stand on the main task   Run tests   Run the app   Debugging   Write down things that we could improve   …   This sheds new light on ‘focused solo programming.’ We are not that focused! We are continuously task-switching from one activity to the other.   Let’s look again at the long-term benefits of mob programming I mentioned earlier      in 2, Continuous Continuous-Improvement. One person focuses on continuous improvement   in 3, Higher quality design discussions. One person is typing, building, and doing many low-level activities. The others concentrate on design   Woody Zuill explains that mobbing enables the mental state of ‘flow.’ The driver is taking care of all the low-level details of writing code. The rest of the mob is left with:      Thinking and making decisions about design   Thinking of performance   Deciding about the next test cases   Keeping track of where you stand on the main task   Keeping track of things that we could improve   That’s already an impressive reduction in Work-In-Progress! But wait, there’s more! You can decide to focus on a single topic, with the assurance that others in the mob deal with the rest! For example, you can concentrate on continuous improvement for half an hour (cf point 2). Or you could spend some time thinking of edge cases for tests.      In the mob, you can stay in the zone!               By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        The first thing we can see is that the same work is happening in both work configuration. Let’s now count how many context switches the team goes through during this 20 minutes work session:      working solo: 28   as a mob: 10   This is 3 times less. Even if these micro context-switches only cost few seconds each, this is enough to explain how a mob can be more productive than solo developers!   I’ve already seen this pattern!   To run fruitful meetings, Sociocracy suggests setting up different roles for attendees (Secretary, Leader, Facilitator, Delegate…). Maybe we should try to explicit ‘roles’ in the mob and see how this works? I could think of:      The tester   The continuous improver   The ‘performancer’   The maintainer   The tracker   …   Nick Tune suggests a similar scheme for Remote Team Flow EventStorming for Retrospectives. They managed to run a remote Flow Event Storming by using a few of these roles. To avoid overspecialization-blindness, they round-rob these roles every 10 minutes.   I’ll write about this as soon as we get the chance to try this, so stay tuned!   What do you think?   I’d really love to read about your own experiences with mob programming.   Maybe you:      Noticed the same effects I have mentioned?   Experienced other surprising long-term impacts of mob programming?   Have experimental data to back up some of these effects?   Noticed the exact opposite of what I wrote about?   Or found new ways to sell mob programming to your team?   Whatever your story, I’d love to read it in a comment below.   Happy mobbing!  ","categories": ["mob programming","business value","continuous improvement","technical debt","team building"],
        "tags": [],
        "url": "/3-long-term-benefits-of-mob-programming-that-make-it-cost-effective/",
        "teaser": "/imgs/2020-05-15-3-long-term-benefits-of-mob-programming-that-make-it-cost-effective/mob-programming-time-capsule-teaser.jpg"
      },{
        "title": "How to set up pairing office hours to avoid teammates interruptions",
        "excerpt":"Part of the tech lead role is to be available for help. Setting up office hours and pair programming is vital to avoid the mental load of task switching.      With everything a tech lead has on his plate, finding quality time to help your teammates is tricky! You know the story:      Every time someone comes for help with a problem, I suffer mental task switching to dive in his work!     It’s difficult to find the mental availability required to give good code reviews!     How am I going to finish this feature when I am interrupted all the time?    If this resonates with you, here are a few tips that will help you!   Task switching is driving us mad!      I remember my first time as a tech lead. Out of habit, I continued to work on the challenging features I used to. The glitch was that now everyone in my team was coming at me with their problems! It quickly resulted in:      Useless task switching   Stressful mental load   Painful long hours   And late deliveries   Some of the primary responsibilities of a tech lead are about assisting your team:      Review teammates’ code   Discuss design with them   And help them to remove a blockage   There is no way you can escape this part of the job.   It took me a while to find a way around this challenge. As we’ll see later, eXtreme Programming made the problem a lot simpler. But I also needed to learn my first lesson as a new tech lead: the fundamental difference in schedules.   Manager’s vs. Maker’s Schedule   Paul Graham wrote a now-classic essay Maker’s Schedule, Manager’s Schedule. It explains that programmers and managers organize their time in very different ways.   Programmers need long slots of uninterrupted time to focus on deep work. Managers need to jump from one meeting to another. The later cope with this task-switching because they don’t need to go deep on any topic.   These different schedules are a particular challenge for Tech Leads, who need to do both! As Jayesh Lalwani wrote on Quora:      A Tech Lead is by definition, someone who is too good to be “just” a programmer […] Once, you get there, you are parked in this halfway house of being part manager who isn’t management, and part architect who isn’t the architect.    As a tech lead, you need to find time for both Manager and Maker schedules.   Office hours to the Rescue   Office Hours definition, according to Wiktionary:      A pre-arranged time when a person whose occupation frequently takes them away from their office during working hours is available in their office to answer questions or provide assistance without the requirement for an appointment.    Setting up office hours to help your teammates is the most effective way to reduce task switching.   A drawing is worth a thousand words. Here is why office hours are important. Without office hours, your day will likely look like that:      After you have set up office hours, it should look something like this:      Booking office hours in your calendar will have 2 effects:      It will contain interruptions within a predefined time slot   It will also group other meetings during the rest of the day. Once you have blocked out a time-slot in your calendar, people won’t set you meetings there!   The main benefit for you is a massive reduction in task switching. At the end of the day, this means less mental load and calmer work!   Max out office hours with pair programming   Office hours work great. Here’s a slight twist to make them even more useful for you and for your team. When someone comes at you for help, default to pair programming.   Why should you favor pair programming over other kinds of help?      It’s the most effective way to up-skill your teammates. If you want to read more, here is what happened in a previous team where we introduced pair programming.   It transforms after-the-fact reviews and validation into synchronous collaboration.   These 2 combined make it the fastest way to build an autonomous team.   I hate doing code review. I love pairing. I don&#39;t want to exert judgement on your code, I want to collaborate on code. &mdash; Jessica Joy Kerr (@jessitron) August 25, 2016   Pairing is a long term investment. In the long run, it will make you dispensable and create more time for your deep work!   In practice, it’s straightforward:      Stop answering questions or doing asynchronous code reviews   Start moving to their desks, or asking them to share their screens   As developers, we refactor code to make it easier to work with. Introducing some pair programming is a first step to refactor how your team works. With time, both kinds of refactoring compound and create an environment for calm work.   Set pairing office hours now!   Here is the 5-minutes fix you should do now to stop this task switching that is making you crazy.   How to setup office hours:      Book a slot in your calendar: 1 or 2 hours, every day, at the same time.   Invite your team as optional participants.   Present your initiative to your teammates. Share your pains and ask for feedback. Use the medium that makes the most sense to you and your team: email, chat, meeting, or anything else.   Have some work ready in case no one needs you. Reviewing code is fine, but you can also use this time to work on tasks that have low priority and don’t need a deep focus.   Regularly ask for feedback from your team to know how it is working for them. If you have retrospectives in place, this can be the perfect time for this. Otherwise, just make sure to get regular feedback!   Use your team’s feedback to iterate and fine-tune the best formula for you and your team.   You could go even further by blocking maker hours, as out of office, in your calendar! That’s a bolder move, though, and a good topic for a follow-up post. Stay tuned!   I’d really love to know how you deal with the tech lead’s task switching. Don’t hesitate to share anything through the comments.  ","categories": ["personal-productivity","management","tech lead","code reviews","pair programming"],
        "tags": [],
        "url": "/how-to-set-up-pairing-office-hours-to-avoid-teammates-interruptions/",
        "teaser": "/imgs/2020-06-22-how-to-set-up-pairing-office-hours-to-avoid-teammates-interruptions/pairing-office-hours-open-teaser.jpg"
      },{
        "title": "How to make your team learn TDD for Legacy Code, and love it!",
        "excerpt":"Legacy code is where TDD is the trickiest. Start practicing the gilded rose kata with your team today! Your teammates will soon feel safe to apply TDD for legacy code.      A few days ago, a former colleague told me his current team was reluctant to give TDD a chance. Here are the typical comments he heard:      “Adding a test in this old code is a three weeks task!”     “All the tests we wrote in this code are mock-ridden, unmaintainable, and useless!”     “How can we go tests first, when the code is already written?”    If you have worked with teams dealing with legacy code, you must have heard similar complaints.   The barriers to TDD for Legacy Code      All these difficulties boil down to two root causes:      It’s impossible to write tests before code that was already written.   Legacy code makes everything more complicated. In our case, it increases the learning curve of TDD for the team.   The plan   To get a team dealing with legacy code to do TDD, you’ll need to address these two fundamental barriers.   One option could be to put more pressure on the team: “From now on, TDD is mandatory.” This strategy is not going to work for long. We want a sustainable pace, and the only way to get it is through regular learning.      The first step is to coach the team to one effective technique to apply TDD in the legacy code context.   The next step is to leverage this momentum to set up regular practice sessions.   A straightforward technique      Here is, in my opinion, the most useful and straightforward TDD for legacy code technique. Let’s say you need to add a new feature:      Refactor the existing code to introduce an extension-point to host your new feature   Implement your new feature using TDD. Make it compatible to your extension-point   Once your new feature is ready, plug it in the legacy code through the extension-point   Check that the whole thing is working. Use whatever test you have, either manual or automated   If it’s not working as you expect, repeat the whole cycle (adapt extension point, TDD your new feature…)   All this might sound very easy in theory. Unfortunately, it can get pretty complicated in practice! Before trying this in production code, it’s a good idea to practice and learn. Let’s see how to do that!   The Gilded Rose Kata   The gilded rose is a famous kata to practice refactoring techniques. I am going to use it to illustrate the above plan.   The gilded rose is an inventory management function. The goal of the kata is to manage a new kind of item: conjured items. Conjured items behave like standard items, except that they age twice as fast. You can check the full instructions on Github.   Let’s jump into the code!   Refactoring to an extension-point      The first step in our plan is to change the existing code before injecting our new feature. This step can be tricky as it touches the legacy code, and we don’t want to break anything. Here are a few principles to cut risks:      Stick to the smallest possible change   Use any validation technique you have at your disposal:            End to end tests       Manual tests       Integration tests       Setting up a Golden Master test for the occasion is also a smart move.           Use many pairs of eyes. Refactoring in pair or mob will drastically reduce the risk for regression.   Fortunately, the gilded rose kata comes with a golden master test. It’s a test that “snapshots” the behavior of your code to let you refactor it. It’s temporary because a precise snapshot is unmaintainable. You can learn more about the golden master technique through this live refactoring.               The gilded rose kata uses the ApprovalTests.cpp library to help us with Golden Master testing. All we need to do is to add a set of test combination to the existing test:   cpp/test/cpp_googletest_approvaltest/GildedRoseGoogletestApprovalTests.cc   TEST(GildedRoseApprovalTests, VerifyCombinations) {      std::vector&lt;string&gt; names { \"Aged Brie\"                               , \"Backstage passes to a TAFKAL80ETC concert\"                               , \"Sulfuras, Hand of Ragnaros\"                               , \"Leather Boots\"                               };     std::vector&lt;int&gt; sellIns { -2, -1, 0, 1, 2, 3, 4, 5, 6, 7 };     std::vector&lt;int&gt; qualities { 1, 3, 5, 10, 49, 50, 80 };      auto f = [](string name, int sellIn, int quality) {         vector&lt;Item&gt; items = {Item(name, sellIn, quality)};         GildedRose app(items);         app.updateQuality();         return items[0];     };      ApprovalTests::CombinationApprovals::verifyAllCombinations(             f,             names, sellIns, qualities);  }   Now that we have a regression test, we are ready to change the existing code, so let’s try to add an extension point. One way to do this is to add an array of ‘item updaters’ that can take care of specific types of items.   cpp/src/GildedRose.h   The first thing we’ll do is to:      Create an ItemUpdater abstract class   Add a list of ItemUpdater objects to the GildedRose constructor   class ItemUpdater { public:     virtual ~ItemUpdater();      virtual bool handles(const Item&amp; item) const = 0;     virtual void updateQuality(Item&amp; item) const = 0; };  class GildedRose { public:     vector&lt;Item&gt; &amp; items;     const std::vector&lt;std::shared_ptr&lt;const ItemUpdater&gt; &gt; itemUpdaters;     GildedRose(vector&lt;Item&gt; &amp; items, const std::vector&lt;std::shared_ptr&lt;const ItemUpdater&gt; &gt; &amp; itemUpdaters);          void updateQuality();      void updateQuality(Item &amp;item) const; };   We’ll just need to adapt the constructor calls from the tests with an empty array of ItemUpdaters {}.   cpp/src/GildedRose.cc   In the implementation, we:      Store the ItemUpdater objects   Extract an updateQuality(Item&amp;) method to handle a single item   Delegate to an ItemUpdater that can handle a particular item   ItemUpdater::~ItemUpdater() { }   GildedRose::GildedRose(vector&lt;Item&gt; &amp; items, const vector&lt;std::shared_ptr&lt;const ItemUpdater&gt; &gt; &amp; itemUpdaters) : items(items), itemUpdaters(itemUpdaters) {}      void GildedRose::updateQuality()  {     for (int i = 0; i &lt; items.size(); i++)     {         updateQuality(items[i]);     } }  void GildedRose::updateQuality(Item &amp;item) const {     for (int i = 0; i &lt; itemUpdaters.size(); ++i)     {         std::shared_ptr&lt;const ItemUpdater&gt; itemUpdater = itemUpdaters[i];         if (itemUpdater-&gt;handles(item))         {             itemUpdater-&gt;updateQuality(item);             return;         }     }      if (item.name != \"Aged Brie\" &amp;&amp; item.name != \"Backstage passes to a TAFKAL80ETC concert\") {          ... }   It is now possible to inject an implementation for conjured items.   Code conjured items with TDD   The implementation of our extension point needs to deal with two aspects:      declare that it will handle conjured items   update quality and sellIn for a conjured item   Here is the implementation I came to using TDD   cpp/test/GildedRoseConjuredItemUpdaterTests.cpp   // Include header files for test frameworks #include &lt;gtest/gtest.h&gt;  // Include code to be tested #include \"ConjuredItemUpdater.h\"  class ConjuredItemUpdaterTests : public ::testing::Test { protected:     ConjuredItemUpdater updater; };  TEST_F(ConjuredItemUpdaterTests, HandlesConjuredItems) {     EXPECT_TRUE(updater.handles(Item(\"Conjured Mana Cake\", 2, 2)));     EXPECT_TRUE(updater.handles(Item(\"Conjured Leather Boots\", 2, 2)));     EXPECT_FALSE(updater.handles(Item(\"Leather Boots\", 2, 2))); }  TEST_F(ConjuredItemUpdaterTests, ConjuredItemsGetCloserToSellIn) {     Item conjuredItem(\"Conjured Mana Cake\", 2, 4);      updater.updateQuality(conjuredItem);      EXPECT_EQ(1, conjuredItem.sellIn); }  TEST_F(ConjuredItemUpdaterTests, ConjuredItemsDegradeBy2PerDay) {     Item conjuredItem(\"Conjured Mana Cake\", 2, 4);      updater.updateQuality(conjuredItem);      EXPECT_EQ(2, conjuredItem.quality); }  TEST_F(ConjuredItemUpdaterTests, ConjuredItemsCannotGetANegativeQuantity) {     Item conjuredItem(\"Conjured Mana Cake\", 2, 1);      updater.updateQuality(conjuredItem);      EXPECT_EQ(0, conjuredItem.quality); }  TEST_F(ConjuredItemUpdaterTests, ConjuredItemsDegradeBy4PerDayOnSellInDate) {     Item conjuredItem(\"Conjured Mana Cake\", 0, 7);      updater.updateQuality(conjuredItem);      EXPECT_EQ(3, conjuredItem.quality); }  TEST_F(ConjuredItemUpdaterTests, ConjuredItemsDegradeBy4PerDayPastSellInDate) {     Item conjuredItem(\"Conjured Mana Cake\", -1, 7);      updater.updateQuality(conjuredItem);      EXPECT_EQ(3, conjuredItem.quality); }   cpp/src/ConjuredItemUpdater.h   #ifndef GILDED_ROSE_REFACTORING_KATA_CPP_CONJUREDITEMUPDATER_H #define GILDED_ROSE_REFACTORING_KATA_CPP_CONJUREDITEMUPDATER_H  #include \"GildedRose.h\"  class ConjuredItemUpdater : public ItemUpdater { public:      bool handles(const Item &amp;item) const override;     void updateQuality(Item &amp;item) const override; };  #endif //GILDED_ROSE_REFACTORING_KATA_CPP_CONJUREDITEMUPDATER_H    cpp/src/ConjuredItemUpdater.cpp   #include \"ConjuredItemUpdater.h\"  namespace {     static const char *const CONJURED_ITEM_PREFIX = \"Conjured \";      int degradation(const Item &amp;item) {          if (item.sellIn &lt;= 0) {             return 4;         } else {             return 2;         }     }      void degradeQuality(Item &amp;item, int degradation) {         item.quality = max(item.quality - degradation, 0);     } }  bool ConjuredItemUpdater::handles(const Item &amp;item) const {     return item.name.find(CONJURED_ITEM_PREFIX) == 0; }   void ConjuredItemUpdater::updateQuality(Item &amp;item) const {      degradeQuality(item, degradation(item));      item.sellIn--; }   Inject into the existing code   This last part is a piece of cake. We only have to provide our implementation to the Item Updated when we instantiate it.   GildedRose app(items, {std::make_shared&lt;ConjuredItemUpdater&gt;()});   In real life, we would use whatever test we have:      to make sure that we did not break anything   and that our new feature is behaving as expected   We would do this using any automated, and maybe slow, tests we have at our disposal. We could also run the system and test it manually.   During our Gilded Rose Kata, there are a few things we can do:      We can run the golden master to make sure that we did not break anything.   We can create a ‘main’ function, add some trace, and make sure that it’s behaving as expected.   We can also run the golden master test and update it. Approvals.cpp will open your diff tool and highlight the changes with the snapshot. It’s a very convenient way to check our work:   TEST(GildedRoseApprovalTests, VerifyCombinations) {      std::vector&lt;string&gt; names { \"Aged Brie\"                                 , \"Backstage passes to a TAFKAL80ETC concert\"                                 , \"Sulfuras, Hand of Ragnaros\"                                 , \"Leather Boots\"                                 , \"Conjured Mana Cake\"                                 };     std::vector&lt;int&gt; sellIns { -2, -1, 0, 1, 2, 3, 4, 5, 6, 7 };     std::vector&lt;int&gt; qualities { 1, 3, 5, 10, 49, 50, 80 };      auto f = [](string name, int sellIn, int quality) {         vector&lt;Item&gt; items = {Item(name, sellIn, quality)};         GildedRose app(items, {std::make_shared&lt;ConjuredItemUpdater&gt;()});         app.updateQuality();         return items[0];     };      ApprovalTests::CombinationApprovals::verifyAllCombinations(             f,             names, sellIns, qualities);  }   Close with a 10 minutes retrospective   Don’t forget why you did this code kata! Participants in a coding dojo often get so caught up in the exercise that they forget it’s only practice. When coaching, it’s your job to get them back to reality! The quick retrospective is there to make sure they can apply what they learned in their day to day job.   I got my recipe for the mini-retrospective from the Cucumber team. It’s only four questions:      What did you do?   What did you learn?   What still puzzles you?   What do you decide?   I won’t go into the full details of how to animate a quick retrospective. (Note: I might write a post about this one day, so stay tuned). The main point is to make sure to be listening, and not speaking, 95% of the time. Here are some meaningful discussions to have about this kata:      How can you apply these techniques (bottom-up TDD and Golden Master) in real life   How can you deal with the lack of end to end tests? It’s an excellent time to suggest trying another kata around BDD Scaffolding. (This too could be the subject of a future post)   What should you do when you need to change the legacy code at many different places to inject your new feature? To summarize, this situation demonstrates that the existing code could do with better modularization. This is a good opportunity to mention the Bubble Context technique:   Create a well-design, domain-driven module for your feature   Inject it in the legacy at different points with the above technique   Very likely using extra adapters   Use your 5% speaking time to hint participants to these discussions if they did not rise by themselves. Suggest running follow-up katas or mob sessions to keep the momentum going. Don’t forget, coaching a team is a long term task.      Side note: Many variants of Gilded Rose     The Gilded Rose kata is particularly interesting to practice different refactoring techniques:          The Bubble Context, as we did     Setting up a Golden Master Test     Refactor using safe baby steps     The Strangler Application     Testing with BDD scaffolding      To summarize, what should you do NOW?   First of all, keep problems in mind:      How to write tests before code that already exists?   Coach enough TDD for legacy code skills to the team.      As often, the solution is in the problem. Next time you hear these problems, suggest trying a refactoring kata. You might have to repeat the suggestion a few times before people accept. As soon as you have an opportunity, run the gilded rose, and coach one TDD for legacy code technique. Make sure to prepare the kata before, though.   Don’t forget the long term plan either:      Continue to practice TDD Katas regularly   Help the team to bridge the gap with their real code, through mob sessions for example   Use the quick retrospective to get the team to try a second kata or a mob session as soon as possible.   You can do it!   You don’t have to be an official XP Coach to do all this:      It’s a great way for tech leads to up-skill their team.   Nothing prevents developers from doing ‘guerilla XP coaching.’   Let’s hack organizations for the better!  ","categories": ["testing","refactoring","coaching","tdd"],
        "tags": [],
        "url": "/initiate-your-team-to-tdd-for-legacy-code-with-the-gilded-rose-kata/",
        "teaser": "/imgs/2020-08-24-initiate-your-team-to-tdd-for-legacy-code-with-the-gilded-rose-kata/tdd-for-legacy-code-mountain-teaser.jpg"
      },{
        "title": "How to deliver a remote training with code-katas and randori in pairs",
        "excerpt":"Randori-in-pairs code-katas maximize learning thanks to code and interaction. With git-handover and breakout rooms, it becomes a practical remote training.      COVID and lock-down have made the training world jump 10 years forward. Remote training is becoming the norm. Unfortunately for us, switching to remote training is not straightforward! Here are a few comments I found on the online extremeprogramming@#groups.io.      I moved one physical training session (2 days) into Zoomland and the result did not thrill me.     I definitely did not like losing the ability to wander around the room and look over people’s shoulders to ask questions.     I’ve adapted one in-person training class to the virtual environment […], but the results to date have been very poor.    Very senior and famous trainers from the TDD / XP community wrote these comments! If these people are struggling to adapt to remote training, no wonder we all are!   The company I’m working for is currently operating in a remote-first mode. Not by choice, it’s a matter of fact: most people now work from home.   This raised a severe challenge for my teammates and me. We needed to find a way to continue to work with the teams. In some way, this meant finding a way to deliver remote training.   Fortunately for us, the company has developers in our 3 main offices: Beirut, Dublin, and Paris. We had already experimented a lot with remote work, so we were not wholly taken off-guard. Here is how we are now delivering our technical agile coaching.   Our coaching sessions   We stopped all-day training sessions long ago. Instead, we have regular sessions with the teams over a few months. Attendees go to all-day classroom training to learn new techniques. Shorter regular sessions become routine that changes how a team operates.   Our current model of technical coaching relies on two kinds of sessions:      Different flavors of Code-Kata practice sessions: presentation kata, all in randori, and randori-in-pairs. We use these formats to introduce new techniques related to TDD and refactoring. If you want more details, check my previous post about the bubble context and the Gilded Rose kata.   We also run Mob sessions in which we work with the full team on their own codebase. During these sessions, we apply, for real, what we have practiced in kata sessions.   These formats are not all as easy to adapt to remote work.      During mobs and all-in-randoris, everyone works on the same codebase. These formats work very well with the git-handover mechanism. (More about the git-handover in Best open source tools for remote pair programming)   On the other side of the spectrum, the presentation kata is like live-coding. Any screen-sharing tool will let the driver present what he’s doing.   The real challenge is in running a successful Randori-in-pairs remote session. Unfortunately, it’s also one of the most learning-effective training formats. Let’s see how this works.   Randori in pairs      As far as training: mobbing has limits, and I’ve done TDD training with 8 folks in a mob… but more than that would likely start losing folks. [Jeff Langr] on [extremprogramming@groups.io]    Here’s a quick reminder of the different Randori formats.   During a randori code-kata, all participants work together on the same problem. Problems usually come from well known online code-kata libraries. When sitting in the same room, participants share the same laptop. A different person becomes the driver and receives the keyboard every 5 minutes. (Check this post if you want to learn How to start a team coding dojo randori today!)   A randori-in-pairs is like a Randori, except that participants don’t work all together. Instead, they work in pairs on their dedicated version of the code.   Randoris improve team dynamics and can introduce new practices and techniques to everyone.   Randoris-in-pairs are better for practice. By working in pairs, participants will have more time at the keyboard.      The randori-in pairs format has a Training from the back of the room flavor. Through discussion and feedback, participants learn many lessons without your intervention! It’s both a way to scale the training and to get your message through. Learning by oneself or from a teammate feels more natural than listening to an expert!   A recipe for remote randori-in-pairs   Regular short sessions      Without the travel to on-site, it seems silly to try to cram things into full days. [George Dinwiddie] on extremeprogramming@groups.io    If you usually provide full-day training, it’s time to split it into 2 hours sessions over many days, or even weeks.   Spending the whole day with a headset on is exhausting. The old 8 hours a day style of training is not sustainable when remote.    As I wrote above, we have been running regular short sessions for a long time now. Experience showed that it’s more effective than full-day classroom training. It leaves time for participants to experiment with their new knowledge. When they arrive at the next session, they have gone through successes and failures. They can share best practices with their teammates or ask more insightful questions.   That’s good news for us. Remote work is forcing us to spread training sessions over many days, but this will be more effective!   Breakout rooms   The pairing setup is easy to reproduce with breakout rooms. Good video conferencing tools provide a way to send participants in private rooms.   I’ve heard a lot of praise about Zoom’s breakout rooms. Due to corporate policy, we had to use Microsoft Teams, though. It turned out to be pretty easy to reproduce a similar feature within Teams. We just had to prepare extra channels for every pair and ask participants to join them.      When I did training, I would have each share their screen and I would watch on “Brady bunch” mode in google meets, flicking between screens. It was exhausting for me but otherwise worked well, especially since everyone was able to learn from each other’s mistakes easier. [Avi Kessner] on extremeprogramming@groups.io       As a trainer, this setup lets you jump from one pair to the other. I’ve found that it was possible to deal with up to 4 pairs this way. Depending on how autonomous your attendees already are, your mileage may vary…   Git Handover   One key issue for an effective remote randori-in-pairs is how to share code within a pair. The most straightforward way to do that is to use the git-handover technique. It relies on simple tools, and developers used to Git will understand how it works in less than a minute.   A pair should create a dedicated branch and collaborate by pulling and pushing code. From then on, the driver can share his screen and work as he prefers. It’s also a good practice to configure a reminder to switch the driver every 5 minutes.   Learn more about the git-handover technique in my post Best open source tools for remote pair programming.      What to do in practice?   Before the session      Prepare a git repo with the code kata to work on. Make sure that it is accessible to all participants.   Setup breakout rooms or sub-channels within your video conferencing system   Prepare git branches for each pair to work on   During the session      Gather everyone in the main room, and introduce the subject   The first time, you might have to explain the logistics of the remote randori-in-pairs   Send everyone to their pair room and start coding.   Jump from room to room. Troubleshoot and re-explain how it’s working until people get used to the setup. It should not take long, though.   When time is over, gather everyone in the main room again for a quick retrospective.   Tooling   Here are a few useful tools:      mob is a tool written in Go that deals with the timer and pulls and pushes to a git WIP branch.   We’ve had success with a simple watch script  that keeps pulling and pushing on the WIP branch. Once started, Participants only have to deal with screen sharing.   There are many online timers to remind you to switch the driver when mob or pair programming. We use https://agility.jahed.dev/, but many others will do.      Why does it work?   Let’s pause a second to understand why this setup works so well.   1st: it keeps all the benefits of randoris-in-pairs:      It maximizes practice time   It maximizes interaction in every pair   Pairs discover a lot of lessons by themselves   2nd: as a trainer or coach, you can jump in and observe what a pair is doing from the back seat. When you do so, you can see both the code and the attendees. You’re in a perfect situation to ask the right question, just at the right time.   3rd: thanks to git-handover, people can use their own IDE and environment. This frees up more brain-CPU for the learning.   4th: as everyone participates from home, it’s easier to spread the training over many days. It’s more sustainable for attendees. As a consequence, they are in a better mindset for learning.   5th: spreading training over shorter sessions is also better for building knowledge!   Bonus: this randori-in-pairs format benefits from all the extra remote advantages:      Saving on commutes   Staying close to our families   Working at our own desks   It’s better for the environment!   Try it yourself!   The current situation has put some of us into serious difficulties! Achieving the level of interaction we rely on during a remote training is difficult! The remote randori-in-pairs gets pretty close. Give this format a thorough look before you deliver your next remote training!   Now is the time to experiment and discover solutions that we did not envision 1 year ago! The nice thing is that remote randori-in-pairs is easy to test:      Find 2 volunteers   Try a mini session to get the feel of it   Fine-tune and repeat until you feel ready   At this point, invite your first participants and deliver excellent remote training. At this point, you’ll be in a perfect situation to improve even further!   If I’d love to read how it turned out for you!  ","categories": ["remote","coaching","learning","coding dojo"],
        "tags": [],
        "url": "/how-to-deliver-a-remote-training-with-code-katas-and-randori-in-pairs/",
        "teaser": "/imgs/2020-09-21-how-to-deliver-a-remote-training-with-code-katas-and-randori-in-pairs/randori-in-pairs-bunch-teaser.jpg"
      },{
        "title": "7 tricks to influence a team resisting to change its technical habits",
        "excerpt":"There are many ways to manage team resistance. Here are some of the technical agile coach’s tricks that influence: testimonies, experiments, and more!      Such initial resistance is typical, but maybe the worst I’d encountered in that class. People don’t learn new habits overnight. [Jeff Langr on extremprogramming@groups.io]       They liked developing big chunks of code on their own private branches, and there was no talking them out of it because they weren’t feeling any pain. [Phil Goodwin on extremeprogramming@groups.io]       Honestly, this is stupid! [A coachee when I presented the very first TDD kata]       I don’t want this change!   As technical agile coaches, it’s our daily job to bring change. Whenever we want to introduce a team to a new practice or habit, we are bringing change. Unfortunately for us, people have many good reasons for resisting a change! They might:      Be tired of repeated change programs that did not live up to their promises.   Be very happy with the current situation, and fear getting a bad deal if anything changes.   Have too much work and don’t want the extra workload of learning new habits and practices.   Some people will offer mild resistance. Others, though, will make the confrontation pretty tough. Trying to push harder when a team is already resisting will only make work painful for everyone.      Here are different strategies to deal with resistance more sustainably.   Try some of these   Walk in their shoes!         This will not work here!    This quote is a sure sign that they don’t see how to apply what you are coaching in their day to day work. The technical environment might be too far from what you have presented. Are they crawling under legacy code? Do the company processes prevent them from changing how they work?   It means you need to understand their context better. You will usually learn a lot by sitting and working with them.      Avoid BUZZWORDS. What if we try this? Sitting and work with team. TALKING ISN’T EFFECTIVE. If they see and don’t want it, OK. Learn from them. [Dave Nicolette on extremeprogramming@groups.io]    Lean practitioners will call this Gemba. By pairing and mobbing, you’ll understand the team members’ situation and constraints. You’ll see and even feel their problems first hand. This will help you to find the angle to use to introduce new practices.   Explain to the team that you will mostly observe and that you will not slow them down. Be attentive to what is going on, and remember to be constructive. A bonus is that you might also be helpful! If ever you recognize a situation where you can help, by all means, do it! This is where your legacy-code testing skills will pay off! This will earn you extra credits that will make change easier down the road.   I used this technique with the first team I coached after I became an official technical agile coach. I spent 2 days per week pairing with the developers. I also attended their daily meetings and other sprint ceremonies. After a few weeks, I had understood and experienced their problems. I then suggested much more time-efficient coaching sessions using mobs and katas.   Be patient      We don’t really have this problem.    Sometimes, people don’t feel a problem. They might be used to it, assume it’s a fact of life, and live with it. On other occasions, it might be because they have more pressing issues to deal with first. Whatever the cause, the change you suggest will not stick if it is fixing an invisible problem. It’s useless to try to change something that people believe is working.   If you still think it’s important, keep your eyes open. The key is to be patient: work with them and leverage on any opportunity you encounter.      Sitting with them as they work through real stuff, and showing how things play out is critical. IT CAN BE MONTHS BEFORE HABITS STICK [Jeff Langr on extremeprogramming@groups.io]        Showing people tiny steps they can take that make them feel better about what they do, over a good long while [Mike Hill on extremeprogramming@groups.io]     For greater effectiveness, make every success as visible as you can. It’s even better to get the people you were working with share it with the team.   For a long time, this is the only type of coaching I actually did. I was not an official technical agile coach, but more of a developer who was coaching his team.   I remember a time when unit testing was still a controversial practice! We would publicize any regression caught by a unit test as an argument to convince the skeptics.   Run an experiment      We are humans… as a result, we don’t always agree! Introducing new practices can lead to many kinds of disagreements:      between you and the team   or among team members themselves   This can be the perfect moment to suggest an experiment. It’s often a lot easier to get people to try something than to commit to it.   I would ask them to try to submit a pull request about one half the size that they do now, then measure how long those remain open. I would measure quietly to avoid influencing their actions. Maybe this would help. &mdash; ☕ J. B. Rainsberger (@jbrains) November 4, 2019   Short, 1, or 2 weeks, experiments are usually enough. Involve the team in the design of your experiment so that the results are better accepted. Also, try to make it as measurable as possible. Fuzzy results might lead to even more debate.   A few months ago, a colleague introduced a team to approvals tests during a kata. The team needed a bit more confidence to add the practice to their official toolbox. A team member ran an experiment on a particular piece of their production code. It went on pretty well, and approvals.cpp has now been promoted to an official company third-party.   Put them in a training mindset      TDD looks fine, but our immediate problem is dealing with Legacy Code!    I have seen that developers who master TDD deal with Legacy Code a lot better from my past experience. Unfortunately, it’s not apparent to people who don’t yet master TDD. Many things seem useless until you understand them!   Instead of trying to convince them, find a way to put them in a training mindset for a while.      I […] Stopped and reminded them that they were here to learn how to build things incrementally, that they should continue to work through the exercises with that in mind, that seemed to calm they down a bit [Jeff Langr on extremeprogramming@groups.io]     When people are in a training mindset, they are more open to      Trying new things   Learning for the sake of learning   And putting their BAU constraints aside   Has the company got some form of slack time, learning, or innovation day? (Even SAFe can be hacked in this way!) This is the perfect occasion to set up a friendly low-ceremony training kata. Look for company processes you can piggyback on. For example, you might prepare a more official training and have all the team attend for a few days.   The refactoring coaching plan I built when working at Murex turns around this idea. The team was struggling with legacy code and refactoring. I prepared a kata plan that consisted of 2 parts:      the first half was a set of katas around TDD   the second half contained only katas around refactoring techniques (Golden Master, Strangler…)   I pitched the full plan as a single unit. I mandate TDD to create the short feedback loop required for efficient learning. This allowed me to shortcut many arguments about TDD: it was only a training tool. By the time the team had reached the end of the kata plan, they knew enough TDD skills to apply it in the BAU work.   Bring a testimonial      It looks interesting, but we are not sure it’s a good investment of our time…    Starting to work with a technical agile coach is a serious time investment. Some teams might feel uneasy about spending all this time with a coach. Does the team need a bit of social proof!   This is the perfect occasion to make them meet a former coachee. The closer to their situation, the better. Good news if you are coaching in a multi-team organization: it should be easy to find someone facing similar day to day issues!   The story of a peer overcoming similar problems is worth a thousand words from the coach.   One developer from the first team I coached as a technical agile coach at Murex later told me:      Unit testing and working in baby steps had changed her life!    Job was not easy for her when I joined the team. 2 years later, she is now happy to come to work and more productive than ever. Keeping contact with someone with such a powerful testimonial is a great asset!   Kick off creative collaboration   All people are different, and we don’t all connect as easily. Sometimes, contact with a team might not be as natural as usual. For whatever reason, it might not have started on good tracks.   A collaborative workshop is a perfect tool to kick-off an effective partnership.      The typical plan would be:      Identify their problems   Present the options you can provide   Brainstorm what to do next   This fits nicely in a 2 phase workshop.   Run the first session for problem identification. For example:      It could start with a problem brainstorming   And continue with an activity like the 5-Whys, to understand their situation in depth.   Starting from their context will prove your interest in them. It will help to improve the relationship.   You can then use the time between the sessions to do a bit of homework. Check what you have in your coaching toolbox that could help them. You might also ask them to collect any ideas they might have on their side.   Finally, meet them again. Start by sharing the ideas you all had to fix their problems. Finish by picking what you want to start with. The solution focus format works like a charm here.   Ask another coach   If you still cannot build a relationship with the team after such a workshop, ask another coach to give it a try. You might just not be the right person for this team. Someone else might have more luck!   We all have different coaching styles, and some fit some teams better than others.      That’s one of a tech-coach team’s forces: you are more likely to find the right coach for each situation. At work, I know that some teams work better with some of us than with others. That’s fine, and it’s also a great way to share the workload!   In last resort: Quit   You might have noticed that this is the 8th trick! It’s not a way to influence, but it’s still something we have to resort to from time to time.   If nothing seems to work, it’s better to stop now and move on to another team. Many circumstances are out of your control and can make your coaching fail:      Has the team got too much work?   Are some key team members having personal issues?   It might just not be the right time, and it would work better in 1 year!   Whatever the reason, you will create more value by helping a volunteer team. It’s actually the first principle of our coaching team at Murex: we only work with teams that volunteer.              From Michael Batko’s blog post summary of the book Crossing the Chasm        It’s especially true if you are coaching in a multi-team organization. You don’t want to spend all your energy fighting laggards. There might be plenty of other teams that will welcome your coaching. Working with them brings more value to the business. Don’t forget that you have a whole organization to refactor! Setbacks are inevitable. Sometimes, it just won’t work. We want our coaching work to remain sustainable. The best thing to do is to accept and move on.   From my experience, it’s when you have to quit that you learn the most. Keep some time to think about it. Ask fellow coach co-workers for help. Get more clarity about what did not work and what you will do better next time!   Experiment and learn   Coaching is not a science. Team and organization dynamics is a very complex topic. If your typical coaching recipe does not work with one team, give one of these tricks a chance.      If it works: I’m delighted that I could help you!   If it did not: I’m sure you learned a lot more about the team, and you’ll be in a better place to pick another trick to try. I’m also glad I helped you.   Part of the joy of coaching is discovering a way to bring some improvement in unknown situations! I’d love to read your comments and own stories about teams resisting coaching!  ","categories": ["coaching","tdd","learning","change management"],
        "tags": [],
        "url": "/7-tricks-to-influence-a-team-resisting-to-change/",
        "teaser": "/imgs/2020-11-02-7-tricks-to-influence-a-team-resisting-to-change/resistance-change-teaser.jpg"
      },{
        "title": "How the Samman Method helps to sell technical coaching internally?",
        "excerpt":"Emily Bache wrote an accessible guide for technical agile coaches. Here is my review, plus my bonus section: How to sell your technical coaching services internally.         […] Often they have a lot of internal knowledge and are more valuable to the organization in their current role than they would be as a more general technical coach. [Emily Bache]       Emily Bache has a great reputation that helps her a lot to find organizations and teams to work with. It is different for us: anonymous and without reputation. [A reviewer on Okiwi]       I no longer focus on the practices when I spread them; I just start using them when I pair with someone, and they catch on (or don’t). Maybe I should have specific sessions where I focus on, say, refactoring, or TDD. [George Paci extremeprogramming@groups.io]    A while ago, Christophe Cadilhac created a channel for technical coaches on the french Okiwi Slack. Our kick-off action was to start a book club. As a first book, we picked Emily Bache’s Technical Agile Coaching with the Samman Method. We met every Monday, Wednesday, and Friday. It took us around 3 weeks to finish the book.      Emily Bache had the freelance coaches in mind when she wrote it. I found most of the text useful for both external and internal coaches… except the Sales aspect. Yet another reason for me to write about the book. This blog post consists of 2 parts:      A review of Emily Bache’s book   Advice about how to sell your technical coaching services when you are an internal coach   You don’t need to read the first part to understand the second. You can jump directly to how to The Unique Sales Challenge of Internal Coaches if you prefer.   What is the book about?   NOTE: Emily’s book was only 90% finalized at the time of this review. Content may have changed a little when you read this.   The book presents how Emily Bache now does technical agile coaching. She calls this way of coaching the ‘Samman method.’ It relies on mob programming and regular mini-trainings, which she calls learning hours.   Why Samman?   Google told me that Samman means ‘Together’ in Swedish. Although British born, Emily Bache lives and works in Sweden. Her coaching style relies a lot on mob programming, which I guess is why she picked the ‘Samman’ word.   Book structure   After 2 short introduction chapters, the book consists of 3 parts:      Ensemble Working. This part contains a crash-course on mob programming and some advice about how a coach should act in the mob.   Learning Hours. On top of mob sessions, Emily uses short 1-hour training sessions with the coachees. The goal is to have them practice the theory they will need to be effective in the mob.   Samman coaching engagement. This last part details how to organize as a coach:      How to sell?   How to work with recalcitrant teams?   How to manage your career?   …   What I loved about this book   Here are the reasons why I enjoyed reading this book.   It’s an easy read   It’s currently only 122 pages, which makes it a short and easy read. We all suffer from information overload. I’m a strong advocate of short books: many of my favorite software books were short. For books, less is more!   It’s a full-spectrum recipe for technical coaching.      In her book, Emily presents her full-spectrum and coherent recipe to technical coaching. It’s not a disparate set of practices that we could copy from an experienced coach. It’s a system of techniques that support and build on each other. For example:      Learning hours provide the theory and practice so that mob sessions can be useful.   In turn, mob sessions will highlight what the team should look into during learning hours.   It’s a crash-course on mob programming.   There are entire books about mob programming. Maybe you don’t have the time to read 200+ pages about mob programming. Emily’s book contains a 10 pages crash-course to get you started with mob programming. It even includes a short section about remote mobbing.   It fills a gap      There are plenty of books about Agile coaching. There are very few books that deal with the technical aspects of coaching. By relying on practices like mobbing, Emily presents a modern way to do technical coaching.   It goes into great details about learning hours.   One of my main takeaways from the book is the extensive description of Learning hours. Emily provides some typical session examples. She also goes through the pedagogic theory behind the practice.   We have been using katas for a similar goal at work. A typical kata lasts for 2 hours, though. We had also been experimenting with ‘mini-katas,’ unfortunately, without much success until now. Learning hours, as described in the book, might be what we have been looking for!   It’s a starter kit   The book also covers some career and business-related topics. It could serve as a guide for someone willing to pick a technical coach career.   Emily writes about what she loves about the technical agile coach job. She also presents her typical coaching day. This short section makes the rest of the book more concrete!   In part 3, she dives into more details:      How to find a customer   How to find teams to coach   How to deal with recalcitrant teams   How to start and grow as a coach   …   It contains everything we need to get started!   She uses a very personal style.   The book is not meant to be an absolute reference. It is instead the return on Emily’s experience of doing technical coaching. This is not the only way to do it, but it’s Emily’s way. It works for her and also works for others. My own current way of coaching at Murex is very close to this style.   Presenting her coaching recipe connects with the reader at a more personal level. This made my reading experience more enjoyable.   It’s also useful for experienced coaches.      If you are an experienced technical coach, you might think this book is not for you. You would be wrong! The previous points might look ‘junior coach’ oriented. Here is why the book remains worth your time:      It will make you think about the way you are currently coaching. You might not jump in Emily’s style right away, but it will shed new light on your current practices. Most likely, you’ll tune or change your coaching for the better.   She also offers advice about dealing with recalcitrant teams, a difficulty we all face at some point.   Throughout the book, Emily addresses the personal organization topic in depth. Many of us (me included) still struggle with this, and any help is welcome. For example, she provides advice about when to take breaks and when to have non-coaching days.   My conclusion   My conclusion is straightforward. If you envision becoming or already are a technical coach: give this book a read. Its advice/page ratio makes it a no-brainer.   What you won’t find in this book      Like with any book, Emily had to select what she put in it. Here are two things that are not in the book that are worth mentioning:   No technical knowledge   It does not contain learning material to teach you all the skills you’ll need to become a technical coach. Instead, in part 3, Emily provides a list of all the skills you’ll need to work on. The book also features a terrific shortlist of references to acquire these skills.   No explicit advice for internal technical coaches   Emily explains that she wrote the book with freelance coaches in mind.      Most of the coaches I know are consultants who engage with a number of different organizations […]. I believe it should also be possible to be an internal coach in a large organization. I don’t have experience of that but I hope you will still find useful material in this section. [Emily Bache, Technical Agile Coaching with the Samman method, Part 3]    Indeed there are a lot of internal technical coaches out there. I am one of them, for example. I know even more developers who do unofficial, ‘guerilla-style’ technical coaching.      I find myself continuously reinstalling [XP and Agile] best practices at the jobs I’m taking on. Continuous Integration, unit testing, refactoring, iterative development… [Slava Imeshev extremprogramming@groups.io]    This is where I can be helpful!   What changes for internal coaches?   Part 3 of the book addresses many different aspects of the coaching engagement. Most of these are pretty similar for both internal and external coaches:      Persuading a team to work with you.   Setting expectations before you start working with a team.   Starting a technical coaching career.   Continuously improve as a coach.   If you are an internal technical coach looking for advice on any of these topics, Emily’s book is for you. There is another aspect that differs a lot, though: Sales!   The Unique Sales Challenge of Internal Coaches   A freelance coach sales challenge is:      How do I find a client to work with?    Sales for freelance coaches is very close to any consultant’s sales. In her book, Emily gives business cases examples to pitch technical coaching:      The department striving for Technical Excellence   The company struggling with defects   The twenty-year-old codebase   Too stressed out and busy   She also details typical coaching proposals. You might also have a look at the ton of literature to help consultants find clients.   If you are an internal coach, your situation is different. Your sales challenge becomes:      How do I start technical coaching with teams in my organization?    Why bother?      This looks like a difficult thing to do! Why should someone even bother? Again, Emily has the answer in her book! For the same reasons that someone would want to become a technical coach:           It’s challenging and interesting. […]     You can have a bigger effect than you would in other roles. […]     Teaching is inherently rewarding […]      Now that we know why, let’s see what changes!   Different Context   Independent and internal coaches face very different contexts. If you are an internal coach, some aspects will be more tricky:      You often don’t start with an official technical coach role! This means that you will have to work your way to this new job.   Independent coaches often negotiate contracts with people high in the hierarchy. This is a fail-fast test that ensures some management support for technical coaching. As an internal coach, that’s a luxury you often don’t have. To make things worse, wannabe internal coaches start from a different job. Which very often makes top managers unreachable!   You will not be able to ‘chose’ your customer! Except if you decide to jump ship, your unique customer is your employer. This can be a pretty tricky constraint if the company does not yet believe in technical coaching.   As Emily noted in her book, wannabe technical coaches are often stuck in their roles. Organizations value contributors too much in their roles to let them change jobs.   On the positive side, you also have advantages in your pocket!      There is already a contract that is binding you with your company. There is no energy to waste on negotiation and bureaucracy.   If you’ve been there for a while, you already know the context and the challenges better. This should help to fit technical coaching to the current goals of the organization.   You should know some influential people. These can prove valuable allies to help you to start technical coaching.   You also know the culture better. You’ll have a gut feeling about what people will like and what they will not.   Finally, you should know the processes better. This should help you to work around the bureaucracy and get things done.      4 steps to sell your technical agile coaching services internally   In the past 10 years, I’ve been both an external and internal coach. I already had a few coaching missions under my belt when I joined my current employer 7 years ago. I started as a senior developer, and I am now an official internal technical coach. I even managed to grow an internal team of technical coaches.   The 4 steps   My success formula has been to act like an intrapreneur and to accumulate momentum, day after day. I would summarize my approach in 4 steps:      Find a pain that you believe you could provide some help against   Hack the organization! This is where your organizational knowledge will help you. You’ll have to be creative to find a way to provide some help. Use your position, your network, the process, or whatever you can think of! A quick chat with the right person at the right moment is often all we need to unlock the situation.   Coach: at this point, you should have some buy-in for some form of coaching. Now is the time to be helpful. Do your best, be humble, be collaborative, and collect feedback. Don’t forget the coaching mindset! You should not do it yourself, but help others to do it by themselves.   Accumulate momentum: if you managed to provide some help, then communicate about it! It’s going to help you in the future. If it did not work as expected, then at least you learned something, so try something else!      At that point, repeat the whole thing! Every iteration should get you closer to doing technical coaching on a broader scale.   My story   A few months ago, I wrote a summary of my own journey from senior developer to technical coach at Murex. To illustrate what these steps might look like, let’s put them in parallel with my story.   Iteration 1   I had joined a relatively junior RTDB team as a senior developer.      Pain: The team struggles with a lot of code, slow tests and is at risk of delivering an ambitious product.   Hack: My senior developer role has a mentoring aspect. Let’s use it to coach other developers during code reviews and pair sessions. I could also try to introduce effective retrospectives.   Coach: I start to coach my teammates to TDD, refactoring, and evolutionary design. A few months of this is enough to ignite a continuous improvement mindset in the team. (You can have more details about this in From Zero to Pair Programming Hero   Momentum: the team is on track to deliver a breakthrough product! Some managers buy-in the agile way of working and are pushing for more.   Iteration 2   An Agile transformation is on its way, for the worse or better… I now had a success story as an embedded coach under my belt! I only had to express my desire to do more coaching to get an official agile (not technical yet) coach role. I am asked for help in the Deadpool team, which is struggling with legacy code.      Pain: I spend a few days within the team to understand that legacy code is not the only pain. It’s a group of experts more than a team. Expertise silos and legacy-code production-issues make the pace unsustainable. Morale was low.   Hack: Let’s use my now official ‘agile’ coach role to embed in the team. I could help them to add tests with katas and pair sessions.   Coach: I try different approaches before we get some results. Pair sessions don’t work as expected, but mobbing with the team on their refactoring does. The team develops a continuous improvement mindset, delivers, and works on T-shaping.   Momentum: This is a significant milestone for me. I come out of this with      A coaching plan to tackle legacy code and refactoring   Credits among other agile coaches   Credibility from other teams      Iteration 3   My style of agile coaching is now known as Technical Coaching. My new goal is to find more teams to coach.      Pain: I know many teams suffer from legacy code and refactoring pains. Thanks to the success with the Deadpool team, I now have something to help them!   Hack: This is the most ‘sales’ intensive work I ever did. I started an open coding dojo. I looked for teams to coach through word of mouth. I used my personal network. Whenever I meet a candidate team, I pitch them what I had done with the Deadpool team. Every time a team said no, my pitch became better. Eventually, I find a team that is ready to go!   Coach: I work with the team for 3 months, going through TDD katas, refactoring katas. We also mob together on their legacy code. I receive excellent feedback. Again, people love working on their production code with the coach.   Momentum: Thanks to repeated good feedback, higher-level managers notice my work. They start to see technical coaching as a tool against tricky issues like technical debt. The end-of-coaching survey is also a way to collect the next pains to tackle!   Iteration 4…   The story goes on… We are now scaling the offer, with more technical coaches, more katas, more problems… If you want to read the end of the story, stay tuned!   Takeaways   If you envision becoming or already are a tech agile coach, go and buy Emily’s book! It’s a short and pleasant read. You will most likely discover new practices, or at least see her perspective on them.      She mainly addresses independent coaches in the book. Most of the content is still relevant to internal coaches, though. Whatever your current role as an employee, you can sell technical coaching internally. You have the unfair advantage of already being in the organization! Start iterating through these 4 steps:      Identify a (technical) pain you could help with   Use your organizational position and knowledge to find ways to do some coaching   Coach people to act against the pain   Assess the results: communicate about successes, review and retry otherwise  ","categories": ["mob programming","coaching","book","change management"],
        "tags": [],
        "url": "/how-the-samman-method-helps-to-sell-technical-coaching-internally/",
        "teaser": "/imgs/2020-11-23-how-the-samman-method-helps-to-sell-technical-coaching-internally/internal-sales-teaser.jpg"
      },{
        "title": "5 pair-programming tips and tricks for coaching a remote team",
        "excerpt":"Remote work is an extra challenge for technical agile coaching. Here are 5 opportunistic remote pair-programming tips and tricks to show a better way of working.         […] nothing works better than sitting with them and test-driving some feature or reshaping their existing code–helping them get their work done. [Jeff Langr on extremeprogramming@groups.io]       Collaboration is generally a disappointing afterthought in so many applications [Jay Bazuzi in extremeprogramming@groups.io]       Programming in pairs is only 20 years old - the industry still needs time to catch up to us. [Phlip in extremeprogramming@groups.io]       […] when remote I have a tendency to forget or hesitate to switch driver [Tim Ottinger in extremeprogramming@groups.io]    Whether we wanted it or not, 2020 has thrown us into remote work. This turned out to be a great challenge for many technical agile coaches:      As firm believers in collaboration, we prefer face to face interaction   How can we do our job and influence a team if we are not sitting with people?   Pair programming tools are not yet up to our standards.   Even though I had regularly been working remotely for a few years, 2020 gave me a hard time to adapt:      COVID lock-downs made my schedule chaotic    Remote coaching made communication low-fidelity   2020 also forced me into re-discovering new ways of coaching.   Here are 5 pair-programming tips and tricks that will increase your influence when coaching a remote team. Some are simple must-haves to compensate for not being together. Others are new possibilities unlocked by remote pair programming! All are opportunities to show the way of working that is more effective and sustainable.   #1 Turn Cameras On      I remember my first remote talk very well. I had absolutely no clue about how the audience was reacting. Midway through the presentation, I had to pause and ask, “Is anyone here?” to make sure that at least 1 person was listening.   The same thing can happen when you remote pair program. Without the cameras on, we wreck what’s left of non-verbal communication. It becomes too easy to lose attention and to let the driver drive alone.   We should start every pair-programming session by asking our buddy to turn his camera on. The answers can be surprising! Someone once said:      I cannot. I’m still in pyjamas!    The quick-fix was a sweat-shirt :-)   Asking coachees to start their cameras highlights the value of face-to-face communication.      Lesson#1: Teamwork and collaboration remain crucial for remote work    (As a bonus, we also help each other to keep their social skills sharp)   #2 Don’t Get Disturbed      Would you ever start browsing your phone while pair programming with a coachee? I’m sure you would not! It remains valid when coaching a remote team. The lack of social pressure makes the browser a real trap when coaching remotely.   Here are a few tricks in case mere willpower is not enough to keep you away from emails:      Disable all notifications on your laptop and your phone.   Close all the apps you don’t need right now.   You might also configure a different browser for pair programming only. Don’t set up your social media accounts or email on this browser.   I’ve even read of people creating dedicated accounts exclusive to pair programming.   At some point, I used a premium version of RescueTime to see how much time I was losing with emails. This motivated me to batch all communication work at the end of the day.   Being at home also comes with its set of distractions. If you don’t have a calm desk, a good pair of headsets is a must-have. I use a Jabra Evolve 75. It does not feature the best noise cancelation out there. Yet my pair buddies thank me for its microphone that filters out surrounding sounds!   Intense pair coaching sessions are a demonstration of the power of energized work.      Lesson#2: Energized teamwork is the most effective way to create value. We must protect this time at all costs.    #3 Stay Focused      The next step after avoiding distraction is remaining focused!   Staying focused for hours of remote pair programming is not a willpower achievement. It’s impossible! If pair programming is already tiring, remote pairing is even more. We need to learn to recognize when we need a break.   As a coach, even if you are paid by the hour, it’s OK to ask for a break. If you don’t, you will not deliver your best.   A good trick is to use a Pomodoro timer to take a break every 25 minutes. As a bonus, you can take this break to switch drivers. There are many online Pomodoro timers available. I remember using marinaratimer.com with a previous team. If you are using Trello, you can also give Pomello a try. You’ll get a log of the time you spend on each task for free.   Taking breaks with your coachee is a way to prove that we stick to a sustainable pace. A pause is also the occasion to discuss non-work-related topics and build relationships.      Lesson#3: A sustainable pace is what makes us effective    #4 Keep Track of the Plan   Here is the Ninety-ninety rule:      The first 90 percent of the code accounts for the first 90 percent of the development time. The remaining 10 percent of the code accounts for the other 90 percent of the development time. [Tom Cargill, Bell Labs]    My “Aha! Moment” with XP was when I understood how it fixes the Ninety-ninety rule. With the right practices in place, when we reach the end of our TODO list, we are done!   Sharing a TODO list is a powerful way to show the power of XP when pair coaching.      Online shared tools give us persisted and historized TODOs for free! I bought a Coggle license to draw a mind map, but free tools like Google docs or MindMup would do the trick.      My whole coaching […] comes down to showing people tiny steps they can take that make them feel better about what they do, over a good long while. [GeePaw (Mike Hill) on extremeprogramming@groups.io]    As coaches, keeping a TODO list is an opportunity to show how:      to remain organized   that XP lets you go through a task with predictability, without an indefinite amount of ‘fix time’ at the end   to do a bit of refactoring at each step   to apply YAGNI by putting items in a ‘Parking lot’      Lesson#4: Good practices make programming more organized, predictable, productive, and sustainable.    #5 Use your machine wisely   Here is another opportunity from remote pair coaching: the navigator has a computer!   There are many ways to become a better navigator when remote pairing:      Run mini spikes, using an online interpreter, like OnlineGDB   Research APIs on the internet   Investigate what people think about a design question   There is a risk of getting out of touch with the driver, though! So:      Keep it short!   Have a dual-screen to keep the driver in sight   Don’t drift to the internet (cf #2)      As an XP coach, this is a perfect opportunity to show what embracing change means in practice.      Lesson#5: One way to embrace change is to use your context to run experiments and discover better ways to work.    Bonus: explain ‘Why’ and ‘How’ so it snowballs      As change agents, we want best practices to spread and stick. Except if we sold mob-programming to the team, we cannot be pairing with everyone all the time! When pair-coaching a remote team, we want team members to stick to best practices without us.   Fortunately, these pair programming tips and tricks are easy to copy. Just spend a bit of time to explain:      Why we use these practices, so coachees will want to apply them   How to use these practices, so they will know how to apply them      Lesson#6: Anyone can nudge others to try better ways of working.    Summary: What to do next time you do remote pair coaching?   As a coach, your goal is to show a better, more effective, and more sustainable way of working to developers. You can show this better way of working when remote pair coaching by doing the following:              By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        Feel free to print this card and stick it on your screen. Give it a try for a while and share your feedback. If you know other remote pair programming tips and tricks, I’d love to read about them too!     If you want to learn more about remote pairing, these posts might interest you:      How to deliver remote training with code-katas and randori in pairs   5 XP practices that will make your remote team more effective   Best open source tools for remote pair programming   7 Remote pair programming best practices Q&amp;A - Part 1   7 Remote pair programming best practices Q&amp;A - Part 2  ","categories": ["pair programming","coaching","remote","change management"],
        "tags": [],
        "url": "/5-pair-programming-tips-and-tricks-for-coaching-a-remote-team/",
        "teaser": "/imgs/2021-01-04-5-pair-programming-tips-and-tricks-for-coaching-a-remote-team/remote-pair-coaching-yaourt-phones-teaser.jpg"
      },{
        "title": "Introduce Team G Morning Learning Sessions to Coach the Growth Mindset",
        "excerpt":"Keeping our skills up to date is challenging. Remote work is a perfect excuse to run group learning sessions and to demo growth mindset to our coachees.               By Philippe Bourgau, under CC BY-SA 4.0, high resolution image           - You haven’t read that book? Ha! That’s nothing. There’s a million books I haven’t read. I’m way ahead of you in the not-reading department!     - I’m putting up some stiff competition in the “books I haven’t read” race. [Discussion on extremeprogramming@groups.io]       One problem I see is that most developers are not so aware. TDD seems still very much the exception rather than the rule. It doesn’t seem to be taught in most university Computer Science programs, which is one of two main ways that most programmers learn to program (the other being self-study). [russgold extremeprogramming@groups.io]    The Story of Good Morning Learning   I’m a learning addict. If I’m not careful, I can get demoralized by the infinite amount of exciting things to learn!   Keeping our skills up to date is crucial for anyone in the software industry. This holds true for us, technical agile coaches, but also for the developers we work with.   A few years ago, during his mob programming training, Woody Zuill shared a great practice:      At Hunters, we started every day with 1 hour of learning.    When the lockdown hit us, my commute time went from 50 minutes to 50 seconds. As we were all saving time on commutes, it was the perfect occasion to give daily learning a try!   My tech agile coach colleagues and I started to meet every morning for 30 minutes. We would spend 20 minutes studying and 10 minutes sharing what we had learned. Some read books, others looked at articles, while others experimented.   A week in, and everyone got hooked at the practice! Some of our part-time coaches started to invite their teammates. The group soon began to be too large to be manageable. We had to fork and start spin-offs sessions in different teams.   Since then, this little idea has been growing! My colleague Jonathan Boccara tried it and has been blogging about it!   Coaching the Growth Mindset!      The pandemic also prodded me to run more short experiments [Jeff Langr extremeprogramming@groups.io]       I am […] starting work with one new client remotely (but who knows where they will go?) [JB Rainsberger extremeprogramming@groups.io]    When I coach a team, installing a growth mindset is one of my main goals. Good Morning Learning sessions are a way to coach the growth mindset in 2 ways:      Continuous learning. After a few weeks, coachees will see that they can now do things they used not to. With time, this will increase their confidence in their abilities to overcome challenges.   Hacking the remote work commutes into learning is a way to show how to embrace change. Not only is it a way to adapt to new disrupting constraints, but it’s also a way to get even better.      Good Morning Learning is not only about coaching! It’s also great at increasing everyone’s knowledge and skills!      What Bode was saying was this: “Knowledge and productivity are like compound interest.’’ Given two people of approximately the same ability and one person who works ten percent more than the other, the latter will more than twice outproduce the former. The more you know, the more you learn; the more you learn, the more you can do; the more you can do, the more the opportunity - it is very much like compound interest. I don’t want to give you a rate, but it is a very high rate. Given two people with exactly the same ability, the one person who manages day in and day out to get in one more hour of thinking will be tremendously more productive over a lifetime. [“You and Your Research” - 1986 Speech by Dr. Richard W. Hamming]    Compound learning is extra-ordinary! (Andrew Barry says magical) Over the long term, it makes us more productive and keeps our bosses happy. Higher productivity also means a more sustainable pace. A more sustainable pace is an opportunity for us, geeks, to have more time to enjoy life!      …One last thing: you, as a coach, will learn too!   How to set it up   Starting your own Good Morning Learning sessions is very easy.      Open your calendar app   Schedule a 30 minutes daily appointment at the time you used to commute   Invite your coachees and a guest   During the session:      Everyone forks off to study something for 20 minutes   Everyone joins back the group after 20 minutes and shares what they have learned   If you have too much to share in 10 minutes, split the session into 2. Every group will now have to invite a guest ;-)   💡 Good Morning Learning is also a nice corporate hack! Are you a ‘guerilla’ coach with no formal authority to set up such sessions? No one will be able to justify that you should be cranking out features at the time you used to commute! (NOTE: check “How the Samman Method helps to sell technical coaching internally?” to spread coaching wider in your organization)   Why does it work?   Many simple reasons make the Good Morning Learning habit work:      It’s motivating! Daniel Pink told us in Drive that Mastery is a crucial element to intrinsic motivation. Growing your mastery on your topic, before anything else, will make every day feel more valuable!         Having to share what you learn with your teammates adds a dash of social pressure. This nudges us into using these 20 minutes of learning efficiently.   Active recall is an essential principle for efficient learning. (If you want to learn more about active-recall, go through the great MOOC Learning how to learn). Reformulating for your colleagues forces you to organize your new knowledge. In the end, you will remember better!   The ‘rule’ of inviting someone from another team to your session is a viral hack. People who tried Good Morning Learning love it and will want to continue. Most of the time, they will replicate it in their teams when the group grows and splits.   Follow the steps!      To conclude, my advice is:   Give it a try!   Starting ‘Good Morning Learning’ is a win-win strategy in almost all aspects:      It’s free in the current #WorkFromHome context. Stick to remote work commutes time, and you will be safe!   It sets up a compounding learning habit for you and your coachees.   It’s a motivating way to coach the growth-mindset   Just do it now!      Open your calendar app   Schedule a 30 minutes daily appointment at the time you used to commute   Invite your coachees and a guest   During the session:      Everyone forks off to study something for 20 minutes   Everyone joins back the group after 20 minutes and shares one by one what they have learned   I wish you all a happy learning time! I’d love to read how it went for you. The comment section is yours, for both problems or successes!      Addendum: Do You Want Slides?   Fabien Hiegel took the time to create a nice presentation that wraps up Good Morning Learning. It’s perfect to introduce it and convince your team of giving it a try.     By Fabien Hiegel, under CC BY-SA 4.0, sources     Here are other similar articles that you might find useful      How to start a team coding dojo Randori today is about starting a similar learning session for coding   How to keep up with software technologies presents how ever-green theory helps us to learn new technologies faster   How I got my feet wet with machine learning with ‘The First 20 Hours’ is the story of my first try at Josh Kaufman’s 20 hours technique of learning.   How to learn a programming language in just 20 hours presents how to use code katas to master a new programming language swiftly.  ","categories": ["coaching","learning","personal-productivity","remote"],
        "tags": [],
        "url": "/growth-mindset-coaching-turn-remote-work-commutes-into-team-learning/",
        "teaser": "/imgs/2021-01-25-growth-mindset-coaching-turn-remote-work-commutes-into-team-learning/Good-Morning-Learning-teaser.jpg"
      },{
        "title": "7 collaboration ideas to make remote pair programming more fun",
        "excerpt":"Prolonged remote-only work can be depressing. Here are 7 remote-pairing collaboration ideas to inject fun into the teams we are coaching!         I can’t wait to meet my colleagues in person again! [Someone interviewed during the COVID lock-down]       I’m the first to agree that no such solution is the same as being in the same room [Larry Brunelle in extremeprogramming@groups.io]       As far as teaching it (Note: evolutionary design), nothing works better than sitting with them […] [Jeff Langr in extremeprogramming@groups.io]    I met my friend Ahmad 6 years ago when he joined our Paris based team from Beirut. He was spending most of his time remote working alone. After a few months, the isolation was wearing on him. We decided that one of us would spend a few weeks at the other office every 4 months. This was a tremendous improvement for Ahmad! Many of us are currently forced into prolonged remote work. Unfortunately, going to the office is not an option!   The default remote break is to remove your headset and move away from the computer. The default in-person break is to find a buddy, walk to the coffee machine, and have a chat! Teamwork builds on an invisible network of tight social bonds. These bounds have very little to do with work topics! Remote work makes it way too easy to only talk about work with your teammates. We need substitute activities!   Solid humane collaboration is the foundation of any teamwork. It is our job to show how to sustain and grow this humane collaboration through remote work. We spend a lot of time remote pair programming with the team members. This is an excellent occasion to show how to inject some fun and humane collaboration into daily work!   The obvious   I’ve already blogged about remote pair programming in the past. Here are two pre-requisite to any form of more humane remote collaboration:      Put your cameras on: “Without the cameras on, we wreck what’s left of non-verbal communication.”   Take regular breaks. It’s impossible to have pleasant or fun interaction when we are tired. On top of that, many of the following fun collaboration ideas happen during breaks!      Now that you have set up face-to-face interaction and a sustainable pace: let the fun begin! Here are remote work collaboration tips to make pair programming more fun.   1. Pause and chat   Take the time for regular pauses and have a non-work-related chat with your coachee.   Are you wondering how to get this started? Here are a few example questions you can ask your buddy:      “What coffee do you drink?” (When you come back after a break with a coffee)   “What book are you reading currently?”   “Have you seen any good movies lately?”   “What is your next holiday plan?”   (If you wonder, I get my incredible inspiration 😉 from The 25 best icebreaker questions for team-building at work.)   2. Have a snack together       […] pretty much anything you can do to get them (Note: team members) to take care of each other will help – team lunches, birthday parties, etc. If they are used to taking care of each other away from the code it will feel natural to take care of each other within the code. [Phil Goodwin in extremeprogramming@groups.io]    Sharing food builds deeper connections between humans. Take a moment to share your afternoon snack together. If you have more time, you could even have a remote lunch!      We had our first remote lunch at the office with Ahmad many years ago. It looked like a wacky idea at the time, but it was actually a lot of fun! We even learned why Lebanese people don’t cover their roofs with solar panels!   3. The Home Tour   Here is an idea I got from Rachel Davies in her talk “Sustaining Remote-First Teams.”      The practice is straightforward. Take a break and carry your laptops to walk your buddy around your home. It’s just as you would do with a first-time visitor. It’s a great way to welcome the collaboration and kick off constructive work!   4. Unboxing ceremony      Just a sec, I have a delivery!       We’ve all heard this sentence a few times during 2020! First, we can use this interruption to take a short break. Breaks are always welcome 😉. Second, we can have an ‘unboxing ceremony’ (this is another excellent idea from Rachel Davies.) Shake your box around and try to have your buddy discover what is in there!   5. Window Views   I’m lucky enough to have a garden and to a nice home-office window-view. Sharing what we see helps us to connect to the other’s point of view.      This can also serve as a great morning energizer!   6. Fun Videoconference Backgrounds   Hopefully, your video conferencing system lets you set custom backgrounds. It turns out this feature is an excellent way to build elaborate pranks. Here are a few things I tried:      Use a weird-for-work background, like a circus, for example.   I am using a walking treadmill for a few hours every day. I set up a “Great Wall of China” background to make it look like I was traveling around the world!      I depend heavily on putting provocative things up on the wall and seeing the reactions. [George Dinwiddie on extremeprogramming@groups.io]       Sticking posters on the walls is a well-known coaching technique. Remote work prevents posters, but videoconference backgrounds are the next best thing! Any presentation software makes it easy to hack messages on top of pictures.   I once had to go to the company building during the lock-down. I seized the opportunity to take a picture of my real office background. Using it as a videoconference background surprised everyone: “How did you go to the office during the lock-down?”   Finally, you can combine any of the above. For example, here is a background I prepared on a late Friday afternoon! It really looks like Darth Vader is looking over my shoulder! This made my colleagues laugh a lot the first time they saw it.      7. Online Kudos   Sending Kudos is a cheap way to show your coachees how to bring more humane collaboration in the workplace.   Kudo boxes are the usual way to send Kudos to your teammates. I remember we used to send Kudos cards to Ahmad by mail. He liked to receive a letter full of kudos. Unfortunately, we did not stick to the practice for very long…      A more straightforward way is to use your team chat! For example, Microsoft Teams has a ‘praise’ plugin that lets you send short kudos. When you notice your pair is doing something worth a Kudo, send him one through a team channel. Everyone will see the Kudo, and the habit is more likely to spread.   Coach by example!   Next time you remote pair program, try some of these fun collaboration ideas. As technical agile coaches, it is our responsibility to show that:      There is a better way   Humane collaboration is the foundation of effective teams   We need to embrace change   Here is a summary infographic that you can print and stick on your wall to keep these tips in mind!              By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        Most of all, I would love to read your own ‘humane’ hacks to remote pair programming! The comment section is yours.     Here are other similar articles that you might find useful      5 pair-programming tips and tricks for coaching a remote team will make your life easier if you are coaching a team remotely   How to run a Remote-First Open-Space Technology Un-Conference is a step by step guide to organize an effective remote workshop to harness the collective intelligence of the team you are coaching.   7 Remote pair programming best practices Q&amp;A contains tips to help you and your coachees run effective remote pair programming sessions.   Best open source tools for remote pair programming presents different open source tools to setup efficient remote pair programming coaching sessions with your coachees in no time   How to avoid unnecessary meetings (a takeaway from Devoxx France 2018) summarizes how open source contributors collaborate effectively to make decisions without meeting!   How to use Mob Programming at the rescue of Pair Programming burnout: if your coachees find pair programming exhausting, you can suggest doing all pairing sessions together as a mob!  ","categories": ["coaching","remote","pair programming","team building"],
        "tags": [],
        "url": "/7-collaboration-ideas-to-make-remote-pair-programming-more-fun/",
        "teaser": "/imgs/2021-02-15-7-collaboration-ideas-to-make-remote-pair-programming-more-fun/Remote-Fun-Pair-Programming-teaser.jpg"
      },{
        "title": "How to coach developers to get a chat with their product experts",
        "excerpt":"Getting these crucial chats with product experts is often a brain-teaser! Let’s show devs how to trigger short, practical, immediate Example-Mapping sessions.         Unfortunately, the tools have taken over the meaning, and I need to chant, “The card is a token for an ongoing conversation” over and over. [George Paci extremeprogramming@groups.io]       Engineers no longer can talk to users. [Slava Imeshev extremeprogramming@groups.io]       What we did was wrong. - [Note: and so we had to] Rip out the wrong code and tests and start over with the right functional test. [Steve Gordon extremeprogramming@groups.io]    Product experts are crucial… Yet unreachable!   Developers won’t do an excellent job without first-hand domain knowledge. It is no coincidence if XP recommends an onsite customer or Scrum a dedicated Product Owner.   When developers and product experts don’t collaborate enough:      The team risks building the wrong stuff. As a result, devs will need to throw away a large chunk of work and rebuild it.   The team creates more bugs. In the end, everyone spends more time bug fixing and less time building new features.   In the long term, these problems erode trust and leave no time for more valuable topics.   Unfortunately, it’s often tough to get the needed information from product experts!      First, product experts are already busy on the other side of the product definition work. Spending time with developers often takes a back seat.   Second, being experts, they forgot what it means to be new on their topic. They tend to omit many details, thinking these are obvious or that ‘everybody knows this’. Unfortunately, developers, who are not product experts, need these details!   On top of that, they have no idea what programming is about. Product experts tend to underestimate the details that writing working software requires.         Finally, the ‘specification’ handover mindset is still pretty alive in the workplace. Many product experts’ job is to write detailed specifications for developers to execute.   Example-Mapping injection!   Here is a recipe to get product experts involved in a conversation about a story.   Are you noticing collaboration issues between your coachees and the product experts? Here is what you can do:      Find 1 or 2 team members who are suffering from a lack of collaboration with product experts.   Make sure they are willing to experiment with you.   Present and practice Example-Mapping with them   Ask them to be on the lookout about situations that would need a product expert. Tell them to call your little group when they spot such a problem.   As soon as this happens:            Call the domain expert as a group.       Tell him you are blocked.       Ask for an on-the-spot 15 minutes remote chat.           If he cannot right now, schedule something with him as soon as possible.   Use these 15 minutes to run an Example-Mapping session.   Help to convert the notes from Example-Mapping to automated acceptance tests.   Repeat until Example-Mapping becomes a habit   It’s as simple as that! Direct phone calls are still the fastest way to get in touch with someone. It’s like if bureaucracy makes us forget that! Fortunately, technical agile coaches are here to challenge the status quo 😉   An Example-Mapping success story      A few weeks ago, I worked with a team that wanted to collaborate more with the product experts. They had tried a different approach, but with mitigated results up to now.   For Example, I had helped them to run a Big Picture Event Storming a few months ago. The feedback from the different audiences showed the communication gap:      Developers had enjoyed the Event Storming. They said it had helped them to understand the domain a lot better   Product experts, though, estimated that they had not shared that much information!   This time, I paired with a developer to experiment around Behavior Driven Development. We started the day by calling a product expert, and we ran a 30 minutes Example-Mapping session.   At the end of this experiment, they decided not to take 100% of BDD but to stick to Example-Mapping sessions! Here is the kind of feedback that we got from product people:      I really liked behind included in the work, engineer and designer working together is still a pleasure.       I already started to look at the domain from an engineering point of view in order to improve my design thinking.    Example-Mapping is excellent at hacking more collaboration between developers and product experts.   What is Example-Mapping?      At this point, you must be wondering what Example-Mapping is? It’s a structured conversation format that was invented by the Cucumber team. It’s a way to capture the business rules and associated examples out of a story.       It captures just-enough details for devs to build the right thing.    It’s then straightforward to convert Example-Mapping notes into automated acceptance tests. No more throwaway and rewrite, way less bug fixing… Example-Mapping is one step more towards calmer work!   In our situation, it also has many extra advantages:      Sessions are quick. There is no need to schedule a formal meeting.   The format is simple. We won’t frighten the product expert.   It’s easy to learn. We can master it in 1 or 2 hours max; we don’t need to organize a long training.   It’s remote-friendly. This makes Example-Mapping perfect for injecting in a telephone call.   What does Example-Mapping look like              Sample Example-Mapping cards on the introductory post about Example-Mapping, by Matt Wynne        Example-Mapping uses colored cards to capture the details about user stories:      A yellow card for the title of the story   Blue cards for business rules   Green cards for example-scenarios associated with each business rule   Red cards for problems or unanswered questions   With this in mind, the group simply discusses the story and tries to find the rules and examples. Everyone should bring their unique point of view to make the exchange more valuable. Someone in the group is the notetaker. This person keeps track of the conversation by writing and arranging cards on the table.   That’s almost all there is to know!   How to coach Example-Mapping   Example-Mapping is straightforward, but it still needs a bit of practice to get used to. On top of that, you want the product expert to have an excellent first experience with it.   First, learn how experts are doing.   There is a lot of existing material to learn Example-Mapping on the web. The Cucumber team created a webinar to teach Example-Mapping. The simplest thing to do is to watch it with your coachees and have a quick follow-up discussion.   You might also have a look at these other references, depending on your styles of learning:      Cucumber’s introductory blog post about Example-Mapping   Another blog post from them   If you prefer to go in-depth, you might have a read at BDD Books: Discovery   There is even a free course on Pluralsight, but I did not went through it   Would you instead give a short crash-course yourself? Here is a presentation made up by my colleague Matthieu Tournemire. We’ve been using it successfully at Murex for a few years now.              By Matthieu Tournemire and Philippe Bourgau, under CC BY-SA 4.0, Full Presentation        Second, practice!   Your coachees should now have a clear idea about how an Example-Mapping session should go. Let’s get them to practice a bit! Find a user story that one of your coachees knows well and that the others don’t. Take 30 minutes to try Example-Mapping on this story. Park any question or blocking point on red cards.   Run as many sessions as needed for your coachees to feel really at ease with Example-Mapping. It won’t take you more than 2 hours in total.   Tools   Example-Mapping was first thought of as a co-localized activity. If you are all in the same room, all the material you need is a set of colored cards and a few sharpies.   Co-localized Example-Mapping is simple, but…      Sometimes, co-localization is not an option!   Even if you are all in the same building, Example-Mapping needs everyone to be in the same room! This is yet another barrier to reach the product expert.   It’s effortless to run an Example-Mapping remotely! I don’t know of a dedicated Example-Mapping online tool, but here workarounds:      An online collaborative spreadsheet might be all that you are looking for. Here is a template you can reuse. Fill any cell, and it will color according to the row’s nature.              By Philippe Bourgau, under CC BY-SA 4.0, full spreadsheet template           An online whiteboard, like Miro, works wonders to simulate cards.   Last, one of the significant advantages of using real cards and pen is that you can draw! A picture is worth a thousand words. It’s not a surprise that drawings are great for capturing scenarios. If you have a device that lets you draw well enough, then, by all means, use it!   Don’t forget to share the screen so that the product expert can see what is being captured. The above tools also enable collaborative editing. Once everyone is used to the format, it’s better to share the link instead of the screen!   Give it a try!   Keep your ears open when you are with your coachees. Next time you hear someone having troubles understanding what they are supposed to do:      “I don’t understand the specs!”   “I have no specs!”   “I don’t know how to test this story!”   “Jim (the product guy) is never available!”   Try to get them into trying Example-Mapping with the product expert.   There is more to Example-Mapping than what I quickly presented. For Example, you can also use it to…      Detect stories that are not ready.   Detect stories that are too big and split them.   Write and automate Gherkin scenarios.   …   If you have questions about Example-Mapping, don’t hesitate to ask! I’d also love to read how your own experience went: the comment section is yours.     Here are other posts that might interest you:      Organization refactoring: Event Storming and DDD injection. It presents a ‘refactorging’ technique to introduce Event-Storming after Example-Mapping   5 Views to Capture the Outputs of an Event-Storming workshop. It explains how to use Example-Mapping to capture the outputs of an Event-Storming workshop   First rule of DDD is: let’s not talk about DDD. Fun infographics explaining that it’s better to do DDD instead of frightening people with it   Event Storming lessons from Post-It haters. A reflection about why stickies are so effective for group collaboration  ","categories": ["coaching","ddd","bdd","refactorging","example mapping"],
        "tags": [],
        "url": "/how-to-coach-developers-to-get-a-chat-with-their-product-experts/",
        "teaser": "/imgs/2021-03-29-how-to-coach-developers-to-get-a-chat-with-their-product-experts/Collaboration%20Injection-teaser.jpg"
      },{
        "title": "How to make change stick by training part-time technical agile coaches",
        "excerpt":"Too often, teams revert to their old habits when the technical agile coach leaves. Training part-time coaches is a powerful way to make the change stick.      Changing the technical practices of the teams is already a considerable challenge. Making the change stick after we leave is even more challenging!      I needed to go out on medical leave for a while, and when I came back, every single practice I had introduced got rolled back, with expected results. [David Kramer on extremeprogramming@groups.io]       “I was shocked when I heard the devs chose to abandon disciplines that had helped to save the business” [Robin Dymond extremeprogramming@groups.io]       The previous team we worked with, which we have now quit, used to mob while we were there, seems not to be mobbing anymore 🤷🏽 [Someone on Okiwi slack]    I guess we have all faced this situation. Wasted work is never fun. If this repeats too often, it can make us question the usefulness of our jobs. If we are not careful, this can lead to burnout!   My team of technical agile coaches and I have had the luck to stumble upon something that works at Murex. It could work in your organization too!    Coaching coaches!   At Murex, we are using an ‘internal start-up’ model to spread technical agile coaching. About 2 years ago, we were only 2 full-time coaches, and we had worked with 4 teams over the previous year. Murex has more than 60 development teams. Coaching all teams with the same recipe would have required 7 more years!      This was too slow. We went through different options: outsourcing, digitalization, and others. We finally agreed on training developers into part-time technical agile coaches. The official goal was to increase our coaching capacity. We also had the intuition that this would create lasting cultural change.   We started with Patrice, a very experienced developer from one of the first teams we had coached. After some negotiation with his managers and a few other parties, he joined us for 20% of his time. We would not have been able to negotiate a company-wide part-time coach role. Negotiating on a case-by-case basis proved to be way simpler!   We started to spend our Thursdays together, working the ins and outs of our material. At the time, this meant going through code katas. We also used these Thursdays to write down the critical learnings of every exercise. (Note: Tell me if you would like me to write in more details about the different plans of katas we are using with teams.)   After a few months, Patrice felt confident enough to run the katas with a group of newcomers.   Since then, Patrice has been coaching other teams and groups. Yet, the primary outcome has not been our coaching capacity increase! Patrice has been coaching his team full-time for more than a year!   For the past years, his team has been improving all business KPIs. Better still, his teammates have attested that work is both calmer and happier!   Since then, we have been working with 3 other part-time coaches: Antoine, Mirna, and Myriam. We are starting to observe positive impacts in their teams too!      We are now using our Thursdays as a kind of coach the coach dojo. Experienced and junior coaches mob through various katas and learn from each other. We have seen yet another benefit of working with part-time coaches! They are pushing us to make our activities more adapted to Murex developers.   How to do it?   Here are the steps we took to make this happen:      When coaching, look for enthusiastic people who get it.   Ask them if they would like to become part-time technical agile coaches.   Together, pitch this career evolution to their managers.   Use this negotiated time to rehearse your main coaching activities with them.   When they feel ready, embark them as pair coaches on new coaching missions.   When they feel ready, let them run their own coaching missions.   As a by-product, they will now be coaching their teams full-time!   Give it a try yourself!   The tricky steps are 2 and 3. No other step ever failed.   Step 2: Expect many NOs.   In our experience, the second step was the most critical. Many people declined for lack of time. We had to approach 11 people to get 4 OKs. People are busy! Don’t abandon at the first refusal. I would ask at least 10 candidates before declaring: “This does not work here!”   Step 3: Organization Aïkido      Step 3 was rocky, especially the first time. We managed through by playing by the organization’s rule:      We knew many would meet this unusual request with skepticism. We started by turning a few critical people into allies to help us get an OK from others.   We met these key people one by one and prepared specific pitches to ‘sell’ our proposal. Here is a list of the typical questions we were asked. I did not provide the answers because they are highly organization-dependent. Answering the questions for your organization is a great way to prepare for the pitch. (Note: My colleague Matthieu Tournemire created a wonderful template to create strong pitches. Don’t hesitate to ask if you would like to learn more about writing compelling pitches.)   Murex has a strong delivery culture. We knew this role had to be official, or developer BAU would eat the 20% up. We formalized a part-time technical agile coach role to follow company standards. This ensured part-time coaching was taken into account during plannings and end-of-year reviews.   Knowing the organization was a clear unfair advantage. You, too, will have to understand your organization’s implicit rules to make your case.   Give it a try now   Organizations are complex beasts. There is no silver bullet for refactorging (Changing organization)! From experience, though, growing part-time coaches has good chances to make change stick. Fortunately, it’s also straightforward to try:      Whenever you encounter coachees that are enthusiastic, and that gets it   Ask them if he would like to become a part-time technical agile coach   Follow the steps above   I’ll continue to blog about our technical agile coaching adventure, so stay tuned. I’d also love to read about your own tips and tricks to make change stick. The comment section is yours!        If you liked this post, you might also learn more about how to make the change stick in the following posts.      Are you interested in the big-picture of our technical agile coaching at Murex? Read: The story about how we do Agile Technical Coaching    Are you an internal or guerrilla coach? Learn How the Samman Method helps to sell technical coaching internally?    Are you wondering what where the activities we practiced with part-time technical coaches? Read: A coding dojo exercises plan towards refactoring legacy code   Coding Standards are another way to make change stick. Here are 3 Good and Bad Ways to Write Team Coding Standards and Conventions   With time, expertise in agile development practices makes part-time technical agile coaches ‘badass’. Learn more in Why we need Badass developers to perform large scale refactorings  ","categories": ["coaching","change management","badass-developer"],
        "tags": [],
        "url": "/how-to-make-change-stick-by-training-part-time-technical-agile-coaches/",
        "teaser": "/imgs/2021-04-08-how-to-make-change-stick-by-training-part-time-technical-agile-coaches/change-glue-teaser.jpg"
      },{
        "title": "How to coach a team that has been burnt by bad TDD",
        "excerpt":"Once exposed to bad TDD, teams are a challenge for any technical agile coach. Instead of trying to sell them TDD, XP, or whatever, fix their pains!      TDD is a cornerstone of what we coach. Unfortunately, there is a lot of misunderstanding about TDD. Search the web for ‘bad TDD’ to get an idea. Unfortunately, after a team has been exposed to bad TDD, it won’t try it again! How can we get around this extra hurdle and do constructive technical agile coaching?      I’ve wasted way too much time learning from those who took it upon themselves to teach something they barely understand themselves. [John Welty on extremeprogramming@groups.io]       I am least successful when people, for whatever reason, are unable to hear my ideas. It may be that they were mandated to attend a training or receive coaching, or it may be that they feel none of the changes I am bringing are in their best interests. [Robin Dymond on extremeprogramming@groups.io]       We’ve already gone through a TDD training before. We found it very dogmatic and difficult to apply. We prefer to sticking to what we are used to rather than to spending more time on TDD. [A developer who attended a previous TDD coaching]    My suggestion is pretty simple:   Don’t talk about TDD! Instead, use TDD to fix their pains!   The infamous TDD training   Murex, the company I work for, used to have TDD training in its catalog. Murex is pretty generous with training, and it’s pretty straightforward to attend one. As a result, many developers had learned TDD this way. Unfortunately, the training was not good.      Many developers had concluded that TDD was expensive and useless. Some even took bad habits after attending it. For example:      Writing all the tests before writing all the code.   Testing every function independently. This leads to heavy mocking and makes the code harder to refactor.   After my first success with the Deadpool team, I started a weekly TDD Coding Dojo to find new teams to work with. Very few people came 😞. I had to find another way.      Forget TDD as a goal   TDD is no longer the ‘killer feature’ that sells agile technical coaching to teams.   In my case, the manager of the Deadpool team found another team for me! He suggested that I check if there was an opportunity with the Kirby team.   As you might already know, if you are a regular reader, I only work with volunteer teams. If I wanted to work with Kirby developers, I needed to find a way to make them want!   Bad TDD is dogmatic, overpromises, and under-delivers. Unfortunately, some Kirby team members had attended the infamous TDD training. I knew that mentioning TDD would play against me.   Understand their pains   The first thing to do is to understand the team members’ pains. There are many ways to identify these pains:      You can meet with all the team to have a group chat or to run a pain gathering workshop   You can also have one-to-one interviews with team members. This is useful if you suspect that not everyone will speak up in front of the group   Otherwise, you can Gemba-pair: spend a few hours to pair with everyone in the team      What is essential at this step is to avoid selling them anything. Just observe, be helpful when you can, but really, observe.   In our case, the Kirby team members were having a hard time maintaining legacy code. Here are examples of pains we have seen in other teams:           We have good test coverage and a lot of tests, but everything turns red when we make the smallest change!     We don’t know what kind of tests to use to test the different parts of our system!     The business is pushing is so hard that we are taking technical debt all the time. How do we find the time to refactor!     The team just faced important turnover. How can we rebuild the team dynamics without losing 1 year?     …      Design a custom coaching plan   Once you understand their day-to-day issues, it’s time to come up with a fix!   Maintaining legacy code is a widespread pain for software teams. I could leverage what I had done with Deadpool to propose something to the Kirby team.   In other cases, you might be able to fix their pains without programming. If you can, then do it! You will save everybody’s time, and the team will be more likely to come back for more.   For example, we have designed a test strategy workshop. It helps team members to agree on how to test the different aspects of their system.   Other times, though, fixing the pain will involve installing new programming habits.      In this case, avoid general agile development practices coaching. Instead, come up with a coaching plan that is fixing their specific pains. For example, we used the following coaching plan to help the Kirby team deal with their legacy code.                  Exercise       How       Learn about…                       Bowling Game kata       Prepared kata Demo       Good TDD                 Roman Numerals kata       Randori in Pairs       Emergent solution                 Mars Rover kata       Randori in Pairs       Emergent design                 Game of Life kata       Randori in Pairs       Bottom-Up (= inside-out) TDD                 LCD kata       Randori in Pairs       Top-Down (= outside-in) TDD                 Game of Life kata       Randori in Pairs       Mix Bottom-Up and Top-Down                 Median of list of lists kata       Randori       TDD on algorithms                 Roman additions kata       Mob       Mobbing                 Gilded Rose kata then production code       Mob       Approval Testing and refactoring                 Gilded Rose kata then production code       Mob       Strangler application                 Trivia kata then production code       Mob       Mikado Method                 Trivia kata then production code       Mob       ACL and Bubble Context           Printable version   It relies on a mix of code-katas and mob sessions. This is one way of doing it. The Samman Technical Coaching method uses a blend of learning hours and mob sessions. On our side, we have settled on the following recipe for most of our coaching plans:      Start with a series of katas around TDD.   Continue by mixing            katas targetting pain-fixing techniques       mob-sessions to apply these techniques on their production code           I hear you:      But you just said to forget TDD and to focus on the pain! And now you start with katas on TDD!    As a matter of fact, we have noticed that part 2 does not work unless the team has gone through part 1! As coaches, we know TDD is essential. The key is to avoid selling it directly.   TDD as a workout exercise   With a clear pain and a coaching proposal, it’s time to pitch it to the team. Here is how to present part 1.      We have different arguments to sell the TDD part to the teams:      From experience, the coaching does not work well if we skip part 1   Part 1 will train you to new team collaboration practices like pair and mob programming   TDD speeds up learning by creating the deliberate practice fast feedback loop   Here is the script I use to explain what deliberate practice is and why it matters:      Deliberate practice is the technique used by athletes to achieve world-class performance. Suppose you are a swimmer and you are not very good at turning at the end of the lane. What will have the most impact on your performance? Swimming 2000 meters? Or focusing 30 minutes on turning only?       It’s the same for us. By setting up a fast feedback loop, TDD lets us learn faster through deliberate practice.       Also, no one learns to swim in the deep end of the pool. We all started at the shallow end. That’s why we will begin with simple exercises. We will increase the difficulty as we go. At some point, you will be able to apply the techniques to your production code.    (Side Note: lately, we have been experimenting with TCR as a new way to do deliberate practice. It looks promising. Tell me if you would like to read more about it!)   With these 3 arguments under your belt, you should be ready to pitch your proposal to the entire team. A good pitch starts with their pain, presents a fix, and continues with extra benefits.   Sometimes the pitch gets them all on board. Sometimes, though, the team needs a bit more time to decide. In this case, I suggest a first test session to help the decision.   Deliver a well-prepared kata demo   We start the TDD part of our coaching sessions with a prepared kata demo. We usually go with the bowling game score kata. It’s a great occasion to show what TDD looks like when it’s well done.               We must, in fact, remain very careful! Most developers are tired of being asked to be more productive. That’s why I don’t mention TDD as a way to be more whatever. Instead, I present it as a way to avoid being woken up by bugs in the middle of the night.   Attendees are usually enthusiastic about the demo. Here are examples of feedback I received:      I’m surprised about how nice and simple the resulting code is!       I already learned more in 2 hours than in the full TDD training I attended!    We’ve never had a team stop after the demo!   Don’t sell… Fix pains!   It ends as it started:      Don’t sell TDD, XP, or whatever… Fix real pains!    Next time you start coaching a team      Take the time to understand their pains   Present them a custom coaching plan that fixes their most important pains   Present TDD as a learning tool rather than a goal by itself   This should get you past any prior misunderstanding about TDD.     If you liked this post, you might also learn more about how to get over bad TDD exposition in these articles:      Wondering what a custom coaching plan might look like? Read: A coding dojo exercises plan towards refactoring legacy code   Want to learn more other problems you might face when starting coaching a team? Check 7 tricks to influence a team resisting to change its technical habits   Interested about how a series of code katas can also help people learn a new language? Read: How to learn a programming language in just 20 hours   Want to get the full story of how we approached teams at Murex? Read The story about how we do Agile Technical Coaching   Interested in live katas I gave? Watch Live Legacy Code Refactoring with the Golden Master at Legacy of Socrates   A complete workshop for your team to see what’s a good test strategy  ","categories": ["coaching","tdd","learning","change management","coding dojo","mob programming"],
        "tags": [],
        "url": "/how-to-coach-a-team-that-has-been-burnt-by-bad-tdd/",
        "teaser": "/imgs/2021-04-28-how-to-coach-a-team-that-has-been-burnt-by-bad-tdd/forget-tdd-fix-pains-teaser.jpg"
      },{
        "title": "5 technical agile coaching tips to fight exhaustion from laggards",
        "excerpt":"Trying to coach laggards will exhaust you. Here are 5 tips to deal with them, sustain you, and make you spend your energy where it matters.         I don’t know what to do precisely. I already sent him many times the wiki link to the coding conventions.       Is this person toxic, or am I too demanding about things I want?       For a long time, I thought I could avoid toxic behavior by spending time with these people.    I have a friend who trains managers as a living. Unfortunately, she had to visit a doctor because she was suffering from insomnia. When the doctor learned what her job was, she said that many trainers and coaches have sleep disorders!   Any activity that involves working with others and bringing change is exhausting. We can only deal with so much active listening and explaining during a single day.              Diffusions of Innovations Bell Curve (Source: http://blog.leanmonitor.com/early-adopters-allies-launching-product/)        Coaching any developer to XP will need a lot of energy. Yet, ‘laggards’ take 10 times as much! Laggards do not believe in what you are coaching. Nothing you will say or do will make them change their minds. Laggards will argue every minor detail, and they won’t change their behavior. They are not ‘bad’ people in any way! It’s just that they’ll be the last to switch to new ways of working. If you are not careful, they will exhaust you, though!      What if you knew how to make all your coaching work helpful, constructive and energizing?    Unfortunately, it’s not all ponies and unicorns. There are laggards in the teams we coach. Yet, we don’t want them to burn us out! Here are 5 tips to help you deal with laggards and make you continue to enjoy your work!   1# Only work with volunteer teams   Before even starting coaching, make everything you can to work with a volunteer team! There are 2 reasons for that:      If the team does not want to be coached, you will likely face many laggards in the team! In this case, it’s better to fail fast. Every time a team refuses coaching, you avoid a difficult mission!   Persuading the team of volunteering will set up a constructive collaboration. Working on a team’s pain points instead of general TDD or XP should do the trick. It will align everyone towards the same goal. It also de-emphasizes XP or TDD, which avoids many opinion-based arguments.   Sticking to volunteer teams decreases the risks that you will have to deal with laggards.   💡 What you can do now: Read my previous post and learn how to pitch teams about their pains.   2# A single laggard is OK   In my experience, a lonely laggard in a team will eventually leave. I now use “the rule of one”:      If there is a single laggard in the team, focus on the rest of the team.    Continue to be friendly with this person, but don’t spend your time trying to persuade them. The team’s new way of working might not suit them. They most likely will be better in another team. At some point, they’ll understand this and will move. Fortunately for us, there are currently plenty of exciting opportunities for software developers.      The best thing to do is to avoid starting coaching a team that has 2 or more laggards. If you find yourself in this situation now, I advise to try a coaching “reset”:      by refocusing on a specific pain (see #1)   or by passing a Last-Chance deal with the team (see #5)   💡 What you can do now: avoid having to coach a team with more than 1 laggard, and spend your time with those who want   3# Avoid personal arguments   Nothing is as exhausting as repeated and never-ending arguments about the same topic. Many laggards will try to get you in a quarrel about TDD, pairing, or mobbing. Don’t let yourself get caught in this trap!   Don’t forget your goal as technical agile coaches. Use the opportunity to set up an environment where the team members can resolve their disagreements together. Here are examples of what you can do:      Use the topic at hand to start an improvement kata. It will begin with data gathering and could move the team into more data-driven discussions.   Gather everyone together into an Event-Storming workshop. It’s an excellent way for a group to make complex decisions. For example, you could use it to decide whether to rewrite or refactor a piece of code.   If people are arguing over coding conventions, set up a randori kata or a mob session. Pick a problem or a piece of code to make the convention come up. Then use everyone’s presence to have an open discussion about it.   Is there an existing model around the topic? If there is, use it to go past personal preferences and have a more objective discussion. For example, Big O() analysis is better than pure intuition to know what part of the code to optimize.              Erik Dietrich author a PluralSight course to help you build a business case for technical practices. As a team, it’s a great way to overcome personal biases and discuss objectively about the pros and cons of some practices.        💡 What you can do now: refuse to enter any personal argument. Instead, call the team on the spot, prepare a custom retrospective, or schedule a workshop.   4# Regular coaching breaks   In her book Technical Agile Coaching with the Samman Method, Emily Bache recommends regularly taking some time away from the team. More precisely, she suggests alternating:      3 weeks with the team   3 weeks away from the team      It’s an excellent opportunity to recharge your batteries before challenging coaching sessions.   There is another subtler advantage. It will also let you observe if the team is improving while you are away. If it does not, it’s a sign that the team is lacking enthusiasm. It’s a warning sign that this coaching mission might end up exhausting you!   💡 What you can do now: own the coaching sessions invitations and schedule regular breaks.   5# Leaving is always an option   If you’ve tried all the above and work is still exhausting you, it is time to consider leaving. It turns out that there are different ways to quit!      You can find another coach. Maybe you were just not the right person for this team.   If you think there is still hope, try to pass a Last-Chance deal with the team. If the team does not hold up to its part of the bargain, then you can leave holding your head up.   Finally, you can also choose to be direct. Progress is too slow, you are exhausted, and you have decided to stop.   Sometimes, we need to let go of a team to focus on others where we will be more helpful. Coaching is not an absolute science, and it does not work every time. Once your mood has improved, take some time to reflect and see what you will do differently next time.      💡 What you can do now: if the situation remains non-sustainable for you, leave.   Be nice to yourself!   We are only humans, and there are limits to what we can handle. Don’t let a few laggards exhaust you! Remember these tips:      Pitch teams about their pains   Spend your time with people who want it   Use team activities to deal with disagreements   Own the calendar invites and schedule some breaks away from the team   Finally, if the situation is non-sustainable for you, leave   These tips will help you avoid “laggards exhaustion.” They will also let you show a better version of yourself to all your coachees, now and in the future. They will make your technical agile coaching work more energizing, constructive, and sustainable.   Oh… one last thing… they might also make you happier!   What about you? How do you deal with laggards in the teams you coach?     Here is a selection of related posts that will interest you:      Wondering how to find volunteer teams by focussing on their pains? Read How to coach a team that has been burnt by bad TDD   Here are 3 Good and Bad Ways to Write Team Coding Standards and Conventions   An oldie, but still relevant: How we introduced efficient agile-retrospectives   Are you looking for a data-driven way to continuously improve? Read How we used the improvement kata to gain 25% of productivity   A few examples of complex decisions you can make with Event Storming:            How to fight priority paralysis       Feature teams or component teams?       Rewrite or Refactor?       How to prototype (micro)services and NFRs       Build or Buy?          ","categories": ["coaching","sustainable pace","change management"],
        "tags": [],
        "url": "/5-technical-agile-coaching-tips-to-fight-exhaustion-from-laggards/",
        "teaser": "/imgs/2021-05-10-5-technical-agile-coaching-tips-to-fight-exhaustion-from-laggards/laggard-exhaustion-teaser.jpg"
      },{
        "title": "How the pandemic made us discover better ways of coaching",
        "excerpt":"This pandemic year has transformed the way we do technical agile coaching. It’s now more asynchronous, self-service, gamified, open, and inclusive!         I would have each share their screen and I would watch[…], flicking between screens. It was exhausting for me but otherwise worked well […]. (Avi Kessner on extremeprogramming@groups.io)       I moved one physical training session into Zoomland and the result did not thrill me. We did well enough with what we had, but I definitely did not like losing the ability to wander around the room and look over people’s shoulders to ask questions (JB Rainsberger on extremeprogramming@groups.io)       Without the travel to on-site, it seems silly to try to cram things into full days. And with the friction of remote work, seems counter-productive. I think there are better models waiting to be fleshed out. (George Dinwiddie on extremeprogramming@groups.io)    ⭐️ Wouldn’t it be great if challenges always resulted in better ways of working?   I would not have bet on that when the pandemic hit us in Spring 2020.   Murex has offices in Beirut, Dublin, and Paris. We had already tried to nudge people into remote-friendly work before the pandemic. For example, we experimented remote-first Community of Practice event… But we received a lot of push-backs!   A coach teammate, Ahmad, was working from Beirut. Fortunately for us, we were already working in hybrid mode, and the transition was easy for us. When all the company started to work from home, we thought: “It’s now, it’s going to work this time!”. We launched a Remote-Work Community of Practice, but again, the audience did not stick.   Lock-down had suddenly made coaching, relying on pairing and mobbing, a lot harder! We had no idea how we were going to manage.   Yet, you know the saying:      Necessity is the mother of invention    Like everyone, we had to adapt, and we learned a ton by doing so! We had to transform our material and pedagogy. Our coaching techniques are now more remote-friendly, asynchronous, inclusive, open, and self-service! Like many, we are suffering from COVID anxiety and Zoom fatigue. Yet, some aspects of our work are now both more sustainable and effective.      One thing is sure: we will never work as before, even when we get back to the office! Here are a few examples of the changes we made:   Remote Mob Tooling   Before the pandemic, we had been using git-handover to rotate drivers in remote pairs and mobs. We relied on the watch.sh script to keep repos in sync. (Find out more in Best open source tools for remote pair programming). When all our coachees started to use this script, we had to seriously improve it.   Here are a few unexpected outcomes that came out of this:      The script is now more portable and lets everyone mob from his preferred setup.   We now have a git handover + TCR script! It turned out we had been experimenting with TCR when the lockdown started. It just made sense to combine the 2 scripts together!   We are planning to open-source this script!      💡 If you struggle with remote pairing or mobbing, give watch.sh or the mob tool a try!   Reusable Remote Workshop Templates   Before the pandemic, we used to rely on everyone being present during workshops. Like every agile coach, we were using a lot of sharpies and sticky notes. Like every agile coach, we used to redesign a new workshop every time.   Everything changed when the lockdown hit us. The company was already using Miro; it felt natural to port our workshops there. It was awkward at first: small screen, clumsy manipulation… After a while, though, we started to notice some benefits:      We were able to reuse and improve activities every time we went through a similar workshop. For example, we have built a Test Strategy workshop board that is being reused a lot at Murex.   We replaced long workshops by successions of shorter and sustainable sessions. We discovered that remote workshops become a total waste of time when they last longer than 1h30. Inspired by Training from the Back of the Room, we now ask participants for some asynchronous homework between sessions. Many actually do them!   With all this experience under our belt, we ran a successful remote code-retreat at DDD Europe!   We are now refining and adding facilitation instructions to our workshop templates. We are slowly getting to self-service workshops!      💡 Do you want to learn more about remote facilitation? Nick Tune wrote fantastic posts about how to create great workshops using Miro:     Remote Workshops Using Miro: What I’ve Learned So Far   Organising Large Miro Boards For Remote Workshops   Written communication for tech coaching   We used to stick posters on the walls to get people’s attention. I tried to do the same next to my desk at home, but no one noticed 😉.      Fortunately, I have been blogging for a long time, and it was natural to start blogging at work too. Blogging is a long-term strategy, though. Plus, intranets are not yet as inbound-marketing friendly as the internet. Yet, a few months in, people are starting to ask for help after reading our posts.   Here again, the pandemic brought unexpected benefits through our internal blogging:      We are more visible! We are getting more inbound work. We only work with volunteer people and teams, and more inbound work means more freedom.   As we reach more people, we can hope it will bring more change to the organization. Especially since some teams don’t want coach interference but prefer to practice by themselves.   As I mentioned above, many Murex teams are spread over different countries. Making all communication online bridges the gap between offices. In the end, it also makes work more inclusive.   We post self-service instructions to other activities like Miro templates or Code katas!   💡 Reading Technical Blogging is an excellent way to start blogging! (NOTE: I never followed any formal course about how to start a blog, but it would have been more effective if I had! I just began to share memo notes, and I would not recommend this as a marketing strategy!)      Video-Gamification for tech agile coaching                  We’ve been dabbling with TCR (Test &amp;&amp; Commit               Revert) among coaches for a while. We started from Thomas’ Denifel scripts and tweaked them.           As I wrote above, the pandemic made us merge TCR and git handover scripts into one! This made it easier to use TCR.   We’ve been using TCR for all our coach-the-coach sessions for a few months, and we love it! Lately, we’ve even started to use TCR in katas with teams, and it’s been working surprisingly well!      People like it!            They say it’s fun, that it’s got a poker-like flavor: “I’m betting my work on its correctness!”       We don’t like others to point towards our mistakes. Having a script revert our failing changes is OK. Having an annoying coach repeating to take small steps all the time is just painful.           Given the TCR success, we are exploring other ways to gamify coaching:            Running Diego Lemos’ extreme-carpaccio workshop.       Running Michel Grootjans’ Kanban Simulator with whole teams.           Gamified, and especially video-gamified, activities are more self-service by design!      💡 If you haven’t already given TCR a try, I urge you to have a look at Thomas Denifel’s scripts.   💡 Don’t hesitate to ask and subscribe if you would like me to write more about any of the topics above.   Call to action   Step by step, we are turning some of our work into self-service material! Self-service material means that we can have       More impact   A more sustainable pace   And more exciting work!   The constraints of the pandemic forced us into innovation. The key lesson here is that there are always better ways to work! Let’s not wait for the next pandemic! Let’s experiment with artificial constraints to trigger innovation!   💡 Are you struggling with the post-pandemic new way of doing technical agile coaching? Give the ideas above a try!   You are welcome to share any new way of doing technical agile coaching that the pandemic forced you into!     Here are a few other posts that might interest you:      Best open source tools for remote pair programming presents our watch.sh git-handover script and other remote pairing open source tools.    Here is How to deliver a remote training with code-katas and randori in pairs.    5 Years of Blogging About Software is an old post where I explained how I started blogging.    Wondering why we only work with volunteers? Check 5 technical agile coaching tips to fight exhaustion from laggards   A complete workshop for your team to see what’s a good test strategy  ","categories": ["coaching","sustainable pace","change management","remote","tcr","gamification","blogging"],
        "tags": [],
        "url": "/how-the-pandemic-made-us-discover-better-ways-of-coaching/",
        "teaser": "/imgs/2021-06-21-how-the-pandemic-made-us-discover-better-ways-of-coaching/better-ways-teaser.jpg"
      },{
        "title": "How to measure (and report 😢) your tech agile coaching effectiveness?",
        "excerpt":"Technical agile coaching is complex. Many organizations would track our work with inadequate measures. Here is how hypothesis-driven goals can keep us safe.   Coaching is challenging to measure, and it can be demotivating.         Measuring performance in software is very tricky […] the coach position is uneasy, maybe even dangerous and counterproductive! And depends a lot on the surrounding culture (Okiwi Slack)       I have a hard time making efforts on something that I know is useless… Coaching is not useless, but it’s so slow! And full of disappointments (Okiwi Slack)       The first disillusion is when you realize that whatever you do, you will change almost nothing! (Okiwi Slack)    Organizations are complex systems, and teams exist in these systems. Complex systems always react to change in unexpected ways. In fact, the behavior of complex systems only makes sense as a-posteriori. (TODO link) This means that change management will be challenging to measure.               Henrik Kniberg explains how he does it in his talk Confessions of a change agent. Eventually, he accepted that we cannot measure the effectiveness of an agile coach. So here is what he does instead:      Are people happy to see me around?    Yet, coaching without knowing if you are effective can become depressing. Unfortunately, things get even worse in command and control organizations!   The extra challenges with Command and Control organizations      Being a coach, mandated from above and who’s job is to tell you the right way to work” (Okiwi Slack)       Coaches and devs don’t share the same position, goals, interlocutors, etc. Indeed, this creates an unbalanced situation between coaches and devs” (Okiwi Slack)    In these organizations, coaches are often mandated to ‘transform’ teams. The underlying goal is to improve productivity. This means that at the question:      How to measure a tech agile coach’s effectiveness?    These organizations will ask:      How fast teams are being transformed?   How much more features can the team deliver after it has been coached transformed?   This mission is problematic in 2 ways:      First, we don’t know how long it will take to coach a team into empowerment. Yet our customers evaluate us on the speed to ‘transform’ teams!   Second, higher productivity is only one possible outcome of team empowerment. Still, our customers are evaluating us only on productivity improvement!              Watermelon metrics are green from the outside, but red in the inside! It’s all too easy to game measures when they are linked to inadequate objectives.        As a consequence, here is what often happens when coaching becomes difficult:      A. The coach games the measures. Ex: Spend the least time possible with each team. Then find an accommodating measure for productivity, whatever the actual outcome.   B. The coach gets screwed because things did not work according to the plan. Ex: The team decides that it will take some time to pay back some of its technical debt. As a result, productivity decreases in the short term.   This can make you doubt your whole coach career!   Actionnable, short-term goals   Here is a template I use whenever I have to provide measurable goals.      We believe that &lt;Intention&gt;     Will result in &lt;Outcome hypothesis&gt;     We will know we have succeeded when &lt;Outcome signal measure&gt;     To run this experiment, I will &lt;Measurable actions or habits I can commit to&gt;    Let’s see where this comes from:   Hypothesis-driven Development   Fortunately, we are not alone. Product managers need complex compliant measurable goals too! The Lean Management community came up with the Hypothesis-Driven Development Story Template.              By ThoughtWorks, original in How to Implement Hypothesis-Driven Development        Your Circle of Control   The idea behind the template is to frame each team coaching as an experiment. To measure one’s contribution to this experiment, I added the last line to the template:      To run this experiment, I will &lt;Measurable actions or habits I can commit to&gt;               By discoveryinaction.com.au, originally published in Circle of Concern vs Circle of Control        In the 7 Habits of Highly Effective People, Stephen Covey presents the Circle of Influence. The Circle of Control is an extension to this model. If you want to learn more about the Circle of Control, check Cam’s post How To Stop Worrying – The Circle of Control   An example   Here is an example we could write when coaching a team that is struggling with legacy code:      We believe that coaching team X to refactoring techniques.     Will result in a reduction of their technical debt.     We will know we have succeeded when:          They spend less time bug fixing on library X     They manage to reduce cycle time on refactoring tasks on library X       To run this experiment, I will:          Spend 2 hours per week going through refactoring katas with the team     Mob with them for 2 hours on refactoring their production code, twice a week     Collect their feedback after every session and use it to improve the next session      Outcomes   I’ve observed 3 consequences of using this template:      You will be safer! As long as you are stick to your actions, you are achieving your goal. You have a lot less risk of being blamed for something that is outside of your control. This makes work a lot more sustainable and transparent for everyone.   You will make your experimental learning explicit! These objectives are no longer useless bureaucratic things. You can actually use them to iterate and improve the following experiment.   You will spread the practice! Chances are that others are struggling with their objectives too. As you share it with them, you might make them aware of the uncertain nature of their environment. Who knows, maybe they’ll start to use this template as well!      Give it a try!   If you currently need to track and report your progress, here is what you can do now:      Write objectives for your current work as you usually do.   Repeat asking yourself the two following questions:            How can I easily measure this experiment?       Is this entirely under my control?           Use the answers to guide you as you try to fill the template:      We believe that &lt;Intention&gt;     Will result in &lt;Outcome hypothesis&gt;     We will know we have succeeded when &lt;Outcome signal measure&gt;     To run this experiment, I will &lt;Measurable actions or habits I can commit to&gt;    In the next post, I’ll present the tips I use to dump measures completely and go #NoGoal whenever I can! Stay tuned   In the meantime, I’d love to read about your own objective-writing tips. The comment section is yours!    Tech coaching can be exhausting at times. Here are other interesting posts that might interest you:      5 technical agile coaching tips to fight exhaustion from laggards   7 tricks to influence a team resisting to change its technical habits   How to coach a team that has been burnt by bad TDD   How to kill Scrum Zombies?  ","categories": ["coaching","personal-productivity","customer relationship","goal setting"],
        "tags": [],
        "url": "/how-to-measure-and-report-your-tech-agile-coaching-effectiveness/",
        "teaser": "/imgs/2021-08-09-how-to-measure-and-report-your-tech-agile-coaching-effectiveness/sisyphus-teaser.jpg"
      },{
        "title": "3 Questions To Let-Go Technical Agile Coaching Measures",
        "excerpt":"Let’s give up trying to measure technical agile coaching! Instead, use these 3 liberating questions to self-assess how well you are doing at any moment.         The coach ‘stamp’ does not increase your chances of meeting someone who wants to learn! (Someone on Okiwi slack)     It’s fulfilling to manage to transmit the ‘XP virus,’ but this happens rarely!  (Someone on Okiwi slack)     I’m planting seeds, and I’m watering, but guess what, lots of other people are also planting seeds and watering and stuff, so when we look at the result, was that because of me, or was it despite me? (Henrik Kniberg, confessions of a change agent)    However hard we try, the impacts of technical agile coaching are impossible to measure.   In the previous post, I presented a template to write measurable objectives. It relies on Hypothesis-Driven development and our Circles of Control. Using this template has many advantages:      It ensures we are doing our best   It is corporate compatible   Unfortunately, deep inside, it still leaves us with the timeless questions:      Is my work helpful?     Am I making any progress?    In this post, we’ll turn the question on its head…      💡 Wouldn’t it be great if you could use the non-measurable nature of coaching to fuel your enthusiasm?    Accept that coaching is impossible to measure      By definition, the complex nature of technical agile makes it non-measurable. In this situation, it can become very demotivating.   Forcing a measure on something that is not measurable only makes things worse! Goals put our attention on the future. The gap between the current and target situations generates discomfort, stress, and anguish.      Once you accept that coaching is not ‘useful,’ it becomes relaxing, and I find it a more enjoyable job than solo dev (Someone on Okiwi slack)       It’s not important to me anymore to know exactly what was my contribution or not (Henrik Kniberg, confessions of a change agent)    If we get rid of the measures, we can put our attention back to the present moment. Caring only about what is happening now frees us from this gap-generated stress. (That’s why Scott Adams says ‘goals are for losers’!) There’s a catch, though, if we want to ‘forget’ the future, we can’t rely on objectives and goals anymore. So, instead of measuring our progress over a plan, we have to ensure we are making great choices NOW!   Here are 3 rules of thumb we can use to self-assess how well your coaching is going on at any moment!      Are people happy to see you around?   Is the situation improving?   Are you acting in line with your WHY?   #1 Are people happy to see you around?               This one we owe to Henrik Kniberg:      I think the closest equivalent to you know if you’re adding value is whether people want to have you around! Are they happy to have you there? In that case, you’re probably doing something right! (Henrik Kniberg, confessions of a change agent)    Whenever you work with your coachees, see how they are behaving. Pay attention to their faces. Do they look happy to see you? Or does it look like you’re pulling teeth?   Another way to know is to ask for frequent feedback! Whenever you have the occasion, just ask! But don’t hesitate to also send surveys if you want more structured feedback!   In our team of tech coaches at Murex, we’ve sent many surveys to all teams we’ve worked with. We’ve sent mid and end-of-program surveys. Some teams have gone through 3 or more programs with us! Lately, we have started to send surveys 3 months after we left the teams to see how things are evolving. (I might write about this in a future post!)   #2 Is the situation improving?      This quote from the book Agile Coaching sums it all:      So, how can you tell how you’re doing as an Agile coach?     • Looking back, is the team more Agile now than it was a month ago?     • Have you had a positive influence on the team?     [Rachel Davies &amp; Liz Sedley in Agile Coaching]       It’s not about replacing the future with the past! Instead, take 10 minutes to assess the differences between now and one month ago every few weeks. If things are better, it’s a sure sign that you are both helpful and making progress.   Once you are done, forget the past and come back to the present!   #3 Are you acting in line with your WHY?   Instead of a goal, you can use a clear WHY! If you spend some time discovering your WHY you will be able to use it as a compass. This compass can help you to make the right choices and drop goals almost entirely.      Helping Geeks feel safe in the world [Kent Beck]    As a bonus, a clear WHY will also make you more authentic to others and at peace with your actions. Also, knowing why you do things will make you more enthusiastic. In turn, enthusiasm will make you more impactful with the teams you coach!   The book Aligned helped me a lot to discover my WHY.      💡 Simon Sinek also wrote a book ‘Find Your Why’ that I did not read, but it has excellent reviews   You might be wondering how to use your WHY once you have it?   Let’s take my example. My WHY is “Showing a more sustainable way.” Whenever I start working on something, I inject a dash of sustainable disruption in there:      I always run many experiments to discover new ways.   I present sustainable pace as the primary outcome of agile coding techniques. I only mention productivity as a second-order effect!   When I make a decision, I tend towards the options that are the most innovative and sustainable.   What if…      …people are not happy to see me!     …situation is not better than one month ago!     …however hard I try, I cannot act close enough to my WHY!    You have just discovered that something fundamental is not working. So it looks like now is the time to have an essential retrospective with the right people!   3 Questions, 3 Benefits      Let’s summarize:      These questions will reduce goal-generated stress.   They will make you more enthusiastic as you’ll work towards your WHY!   Guess what? They will make you more effective! Letting go of the ‘target’ will help you to make better decisions when setbacks happen.   Agile Technical Agile Coaching?   Let’s take a step back:      Are coachees happy about your work? ~ Customer value   Are the team doing better? ~ Continuous improvement   Are you following your Why? ~ A compelling mission   Forget the long-term target. ~ Reacting to change   This is pretty close to the agile principles! We should also be agile in the way we do technical agile coaching!   Don’t wait to give it a try! You can start with questions #1 and #2 right now. You can also schedule some time to discover your WHY as soon as you can. The sooner you start, the sooner you’ll be able to coach with #NoGoal, and enthusiasm!      💡 What you can do now: Discover your WHY and try one of these techniques as soon as you can.    You might also like:      How to measure and report your tech coaching effectiveness   If you want to learn more about leveraging setbacks How the pandemic made us discover better ways of coaching   If you are curious about how to deal with outside environment How to coach a team that has been burnt by bad TDD  ","categories": ["coaching","goal setting","personal-productivity","sustainable pace"],
        "tags": [],
        "url": "/3-tips-to-drop-technical-agile-coaching-result-measures-and-relax/",
        "teaser": "/imgs/2021-09-20-3-tips-to-drop-technical-agile-coaching-result-measures-and-relax/goals-or-no-goal-teaser.jpg"
      },{
        "title": "5 Whole-Team Workshops To Increase Developers' Role In Sprint Planning",
        "excerpt":"Run these activities with the whole team to increase not only the developers’ role in sprint plannings, but also collaboration, engagement, and value creation!         Tools have taken over the meaning. I need to chant “The card is a token for an ongoing conversation” over and over. (George Paci extremeprogramming@groups.io)       The devs had no say in what stories went into the sprint (David Kramer extremeprogramming@groups.io)       [ndlr: In Parallel to] The rise of the ‘Product Manager’ job, which is a strange combo of a member of the sales organization and a surrogate user. Engineers no longer can talk to users. (Slava Imeshev extremeprogramming@groups.io)    Nowadays, it looks like developers are often stuck in their development roles. They are rarely involved in product management decisions. In this context, they receive user stories like “tasks to do.”   From a rational point of view, we can explain this situation with two typical human biases:      We tend to underestimate the importance of soft topics. Here, we overlook the impacts of collaboration, trust, understanding, and implication.   We tend to keep things for ourselves because we want to stay in control.   Yet, as coaches, we see teams and developers suffering from this situation. Sure, we understand where this is coming from… We are humans, not 100% rational robots, though! As a result, it’s easy to fall into disillusion or rebellion!      Wouldn’t it be great if you knew ways to make all stakeholders – product experts, developers, and quality people – collaborate to plan what goes into the sprint?    Here are 5 whole-team workshops to build team spirit and trust between stakeholders. Eventually, they should make everyone collaborate towards the same direction:      Practice an Example Mapping kata   Agree on a scope with a Big-Picture Event Storming   Play the built-in quality game   Agree and improve a whole-team test strategy   Address technical debt from a business perspective   Each of these activities should increase the developers’ role in sprint planning. And guess what, greater collaboration on planning is not only good for developers! As a second-order effect, it will:      Improve product management decisions   Increase developer motivation   Reduce developer stress   And finally, increase the value creation.   It’s a clear win-win for everyone!   1. Practice an Example Mapping kata              Sample Example-Mapping cards on the introductory post about Example-Mapping, by Matt Wynne        I already wrote about how to inject Example-Mapping into your process. It is also possible to run a kata to learn Example-Mapping. Any kata with enough ‘domain’ will do. For example, the Mars Rover Kata is a great fit. Here is how to do:      Invite the product and quality experts.   Hand the kata specs to the product person, but don’t say a word to others.   Make them run the example mapping to discover the specs.   If you don’t know the Example Mapping workshop yet, you can learn more about it in these posts.   2. Agree on a Scope with Big-Picture Event Storming      A new target is a perfect occasion to run a Big-Picture Event Storming. Propose one next time the team starts a new product, a new important feature, or some rearchitecting. There are at least three collaboration benefits:      Participants will collaborate closely during the workshop. As a consequence, all stakeholders will know and trust each other more. Trust is the foundation for more collaboration on planning.   Often, developers accumulate a huge domain-knowledge debt. Big-Picture Event Storming is an accelerated way to pay it back. With more domain knowledge, developers’ role in the sprint planning will increase.   Participants will map out all domain events on the design board. This makes negotiating the scope of the next milestone straightforward. It’s a perfect occasion for developers to have a massive influence on planning. Here is a post explaining how to use Big Picture Event Storming to agree on a target scope in more detail.   I’ve written a lot about Event Storming in the past. Misadventures with Big Design Up-Front is a good starting point. (Note: My colleague Matthieu Tournemire and I have created the Event Storming Journal, a blog dedicated to demystifying Event Storming)   3. Play the built-in quality game              From the Built-in Quality Game under Creative Commons Attribution-ShareAlike 4.0 International License        Often, developers are absent from planning by sheer ignorance. Many business stakeholders ignore the impact of omitting some technical practices.   Nobody Ever Gets Credit For Fixing Problems That Never Happened explains this situation. It concludes that serious games are a way to get everyone more aligned. The Built-In Quality Game could be such a game. We invented it as a fun alternative to formal training. It lets everyone experience how investing in engineering practices speeds up feature delivery.   Here are two other games to raise awareness about technical debt:      Dice of Debt Game   Catch the moon      Hopefully, these games should make business people care more about sound engineering. As a result, they should pay more attention to developers during sprint plannings.   The Built-In Quality Game is open source, and you can download all the material to run it from GitHub.   4. Agree and improve a whole-team test strategy   At work, many teams have the long-term goal of ‘inverting the test pyramid.’ Although a worthy goal, teams don’t necessarily know how to get there by themselves. Without enough collaboration, product, development, and quality experts were only optimizing locally. They need more coordination to tackle the problem as a whole.      We created and have now run the Test Strategy Workshop with a few teams. The goal is for them to embark on a test improvement journey together. It’s a multi-stage workshop that spans many short sessions. Here is how the workshop goes:      Introduction to the topic.   Homework and presentation of the different kinds of tests in the industry.   Drawing of the industry’s ideal test pyramid.   Drawing of the team’s current test pyramid.   Collaborative “Test Strategy Canvas” to agree on how the team is currently testing.   Solution Focused activity to define the next improvement to “invert their pyramid.”   Product and quality people take part in defining the next improvement. As a result, they are more likely to sponsor test improvement during sprint plannings.      It was also an occasion to learn more about our own tests. It was as if we climbed up a tree and saw the whole testing forest unfold before our eyes. (Mirna)       Even if some team members were already aware of testing techniques, testing best practices, this workshop gives the possibility to share a common vision between all the team members. (Patrice)    This workshop is Toyota’s Improvement Kata in disguise: it’s a long-term quest! Once a team delivers an improvement, they should re-run the workshop to agree on the next step! Every run will strengthen the collaboration among the stakeholders.   Tell me if you would like me to write about this workshop in more detail.   5. Address technical debt from a business perspective               In this talk, Jürgen De Smet presents a workshop to make everyone agree about how to deal with bugs. It uses bugs as the visible part of the technical debt iceberg. Its structure is close to that of the test strategy workshop I wrote about above:      The whole team gathers to agree on how to classify and deal with bugs. Some categories of bugs will be fixed, while others will be parked.   After a while, they meet again to analyze parked bugs. The goal is to identify a sub-system where refactoring would fix a lot of these bugs.    They then run a Solution Focused activity to agree on a small refactoring increment.   They regularly go through the workshop to keep the momentum.   I have not yet tried this workshop with teams. Yet, Jürgen obviously did, and it looks promising enough that I mention it.   Call to action      Are you coaching a team in which developers are the mere executors of the business people’s plans? In this case, give these workshops a try!   The first one might not create a revolution, but it will start to move the needle. As you run more of these activities, the developers’ role in the sprint planning should grow.   What about you? Please share your tricks to increase the developers’ role at Sprint Plannings?   Other articles that might interest you   Here are other related articles that you read to dig further:      How to convince your business of sponsoring a large scale refactoring   If you want to learn other ways to inject DDD in your organization, check Organization refactoring: Event Storming and DDD injection   If you want to read how we invented the Built-in Quality Game at work, read A serious game for learning ‘built-in quality at the source’   My complete guide about how to run your first Big Picture Event Storming starts with Misadventures with Big Design Up Front   How to run your first improvement kata   A complete workshop for your team to see what’s a good test strategy  ","categories": ["bdd","ddd","coaching","refactorging","example mapping","event storming","technical debt","gamification"],
        "tags": [],
        "url": "/5-whole-team-workshops-to-increase-developers-role-in-sprint-planning/",
        "teaser": "/imgs/2021-10-30-5-whole-team-workshops-to-increase-developers-role-in-sprint-planning/5-activities-for-developer-voice-teaser.jpg"
      },{
        "title": "Leverage Scrum to workaround feature-factory sprint planning",
        "excerpt":"Is “Business Value” the only thing that matters for sprint plannings? Here are 3 Scrum practices that will buy developers some time for technical work!.         The main problem is not agreeing that technical excellence is essential for the success of the delivery. Corporate culture is 100% focused on feature factory, delivering features at the expense of technical excellence.       Teams are practicing either ‘run as fast as you can’ SDLC, which is essentially a no-practice, or full-blown waterfall, as in ‘let’s-plan-a-year-out with all consequences. (Slava Imeshev on extreme programming groups.io    Without enough technical work, the code degrades, maintenance becomes more difficult, and the pace gets unsustainable!   Non-developers are usually not aware of the importance of technical practices. (cf Nobody Ever Gets Credits for Fixing Problems that Never Happened)   It’s a challenge to make business people care about technical excellence! As a result, technical agile coaches can end up disillusioned and rebellious against the status quo.      Surprisingly, even teams officially using Scrum overlook technical work! Fortunately, in this case, we can use Scrum as an argument of authority to inject change.       Wouldn’t it be great if the team could manage most of the technical work outside the sprint planning?    Here are three Scrum-Aïkido moves that you can do with an official Scrum team. In the long term, they should let the team reach a calmer and yet more effective pace:      Agree on quality in a Definition of Done   Forecast instead of commit to user stories   Leverage Planning Poker to buy time   Technical excellence takes some time, especially to get started. Nothing will change if we don’t take this time. Here are 3 ways that Scrum can help us.   Agree on quality in a Definition of Done   Updating the Definition of Done (DoD) might be the first thing to do. Here is how the Scrum Guide highlights the importance of technical excellence:      As Scrum Teams mature, it is expected that their definitions of “Done” will expand to include more stringent criteria for higher quality. (Scrum Guide 2017)       Instilling quality by adhering to a Definition of Done; Commitment: Definition of Done The Definition of Done is a formal description of the state of the Increment when it meets the quality measures required for the product. […] The Definition of Done creates transparency by providing everyone a shared understanding of what work was completed as part of the Increment. If a Product Backlog item does not meet the Definition of Done, it cannot be released or even presented at the Sprint Review. (Scrum Guide 2020)    The trick is to discuss the DoD outside the context of the current stories and urgencies. The business stakeholders will better listen to the long-term concerns raised by developers.   There are many resources on the web about how to run a DoD workshop. For example:      Definition of Done Workshop by Wayne Grant   Virtual Exercise to Create or Update a Definition of Done by Philip Rogers   The DoD will evolve over time. So you’ll have to regularly run update workshops to make sure the team reviews and adapts its DoD.   Forecast instead of commit to user stories      The intended commitment in Scrum was:      Do everything we can to deliver what we said we would without sacrificing quality.    Unfortunately, the last part was often forgotten, and it became:      Do everything we can to deliver what we said we would without sacrificing quality    That’s why Scrum changed its wording from “commitment” to “forecast.”      One of the most controversial updates to the 2011 Scrum Guide has been the removal of the term “commit” in favor of “forecast” in regards to the work selected for a Sprint. (Commitment vs. Forecast: A Subtle But Important Change to Scrum)    As coaches, we should be the background consciousness of the team. We should keep our ears open and point out when a “commitment” should be a “forecast.” We should be particularly attentive towards the end of sprints! This is when it’s all too easy for humans to default to ship-at-all-cost mode!   Leverage Planning Poker to buy time   Planning Poker is a way to use the wisdom of the crowds to estimate the complexity of user stories. If you don’t know about Planning Poker, the Wikipedia article is a good starting point.      Planning poker relies on team-relative ‘complexity story points’ estimates rather than traditional man-days. One benefit of story points is that it makes micromanaging man-days more difficult. The alternative, forecasting in man-days opens the door for stakeholders to:      Challenging estimates.   Organizing tasks in everyone’s calendar to get more stories into the sprint.   To summarize, story points estimates increase the team’s control over its time.   Planning Poker is simple: plan as many story points as you delivered in the previous sprint. This can help the team and the stakeholders to improve forecasts in two ways:      Time for recurring unplanned work is ‘automatically’ reserved. This is the magic of the auto-adjusting factor of story points and planning poker!   When the team finishes its sprint stories, it should work on improvements, not on new stories! In the long run, it reduces the variability that comes from technical debt.      I remember when I explained this graph to the Deadpool team members. They were looking at me with bewildered eyes when I told them that this was all they should forecast. After a few months, though, the team was set on a continuous improvement dynamic. Today, the team is a company-wide example!   These practices have many other extra advantages. Check these resources about how to learn how to run a planning poker and more:      Learn how to use story points and run a planning poker (https://www.mountaingoatsoftware.com/agile/planning-poker) (I even created an open-source remote poker app a while ago…)   Learn more about slack time in (James Shore’s Art of Agile Development offers a good reference)   Too big to swallow?      This Scrum-Aïkido should get us a long way into finding time for technical work. We can deal with most technical “house cleaning” this way. James Shore said a former team managed to internationalize an app in one day thanks to the continuous refactoring! (cf The Art of Agile Development)   Unfortunately, there will be times when we will unearth large technical tasks. It’s surprising, but this is how the complex world is! Always throwing new technologies, disruptive features, and deeper domain insights at us!   When this happens, we’ll need to agree on a plan with business people. That’s why it’s so essential to build trust and collaboration with them.   Try this!   Are you working with a team that is struggling to find some time for technical work? Regular technical work is the developers’ responsibility. It should not be subject to negotiation at the sprint planning! If this team is officially using Scrum, it’s time to use these “Scrum-Aikido” techniques!      Make sure all parties agree on a definition of done with high quality.   Explain the difference between forecasts and commitments in Scrum. Start to talk and act based on ‘forecasts.’   Use story points and planning poker to carve out some time to improve the code health.   In the long run, these tricks should make the team’s pace more sustainable, yet more effective too!   Do you know more about “Scrum-Aikido” to keep technical work under the developers’ responsibility?     Here are a few other posts that you might find interesting:      How to play planning poker… Badass Mode! A fun infographic showing how teams can get their way during the sprint planning.   How to kill Scrum Zombies ? A collection of tricks that could help in the case of a team brainlessly following Scrum   5 Whole-Team Workshops To Increase Developers’ Role In Sprint Planning.  ","categories": ["agile","coaching","scrum","continuous improvement","technical debt","refactoring"],
        "tags": [],
        "url": "/leverage-scrum-to-workaround-feature-factory-sprint-planning/",
        "teaser": "/imgs/2021-12-06-leverage-scrum-to-workaround-feature-factory-sprint-planning/sprint-planning-teaser.jpg"
      },{
        "title": "How to estimate velocity in Scrum to escape the feature factory",
        "excerpt":"Poker Planning is a collaborative game… that developers often lose! Here are the original fair rules that will provide developers time for technical work.        Some teams are somewhat experienced in Agile &amp; DevOps (but mostly from the cadence &amp; ceremonies perspective)       The team uses sprint planning and velocity by the book, yet they never have time to work on technical excellence!       Technical excellence, the practice of improving the structure of the code by doing continuous refactoring is completely foreign to most developers, who are being told that if they are good at what they are doing, they should be able to do it correctly on the first try.    We get messier code, more complicated maintenance, and an unsustainable rhythm without enough technical work. But, unfortunately, non-technical people are usually not aware of the importance of technical practices.   I said in the previous post that running planning poker sessions can help us here. The idea is to plan as many story points for the next sprint as delivered during the previous one. This will accommodate variability and create time to catch up on technical debt.   The number of story points delivered per sprint is the Velocity. Unfortunately, the general recommendation about estimating velocity in Scrum is terrible! As a result, many Scrum teams over-commit and skip important long-term technical work. That’s how agile feature factories are born!      Did you know that developers would get time for technical excellence by forecasting just as many story points as delivered during ONLY the previous sprint?    Let’s see how to leverage Scrum Velocity to reach a sustainable and effective pace.      The typical (but flawed) way to estimate the velocity   What’s the problem with averaging velocity?   Just use the previous sprint to estimate velocity!   It’s not that easy, though!   The typical (but flawed) way to estimate the velocity         Add up the total of story points completed from each sprint, then divide by the number of sprints. (Google answer’s to “How to compute velocity scrum”)    What’s the problem with averaging velocity?   A classic situation   A few years ago, I worked with a team facing delivery difficulties. Here is what I noted about its sprints and velocity:      The amount of story points the team delivered (its velocity) varied a lot from sprint to sprint.   Yet, the team members were doing anything they could to deliver what they had planned:            They were working full-throttle on stories right till the end of the sprint.       Things turned out well from time to time, and they would manage to finish before the end of the sprint. They would then catch up on what they had not delivered on previous sprints.           As a result of all these efforts, the team members were exhausted.   They were also aware that they were pilling up technical debt.   Unfortunately, the situation looked to be getting worse!   I guess you can relate to this story: most of the teams I have worked with have been in this situation!   The vicious circle of averaging velocity   Let’s see what happens when we use the average velocity in the previous situation.   Here is a sample velocity history:      The average velocity over the 4 previous sprints is 42. Let’s assume that the team now plans to deliver this average: 42 story points. During these 4 previous sprints, the team produced:      more than 42 story points twice   less than 42 story points twice   If the team does not change anything else in its way of working, history will repeat itself! So here is what we can expect over the following sprints:      The team will not deliver what they planned on half of the sprints. This will generate stress for everyone and degrade trust with business stakeholders.   On the other half of the sprints, the team will manage to deliver what they said. But team members are likely to start catching up on what they did not tackle in previous sprints! Unfortunately, from the eyes of business stakeholders, it’s only late payback!   In the end, the team is in a constant state of catching up. This constant struggle to meet the plan makes:      The team look unreliable and non-trustworthy in the eyes of business stakeholders   The team members cut corners and take technical debt   The team members work under permanent stress              Retweet this image by Philippe Bourgau, under CC BY-SA 4.0, high resolution image        In fact, averaging velocity does not change anything to the previous situation! On the contrary, it degrades trust, generates stress, and pill-up technical debt.   Why are people averaging velocity?   The only, conscious or unconscious, reason to average the velocity is to game the metrics! The team can show a ‘predictable’ face by communicating about the average.   Unfortunately, this strategy does not address the root causes of the variability. Instead, it only makes things worse.         Averaging the velocity makes everything looks ‘green’ from the outside, while the situation is still ‘red’ inside, like a watermelon.    Just use the previous sprint to estimate velocity!   Let’s resume the story about the team I mentioned above. The team needed to plan less per sprint. Fortunately, the organization had a goal to increase “predictability.” The team could leverage this to reduce what it planned every sprint. This picture shows how we cut the planned velocity:      This was a hard cut to swallow, but the team soon had time for new activities:      pair and mob programming   pairing between the different profiles in the team to increase generalist skills   investing in a test framework to increase coverage of legacy code   taking more time to write clean code   investing in documenting some critical but forgotten parts of the codebase   After a few sprints, stakeholders noticed that the team was getting in control. In addition, limiting the planned work had kicked off a continuous improvement mindset in the team. As a result, the team member’s mood improved significantly in a handful of weeks!   How to calculate velocity and capacity in Scrum   Here is how the literature tells us to calculate velocity and capacity in Scrum:      To predict your next iteration’s velocity, add the estimates of the stories that were “done done” in the previous iteration. (James Shore in The Art of Agile Development, 1st Edition’s)    It’s that simple! Over a few sprints, the team will reach a velocity close to the minimum in their sprint history. If the team already has its history, it could start with this minimum. Here is what happens with this strategy:      The team will deliver what it planned in 95% of the sprints. This builds trust with business stakeholders and reduces stress.   The product owner will be able to predict what the team can deliver.   Sometimes, the team will finish its stories before the end of the sprint. The team members will have some time to refactor, test, or tackle other significant improvements. This addresses the root causes of variability. It makes work more sustainable and less stressful for everyone in the long term.   As continuous improvement compounds, the productivity of the team will also increase. The team will increase what it can deliver every sprint in the long run. Again, this will build trust with business stakeholders.   In the long term, this strategy beats averaging velocity in all aspects. Not only for developers but for business stakeholders too!   It’s not that easy, though!   Even if we know things will be better in the long term, getting there, though, is tricky. At first, it will deliver less than the ‘Run As Fast As You Can’ strategy.   Here are the main challenges the team will have to overcome.   Negotiating the change with the Product Owner   This “long-term” strategy stresses the product owner’s duty of excluding what not to do!      Simplicity–the art of maximizing the amount of work not done–is essential. (The Agile Manifesto Principles)    Some product owners will push back! As coaches, we can help by training and preparing the team to negotiate. We can also facilitate the discussion to turn it into constructive brainstorming.   Parkinson’s Law      Work expands so as to fill the time available for its completion. (Wikipedia)    Unfortunately, Parkingson’s Law is real! As coaches, we can help by adding visibility. For example, we can show the Scrum Master how to track the day of the sprint when all the stories are closed.   Scope Creep   Developers will start new user stories when everything planned for the sprint is done!   This one might sound a bit crazy! But, indeed, I’ve observed this behavior in the large majority of teams I’ve worked with! User stories are great, but we want time for technical improvement! Here is a remedy that Michael Feathers shared during a training:      Always have a small backlog of small and precise refactoring tasks ready to be worked on.    We can work with the Scrum master to organize the grooming of a refactoring backlog. It needs to be small and visible!   Time for a change!      Are developers losing the planning poker game in the team you are working with? Here is what you can do:      Present the average and previous sprint velocity strategies to the Scrum Master. Take the time to discuss the matter in-depth, and share the references from this article.   Together, present to the team and discuss how to move to the previous sprint velocity strategy.   Help the team to negotiate with the Product Owner. Suggest ways to ease the change. For example, the team might run an experiment over a few sprints. It could also gradually reduce the duration over which the velocity is averaged.   Don’t forget to watch out for Parkinson’s law and scope creep!   I would love to read about your own developer tricks to pitfalls around Scrum Velocity! The comment section is yours.    Here are other posts about how developers can get more time for technical work:      Leverage Scrum to workaround feature-factory sprint planning presents other Scrum-Aïkido techniques to take more time on technical excellence   5 Whole-Team Workshops To Increase Developers’ Role In Sprint Planning   How to coach developers to get a chat with their product experts   Make Testing Legacy Code Viral: Mikado Method and Test Data Builders presents a low investment, long-term, but viral way to inject testing in a legacy codebase   How to convince your business of sponsoring a large scale refactoring  ","categories": ["agile","coaching","refactoring","scrum","technical debt","planning"],
        "tags": [],
        "url": "/how-to-estimate-velocity-in-scrum-to-escape-the-feature-factory/",
        "teaser": "/imgs/2022-01-13-how-to-estimate-velocity-in-scrum-to-escape-the-feature-factory/escape-from-the-feature-factory-teaser.jpg"
      },{
        "title": "How to handle \"TDD does not work in real life!\" during code katas",
        "excerpt":"Doing code katas to learn real-life TDD can at first look like a scam to many developers. So here are three techniques to maintain a constructive mood.      If you have run coding dojos, I’m sure you’ve heard developers say things like:      All these small tests are a waste of time in real life. A few broad tests and good debugging skills are all I need!       This is stupid! There is no point in returning hard-coded values just to make the test green!       This works well for toy projects but won’t work on real-life and large codebases.    I cannot remember how many times I heard this kind of comment. Unfortunately, coding dojos have a Karate Kid flavor. In this old movie, the hero, Daniel, is taught Karate by practicing chores like painting a fence, waxing a car, etc. Eventually, Daniel wins his combats by repeating the chore moves. Developers I know who have practiced enough code katas are now very effective with TDD in their day-to-day work.      Midway in Karate Kid, though, the hero loses faith in the teaching and rebels against his master.   Unfortunately, developers can rebel against their coach during a code kata too!      Wouldn’t you love it if developers always remained constructive during and after code katas?    With time, I’ve discovered three habits that keep everyone in a constructive mood:      Listen to understand   Explain the pedagogy behind the coding dojo   Mob as early as possible   Listen to understand   When people complain about the kata, they say it for a reason. It can be fear, fatigue, or other emotions. As long as we don’t understand where this is coming from, they will accumulate resentment against us. So instead of arguing, it’s a more effective strategy to get everything out. The critical skill here is listening.      There is a glitch, though. A dojo is a group session. It’s OK to spend a few minutes so that someone can explain their point of view. However, it’s different to let this drag on and undermine everyone’s mood. In this case, it’s better to schedule a private chat with the person. Here is how to say this:      I’m sorry, I have to interrupt. I’m very interested to understand your point of view. I’m sure it is an opportunity to make these sessions more effective. But unfortunately, our discussion is turning into a dialogue, and the dojo is not the place for this. Are you available after the session to continue this discussion between us? Would you prefer to schedule a meeting together later today?    Listening is not about bringing arguments or trying to persuade. It’s about understanding the other’s point of view. Sometimes, a compassionate ear is all people need to remain constructive in the dojo.   Explain the pedagogy behind the coding dojo   Thinking of applying what they practice in kata in real life might be too much of a stretch for them at first. Let’s accept that. Yet, we want them to be in a ‘learning mindset’ during the sessions. The goal is to have them forget their doubts during the session to learn. Then, once they have mastered the techniques, applying them in real life might not seem so crazy.   We could explain all the pedagogy theories behind the katas. As humans, though, we love stories. It’s hardwired in our brains. So I’ve found it is more effective to explain why katas work through stories. I tell these stories before the first code kata. After that, it’s a good habit to remind passages whenever it feels that attendees have forgotten.   The coding dojo was invented in early 2000 by a few French eXtreme Programmers living in Paris. Emmanuel Gaillot, Laurent Bossavit for sure, and maybe others. They knew how to practice TDD and XP practices in real life, and we’re looking for a way to teach it. They got some inspired by martial arts and sports.   A dojo is a safe practice place   Martial artists might get hurt in competition, yet they don’t expect to get hurt in the dojo. Sometimes, real-life programming hurts: we get emergencies, burn the midnight oil, and go under a lot of stress… But not in the dojo. Failing at the exercise does not hurt and is of no consequence!   As coaches, we must do all we can to keep the coding dojo a fun and enjoyable experience. We should often remind that the goal is not to “fix” the problem but to practice and that it’s OK to “just try something.”      A kata is a developer’s real-life condensed into 2 hours.   There is a spark of genius in the coding dojo format. It squeezes many aspects of day-to-day programming in 2 hours!      In real life, developers discover significant specification changes about what they built… last week! During a code kata, we stick to YAGNI (You Aren’t Gonna Need It). We only make the current tests green. We deliberately do not design for future tests.   In real life, developers often inherit code written by someone else. During a code kata, we deliberately switch the driver every 5 minutes.   In real life, developers have to collaborate on design or code. During a code kata, we pair or mob all the time.   This shows the participants that the point of these weird kata rules is not to bother them. On the contrary, these rules make them practice and improve fundamental programming skills.   Deliberate practice of refactoring skills      If it hurts, do it more often!    In martial arts, katas are sequences of moves that people practice until perfection. The notion of deliberate practice is present in all sports. Here is an example I often share with participants:      Suppose you are training for a swimming competition and you need to improve your flip. Here are two practice strategies:         Swim for 1 hour, and practice a flip every 50 meters     Swim 5 meters, come back, flip, repeat       If you want to improve your flips, the second is more effective:         you’d be less tired     you’d practice more flips     you’d have a shorter feedback loop     your coach could provide you more advice        The same thing happens in the coding dojo. The muscles we want to develop during coding dojos are TDD and coding with baby steps. The coding dojo is an accelerated way to grow refactoring muscles.   An opportunity to practice to the book      Be patient, don’t try to speed up before you can perform a technique slowly and correctly. (My Aïkido sensei)    Here is something I often say to coachees:      Now that you are taking the time to give TDD a try, you might as well be as disciplined as possible! Once you can apply it by the book on easy problems, you’ll see how to apply TDD in real-life.    The dojo is a unique occasion to experience TDD by the book. Participants will experience what TDD feels like in an ideal setting. They will assess the benefits of such a way of working. This will help them envision integrating parts of it into their daily work. With time, they might even be able to transform their day-to-day work into this ideal!   Additionally, practicing a watered-down version of TDD might not be motivating enough.      Learning TDD is like swimming front crawl when you only know how to swim breaststroke. It’s slower at first, but it eventually becomes faster and less tiring. (Someone on Okiwi slack)    Mob as early as possible      The most straightforward way to debunk “It does not work in real life” is to actually do it “in real life.” Alternating between code katas and mob sessions helps coachees to bridge the gap.      We show mob programming to the teams during the first days of coaching (Someone on Okiwi slack)    The easiest thing to do is: pick a user story the team is currently working on and run a mob session around it. Many people are still reluctant to mobbing, though. They are afraid that they’ll be wasting time by working all together. So instead, ask them what work would create the most knowledge sharing or is critical to do well.   It’s even more true after a refactoring kata. Ask the team members for a piece of code to apply the refactoring technique they practiced. Again, it works great to get advanced refactoring techniques into a team’s habits.   Running mob sessions is a perfect answer to comments like “TDD does not work in real life” during code katas. So here is how I tease the participants:      This session is for practice in an accessible setting. We’ll apply it to your codebase during the next mob session. No one learns to swim at the deep end of the pool!    Three techniques      It’s no surprise that a topic called eXtreme Programming will generate strong reactions! The most common critique we receive when we train people to XP practices is:      XP and TDD will not work in real life!    Dealing with these remarks is part of our job. Next time you face this remark, remember to apply these three techniques:      Listen (more than you think)   Remind everyone of the pedagogy   Apply in the mob as soon as possible   If you know other techniques to deal with “It won’t work in real life,” please share them in a comment!   Happy coaching!     Here are other posts you might that could interest you:      5 technical agile coaching tips to fight exhaustion from laggards   How to coach a team that has been burnt by bad TDD   Coding Dojo Troubleshooting   A coding dojo exercises plan towards refactoring legacy code   3 compounding benefits of mob programming that make it cost-effective   ","categories": ["tdd","extreme programming","coding dojo","coaching","mob programming"],
        "tags": [],
        "url": "/how-to-handle-tdd-does-not-work-in-real-life-during-code-katas/",
        "teaser": "/imgs/2022-02-18-how-to-handle-tdd-does-not-work-in-real-life-during-code-katas/kata-real-life-teaser.jpg"
      },{
        "title": "How to help a team to find their preferred mob programming rules?",
        "excerpt":"A dysfunctional mob makes coaching impossible! So here is a workshop for team members to try and find the mob programming rules that work for them.         Have you got tips to put people at ease during mob programming sessions?       The same driver keeps driving for the entire session, the other participants look passive most of the time. (But I believe they are actually afraid to say something stupid!)       I would love to reduce the latent stress in the mob, but just saying “There is no stress” is visibly not enough!       In the past, I had some unhappy experiences with fast driver rotation on a topic that required much setup. This generated quite some frustration!    Introducing mob programming to coach teams is not easy!      Wouldn’t it be great if all mob programming coaching sessions were high-energy? If roles rotated smoothly? And the mob demonstrated the calm and natural collaboration of a well-oiled team?    A Code Retreat is a workshop where participants experiment many times with a topic. Let’s see how running a code retreat around mob programming helps members of a team to both:      Kick-off their mob programming habit   And discover how they want to mob.   We’ll see      How to run a code retreat with Mob programming experiments   Woody Zuill’s mobbing experiments   More experiments!   How to run a code retreat with Mob programming experiments.   A code retreat typically lasts for half a day to a full day. It was first designed with programming in mind. You can learn more on codereatreat.org.   Here is how we run a code retreat of Mob programming experiments:   Before the event:      Book half a day or a full day with all the team.   Ask participants to imagine experiments of mob-programming rules, flavors, or styles. It could also be collaboration best practices that they would want to try.   Make them vote on the experiments to rank what they want to try first.   During the event:         Run a series of one-hour iterations:            Take 5 minutes to make sure everyone understands the next mobbing experiment.       Experiment with this flavor of mob programming for 40 minutes using a code kata or a piece of their code.       Take 5 minutes to write down the pros and cons of this way of mobbing.       Have a 10 minutes break.       Repeat with the following experiment on the list.           At the end of the day, help the team members run a whole-day retrospective. Make sure they agree on their mob programming rules.   It’s incredible how actually trying something stops dead-ends discussions between team members. In a few hours, we’ve seen groups go from dysfunctional muddles to calm and effective mobs!      💡 Don’t forget to slip in words about the bigger picture. The team can run code-retreats to experiment and make any collective decision!    You must be wondering what mob programming experiments might look like, so read on!   Woody Zuill’s mobbing experiments   I had the chance to attend a training by Woody Zuill about mob programming. He wanted to smoothly introduce strong-style mobbing. So he made us go through a kata using more and more elaborate mobbing styles:         First, we practiced round-robin the driver, navigator, and observers roles. We did not write any code, but we used super short cycles for a few minutes.   Then we tried group programming with a driver, a single navigator, and observers. Everyone was silent except the navigator. We practiced for a few turns of 4 minutes on a code kata.   Then observers were allowed to speak to the navigator after raising their hands.   Finally, we practiced a “real” mob, with one driver, ‘polite’ navigators, and no observers.   The nice thing about learning to mob this way is that I understood what every variant adds to the previous one.   Three of Woody’s mob programming styles also make great experiments to run with teams:      1 Silent driver, 1 navigator who speaks, others are silent observers   1 Driver, 1 navigator, and civilized observers   1 Driver, All navigators   These experiments are a must-do for a team starting mob programming.   Surprisingly, many teams prefer option 2 to option 3! It looks like it makes space for thinking, and it leaves introverts a chance to speak up.   More experiments!     Here are other examples of the mob programming experiments:   Remote Mob Programming   If the team is mobbing remotely, they could test different mobbing tools. Or, if they don’t yet have the habit of turning their cameras on, they could give it a try and see what changes.   Giving Feedback   It’s a great idea to experiment with how to share feedback in the mob. For example:      Use the feedback-sandwich technique:            Start with positive,       Follow with constructive,       And end on a positive note again.           Or be inspired by Non-Violent Communication:            Start with your observations,       Then your feelings,       Follow with what your needs,       And end with a request.           How to interrupt to give feedback: raise a hand, change video background…      Many more!   Ideas for mobbing experiments are countless! For example, **if performance is crucial, a team could experiment with a dedicated role in the mob. **It could also explore mobbing in different contexts, like legacy or greenfield code bases.   As coaches, we can use this experimentation workshop for other aspects of collaboration. For example, we could run pair programming or reviews code retreats. This might even be a chance to nudge a team towards fewer reviews and more pairing and mobbing!   If you are looking for more disruptive and thought provoking mobbing experiments, check out this Twitter tread by Romeu Moura.   I went very quickly over these experiments. Tell me if you would like to read more about some of these!   Give it a try         Getting people to try something is already 90% of the change done!    Nothing beats experimentation when trying to find good ways to work together. Are you facing collaboration glitches when coaching with mob programming? Suggest this mobbing Code Retreat! The team will learn more in 4 hours than in many days of arguing.   What about you? Please share your mob programming best practices that we could experiment with!     Here are other articles to dig further:      How the Samman Method helps to sell technical coaching internally? Also presents Emily Bache’s Samman Method, which relies a lot on mob programming.   Looking for arguments to explain to coachees how mob programming is cost-effective? Read 3 compounding benefits of mob programming that make it cost-effective.   Best open source tools for remote pair programming. Here are simple, effective, robust, and free techniques for remote mobbing!  ","categories": ["mob programming","team building","remote","coaching","continuous improvement"],
        "tags": [],
        "url": "/how-to-help-a-team-to-find-their-preferred-mob-programming-rules/",
        "teaser": "/imgs/2022-03-24-how-to-help-a-team-to-find-their-preferred-mob-programming-rules/mob-programming-under-scrutiny-teaser.jpg"
      },{
        "title": "3 Things You Need to Master to Get Your Team Ready to Work With Serverless",
        "excerpt":"In this guest post, serverless expert Marco Troisi highlights 3 key skills to go serverless, and how, as coaches, we can help a team to learn them.      There are many reasons for wanting to adopt serverless technologies as part of your development lifecycle. From reduced costs to automated scalability to improved developer experience.   Of course, this is not to say that serverless is a good fit for every use case. Your team or organisation may well have very good reasons for not adopting serverless. Increasingly, though, such organisations are the ones that are going to need to justify their decision, not the other way around.   Once you have established that serverless technologies would give your team an advantage as well as deliver a better product for your customers, you are now looking at a transition.   How can you coach a team used to work with more traditional technologies and patterns towards full productivity and enjoyment of serverless development?   A few years ago, my team made the transition from a gigantic monolith to a 100% serverless architecture. During that process, I learned and observed what skills are required for a team to succeed with serverless.   The following 3 key steps will accelerate your transition to serverless technologies:      Infrastructure-As-Code   Event-Driven Architecture   Solution Architecture Diagrams   #1 Learn Infrastructure-As-Code   Infrastructure-as-Code (IaC) allows you to manage infrastructure (networks, virtual machines, databases, etc) through code rather than via manual processes.   By allowing developers to write, run, and maintain their infrastructure, you set yourself up for success as your team embarks on the serverless adoption journey.      Here are 3 benefits of embracing Infrastructure-as-Code in your team:   #1 It ensures consistency across environments   In an interview, Boyd Hemphill, former director of evangelism at StackEngine, describes Infrastructure-as-Code as follows:      code should be written to describe the desired state of the new machine. That code should run on the machine to converge it to the desired state. The code should execute on a cadence to ensure the desired state of the machine over time, always bringing it back to convergence.    A solid Infrastructure-as-Code setup should reduce the unfortunate instances where code appears to work on a developer’s machine but everything breaks down as that code is pushed to production.   With Infrastructure-as-Code, the system and configuration required to run your application are clearly visible to everyone; everything is perfectly replicable across any number of environments, whether the developer’s machine, staging or production.   Of course, there may be slight differences in place. For example, the machines running your production workload may be more powerful than the ones used for local testing. Similarly, network conditions may vary and affect the outcomes. These are all real factors that need to be kept in mind, but they are also minor in the grand scheme of things. And they can be mitigated greatly by adopting serverless and cloud-based development (that is, developing and running your applications directly in the cloud rather than on a local machine).   #2 It makes infrastructure reviewable by developers   Once infrastructure becomes part of the team’s development lifecycle, any code necessary to alter that infrastructure will need to be reviewed. The benefits of reviewing each other’s code are well known, so we don’t need to discuss them here. I do want to touch on a particular benefit of code reviews though: learning.   Regardless of the framework that you choose to handle your Infrastructure-as-Code, there is going to be a bit of a learning curve. But you can make this curve less steep and accelerate the process with code reviews. All it takes is a team member who (more or less) knows what she’s doing. As other people get involved with the reviewing process, they will soon start to get a sense of what things look like.   For example, let’s imagine that your team uses AWS CloudFormation as a framework; a team member creates a little PR that introduces a new Lambda. The title of the PR is Lambda for processing payments. The code introduced in the PR looks something like this:   Resources:   paymentLambda:     Type: AWS::Lambda::Function     Properties:       Runtime: nodejs14.x       Handler: src/handlers/payments.handler       Code:         S3Bucket: payment-lambda-bucket         S3Key: function.zip       Description: Payment processing Lambda   At this point, you have the context from the title of the PR, so you know at a high level what this code is trying to achieve. And even though you may not be completely sure of how the whole thing hangs together, you can definitely recognise some things in the YAML above. There is a Type, there is a Runtime, and there is a Handler which seems to point to the code that gets executed inside the Lambda.   Next time you’re asked to review someone else’s CloudFormation, you are much more familiar with it. Moreover, when the time comes for you to write some infrastructure code, it won’t be as intimidating. You can refer back to other people’s code and look for the elements and components that you need, with the necessary modifications.   #3 It increases the speed of development      In the “cloud age” of IT […] changes can be made in minutes, if not seconds. Change management can exploit this speed, providing better reliability along with faster time to market. ~ Kief Morris (Infrastructure as Code)     Infrastructure-as-Code enables you to increase the rate at which you develop your products and ship them to market. There is both an organisational and a technical reason for this speed uptake.   Organisationally, as engineering teams own the capabilities necessary to make infrastructure changes, you have fewer blockers. Much like with the adoption of a microservice architecture, with Infrastructure-as-Code you remove a major dependency, namely the infrastructure “experts”. A team can build a new feature or component from the ground up, define its infrastructure, and ensure their work is deployed and available to customers.   Technically, with Infrastructure-as-Code you get far more reliable systems. Since what’s in production is consistent with what’s been tested locally, and given that everything has gone through the standard review process, it is simply less likely for things to go wrong. Coupled with an effective CICD pipeline, you can ensure that changes are shipped consistently and bugs are caught and resolved quickly.   Learning resources   If your team is on AWS, I highly recommend reading the CDK Book and watching this video. The ever-popular Terraform is a solid alternative that supports other cloud providers too; this video is a good place to start. Group activities such as a “lunch &amp; learn” session can be very helpful; the team could simply watch the video together and pause at various points to discuss.   #2 Understand Event-Driven Architecture      Source: AWS   Event-Driven Architecture is a model for designing your software. An event-driven system is one where events are at the core of the system, as opposed to requests and responses.   Typically, an Event-Driven system consists of event producers that generate a stream of events, and event consumers that are listening to events coming in.   Amongst the benefits of an Event-Driven Architecture is that it enables you to build loosely coupled applications that, if well designed, can be very resilient as well as deliver improved performance to the end-user.   Serverless has Event-Driven as part of its DNA. Serverless functions (like AWS Lambda) are on-demand compute services that run custom code in response to events. Events can be generated by your own application, by other cloud services (for example, most AWS services generate events), or by third-party services.   Event-Driven systems don’t need to be too complex, to begin with. For example, AWS offers an extremely powerful, yet easy to use, service called EventBridge. Many companies are using EventBridge along with Lambda to power Event-Driven applications that are highly performant.   Learning resources   Marcia Villalba from AWS has made several very useful videos about EventBridge. This video, in particular, I cannot recommend highly enough. It is also a great one to watch as a group, and could easily stimulate a follow-up team exercise.   #3 Appreciate the Value of Solution Architecture Diagrams      a persuasive solution architecture diagram must be able to convey the business values to key stakeholders and the technical details to developers. ~ [Tuan Lee](https://medium.com/@tuanvietle/a-simple-method-to-create-solution-architecture-diagrams-6a127b93a122), solution architect at NashTech     In my experience, a Solution Architecture diagram is a fantastically powerful way to bring about clarity and keep stakeholders aligned.      Source: AWS Reference Solution Architecture for High-Performance Computing   Building a Solution Architecture diagram can be intimidating at first. One way to overcome this anxiety is to remember that, unless your job title is a Solution Architect, your diagram doesn’t need to be perfect or even pretty to look at. Most engineers will only ever design diagrams for internal use.   There are 3 things that need to be highlighted by any useful Solution Architecture diagram:      Users   Technologies used   Flow of information   Start with the user: who is going to use this system/feature/component? Then, line up all the technological components used. Lastly, draw lines between them (and make notes to the side, if necessary) to bring attention to the flow of data and the sequence of actions.   You now have a perfectly useful Solution Architecture diagram that your colleagues can use as a point of reference when reviewing your work.   Learning resources   The best way to learn how to draw Solution Architecture diagrams is to get your hands dirty.   Here’s a good team exercise: pick any diagramming software (draw.io, for example, is free; this video gives you a quick introduction), find a simple component that everyone is familiar with (e.g. the signup process), and start drawing it out by highlighting the 3 areas mentioned above.   Lastly, show the diagram to someone who is only marginally familiar with the system; is the general idea clear to them? If not, what can the team do to make the diagram clearer? Feedback like this is priceless since the main purpose of a Solution Architecture diagram is to make a solution crystal clear to every stakeholder.   BONUS: Get AWS Certified!   According to the 2021 Global Knowledge survey, 80% of interviewed IT professionals have seen an increase in their job effectiveness, including improved quality of work, increased engagement, and faster job performance, as a result of obtaining an IT certification.      IT certifications are not a guarantee of competence if taken in isolation, but they can be a cheap and effective way for companies to ensure a knowledge baseline among teams.   I recommend that anyone who is working in the cloud and adopting serverless get an AWS certification (you may want to look at equivalent GCP or Azure certifications if your company has reasons not to go with AWS).   A great place to start is the AWS Developer Associate certification; this certification assumes a couple of years of experience working with AWS, but you can study a little more to make up for the lack of practical experience.   Other certifications exist and can be pursued, depending on the specific role. For example, the Machine Learning, Database, Security, and other “specialty” certifications are especially focused on those topics.   More advanced certifications such as the Solutions Architect Professional and the DevOps Engineer Professional are also available to those who want to go above and beyond. They are more difficult to achieve, but obtaining them guarantees considerable knowledge in how to use and architect cloud applications.   Conclusion   In this article, we have looked at 3 areas that you should encourage your team to invest in if you’re considering adopting serverless technologies:      Infrastructure-as-Code   Even-Driven Architecture   Solution Architecture diagrams   By starting small and avoiding overcomplicating your application, you can gradually introduce these 3 practices.   As you think about the next feature that you or your team are going to tackle:      Draw a simple diagram outlining the various components and attach it to the ticket and any Pull Requests, for everyone to see.   Think about whether there is a way to delegate actions to asynchronous events rather than leaving your end-user waiting for a response.   Finally, look at pre-made templates so that all of your infrastructure is clearly defined (and reviewed) as code.   Have you or your team recently adopted serverless technologies? What have you found useful and what challenges have you faced?    Marco Troisi is a cloud architect and serverless expert. We have been following each other’s blogs for a long time. He now blogs on The Serverless Mindset.  ","categories": ["architecture"],
        "tags": [],
        "url": "/3-things-you-need-to-master-to-get-your-team-ready-to-work-with-serverless/",
        "teaser": "/imgs/2022-05-11-3-things-you-need-to-master-to-get-your-team-ready-to-work-with-serverless/serverless-skills-teaser.jpg"
      },{
        "title": "How to make a team facilitate agile architecture workshops",
        "excerpt":"In The Serverless Mindset, I blogged about Event Storming and Example Mapping for agile architecture. How do you coach teams to adopt these workshops?         3 wonderful workshops that will make your serverless architecture agile     Successfully building a serverless system as a team is far from easy. So, here are 3 DDD workshops for everyone to do collective and incremental architecture.          It’s actually harder to do serverless development with two engineers than it is with one engineer. And [the problem] scales up ~ Nathan Taggart        (From 3 Wonderful Workshops That Will Make Your Serverless Architecture Agile)    A few weeks ago, I wrote a guest post for Marco Troisi on his blog, The Serverless Mindset. It presents 3 workshops: Big-Picture Event-Storming, Design-Level Event-Storming, and Example Mapping. It explains how these workshops avoid command-and-control or chaotic architecture in serverless teams. In fact, they unlock a middle way that is calmer, more collaborative, and more effective! It’s agile architecture!      As much as management wanted to benefit from Agile, they didn’t want to give up control. They could not give up micromanaging. It was more important for them to be in control than it was for the company to be successful. (Someone on extremeprogramming@groups.io)       I stopped believing in team self-organization long ago. […] There are amazing individuals who can do it, and I worked with them. Those usually make up 5-10% of the team. (Someone on extremeprogramming@groups.io)       I needed to go out on medical leave for a while, and when I came back, every single practice I had introduced got rolled back, with expected results. (Someone on extremeprogramming@groups.io)    Guess what? These challenges and workshops are not restricted to serverless contexts. So most teams will find agile architecture helpful.   As coaches, our job is not to facilitate these workshops (despite what we are often asked 😕). Our job is to make these workshops a habit in the teams! So how do we coach teams to run these workshops regularly by themselves?      Wouldn’t it be great if you knew how to coach teams to do transparent and collective architecture?    How to teach Event Storming      Big-Picture Event Storming     Big-Picture Event Storming is a whole-team workshop whose purpose is to visualize everything! It’s about sticking all the domain events of the system on a giant design board. It’s Domain-Driven Design in practice! It transfers tremendous knowledge between the domain experts and the developers.          An architecture accelerator     Event Storming can achieve over a few days what would take months with traditional architecture. The idea is not to get a perfect up-front architecture. Instead, it’s to design just enough for everyone to pull in the same direction.     (From 3 Wonderful Workshops That Will Make Your Serverless Architecture Agile)    I can think of 3 learning steps to be able to teach Event Storming to a team:      Facilitating Event Storming yourself   Injecting Event Storming into your team as a team member   Finally, installing Event Storming into the organizations that you are coaching   I already wrote about how to facilitate Event Storming workshops. I also wrote about how to inject Event Storming into your organization.   The third step is particularly tricky because the workshop often lasts for a few days. As a result, people will be afraid to take on the facilitation!   Let’s see how to hand over the facilitation to the team.   Start with Immersion      It’s useless to expect someone else to help with the facilitation during the first Event-Storming. People usually see it as an ‘extraordinary’ event that is not worth learning.   Don’t bother and facilitate the workshop yourself! As often as you can, remind them that they should regularly re-run this workshop themselves.   Use this first taste of Event Storming to spot people who “get it.” Then, after the workshop, try to involve these people in facilitation and agile architecture.   Repeat Shorter Event Storming Workshops   Take the habit of suggesting short Event Storming workshops:      Flow Event Storming for retrospectives   Or Design-Level Event Storming to dive into the design of a core part of the code.   These smaller-scale workshops don’t need to involve everyone.   As soon as the team agrees to run another workshop, find someone to help you with facilitation. Take 2 hours with your apprentice facilitators to rehearse a simple scenario.   Continue suggesting regular Event Storming. The next time you run the workshop, handover facilitation completely. Be present as a backup, though. When that works well, you have successfully made Event Storming a habit in the team.   Example Mapping      Example Mapping     The promise of serverless is that the cloud provider will take care of all the plumbing for you. This means that your lambdas should contain almost only domain-oriented code. Example Mapping is a neat activity to clarify the Lambda’s business rules.     Example Mapping only takes 20 minutes, so you can run it very often. For example:          When starting a new user story.     Or after a Design-Level Event-Storming, to clarify what happens inside an aggregate.       Example Mapping is a structured conversation between Developers and Domain experts. Other specific profiles (e.g. quality engineer) can also join if need be.     (From 3 Wonderful Workshops That Will Make Your Serverless Architecture Agile)    Here’s a detailed guide about how to inject Example-Mapping into a team. The practice is simple, so people should be able to adopt it quickly.    Picture from Cucumber blog   Some teams will find Example Mapping is overkill for their context. So don’t hesitate to simplify it to accommodate the team’s constraints. For example, replace Gherkin with literate programming tests.   Make these workshops a habit!      Don’t wait!     Are you involved with building a serverless system? Whether you are just starting or already deep in, you should run a Big-Picture Event Storming! It’s a starting point that will lead you to Design-Level Event Storming and Example Mapping. These workshops will result in:          Alignment between everyone.     Just-enough architecture to keep your system healthy and to avoid centralized decision-making.       (From 3 Wonderful Workshops That Will Make Your Serverless Architecture Agile)       Is a team you are coaching leaning towards command-and-control or chaotic architecture? Suggest Event Storming or Example Mapping to do more agile architecture! Remember the 3 steps to make these workshops a team habit:      Start with immersion   Spot early adopters and potential facilitators   Involve them more and more in regular sessions   What do you think? What are your tips for handing over the facilitation to coachees? I’d love to read from you in the comments.    Other articles that might interest you:      Organization Refactoring: Event Storming and DDD injection   # How to coach developers to get a chat with their product experts   A detailed facilitation guide about Big Picture Event Storming that start with Misadventures with Big Design Up Front   Why should we use Design Level Event Storming for DDD? A detailed facilitation guide about Design-Level Event Storming   Preview of The 1-hour Event Storming book Matthieu Tournemire and I are currently writing   If you are interested in more workshops, here are 5 Whole-Team Workshops To Increase Developers’ Role In Sprint Planning   If you are starting on a serverless team, read 3 Things You Need to Master to Get Your Team Ready to Work With Serverless  ","categories": ["event storming","example mapping","architecture","serverless","ddd","learning"],
        "tags": [],
        "url": "/how-to-make-a-team-facilitate-agile-architecture-workshops/",
        "teaser": "/imgs/2022-06-03-how-to-make-a-team-facilitate-agile-architecture-workshops/Facilitation-Keys-teaser.jpg"
      },{
        "title": "A Surprising Way How To Teach Evolutionary Design",
        "excerpt":"Evolutionary Design might be the most valuable yet impenetrable XP practice. Use TCR to practice baby-steps coding, the cornerstone of Evolutionary Design.         Evolutionary Design is my very favorite XP practice, but also the one that took me the longest to accept. It’s also the hardest to teach.       More than just software development skills, it [Note: evolutionary design] also takes humility, patience, restraint, discipline, and confidence.       People don’t learn new habits overnight. Three days barely scratches the surface, so sitting with them as they work through real stuff, and showing how things play out, is critical. It can be months before better habits stick.    Performing Evolutionary Design makes the lives of developers better! With it, a team can re-orient its software design according to the latest challenges. This makes progress steadier, more predictable, and the pace more sustainable!   For Kent Beck, master programmers split large code changes into small steps. Baby steps are the building blocks of continuous software design evolution. Baby-steps programming is the fundamental skill for evolutionary design.   Unfortunately, teaching baby steps is not easy. Participants tend to stick to wide steps. Even in a TDD coaching session and even as we remind them!      Wouldn’t you love to run baby-steps training sessions and teach evolutionary design faster?    It turns out that using TCR (test &amp;&amp; commit || revert) in practice sessions does that!   How we started to run TCR practice sessions   Thomas Deniffel’s post about TCR Variants made a massive impression on me. I tried TCR with 1 or 2 katas and then suggested to my coach colleagues to experiment too. Everyone liked how it ‘forced’ us to stick to baby steps.   Here is what our TCR flow looks like:      It watches the file system for changes to the code   As soon as it detects some changes, it builds and runs the tests            If the tests fail, it reverts       If the tests pass, it commits           It repeats   If we are too bold or ambitious (or pretentious) with a code change, the tests fail, and the change gets reverted. One quickly learns that the easiest way to go through an exercise with TCR is to stick to baby steps.      Having participants stick to baby steps during a code kata has always been an issue. So we agreed to experiment with TCR in a few training sessions.   This was a success! The feedback after our test sessions was excellent! As a result, TCR has become the default for every session we run.   What do participants say?   As I said, up to now, feedback has been great. Here are some examples of what people said at the end of a session.   It’s fun!      It’s almost a ‘gamified’ way to go through code katas!       There’s a poker flavor of ‘betting’ on your change.    It stretches coding muscles.      It’s TDD on steroids!       I learned how to do more rigorous katas.       I pushed baby steps to 11!               Screenshot from Spinal Tap movie, photo from Wikipedia           At first, I was skeptical about the Revert thing. But once I realized it’s meant as a practice tool; I started to appreciate its ruthlessness 😀    It yields more profound lessons.      We were lost in endless refactoring, and the tool taught us a lesson: to let go!       It’s good for the focus because you must stick to what you are doing now. And it’s good for the ego because it’s a script, not a human, that tells you to use smaller steps!       It creates a real feeling of progression!       I learned to focus more on each step rather than wanting to solve the problem.    TCR is like a workout machine!   TCR is a bit like a workout machine for TDD practice sessions. It makes the sessions more challenging and interesting. As a result, it’s a learning accelerator.      What exactly changes with TCR?   A classic TDD practice session   TDD-rookies tend not to run the tests often enough and spend too much time out of green-state. Here is the situation I have often observed:      Participants write a test.   They write way more code than needed to pass the test.   They manage to pass the test with a lot of thinking.   Unfortunately, they broke other tests in the process.   Eventually, they take 15 minutes or more to debug everything back to green.   As coaches, we have to point out every time they start on such a difficult path, but:      It’s exhausting to be the killjoy all the time.   We cannot keep an eye on everyone at the same time. As a result, many participants struggle with long steps.   TCR as a coach bot!   Things are different in a TCR practice session:      Are participants making an ambitious giant step that fails? =&gt; The code is reverted.   Did they break other tests while passing the new one? =&gt; The code is reverted.   TCR keeps the code in a green state. It “forces” participants to stick to baby steps. It does that without us having to remind people all the time. It does that without us having to be there!   As a bonus, we can have more people in a single session!   From TCR to evolutionary design      J.B. Rainsberger explains that practicing micro-steps teaches how to ‘chunk’ them into larger ones. With TCR, participants experience going through a programming task with baby steps. They can even check the git-log to see the detailed steps they used. This is how they learn evolutionary design.   See for yourself!   The simplest way to give TCR a try is to run \"test\" &amp;&amp; \"commit\" || \"revert\" instead of just test.   If you are using git, you can commit with git commit -am \"TCR\" and revert with git restore .. The test command depends on the language and toolchain you are using. Pick your favorite kata and give it a go! If you lack inspiration, go with the classic Bowling Game. I have seen TDD veterans get reverted on this kata and learn from TCR. There’s definitely something for everyone here. I’m sure you will have fun too!   Batteries included!   At work, we have been working on a more advanced TCR tool. You can find it at https://github.com/murex/TCR. You can also find a sample kata repo at https://github.com/murex/Kata-BowlingGame.   Compared to terminal commands, it also:      watches the file system, and runs TCR as soon it spots some file changes   pushes and pulls through an online repo to enable remote mobbing with git-handover   provides a mob timer to switch the driver role during the session   can commit reverts for participants to retrospect on what got reverted   finally, it’s also gentler than strict TCR because it does not revert the tests   The tool currently supports git, Java, C#, C++, and Go, but it’s open to contributions!      Give TCR a try and share your experience! I’d also love to read about your tips for teaching evolutionary design.     Here are other posts that you might find interesting:      How to deliver a remote training with code-katas and randori in pairs   Incremental Software Development Strategies for Large Scale Refactoring #2: Baby Steps   A coding dojo exercises plan toward refactoring legacy code   Why you should start a team coding dojo Randori right now   How to start a team coding dojo Randori today  ","categories": ["tcr","coding dojo","coaching","tdd","learning","architecture"],
        "tags": [],
        "url": "/a-surprising-way-how-to-teach-evolutionary-design/",
        "teaser": "/imgs/2022-07-05-a-surprising-way-how-to-teach-evolutionary-design/tcr-gardening-teaser.jpg"
      },{
        "title": "How to make TCR evolutionary design practice sessions irresistible",
        "excerpt":"Getting participants in a TCR code kata exercise seems complicated! Yet, given what TCR is, these sessions can radiate genuine fun and direct applicability!      A few weeks ago, I wrote that TCR code kata exercises are great for teaching evolutionary design.   Have you suggested TCR sessions to the teams you are coaching? Great! Unfortunately, “Selling” TCR is not easy. Here is what you might notice:      The big pushback to continuous design is of course from devs who insist that they need an end-all solution now, and that it has to be completely implemented now even if the design represents a speculative need.       The harder sell is with people who have been taught that changing the design is “rework waste.”       They used their code review process as a hazing ritual, forcing new programmers to prove they could write everything perfectly the first time.    I’ve also written about “TDD does not work in real life!” complaints. It becomes even worse as we push TDD to the TCR extreme! TCR code kata exercises don’t appeal to developers who believe in “perfect code first.”      What if you had a trick to hook developers on TCR evolutionary design practice sessions?    Lately, my colleague Ahmad started to run kata sessions with a group of newcomers at Murex. One of the first sessions was to practice TDD and TCR with the Mars Rover kata. The group set out for an ambitious test and started to write a lot of code. Despite one mobster complaining about the long step, they pushed on for more than 15 minutes. Eventually, they were confident that their code was working! They saved… TCR ran… and reverted everything!   From then on, the dynamic of the group changed. It went from ego-hero to collaboration against the new TCR opponent. During the end-of-session retro, everyone saw that baby steps were safer and calmer. They also understood that TCR was a great teacher.   To our own surprise, my team and I have discovered that katas with TCR sell better than those with TDD!      Before, Present sessions like fun drills   Make sure to Run a genuine deliberate practice session   Finish the session with an effective retro to transpose practice to real work   Present sessions like fun drills!      How you introduce the sessions greatly impacts the participants’ engagement.   Leverage your pedagogy-expert posture   TCR sounds like a ‘drill’. The word ‘drill’ has an ‘improvement’ connotation and people understand that:      Drills are different from real-life work.   Yet, drills positively impact real-life work.   Using such vocabulary also puts you in the position of a pedagogy expert. (Side Note: I am no pedagogy expert, but as a tech coach, I know more than most developers I work with. In the country of the blind, the one-eyed man is king!) As a result, people are more likely to trust you about the learning value of our exercises.   Highlight the fun!   One of the main observations we made about TCR katas is that they are fun! They have a poker-like twist, as we ‘bet’ our code on green tests. We felt this ourselves, observed it in group dynamics, and heard it through feedback.   💡 TDD code katas can be fun, but TCR code kata exercises are even more fun!   Run a genuine deliberate practice session   Keep them in the practice mindset.   Always repeat that this is practice. At first, participants might be unable to use TCR or TDD in their day-to-day work. Remind them that a code kata is a unique occasion to practice by the book in a safe context.      I stopped and reminded them that they were here to learn how to build things incrementally, that they should continue to work through the exercises with that in mind (Jeff langr on extremeprogramming@groups.io)    This should help them to stay engaged at times when they are doubting.   But TCR has yet another advantage. It’s weird and very different from how most developers write code. At first, it’s almost impossible for participants to envision using it in their day-to-day work. As a result, they tend to take TCR code katas as pure exercises. In the end, it’s easier for participants to stay in a constructive practice mindset!   Wait for reverts!      At the risk of sounding sadistic, TCR gives its critical lesson when it reverts our code! If we were running a traditional TDD session, we would have to remind the participants to take small steps.   # Traditional TDD Code Kata while(true)    coach.say(\"Remember not to write too much code before running the tests!\")    sleep(1.minute) end   💡 With TCR, the best practice is to let participants take long steps!   Most of these extra-long steps end up with TCR reverting the code. Participants might want to ‘cheat’ to recover the code at this point! (For example, you can get it back with a Ctrl+Z in many IDEs). This is the moment to put on your “sports coach” cap:      Hey! This is cheating 😉! This is the crucial moment of TCR. You just learned that this code does not quite work, and TCR is telling you that you need to go slower.    This can be a difficult lesson for some participants. Others in the group will be happy to start from a blank page again, though. Usually, everybody gets over it after a few minutes.   Lean on TCR   As illustrated above, we don’t need to talk much during a TCR code kata exercise. Instead, participants can learn by themselves through the TCR script.   Developers, used to picky compilers, will get far less annoyed by the machine than by a human coach. As a result, it will be easier for them to take a step back and reconsider their opinion.   Also, they will learn what is meaningful to them at this moment. It’s different from us trying to guess what is worth remembering!   To summarize, TCR sessions are more intense and effective and need less coach intervention!   Finish the session with a compelling retrospective   We always end kata sessions with a quick retrospective. It’s essential to take 5 or 10 minutes for participants to discuss what they did and learned. It’s also the best moment to make the decision to change how they want to work. TCR has some critical lessons to bring to these quick retrospectives.   Stop biting more than you can chew!      TCR is a cruel teacher: whenever we try to make a change that outgrows our abilities, TCR reverts it.   After a few hours, one will get a gut instinct about how much they can change in one step without making mistakes!      The competent programmer is fully aware of the strictly limited size of his own skull; therefore he approaches the programming task in full humility, and among other things he avoids clever tricks like the plague. (Edsger W. Dijkstra)    As Dijkstra said, this is a fundamental lesson that is always applicable. We can apply this knowledge to whatever we are working on:      a code kata or a commercial product   a greenfield or a legacy codebase   a professional app or a Sunday pet-project   We’ll have learned this for the rest of our lives! (Note: use TCR regularly to keep the lesson up to date!)   As a coach, we can use the retro to ask powerful questions about this topic:           What did you notice about the changes that passed?     What about the changes that TCR reverted?     How can you transpose this to your day-to-day work?      We have added a new feature in our TCR tool to help participants answer these questions. It’s a pure didactic feature that adds fail and revert commits. Then, through the Git log, participants can retrospect the failed changes. To try it, run the TCR tool with tcr --commit-failures. It does not change anything to the TCR flow, except that you’ll get extra commits to retrospect.   For example, you can inspect the failing changes with git log --oneline --full-diff -p .   4e9bbf5 ❌ TCR - tests failing diff --git a/java/src/main/java/com/murex/BowlingGame.java b/java/src/main/java/com/murex/BowlingGame.java index d49fe79..df25c72 100644 --- a/java/src/main/java/com/murex/BowlingGame.java +++ b/java/src/main/java/com/murex/BowlingGame.java @@ -30,15 +30,20 @@ class BowlingGame {      static int score(int... rolls) {          int score = 0;   -        if (rolls[0] == TOTAL_PINS) -            return 28; -        if (rolls[2] == TOTAL_PINS) { -            return 20; -        } +//        if (rolls[0] == TOTAL_PINS) +//            return 28; +//        if (rolls[2] == TOTAL_PINS) { +//            return 20; +//        }            int iRoll = 0;          for (int iFrame = 0; iFrame &lt; NB_FRAMES; iFrame++) { -            if (isASpare(rolls, iRoll)) { +            if(rolls[iRoll] == TOTAL_PINS) { +                score += TOTAL_PINS; +                score += rolls[iRoll] + rolls[iRoll + 1]; +                iRoll++; +            } +            else if (isASpare(rolls, iRoll)) {                  score += rolls[iRoll] + rolls[iRoll + 1];                  score += rolls[iRoll + 2];                  iRoll += 2;   You can also get a birds eye view of your commit history through your online Git interface:   .   Suggest TCR when refactoring production code   You can also use the retro to nudge participants into using TCR for their day-to-day work. With fast feedback tests, refactoring production code with TCR is pretty straightforward. For example, you can say something like:      If you happen to be refactoring a piece of code that has fast tests, give TCR a try! It makes refactoring calm and practical.    Let’s summarize      Here is my advice to get people engaged in TCR practice sessions:      don’t mention TCR   set a “fun and collaborative drills” atmosphere   let people be reverted   run impactful session retrospectives   As I wrote above, TCR learnings are surprisingly production-ready! Coachees will feel the benefits pretty fast! After that, they’ll start to look forward to more practice. And as they go through sessions, participants will find TCR increasingly more attractive.   At Murex, we have been building a TCR tool for code kata exercises. You can find it at https://github.com/murex/TCR. You can also find a sample kata repo at https://github.com/murex/Kata-BowlingGame.   What about you? I’d love to read your feedback about getting people engaged in TCR practice sessions! Or maybe you have been using TCR on a day-to-day basis? If you get to try these tips, I’d love to hear how it went!     Here are other posts that you might find interesting:      A surprising way to teach evolutionary design   How to handle “TDD does not work in real life!” during code katas   How the pandemic made us discover better ways of coaching   How to coach a team that has been burnt by bad TDD   How the Samman Method helps to sell technical coaching internally?   7 tricks to influence a team resisting to change its technical habits  ","categories": ["coaching","tcr","coding dojo","change management"],
        "tags": [],
        "url": "/how-to-make-tcr-evolutionary-design-practice-sessions-irresistible/",
        "teaser": "/imgs/2022-08-07-how-to-make-tcr-evolutionary-design-practice-sessions-irresistible/tcr-donkey-teaser.jpg"
      },{
        "title": "A complete workshop for your team to see what's a good test strategy",
        "excerpt":"Teams often disagree about what is a good test strategy! So here is an all-in workshop to capture practices, define testing terms and agree on improvements.      One of the first teams I worked with as a full-time coach struggled with testing. Let’s call this team Phoenix. It was relying almost only on slow end-to-end tests. The team had just welcomed a few members to make it cross-disciplinary. In addition, the team aimed to speed up the testing feedback loop. So, developers were xUnit-testing while quality engineers were speeding up systems tests.   Around 2 years later, the situation had improved a lot. Developers were getting more confidence in their tests, and end-to-end tests were faster. Yet, the team was still relying on those slow tests. There was yet no coordinated effort towards an improved test strategy. As a result, the team had to overcome a new threshold to further improve their testing.   I also noticed that the team used its own DIY vocabulary for the different kinds of software tests:     “TPKs” for end-to-end tests   “offline tests” for any type of developers tests      One thing I’m still unclear on is what you mean by “Integration Testing: and whether it’s similar enough in meaning to what the rest of the folks mean by “integration testing”       I’ve learned to define testing terms before having testing conversations. At one large-sized company, engineers had different definitions for each testing term. E.g., one Pearson’s “integration” test was another person’s “integrated” test.       I agree that “integration Test” has a lot of meanings.    Blurry testing terms are a common symptom of not thinking about “What is a good test strategy for our team?” A lack of testing strategy leads to many problems, like:     Holes in the test harness, which lead to bugs and rework   A slow testing feedback loop, relying on old habits, like end-to-end integrated tests   Increased test maintenance burden because different kinds of test overlap coverage.   With time, these “testing problems” increase the risk of abandoning testing altogether!      Wouldn’t it be great if engineers regularly aligned on testing terms?    Let’s teach a dedicated workshop to the teams!   Phoenix tackled testing issues with an all-in workshop to discuss the testing strategy. During the workshop, all team members were able to:      Learn industry’s testing best practices and compare their own practices   Agree and capture testing terms   Agree about who, how, and when they should test the different aspects of their system   As a whole team, define and plan an improvement to their testing strategy   Here is what they said about the workshop:      This workshop made me think about many things, thanks to the different questions.       Thanks to this workshop, I enriched my testing vocabulary with new words, such as subcutaneous tests.       It’s like a game; we had fun but also came up with new solutions.       I liked the fact that the whole team participated in the workshop.    More specifically, QA engineers and developers saw that some parts of the code were tested twice. It was clear that to reduce the testing feedback loop, they had to collaborate more. They had to start replacing end-to-end tests with developer tests. At the end of the workshop, they decided to write a first decoupled acceptance test.   A few months after the workshop, the team was now regularly removing end-to-end tests. As a result, developers could enjoy a faster testing feedback loop.   🎁 With better testing comes more sustainable and calmer work!   The Workshop Agenda   You might be wondering what was inside this workshop? Here is a summary of the steps that the Phoenix team went through. The workshop is a succession of 7 steps. All in all, it takes between 5 to 7 hours. Therefore, running this workshop as a series of 1-hour sessions is best.   Step 1: Learn what industry agile trailblazers do   Before the session, we sent homework to the team members. We shared links like:      DevLin2017 - Aslak Hellesøy - Testable Architectures - YouTube   7 Reasons to Choose Consumer-Driven Contract Tests Over End-to-End Tests (reflectoring.io)   The homework was to search the web and these references for different kinds of software tests.   During the workshop, we asked each team member to present a kind of test. To illustrate this, they had to:      Present a definition to the rest of the team   And place it on this duration X maintenance X coverage quadrant.      The idea is to learn how the different kinds of tests compare.   Step 2: Draw the ideal pyramid   With this new knowledge, team members could build an ‘ideal’ test pyramid.      It’s funny that this ideal pyramid does not look too pyramidal. Martin Fowler said we should not bother so much about the shape anyway! There is no good-or-bad result. This step is yet another way for the team to apprehend different kinds of software tests.   Step 3: Draw your pyramid   After digging through their test metrics, the team could draw its own test pyramid. We sent the team more homework for this activity. We asked team members to come up with metrics:      What are your different kinds of tests?   What is the total execution time of each of these kinds of tests?   How many assertions does each of these kinds of tests check?      This big rectangle in the middle represents end-to-end tests. It was clear to all the team members that they relied heavily on those!   Step 4: Compare and discuss   Participants had 2 pyramids and a better understanding of the test vocabulary. To continue, they could now map their tests to the industry’s standards.      As you can see, they had to get creative! There is no one-to-one mapping between their tests and the kinds of tests we can find in the industry. This mapping showed how end-to-end tests verified too many aspects of the system.   The team members could now imagine what a good test strategy would be for them. Here are examples of what I heard during the workshop:      We are missing decoupled acceptance tests!       We won’t be able to remove end-to-end tests until we ensure coverage differently!    Step 5: Fill a test Strategy Canvas   The Test Strategy canvas is a one-page template that summarizes how a team tests its software.   One of the goals of this workshop is to agree on names for the different kinds of software tests. That’s done in this step. The Phoenix team members also agreed to a unique test strategy thanks to this activity. This canvas clarifies precisely:      The purpose of testing in the team   What is inside or outside the team’s responsibility   The different kinds of tests: name, constraints, technologies, who runs and maintains them…   Having a whole team converge to a single canvas can look daunting! However, mixing the 1-2-4-all and asynchronous sessions is an effective way to do it.   Here is what the Phoenix canvas looked like:      The grid you see in the middle of the canvas describes every different kind of test. As an example, here is the line for Unit Tests:      This canvas can also serve as a living convention. Fortunately, updating the workshop board should not be too long. Re-running the workshop is also a good onboarding activity for new joiners.   Step 6: Solution focus to get 1 step closer   At this point, all participants agreed on the current situation. They also knew the industry’s best practices. The next step was to go through a Solution-Focus activity to draft an improvement.      This activity constrains the size of improvements to something small and manageable. This lets the PO and the rest of the team find a compromise that:      Delivers value early.   Without blocking feature delivery.   While still going in the right direction.   Step 7: Draft an improvement as a team!   The solution focus activity ends with participants brainstorming the “10% improvement”. Here is Phoenix’s improvement draft:    Final testing improvement the team agreed on.   A key benefit of coming up with this improvement with the whole team is that it is already prioritized! During the workshop, the PO had all the time to build empathy with everyone. He understood how this improvement would make the team more efficient.   💡 A nice side effect is that putting tests in place often involves paying back technical debt!   This workshop was also a way to break silos and give non-developers a look inside the system.   To summarize, such technical whole-team workshops can show a different way:   💡 The way of communication, collaboration, and collective decision-making   Is your test strategy clear?      Did you notice any miscommunication around testing between team members?      Do they disagree about testing terms, how to test, or how to improve testing?   Do the different profiles in the team disagree about the priority to give to testing?   If any of these is true, run this Test Strategy Workshop! Find someone interested in facilitating the workshop and help them the first time. They should be able to re-run it later for an update. Running this workshop now and again will result in continuous improvement in testing.   What about you? I’d love to read about other coaching techniques to handle testing disagreements! Finally, if you try this workshop, please comment on how it went or what could improve it.    If you liked this post, you might also be interested in:     How to make a team facilitate agile architecture workshops   How to help a team to find their preferred mob programming rules?   5 Whole-Team Workshops To Increase Developers’ Role In Sprint Planning   3 Good and Bad Ways to Write Team Coding Standards and Conventions   How We Started Exploratory Testing  ","categories": ["testing","coaching","team building","collaborative work","improvement kata"],
        "tags": [],
        "url": "/a-complete-workshop-for-your-team-to-see-what-s-a-good-test-strategy/",
        "teaser": "/imgs/2022-09-24-a-complete-workshop-for-your-team-to-see-what-s-a-good-test-strategy/different-test-vocabulary-teaser.jpg"
      },{
        "title": "A \"Slow Code Retreat\" to be less in a hurry",
        "excerpt":"“Developers will always have more work than time” We can race our TODO lists or accept and slow down NOW. The “Slow Code Retreat” helps us do that!         The team is good, but the organization mainly pushes them for output!       Developers have as much difficulty changing their habits as anyone else. Besides, we’re too busy.       This manager was in my interview and kept asking me questions like: “what if devs feel they are being slowed down?”.    We all know this story. Learning new practices or taking on new habits takes time. And developers always have something more urgent to do.   Lately, my colleagues and I almost worked with a team struggling with onboarding. We helped define knowledge-sharing actions, trained seniors to run team mobs or katas, etc. Yet, at the moment of setting up team sessions, senior developers were too busy!   There is no workaround: Sustainable pace starts NOW!      In its most trivial formulation, it’s as simple as this. To go from doing A to get to doing B, we have to unlearn A and learn B. That takes time. If we don’t have time, we can’t afford to change. (Slash the Load, Mike Hill).    Teams only have so much control over their time! There are always urgent and valuable projects to deliver. Even productivity and sustainability improvements only add more work in the short term! Very often, management will not grant the team some time for learning.   Sometimes, it might look like there are no ways out!   At every moment, we have the choice to step back and take it easy! Nothing prevents us from injecting some ‘sustainable’ seconds into work. It’s not about what happens. It’s about our state of mind and how we live what is happening.   At first, it can be only a few seconds here and there during the day. With time though, we can train our minds to do this more regularly.   How to slow down at work as developers?   “Inject sustainable instants throughout our day of work” might sound a bit too vague… So let’s see how to do this more practically.      The Slow Code Retreat is a workshop I ran at XPDays Benelux. The idea is to let people experience different ‘slow working’ techniques.    Here is the feedback I received at this workshop.   Here is how the workshop is structured:      Welcome   Connect with the topic and other participants with these 4 questions:            What did you say no to to attend this session?       According to you, what could slow code be?       From your experience, list a few situations where slow code could have been helpful.       What is the big question you bring to this session?           Present 6 slow code practices (see below)            One or two rounds of slow code practice on code katas                    Participants pick 1 or 2 practices they want to experiment with           They self-organize in solo, pairs, or mobs           For 30 minutes, they go through the simplest katas like Fizz Buzz or Pascal’s Triangle                           Takeaways   Q&amp;A   The complete workshop lasts between one and a half and 2 hours.   6 slow working practices      Participants can try the 6 slow working practices during the workshop. I’ve split them into 3 categories: coding, communication, and mobbing.   Slow Coding   We can use these whenever we are programming solo, in a pair, or in a mob.   Slow TODO List   Take the time to focus only on the present task:      Keep a TODO list while you code   Keep track of everything that remains to be done   Check or strikethrough steps as you do them   Indent sub-tasks   Can use a .txt, a .markdown, a shared online doc, or a mind map   Reorganize your tasks when needed   If you revert steps that you cannot do yet, it becomes the Mikado Method   Benefits:      This removes all the mental load involved with keeping track of the status of work   The map serves as a communication tool for handovers, pairing, or mobbing   Helps us to drop things that we eventually decide not to do!   References:      code-in-flow/mindful-programming (github.com)   TODO list or Mind Map for programming   Slow Baby Steps      Take the time to practice egoless programming and baby-steps programming with TCR:      Start coding with TCR   Whenever you get reverted            Notice the feelings you experience and accept that error is humane.       Practice humility and Egoless Programming       Try again with a smaller step           As the day goes by, notice how your capacity for flawless work decreases!   Benefits:      With time you’ll learn how much you can do without mistakes   This teaches you to do extra small steps that cannot go wrong!   This helps to understand when to call it a day and leave   This avoids writing low-quality code when you are too tired   References:      code-in-flow/mindful-programming (github.com)   The 10 commandments of Egoless Programming   Slow Self-Retros   Take the time to observe what happens inside us:      Set a timer to ring every 5 minutes   Every time it rings, take a short pause and fill out this log individually:                  ROTI       Did       Learnt       Puzzles       Feelings       Need       Decision                       …                                                              Use cheat sheets for feelings and needs references.   Benefits:      Helps us to spot our non-constructive reactions to events like:            “I don’t know how to do that!”       “Damn, I was sure this test would pass! I’ve got no idea why it’s failing!”       “How stupid I am!”       “I don’t understand anything about this code!”           Develops our knowledge of inner feelings and needs.   Provides plenty of data for continuous improvement and retrospectives of all kinds.   References:      code-in-flow/mindful-programming (github.com)   Slow Communication   For the sake of organization, I put only Slow Code Reviews in the category of slow communication. Yet, Slow Navigating could also go in this section.   Slow Code Reviews   Take the time to offer non-violent feedback to your colleagues:      When writing reviews   When voicing feedback in the mob   Follow the Non-Violent Communication pattern:            List the facts       Share your feelings       Share what needs you have       Suggest a new way of doing things that meets everyone’s needs           Benefits:      It prevents conflicts from building up for the wrong reasons   It improves teamwork, psychological safety, but also the efficiency of reviews   It leaves the door open for more creative solutions   It builds our feelings and needs fluency and self-awareness   It is a way to start learning Non-Violent Communication   References:      Nonviolent Code Review   Five Strategies For removing Violence From Code Reviews   Non-Violent Communication In Code Reviews: Receiving Comments   Resolving Code Review Conflict In A Hippie-Dippie Way   Slow Mobbing   Finally, here are two practices specifically suited to mobbing or pair programming.   Slow Driving      Take the time to be aware of your feelings and thoughts when driving:      Accept, or even better, relax into being navigated!   Stay focused on typing and do what you are asked to do   Focus on the touch of the keyboard and what displays on the screen   Don’t judge what you are asked to do   Breath while compiling or running tests   Benefits:      This driving ‘pause’ makes the mobbing more sustainable   It helps to take a step back:   Let your creative mind work in the background   Come back to navigation with a new perspective   References:                                      [Mindfulness For Programmers. 3 simple exercises to be more present…           by Julia Di Russo           Towards Data Science](https://towardsdatascience.com/mindfulness-for-programmers-da6f92147b8f)                           Slow Navigating   Take the time to practice egoless programming while navigating in the mob:      When other mobsters seem to be struggling with something you’ve figured out:            Observe any criticism or judgment that you may feel.       Instead, try to be patient and compassionate.           When you struggle to understand or solve something:            Observe any self-criticism or self-judgment that you may notice       Instead, try to accept these feelings and let them go.           When other mobsters come up with different solutions than you:       - Strive towards egoless programming and humility by welcoming their solution    Here is a variant:      Take a step back in the mob:            Strive to remain a neutral observer of the mob       Keep a log of what is happening and the mood of the mob. (a bit like Slow Self-Retro)           Benefits:      A compassionate behavior builds psychological safety, improving decision-making and team performance.   A “mob log” can be handy for continuous improvement and retrospectives.   References:      Extreme Programming Creator Kent Beck: Tech Has a Compassion Deficit   The 10 commandments of Egoless Programming   Try the workshop!   Here are the slides for the workshop I gave at XPDays Benelux.      My first recommendation is to try the workshop yourself. My first try at “slow self-retro” was mind-blowing. I was dumb-struck to see the emotional roller coaster a simple kata would get me through! If you have close peers to try it with, slow mobbing will teach you even more!   Also, next time you face a team with no time for coaching, don’t try to force it into their calendar. Has the team got “down time” in its calendar: Scrum slack-time, SAFe Innovation &amp; planning? (Or August in France 😉?) Suggest volunteers try this 2 hours workshop during this downtime:      “Slow Driving” and “Slow TODO List” make work more sustainable and productive from day 1!   The other techniques need more time to install (that might be a topic for a follow-up post)   All practices will get them to explore an ignored facet of software development   Whatever you try, I would love to read your feedback!    Here are other articles that might interest you:      Technical coaching can be exhausting at times. Here are “3 Questions To Let-Go Technical Agile Coaching Measures” and “5 technical agile coaching tips to fight exhaustion from laggards.”   Here is another post containing other experimental technical coaching practices: “How the pandemic made us discover better ways of coaching.”   100% pairing can be difficult for some people. Here is “How to use Mob Programming at the rescue of Pair Programming burnout.”   Slow TODO List is a direct use of the “TODO list or Mind Map for programming,” the only difference being a shift of intent from productivity to calm.   “How to help a team to find their preferred mob programming rules?” presents how to use a mobbing code retreat to experiment with new rules for mobbing. This could be the perfect occasion to try slow mobbing!  ","categories": ["coaching","programming","tdd","pair programming","tcr","personal productivity","coding dojo","mob programming","sustainable pace","continuous improvement"],
        "tags": [],
        "url": "/a-slow-code-retreat-to-be-less-in-a-hurry/",
        "teaser": "/imgs/2022-12-27-a-slow-code-retreat-to-be-less-in-a-hurry/developers-life-race-teaser.jpg"
      },{
        "title": "How to avoid the \"large-scale impact\" pitfall\n",
        "excerpt":"Applying willpower to have a “wider than team” impact on an organization is a recipe for burnout! Check the “Local Actions, Global Impact” strategy instead.         I used to share and bring new knowledge through the organizations I have been in. But in my new job, it looks like no one cares!       Sadly, I spend less and less time coding for the project. But I find that people respect coaches who are in the trenches with them!       Damn, I again got caught in an abstract ‘large-scale agile transformation’ discussion… This leaves me anguished every time!    As change agents, it’s natural to wonder how we could have a broader impact on the organization where we work. Isn’t our goal to transform how people work?   Unfortunately, you are among many parameters that influence an organization’s evolution. Focusing on wide-scale organizational transformation is no guarantee for results. In fact, this behavior can lead to exhaustion!      Wouldn’t it be great if you had the formula to refill your energy for sharing with people? Whatever is the organization evolving into?    Let’s see how to avoid the trap and still make a difference.   “Local Action, Global Impact”   I borrowed this motto from environmental protection movements. I recently read Let my people go surfing by Yvon Chouinard, the founder of Patagonia. The company has been subsidizing environment-protecting activists almost since its creation. One key point they make when selecting a group to support is that it is local and does local actions. Here is why:      These people know the situation better.   They have “skin in the game” to protect their homeland.   They will be able to defend their cause better.   Small groups waste less time and money on bureaucracy.   As small wins pill up, they tilt the public opinion and the law.   Let’s copy these hardcore change agents!   Change only happens one person at a time, as they switch to a different point of view. Change happens locally, by nature! Simon Sinek supports this idea in How to Make a Cultural Transformation. He explains that the only sure way he knows to make cultural change is to win over 15% of people. You do this by meeting them and making their lives better.      It means spending as much time as possible working with or for teams. It also means limiting the energy we burn for “large-scale agile transformation.” It’s too easy to get caught up in this topic. I try to remember to reassess my priorities regularly. I adapt whenever I see that I am disconnecting from teams and talking about change that I cannot control!   Not only does the “Local actions, Global change” strategy works, but it will also make your life lighter!      First, you will stop worrying about the unachievable goal of “large-scale transformation.” Instead, enthusiastically work with people and consider broad change a bonus.   Second, you don’t have to ask for permission to bring change! Just make things better for you and the people around you.   Local Actions   If you had not yet understood 😉: Local actions are where you should invest your time and energy!      Examples of local actions   The prominent local actions are about helping team members with their day-to-day issues. For example:      Running whole-team workshops to tackle particular topics and to break silos. Check out 5 Whole-Team Workshops To Increase Developers’ Role In Sprint Planning. (We have recently experimented with a workshop to build quality views, let me know if you want me to write about it)   Facilitating mob programming or coding dojo sessions around legacy code or maintainable testing challenges.   Preparing for any of the above also counts as local action. Be careful to get regular feedback and remember that you should be slightly embarrassed the first time you run the session.   You may have heard me say: If you&#39;re not embarrassed by the first version of your product, you&#39;ve launched too latehttps://t.co/r4JyKzzyWO &mdash; Reid Hoffman (@reidhoffman) March 29, 2017   Local action is not limited to teams! Sessions with managers or product managers can be local too. However, keep the focus on their behavior and not that of others!   Another local action is to be present and embody XP’s calm and steady rhythm! This behavior has a lot more impact than we usually think!   You can also host ‘open sessions,’ which anyone from the organization can join! These sessions can be code katas or educational workshops. It’s an excellent way to generate awareness, connect, and provide help.   The direct benefits of local actions   The main obvious benefit of local actions is that we can directly improve some people’s lives! Many developers told me how learning XP practices changed their work life. Their days had become more effective, engaging, and, most of all, calmer.   So, keep in mind that XP improves people’s lives. Focus on local actions. Your energy will follow! Plus, people will listen to you if you remain optimistic!   From another perspective, every local action is a potential conference talk or workshop. Sending proposals or attending meetups is another chance to improve developers’ lives.   Finally, the local action strategy is perfect if you are an embedded coach or a developer! All local actions can be part of your day-to-day job. Make sure to ‘sell’ it the right way:      Organize problem-solving workshops to make your team more effective.   Run mob or kata programming sessions to increase the skills of newcomers (and others).   Run sessions at public conference events to help your organization with hiring.   Global Changes      Focusing on local actions keeps us energetic and optimistic. Moreover, local efforts maximize our long-term chances of inspiring others and triggering more change! Here are examples of broad changes that local actions can trigger.   I’ve said above that XP practices equip developers with a calmer and more productive work life. Less stress at work also means a happier family at home! Who knows? You may contribute to saving couples and healthier kids through technical coaching!   You might inspire others to start local actions too! Don’t forget that any movement begins with a first follower.               When the people you coach move to a new team or company, they will also impact those around them! Maybe they will, in turn, trigger large-scale changes later.   Finally, it is possible that your coaching leads to organization-scale change. It’s a long shot: a lot is outside your control, especially in large organizations. So, please don’t count on it! Yet, if the organization is supportive and you get enough “followers,” you have a chance! Be patient.   Again, all these are bonuses! Don’t expect them. You can expect a local impact if you stay focused on local actions and maintain your enthusiasm. Any local result already has enormous value! And wider change will happen from time to time. So celebrate your local acts, and share stories when a broader impact happens.   Make it a habit   XP started as a local experiment in the C3 project. If the goal had been to spread to every dev team, XP would be a failure. Yet, XP has changed the face of the entire industry. Practices like unit testing and Continuous Integration have become standards. Others, like pair programming, are still spreading. So let’s stick to our local actions! Who knows what will happen?   Here is my final trick to remember to “Act Local.” First, print and keep a large calendar on your desk (you’ll find plenty online). Then, every work day, just before you leave, write down three local actions you did.      It’s a bit like Self-Kudos. Don’t be falsely modest: you are only writing for yourself! Once you are done, step back, and be proud of what you did!   Finally, read back through all the actions you did when you need to lift your mood!   You might also love…      3 Questions To Let-Go Technical Agile Coaching Measures   How to measure (and report 😢) your tech agile coaching effectiveness?   5 technical agile coaching tips to fight exhaustion from laggards  ","categories": ["coaching","sustainable pace","change management"],
        "tags": [],
        "url": "/how-to-avoid-the-large-scale-impact-pitfall/",
        "teaser": "/imgs/2023-02-22-how-to-avoid-the-large-scale-impact-pitfall/change-dominos-teaser.jpg"
      },{
        "title": "A Quality View Workshop to Discuss Technical Excellence",
        "excerpt":"Here is a step-by-step workshop to guide a team to drawing a Quality View. Use it to discuss investment in agile technical excellence with stakeholders.         The main problem is not agreeing that technical excellence is essential for the success of the delivery. Corporate culture is 100% focused on feature factory, delivering features at the expense of technical excellence.       Tools have taken over the meaning. I need to chant “The card is a token for an ongoing conversation” over and over.       The devs had no say in what stories went into the sprint.    Developers are often stuck in their development roles. They are rarely involved in product management decisions. Non-technical people are usually not aware of the importance of technical practices.   Without the required technical work, code degrades, and maintenance increases. Eventually, work becomes less sustainable.   As coaches, we see teams and developers suffering from this situation. From experience, we understand where this is coming from. Yet, we are often at a loss about how to make the business care about agile technical excellence! Without enough attention, trying too hard will make us disillusioned and rebellious!      Wouldn’t it be great if you had a trick to make people collaborate? So that they found solutions to deliver features with technical excellence in engineering?    Quality Views could be this trick! Let’s see:      How I began to work with Quality Views   A workshop guide to collaboratively build a Quality View   What to do after the workshop   Quality Views as a trigger for discussions   The company I work at, Murex, has been and is still hiring a lot. Therefore, onboarding of new developers is a critical topic. New joiners have a lot to learn:      Technical skills   The financial domain   And a few inevitable obscure pieces of code.      Some teams asked if my team and I could help them to onboard new team members. We had heard how Colin Breck used Quality Views to tackle technical debt. We had the idea to use Quality Views in a workshop to get the whole team to discuss how to speed up onboarding.      I think the most valuable aspects of employing Quality Views are in the discussions that take place in forming the quality dimensions, evaluating the various components, and prioritizing the work. (Colin Breck - Reflections on Using Quality Views)    Our goal was to trigger onboarding discussions between developers and non-technical stakeholders. It should lead to knowledge-sharing or onboarding-time-reduction actions being prioritized.   To learn more about Quality Views, I highly recommend you to read these two posts from Colin Breck:      Using Quality Views to Communicate Software Quality and Evolution   Reflections on Using Quality Views   If you prefer videos, you can also check his QCon talk.   We came up with a workshop involving the whole team, including Product Owner. Here is the skeleton:      Developers collaboratively build a Quality View of their codebase.   They annotate it with their capability to work in the different modules.   Then, the view is decorated with the expected impact of upcoming features.   With this complete Quality View, participants agree on the hotspots to work on.   Finally, they come up with concrete actions to make onboarding easier.              Here is an example of what the Quality View looked like when we ran it on the TCR tool my team at Murex is building. I’ll be using this as example throughout the blog post to illustrate the steps.        Different teams did the workshop. Here is the feedback we collected a few months later:      The graph output helps to iterate and can be used repeatedly to inform decisions. (A Product Owner)       What I liked the most about the workshop was the interaction between all the team members (A Tech Lead)       Following the workshop, we did some actions regarding documentation and API simplification. (A Tech Lead)    Building a Quality View as a team is an occasion to discuss crucial yet overlooked topics.   Technical excellence in engineering falls in this class of topics! In our case, the starting point was onboarding, but people talked about:      Business risks   Technical debt management   Workload, stress, and sustainable pace.   A whole-team workshop to create a Quality View   Tech-Leads have a pretty good knowledge of their codebase. So they could draw an accurate Quality View by themselves. Through our coach’s goggles, though, this is a missed opportunity for collaboration!      People and collaboration over processes and tools (Agile Manifesto)    That’s why it’s better to build the Quality View together. Below are detailed steps to collaboratively draw a Quality View.   I usually run workshops in a series of 90 minutes sessions:      It’s a sustainable pace for remote.   And it leaves time for people to do “homework” or data gathering between sessions.   Two sessions are usually enough, but your mileage may vary.   1. Define your quality characteristics   First, before the workshop, define the quality characteristics you will be using. You can select them alone as a workshop designer if they are obvious. You can also have a short brainstorming and voting session if it makes more sense.      The categories that I developed are not prescriptive—the categories were simply the most important aspects to my team at the time. (Colin Breck - Reflections on using Quality Views)    Here are the characteristics we used for a team working within a legacy monolith:      Size of module   Code complexity   Testing   Number of bugs in the last six months   The capability of team members to work in a module   2. Homework   Before the workshop, ask people to collect data on the quality characteristics.   If you have read Colin Breck’s posts, you might be thinking: “Wait! That’s not what Colin said!”:      Objective measures are no panacea. It is not hard to envision a component with exhaustive unit-test coverage, that is difficult to evolve and deploy, and fails to meet business requirements in terms of performance, high-availability, or security. This is why quality needs to be represented more comprehensively, often with qualitative measures. (Colin Breck - Reflections on using Quality Views)    Metrics are great conversation triggers. Ask different people to collect different metrics. This way, data will serve as a basis for discussion but will only weigh a little. As you’ll see later, visualization will remain very qualitative.   3. Prepare a formalism   To collaboratively design a Quality View of a codebase, people need to agree on how to represent it. Of course, you can do this alone as a workshop designer, but I encourage you to get as much feedback from the team as you do.      This visual representation can become even richer, however, with the addition of information related to software quality and how the system is evolving. (Colin Breck - Using Quality Views to Communicate Software Quality and Evolution)    Here is what we used in our example:         Boxes for code modules   Size of the box for the size of the module   Color of the box for the code quality   Weather icons to represent the testing   Lines between boxes for dependencies.   Bug icons for bugs in a module   Colored avatars for the capability to work in a module   Alien icons to tag modules that are not owned by the team   The goal is not to have a formal design document. But you must have something visual that creates shared understanding and triggers conversations.   4. Overall view         I wanted to experiment with extending it to combine 1) our architecture diagram with 2) Tufte’s dense visual presentation of information and 3) my colleague’s stacked bar-graph approach, to show both the quality of the components and the evolution of our system. (Colin Breck - Using Quality Views to Communicate Software Quality and Evolution)    Let’s start the workshop finally! The first step is to draft the system, using only boxes for modules and lines for dependencies. So use plenty of stickies or an online whiteboard (Miro, Mural, Powerpoint online, or Google Slides…).   Lines don’t need to be oriented. Rough dependency visualization is good enough to have a conversation.   The 1-2-4-all scheme does wonders:      Everyone gets 5 minutes of solo work to roughly draft the system.   10 minutes in pairs to merge their draft   20 minutes in groups of four to merge further   20 to 30 minutes to merge with the whole team   If the group cannot agree, either redo a round or stop the session and ask for homework to clear things out.   Note: if you are running the workshop remotely, use 1-3-all instead of 1-2-4-all. It’s much easier to manage with breakout rooms.   5. Size of modules      You now have an agreement on the high-level design. So, do another round of 1-2-4-all to resize the boxes according to the size of the modules. This step is faster than the previous one, so shorten the timings for the 1-2-4-all steps.   In remote, it’s easy to clone the model from the previous step so that everyone can have their own.   If co-located, pick a mid-sized module and agree on it as the standard. Then ask participants to draw modules of relative sizes on paper. They don’t need to replicate the dependencies.   Some participants will know the exact number of lines of code. Others will use their intuition. This will generate interesting discussions.      It is the qualitative aspects, involving our intuition and experience, that are more valuable than any set of quantitative metrics. (Colin Breck - Reflections on using Quality Views)    Finish by updating the central model.   6. Code quality         Is the code itself in good shape? Is it well factored? Is it maintainable? Is it reasonably easy to extend the application to add new functionality? (Colin Breck - Using Quality Views to Communicate Software Quality and Evolution)    Repeat the same process as above to color the modules according to the quality.   7. Testing         Does the component have tests to support making changes without introducing regressions or undue risk? Are the tests repeatable and reliable, and can they be executed in a reasonable time-frame? (Colin Breck - Using Quality Views to Communicate Software Quality and Evolution)    Again, do the same thing for testing. Decorate the modules of the central model with weather icons to show how they are tested.   8. Capability         Does the component carry significant business risk? This might include code that was developed by an individual that has departed the company that no one else is familiar with; code that is not in one of the primary programming languages used by the development team; (Colin Breck - Using Quality Views to Communicate Software Quality and Evolution)    In our story above, the workshop’s goal was to tackle onboarding and knowledge sharing. It’s associated with the “Bus Factor” risk. Here is an example of how you can track this risk.   Everyone annotates each module with their confidence to add a feature there.   Murex has been using Situational Leadership 2 for a long time, so we piggybacked on it. It provides four development levels of capability that are easy to self-assess:      D1: “I know nothing, but I am eager to start!”   D2: “I know enough to know that I’m going to suffer when I try.”   D3: “I guess I’ll manage, but I’m always a bit scared to do this…”   D4: “I’ve done this many times, don’t worry, I’ll handle it.”   Before session homework! Everyone was to watch this short video of Situational Leadership 2.     We then asked participants to place a small colored box with their names on the module. We used red for D1, orange for D2, yellow for D3, and green for D4. We reorganized these icons as histograms. These histograms represent the Bus Factor risk profile of every module.   This was just one example of our situation. For each relevant business risk, take the time to prepare a specific activity.   9. Bugs      This is when the Quality View starts to receive delivery data. At the beginning of the workshop, we gave homework to track lines of code affected by bug fixes in the past six months.   Have the people who collected the bugs info place one 🐞 icon on a module every time it was modified in a bug fix. In the end, you should get a good idea of which modules are sources of bugs.   This is also an excellent occasion to take a step back and discuss this view with everyone.   10. Upcoming features      Since becoming a team lead, I have been looking for ways to communicate two things regarding the software our team is responsible for. The first is the current state of the software. I want to communicate the quality of the system in terms of business risks—the reliability, scalability, and security of the system, as well as how receptive the software is to change, in order to meet future business needs. (Colin Breck - Using Quality Views to Communicate Software Quality and Evolution)    This is the moment to get product stakeholders in. Before anything, please take a moment to present the Quality View and answer their questions. They will love it! It’s often the first time they can understand what “Under the Hood” is.   Ask them to list the upcoming features they are thinking of. Target something like 3 to 6 months, depending on the rhythm of your business.   Also, ask them to sort these features into three value buckets:      High value   Medium value   Low value      Wait to try to map these features to the Quality View. The team will do that just after.   11. Future impacts         I want to communicate the quality of the system in terms of business risks […] A legacy component that lacks test coverage, or developer expertise, could be extremely difficult, risky, and costly to change, whereas a well-designed, well-factored application, with good test coverage and developers familiar with the code, might be evolved reliably and efficiently. (Colin Breck - Using Quality Views to Communicate Software Quality and Evolution)    Developers will try to predict what modules will be changed soon. Ask them to list, for each feature, the affected modules. Use rounds of 1-2-4-all (see Steps 4 and 5) to aggregate predictions.   Finally, for each feature, add diamonds to every impacted module:      Each module involved in a high-value feature will get three diamonds (💎💎💎)   Each module involved in a medium-value feature will get two diamonds (💎💎)   Each module involved in a low-value feature will get one diamond (💎)   Some modules might get plenty of diamonds!   12. Hotspots         Quality views are not about features versus quality. They arise from the understanding that in order to deliver high-quality systems, business objectives and customer requirements must be met, or exceeded. It is fine to keep focusing on features, but if customers cannot realize those features reliably, then nothing else matters. (Colin Breck - Reflections on using Quality Views)    At this point, all participants should have a 360º of the situation.   Unfortunately, you won’t be able to fix everything. Yet participants have all the information they need to make wise investments.   Give each participant 3 “dots” that they can place on the module (or dependency) they want to focus on. For example, in our workshops, we use flame icons (🔥).   The modules with the most dots are your “Hotspots.” Hotspots are areas of the code that are at risk of slowing down delivery.   13. Discuss and Decide what to do      The Quality View invites a holistic view of the system. When I presented a similar scenario at a quarterly-planning meeting, our Director of Product Development remarked: “How come those two components are still so red!?”. A constructive discussion followed, where we all came to the agreement that investing in these components would not be cost effective. We were comfortable accepting the short-term risks. (Colin Breck - Using Quality Views to Communicate Software Quality and Evolution)    The Quality View is complete, and everyone understands it. So it’s an excellent time to have the extensive all-in discussion you wanted to trigger.   Bring the prioritization topic forward. Stakeholders will be more conscious of technical tradeoffs now. As a result, the team should be able to build a more realistic and long-term plan. Decisions should take both features and agile technical excellence into account.   After the workshop   Strike the iron while it’s hot!      Feel free to run a follow-up workshop focusing on different aspects. For example:      Mapping current to vision To decide if you should rewrite or refactor a component.   Building a Core Domain Chart To identify a refactoring strategy for a component. Example: thorough refactoring, cosmetic refactoring, or replacing in-house code with a third party…   You can invent any follow-up workshop for your specific challenges. For example, here is how we ran activities to speed up onboarding.   Onboarding at Murex   Remember how this post started. We ran these workshops to speed up onboarding and increase knowledge sharing. So in our case, we ran two extra activities after the Quality View:      A give-and-take matrix to collect everyone’s needs and potential help.   An activity to refine help suggestions into SMART and collective actions.   Teams ended up with very focused and high-impact actions like:      Next time Sally needs to debug module X, we will pair. I will show the key places to add breakpoints, and we will go through debugging together.    Participants liked that these actions were concrete and laser-focused. Compare this to the typical evasive knowledge-sharing actions: “Let’s write some documentation.”   Follow-up      As coaches, your main work after the workshop is to make sure these actions are prioritized and done. Insist that the team adds them to its backlog. Don’t hesitate to gently provoke team members if you see that nothing happens.   Finally, nudge the team to re-run the workshop regularly. Updating a Quality View is much faster than creating it for the first time. Please don’t make it too short, though: new joiners should try to sketch a Quality View by themselves. It’s an excellent chance to discover the system one is part of.   Here is what you can do now   Next time you work with a team that has no time for ‘back-office’ technical work, give this workshop a try! It’s bound to trigger interesting discussions with all stakeholders.   On top of that, drawing a Quality View collaboratively is a great way to share knowledge within the team.   Other posts that might interest you      5 Whole-Team Workshops To Increase Developers’ Role In Sprint Planning   How to coach developers to get a chat with their product experts   How to estimate velocity in Scrum to escape the feature factory   Leverage Scrum to workaround feature-factory sprint planning   A complete workshop for your team to see what’s a good test strategy  ","categories": ["agile","quality","continuous improvement","workshop","architecture","coaching","refactoring","team building","collaborative work"],
        "tags": [],
        "url": "/a-quality-view-workshop-to-discuss-technical-excellence/",
        "teaser": "/imgs/2023-04-26-a-quality-view-workshop-to-discuss-technical-excellence/missing-a-quality-view-teaser.jpg"
      },{
        "title": "How To Fix Bad Agile By Discussing Baby Steps",
        "excerpt":"How to fix bad agile by starting neutral discussions about baby-steps programming. Baby steps are a sustainable and productive approach to writing code.         The average Scrum “installation” is not delivering what it could for the organization, and quite often making the lives of the people at the code face worse. I also believe that the practices and understanding of what was called XP can help developers with that, if only we could reach them.       It may be true that Scrum’s customers like Scrum. I believe that by and large, Scrum’s users quite often do not.       We may need to literally form a new “anti-Agile” community to support these ideas, abandoning the word “Agile” to the many ways it has been watered down and perverted.    As a technical Agile coach, working with a team that bad Scrum or Agile practices have hurt can be challenging. These negative experiences can lead to skepticism and resistance towards anything “agile” related. As a result, gaining their trust and engaging them in XP practices becomes challenging.      🤣 Regardless of what the SAD MF says, there is no need for training the resources since everything is readily available on the SAD website. Management probably also forgot possible training expenses in their YSVIBAICS. (Satiric Transition to Scale Agile DevOps Maturity Model. Check it out for a good laugh)    I’ve had my share of failures when working with teams that had been imposed an Agile-Method™. They were viewing all agile stuff, me included, as:      At best: useless bureaucracy   Or worst: a sneaky way to spy on them and ensure they remained busy and productive 100% of the time      Wouldn’t it be great if we had ways to connect with developers without mentioning Agile or Scrum?    What can we do?   Let’s talk about Baby Steps.              By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        XP or TDD might be associated with traditional Agile and Scrum, which often leave a sour taste for the team. However, we can bridge the gap and rebuild trust by starting discussions around the Baby-Steps programming. By focusing on Baby Steps, you can introduce technical practices from a neutral point of view.      🤣 90% of Scrum or Agile implementations are crap! (My adaptation of Sturgeon’s Law to Agile)    Just highlight the practical benefits of small, manageable changes to the code:      Baby steps allow developers to learn on the way, adjust, and steadily build new features.   Baby steps make decisions reversible. Baby Steps save developers from being dragged by previous choices.   Team members will regain a sense of control and autonomy over their work.   With time, they will understand that Baby Steps also apply outside the codebase.   Baby Steps are not a rigid recipe! Instead, baby Steps are about continuous learning, adaptation, and delivery at a steady pace.              By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        My experience at Devoxx   I recently had the opportunity to present a talk on Baby Steps at Devoxx Paris 2023. While preparing for the session, I searched for the most effective ways to pass a message through. I picked advice from How Minds Change and combined them with Liberating Structures. My goal was to create an interactive but also engaging session! The talk triggered a lot of exchanges and storytelling. (Note: Tell me if you want me to share more about this talk recipe in a new post). Here are some of the feedback I got:      Nice format that made the wide applicability of baby steps emerge       I liked the participatory format, which was original”    This approach could work with your team as well! It encourages active participation so that developers express their concerns, challenges, and aspirations.     After the talk, some people asked me how to take Baby Steps within their teams. Some even shared personal blockages about Baby Steps and sought advice. The fact that people are ready to share personal stories is a clue. “Listening and storytelling” is a powerful strategy to interest teams in Baby Steps!              By Philippe Bourgau, under CC BY-SA 4.0, high resolution image        What you can do   How to fix bad Agile?   Are you doing technical coaching with a team that bad Scrum or Agile has hurt? Try to talk and exchange about Baby Steps instead of trying to persuade them about Agile or Scrum.   Baby Steps are more ‘neutral’ and will make developers more open and receptive. Focus on the practical benefits of Baby Steps on continuous code improvement. In the long term, developers will transpose continuous improvement to teamwork and processes. That’s how they will finally overcome the negative connotations of traditional methodologies.      Feel free to use my sketch notes as visual support in your discussions.   Let’s embrace the power of small and incremental steps! Baby Steps can empower developers to work with confidence and clarity!   Here are other posts that might interest you:      How to coach a team that has been burnt by bad TDD   How to handle “TDD does not work in real life” during code katas   A surprising way to teach evolutionary design   How to make TCR evolutionary design practice sessions irresistible   Make Testing Legacy Code Viral: Mikado Method and Test Data Builders   Incremental Software Development Strategies for Large-Scale Refactoring #2 : Baby Steps  ","categories": ["change management","agile","coaching","continuous improvement","infographic","psychology"],
        "tags": [],
        "url": "/how-to-fix-bad-agile-by-discussing-baby-steps/",
        "teaser": "/imgs/2023-06-04-how-to-fix-bad-agile-by-discussing-baby-steps/baby-steps-aha-moment-teaser.jpg"
      },{
        "title": "Liberating Structures: the Slow Code Retreat's little facilitation secret",
        "excerpt":"You can now run the Slow Code Retreat by following the facilitation guide. By doing so, you’ll also learn 6 Liberating Structures for your future workshops.         After discussing with a few participants, we would like to spread this workshop. Could you come to run meetups in the area where I live?       I liked how you made us go around the room and meet each other.       The facilitation was excellent, better than anything I have seen before!    Previously, I introduced the Slow Code Retreat against their never-ending backlog of tasks.   I recently ran the Slow Code Retreat at Devoxx Paris, Newcrafts, and DDD Europe. Feedback has been great. Some people even wanted to run the workshop in their communities and teams! Unfortunately, the workshop may seem challenging to facilitate at first.      I want to facilitate the slow code retreat workshop myself. Wouldn’t it be great if I could grow my facilitation skills simultaneously?    Detailed Instructions for Facilitating the Slow Code Retreat Workshop   Matthieu Cans was an enthusiastic participant in the workshop at Newcrafts. After the session, he asked me if he could re-run the workshop at Alpes Craft the week after. With the slides, Matthieu ran the workshop both at Alpes Craft and at his company with his co-workers. He received similar feedback as I did. Again, some participants said they wanted to spread this wider.      We need to re-run this workshop in our communities, companies, and other conferences!       Here is a detailed PowerPoint presentation (.pptx). The presenter notes include step-by-step facilitation instructions. I licensed the slides under Creative Commons. You can freely use and adapt them as long as you mention the source and keep the same license.   Spread Slow-Code AND learn Liberating Structures   I prepared the Slow Code Retreat with the Liberating Structures facilitation system. The workshop’s goal is to help developers achieve a sustainable pace. But as a bonus, it will also teach you how to facilitate using Liberating Structures!   You are safe: your audience will love the slow-code-retreat      The feedback from workshop participants has been consistently positive. Many expressed how the workshop has had a profound impact on their work.      I will take his advice and start doing the things he taught us. It will make me a happier and more calm engineer. - Participant at DDD Europe       Big impact! How to look at development as a training system. - Participant at DDD Europe       Thanks a lot for this workshop. It had a real impact on my way of working. - Participant at NewCrafts       😂 I attended the Slow Code Retreat because I wanted to ‘commit’ to a more sustainable coding lifestyle! - A joke by Chat GPT    This workshop is particularly well-received by experienced developers. Juniors may not be ready to embrace that there will always be more work than time 😱 ! Consider your audience to determine if this workshop is the right fit for them.   You can do it: the slow-code-retreat is reusable by design.   )   The workshop is a string of Liberating Structures:      Get in the mood with Spiral Journal (in english)   Meet and share with Impromptu Networking   5 minutes of storytelling to explain how I came up with this idea of Slow Coding   Sharing what Slow Coding is with Gallery Walk (in english)   Experiment with different flavors of Slow Coding using Improv Prototyping            Live coding demonstration       Setting up groups and development environments       Approximately 30 minutes of slow coding       Retrospective using 1-2-4-All       You can start with an optional solo round if time allows           5 minutes of storytelling to explain what I have learned with Slow Coding   Takeaways with 15% Solutions   If you have a co-facilitator, you can even replace the two storytelling steps with a Celebrity Interview, which would be a 7th Liberating Structure.   Liberating Structures create high-quality and reusable workshops. For proof, other facilitators ran the Slow Code Retreat with success.   You will learn: the slow-code-retreat is a Liberating Structures training.      We typically learn Liberating Structures through practice. The idiomatic Liberating Structures training is a 2 or 3 days Immersion Workshop. The Slow Code Retreat can act as a mini immersion workshop.      💡 Run the Slow Code Retreat and get a taste of the surprising power of Liberating Structures!    I first heard of Liberating Structures at XPDays Benelux 4 or 5 years ago. They looked interesting, but I classified them as “yet more facilitation activities.” As a result, they always ended up at the bottom of my learning list. I understood my mistake when we looked at how to train Murexians to workshop facilitation.      🤣 Am I becoming a Liberating Structures fanboy?    So, why should you learn Liberating Structures instead of other facilitation techniques? Liberating Structures are a disruptive language for collaboration. They radically shift how people work together!      They encourage contributions from everyone.   They thrive in complex and diverse environments.   They have a viral aspect, and they propagate and spread by design.   They can scale to accommodate groups of 100 or more.   The Slow Code Retreat workshop is mostly a sequence of Liberating Structures. That is why facilitating this workshop will teach you your first Liberating Structures.   As written above, Liberating Structures are viral. So you are likely to see some of the participants reuse 1-2-4-all in a later meeting!   Run the Slow Code Retreat and Incorporate Liberating Structures in Your Workshops   Like Matthieu Cans, run the workshop, and spread slow coding while becoming a better facilitator!   Find your audience and send an invitation! Before the session:     Check the slides’ notes for the invitation and purpose of each structure.   Read the official descriptions of these Liberating Structures.   Observe how they are adapted to the Slow Code Retreat. That should give you the knack of Liberating Structures.      In your future workshops, add high return-on-investment structures such as:     1-2-4-All   Impromptu Networking   15% Solutions   These structures are excellent starting points to enhance your facilitation skills. Once you are at ease with those, pick another in the Liberating Structures menu.   Finally, if you improve the slow code retreat, please contribute back through Github. This way, we will get the workshop better together.   Other posts that might interest you:      Learn why you would want to run a slow code retreat: A “Slow Code Retreat” to be less in a hurry.   Another workshop guide: A Quality View Workshop to Discuss Technical Excellence.   And another: A complete workshop for your team to see what’s a good test strategy.   5 Whole-Team Workshops To Increase Developers’ Role In Sprint Planning.   A guide to run TCR sessions code katas: How to make TCR evolutionary design practice sessions irresistible.  ","categories": ["coaching","workshop","sustainable pace","continuous improvement","facilitation","liberating structures"],
        "tags": [],
        "url": "/liberating-structures-the-slow-code-retreat-s-little-facilitation-secret/",
        "teaser": "/imgs/2023-07-22-liberating-structures-the-slow-code-retreat-s-little-facilitation-secret/snail-carrying-liberating-structures-teaser.jpg"
      },{
        "title": "The Secrets Of Tech Coaching: Better Communication Skill",
        "excerpt":"Running code katas or mobs with groups that lack teamwork is almost impossible! Here are 20 communication and collaboration techniques to coach by example.         When team members are not used to or want to show humility within the confines of their team’s space, then they are not functioning as a team. That’s where we can and should help. Until that starts to happen, any new learning is an uphill battle.       I spent some time two years ago with a team group who asked me about delays in reviewing their pull requests, and I recall the central issue lay in a lack of self-confidence, resulting in not wanting to criticize someone else’s work.       I would love to reduce the latent stress in the mob, but just saying “There is no stress” is visibly not enough!    Even though we do “technical” coaching, collaboration skills can make or break our work! Most of us have coaching stories that support that. Situations where horrible teamwork made it impossible to pass anything to a team! Or teams with newly emerging technical skills but turning into super-performing thanks to collaboration! The impact of collaboration and teamwork is incredible!   Bad teamwork and collaboration also block mobbing and pairing coaching sessions. How can we help teams to improve collaboration so that coaching becomes effective?      Wouldn’t it be great if we could coach teamwork and collaboration by example? So that all the rest followed?    Let’s grow our collaboration skills first!   Coaching by example, aka walking your talk, is a fundamental coaching technique. By improving our communication and collaboration skills, we’ll showcase good teamwork. As a result, we’ll inspire our coachees to follow suit!      Before jumping into how to do that, let’s see why it works.   Why is team collaboration essential?      No matter how it looks at first, it’s always a people problem. (Jerry Weinberg)       Demarco and Lister demonstrate that the major issues of software development are human, not technical. Their answers aren’t easy–just incredibly successful. (Back-cover of the seminal Peopleware: Productive Projects and Teams)       Junior programmer’s bookshelf: 90% APIs and programming languages; Senior programmer’s bookshelf: 80% applied psychology. (J.B. Rainsberger)    I’m not the only one saying that team collaboration is paramount to success!   It’s not only what people say. The Aristotle project at Google concluded that psychological safety is the main factor in team performance. Psychological safety makes it OK to take risks and show vulnerability in front of others. Here again, human interactions come out as critical to team performance.   It’s clueless to coach a team without dealing with teamwork!   Communication Skills Are Contagious   Communication and collaboration skills are contagious. Good communication is inspiring, and leading by example is a powerful coaching tool:      Demonstrate through your own actions, your responses, the way you conduct yourself, the sort of mindset you are trying bring out in them and their teammates. That’s all. Not rocket science, but sometimes very challenging. (Bob Allen on WeDoTDD slack)       Be the change you want to see in the world (misattributed to Gandhi)    Some collaboration practices will help. For example, Liberating Structures, a set of facilitation techniques, are viral by design. Participating in a liberating structure is usually enough to facilitate it later!   Here is another example. Someone good at negotiation and conflict resolution can protect a whole team!   Finally, as we improve our collaboration, we will become better facilitators. As a result, we will run more effective workshops, which will have more impact on the teams we coach.   Your collaboration practices might spread to the CEO! (cf Local Action, Global Impact)   Better communication will make us happier!      Not only will investing in collaboration make you a better coach. Improving your communication and collaboration skills also makes you happier!      A Harvard study revealed that the quality of relationships is the #1 factor for happiness! Any communication technique will improve some aspects of relationships. The communication skills we grow for work will also serve us in our personal lives. So, improving our collaboration skills for tech coaching will make us happier humans!   Relationships at work are crucial to job performance and job satisfaction. Having a bully at work can make your life miserable. The opposite is also true! Gallup studies demonstrated that having friends at work improves performance and job satisfaction. Other research states that at work, relationships make us happier than purpose!   And… As communication skills are contagious, we will also impact the lives of our coachees!   Always bet on communication skills! Not only for your coaching gigs but for your general well-being.   I hope you understand how vital communication skills are. Let’s see a few “Communication skills.”   My collaboration skills journey   I was not born a good communicator… and I’m still learning! I remain shy and usually feel awkward in social situations. Yet, it feels way easier than it used to be. In particular, I manage well at work and with my close ones.   Here is the “story” of the communication techniques that impacted my life most. It’s not an exhaustive list of everything that exists. But you might find something that spikes your curiosity!   Code Like Prose   When I started as a programmer, solo coding felt safe and relaxing. There was only the logic of the program to deal with.   However, I had to change my mind after a few months in my first job. I understood that my career would not be the “introvert paradise” I had envisioned. Programmers need to communicate a lot! And the primary way programmers communicate is through code! With time, I learned that writing Code Like Prose made my colleagues and future self much happier.   💡The Refactoring and Growing Object Oriented Software Guided By Tests books were essential to my learning.      🤣 Always code as if the guy who ends up maintaining your code will be a violent psychopath who knows where you live. Code for readability. (John F. Woods)    Visual Management   As I read more about TDD and refactoring, I became interested in the agile literature. I discovered how Visual Management can reduce the painful boss-subordinate relationship. Visualizing work and progress enhances team alignment, transparency, autonomy, and self-responsibility. Let’s not take Visual Management for granted! It still could be used more.   💡 Reading the Kanban book was an eye-opener on this one.   Pair Programming   We often look at Pair Programming through the lens of code quality and productivity. Yet, it is also an incredible workout for communication skills. My pair programming years made my listening, negotiation, and self-awareness skills soar.   💡I wrote a lot about pairing, but my first advice to get better at it would be to start attending (or creating) a coding dojo.   Facilitation   As I became “the agile guy,” my teammates asked me to facilitate retrospectives. I learned the facilitation posture: a communication catalyst with a touch of mediation. Good facilitation will directly improve teamwork. Facilitation provides the incredible leverage of collective intelligence to tackle complex tasks.   💡The Agile Retrospectives: Making Good Teams Great was invaluable to get me started. (Note: They are working on a second edition!)   Writing      I started this blog as “Notes to my future self.” The first posts were rough drafts. I’m still no Hemingway, but my writing skills have improved a lot since then. Writing is an overlooked skill for developers. There is a motto that says “Writing is Thinking”! Writing skills yield more readable code, helpful documentation, and fewer email back-and-forths. Full-remote companies like Basecamp use writing skills as hiring criteria! Also, it does not matter where you start: I used to have abyssal grades in writing at school…   💡To improve your writing skills, start writing a blog or taking notes. You can also study classic books like Elements of Style. Finally, apps like Hemingway and Grammarly have helped me a lot to improve on my mistakes.   Living Documentation   Living Documentation is what you get when you combine TDD with Writing. The initial idea is to use human-readable text to write domain-oriented tests. The goal is to involve non-developers. It’s all about communication. Used well, Fit, Cucumber, or literate programming can transform collaboration with stakeholders. Who’s never wasted time building the wrong thing right?   💡I first learned about Living Documentation with the now-outdated Fit book. Since then, I have enjoyed:      Specifications by Example   Writing Great Specifications   The Cucumber book   And Living Documentation.   Remote Pairing      We were all forced into remote work during the Covid pandemic. Yet, I had already been doing regular remote pair programming for a few years. This experience proved full of lessons. Remote work forces us to make a lot of the implicit communication explicit. The same goes for remote pairing. Remote pairing makes us aware of how we communicate and allows us to improve on our flaws.   💡I remember enjoying the Remote Pairing book. Otherwise, I recommend learning the git-handover mechanism to get started with remote pairing. It’s also a good idea to run daily ‘mini-retros’ with your pair to fine-tune your pairing.   Non Violent Communication      🤣 No, Non-Violent Communication is not a rehab program for bullies…    The company I work for, Murex, has a Non-Violent Communication course in its catalog! I was lucky to attend one session a few years ago. NVC lessons can be life-changing:      The difference between facts, feelings, and needs   The fact that we are all responsible for our state of mind   How to give non-violent feedback   Don’t expect an overnight change, though. NVC is a straightforward idea that takes years to master. Yet, the sooner you start this path, the better. The more we improve our NVC skills, the less conflict we will see around us!   💡If you have the chance, attend a course, an immersion retreat, or try to attend a regular practice group. If you prefer books, Language of Life is the reference. Kent Beck also mentioned The Language of Emotions, which connects to the topic.   Event Storming and Collaborative Design Workshops      Team-wide collaborative design is challenging even with workshop facilitation and software design skills. I struggled with that for a few years when I stumbled on Event Storming. I was immediately hooked. Collaborative design is a way to do just enough design up-front in a few days. It also ensures everyone contributes, understands, and aligns on the direction! It has a tremendous impact on the motivation and productivity of a team.   💡I learned about Event Storming from Alberto Brandolini’s work-in-progress book. Matthieu Tournemire and I are also currently blogging The 1-hour Event Storming book. Other standard collaborative design workshops include Example Mapping, Domain Storytelling, User Story Mapping, and Impact Mapping…   Visual Facilitation   We say that a drawing is worth a thousand words. It’s incredible how a few drawings make presentations more engaging. Sketching also saves a lot of time. The audience will get the idea faster. And you’ll save time scribbling rather than writing or talking. You can use sketches in your notes, presentations, workshops, and documentation. The surprise is that you don’t need to be good at drawing to do visual facilitation!   💡If you have the chance, attend a training on Visual Facilitation. But you can also start sketch-noting right now: you only need a pad and a pencil. I began by combining:      Cornell’s note-taking system   With mind maps    And small drawings to decorate ideas.   Mob Programming   Mob Programming is extreme pairing. As a coach, mobbing has proven much more time-effective than pairing. We can exchange practices with the whole team instead of a single coachee. More than that, we can also observe and work on team dynamics! Mobbing will test many of your communication and collaboration skills: listening, facilitation, self-awareness, negotiation, collaborative design…   💡You can practice mob programming by running team coding dojos with strong style pairing. After a while, coachees might suggest trying it with production code. A Mob Programming code retreat is also a safe and quick way to experiment with various flavors.   Public Speaking      Public speaking is about impacting an audience in a limited time. This is useful to nudge management or a team into healthier work habits. Public speaking is also an occasion to get out of our comfort zones and master stress. I know developers who have used public speaking as self-therapy against shyness. Being able to stay calm in stressful situations is a communication superpower!   💡Ask a local or remote meetup if you could do a short presentation. The audience is usually friendly, and the entry bar is low. Then, learn how to answer a Call For Presentations (CFP) and try your luck in more significant events. The Present! A Techie’s Guide to Public Speaking book also has good reviews, but I must warn you that I did not read it.   Verbal Aikido   A few years ago, I started attending a weekly Verbal Aikido dojo. The goal of Verbal Aikido is to transform conflicts into constructive conversations. Conflicts tend to shut our neocortex off and trigger our reptilian brain to freeze, fight, or flee. Regular real-world conflict practice taught me to deal with difficult situations more peacefully.   💡To try Verbal Aikido, join the online dojo every Monday. I will also give intro workshops for developers at public events—for example, XCraft Lyon and XPDays Benelux in November 2023.   Non-Violent Code Reviews      (Bully) Why do you do that? (Author) … (Bully) No, I mean, as a job.    I wouldn’t say I like code reviews. Whenever I can, I replace them with pairing. Yet, Matthieu Tournemire and I are writing “The 1-hour Event Storming Book” together. We don’t have overlapping writing time slots, so we resorted to text reviews. Like code reviews, it was a collaboration challenge. How can we give valuable reviews while keeping the author’s intrinsic motivation? The Non-Violent Communication pattern of observation, feeling, need, and suggestion does wonders for reviews. Guess what? It also works like a charm for code reviews!      It does not trigger conflict.   It gives autonomy to the author to improve the code and learn.   As a complement, Verbal Aikido techniques are great for de-escalating harsh reviews!   💡Try it! Next time you give a review, for example, in a coaching session, use the NVC pattern to express your point. There are NVC cheat sheets for feelings and needs! Remember also that suggestions are only suggestions!   Liberating Structures     Liberating Structures were developed by Henri Lipmanowicz and Keith McCandless. Liberating Structures are licensed under a Creative Commons License.   I had the chance to learn Liberating Structures during an Immersion Workshop. It was a transformative experience. Liberating Structures are facilitation bricks to create inclusive and engaging experiences. It transformed all the collective interactions I have. Liberating Structures are the best tool I know to leverage collective intelligence. They also scale very well to large groups. They have made all my meetings and workshops shorter and more effective. Even my public conferences now rely on Liberating Structures!   💡To get started, try to join a local user group or register for an immersion workshop. You can also check the Slow Code Retreat. It has detailed instructions and can serve as a Liberating Structure Immersion Workshop.   Communication Dojos (In progress)   We are getting to my current ongoing initiatives. As I wrote above, some skills, like NVC or Verbal Aikido, take time to master. The key, though, is practice. The more we practice, the better we get at them. I am currently experimenting with communication dojos. The idea is to regularly and safely practice real collaboration or communication issues. It could be a harsh code review, a pushy salesperson, or making the case for refactoring. I hypothesize that a team could protect itself against a stressful environment!   💡The Improv Prototyping liberating structure is the perfect fit to run such events. Getting someone with experience in NVC or Verbal Aikido would also greatly benefit the group. I’ll keep you updated as I learn more from this practice.   Open Space Architecture (In Progress)   Event Storming is the only collaborative design workshop I know that works with more than ten people. Unfortunately, it is not the answer to all architectural questions and challenges. Fortunately, Liberating Structures also scale very well to large groups. I am experimenting with combining liberating structures to run large collaborative architecture workshops. Such workshops should yield better results in less time. If I am right, it will scale engagement, understanding, motivation, and alignment to large audiences!   💡I will give a workshop on this topic at XPDays Benelux 2023. I’ll keep you updated as I learn more.   Core Protocols (In Progress)      Core Protocols are not new, but I never took the time to look into them. I have lately been reading and collecting feedback from real users. I’d like to see how I can combine them with other practices. In particular, I’m thinking of Liberating Structures, Verbal Aikido, and mob programming. Could Core Protocols boot a better team dynamic in a short time?   💡Software For Your Head is the reference book about Core Protocols, but it is said to be a challenging read. Core Protocols experts say the best way to learn is to attend a course or a “boot camp.” So my best advice would be to find a trainer!   Combine and Compound   An interesting aspect of communication techniques is that we can innovate by combining them. For example:      I replaced all my “gathering” with Liberating Structures.   Improving my writing showed everywhere, from code to emails.   Inspired by Core Protocols, I designed “mobbing cards” to silently express common reactions in the mob. (Note: I might write more about that later).   My team and I are also experimenting with “1-2-4-all-Design!” to ‘pause’ the mob for 1 or 2 rounds of introvert-friendly design time.   Almost every day, I spot times when my communication could have been better. And I’m sure my list will continue to grow! The better our communication skills, the more we can combine them to create beautiful experiences for us and those around us. Every new technique we learn does not add to our impact; it multiplies it!   Always bet on communication!   Team collaboration and communication can make or break technical coaching! We can coach by example and fix broken teamwork by being good collaborators. It starts with us!   There is an almost infinite list of communication techniques we can learn:      From deep listening and connection, with practices like Non-Violent Communication and Verbal Aikido   To disruptive group collaboration with Event Storming and Liberating Structures!   Whatever your interest and need, there is a fun way to grow your collaboration skills! Select a topic, start learning, experiment, and share your experiences.   PS: I am thinking of writing a book about communication and collaboration activities for teams. The title could be “Collaboration Activities to Fix Software Problems.” It would link collaboration techniques to common software problems and provide step-by-step fixes. I’d love to know what you think of the idea.  ","categories": ["coaching","collaborative work","coaching by example","continuous improvement","team building","collaboration","communication"],
        "tags": [],
        "url": "/the-secrets-of-tech-coaching-better-communication-skill/",
        "teaser": "/imgs/2023-11-07-the-secrets-of-tech-coaching-better-communication-skill/human-human-stack-teaser.jpg"
      }]
