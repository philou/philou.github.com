<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: testing | Philippe Bourgau's blog]]></title>
  <link href="http://philippe.bourgau.net/blog/categories/testing/atom.xml" rel="self"/>
  <link href="http://philippe.bourgau.net/"/>
  <updated>2017-10-05T06:48:55+02:00</updated>
  <id>http://philippe.bourgau.net/</id>
  <author>
    <name><![CDATA[Philippe Bourgau]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How we used the improvement kata to gain 25% of productivity - Part 5]]></title>
    <link href="http://philippe.bourgau.net/how-we-used-the-improvement-kata-to-gain-25-percent-of-productivity-part-5/"/>
    <updated>2017-09-27T07:00:00+02:00</updated>
    <id>http://philippe.bourgau.net/how-we-used-the-improvement-kata-to-gain-25-percent-of-productivity-part-5</id>
    <content type="html"><![CDATA[<p>This is the fifth (and last) post of a <a href="/blog/categories/first-improvement-kata-series/">series of 5</a> about the improvement kata. If you haven&rsquo;t read the beginning of the story, I recommend you start from <a href="/how-we-used-the-improvement-kata-to-gain-25-percent-of-productivity-part-1/">part 1</a>.</p>

<p>In the <a href="/how-we-used-the-improvement-kata-to-gain-25-percent-of-productivity-part-4/">previous post</a>, we decided to adjust our definition of a bug to limit the time lost on nice-to-have bug fixes.</p>

<p>It would take a while to know if adjusting the definition of a bug would help us or not. At the same time, we knew it would not help us to reduce the number of bugs we escaped to other teams.</p>

<p><img src="../imgs/2017-09-27-how-we-used-the-improvement-kata-to-gain-25-percent-of-productivity-part-5/success-banner.jpg" alt="A 'SUCCESS' banner in the wind" /></p>

<h2>Idea 3 : More exploratory testing</h2>

<p>We decided to push on this matter as well. This means that we would be running two <a href="https://en.wikipedia.org/wiki/PDCA">PDCAs (Plan-Do-Check-Act)</a> at the same time. This is not the improvement kata procedure by the book. That could have been an error from our side, as first time users of the kata. Or maybe it&rsquo;s a pragmatic habit to adapt the kata to real life &hellip; I guess we&rsquo;ll know better as we apply the kata more often. The danger is that the different experiments can conflict with each other. When measuring the results, it becomes difficult to link observations with experiments. Anyway, back to our own situation, as you&rsquo;ll see, it ended up well for us.</p>

<p>The first thing was to know a bit more about our bugs. Checking the recently closed bugs yielded suspicions about a recent features. Analyzing further proved our gut feeling.</p>

<h6>Curve of how bugs were fixed on the last 2 months</h6>

<p><img src="../imgs/2017-09-27-how-we-used-the-improvement-kata-to-gain-25-percent-of-productivity-part-5/fixed-bugs.png" alt="Curve of how bugs were fixed on last 2 months" /></p>

<h6>Curve of the origin of bugs on the last 2 months</h6>

<p><img src="../imgs/2017-09-27-how-we-used-the-improvement-kata-to-gain-25-percent-of-productivity-part-5/kind-of-bugs.png" alt="Curve of the origin of bugs on the last 2 months" /></p>

<p>Ignoring the Christmas drop at the middle of the curve, we concluded 2 things from these graphs :</p>

<ul>
<li>We were leaking bugs to the product</li>
<li>Bugs mostly came from newly added features</li>
</ul>


<blockquote><p>Despite all our automated tests and regular Â exploratory testing, we were leaking bugs.</p></blockquote>

<p>We decided to do more exploratory testing for a while ! We were already doing exploratory testing at the end of every story. We added an extra 1 hour team session of exploratory testing every sprint.</p>

<h2>Do, Check &amp; Act</h2>

<p>We used these new conventions for a few weeks. We did more exploratory testing, and would be more strict about what a bug was. We stuck to our prioritization : first improvements, then bugs and only after, stories.</p>

<p>After a few weeks of that, we were able to update our bug trend and do a linear regression on it again. Here were the results :</p>

<p><img src="../imgs/2017-09-27-how-we-used-the-improvement-kata-to-gain-25-percent-of-productivity-part-5/final-bug-trend.png" alt="Curve of the origin of bugs on the last 2 months" /></p>

<p>Hurray ! As you can see, we were to be done with bugs around April 2017, which was 3 months away at that time.</p>

<blockquote><p>ðŸ’¡ Quality is free, but only for those willing to pay for it ! [<a href="https://en.wikiquote.org/wiki/Tom_DeMarco">Tom DeMarco</a> in <a href="https://www.amazon.com/Peopleware-Productive-Projects-Teams-3rd/dp/0321934113/ref=pd_lpo_sbs_14_t_0?_encoding=UTF8&amp;psc=1&amp;refRID=9SX9Y3RG61NB7N9VJ6KS&amp;dpID=61lAwzXfQiL&amp;preST=_SX218_BO1,204,203,200_QL40_&amp;dpSrc=detail">Peopleware</a>]</p></blockquote>

<p><a href="https://www.amazon.com/Peopleware-Productive-Projects-Teams-3rd/dp/0321934113/ref=pd_lpo_sbs_14_t_0?_encoding=UTF8&amp;psc=1&amp;refRID=9SX9Y3RG61NB7N9VJ6KS&amp;dpID=61lAwzXfQiL&amp;preST=_SX218_BO1,204,203,200_QL40_&amp;dpSrc=detail"><img src="../imgs/2017-09-27-how-we-used-the-improvement-kata-to-gain-25-percent-of-productivity-part-5/peopleware.jpg" alt="Cover of the 'Peopleware' book by Tom DeMarco &amp; Timothy Lister" /></a></p>

<p>We confidently adopted these practices as part of our working agreements. This brought our first improvement kata to its end.</p>

<blockquote><p>ðŸ’¡ The improvement kata not only brings improvements, it also teaches you why they work.</p></blockquote>

<h2>3 months later</h2>

<p>As you know, April 2017 is long gone. I can now give you a more up to date report of the actual effects on the team&rsquo;s daily work.Â </p>

<p>First, the backlog does not contain bugs anymore. We payed the bug debt back. Second, we still discover some bugs from time to time, but a lot less than we used to. To summarize, there is now a pair of developers (25%) of the team that can work on user stories instead of fixing bugs.</p>

<p>As we are still fixing bugs as they appear, the 25% productivity gain claim might be an overstatement, but 20% is not. At the same time, less bugs are now escaping. This means that the whole organization is saving on interruptions and rework. 25% might not be such a bold claim after all !</p>

<h2>This is it !</h2>

<p>This was post 5 in a <a href="/blog/categories/first-improvement-kata-series/">series of 5</a> about the improvement kata. I&rsquo;m not completely done writing about this improvement kata though. In the coming weeks, I&rsquo;ll post about the lessons learned and how to start your own improvement kata.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Forget unit tests, only fast tests matter]]></title>
    <link href="http://philippe.bourgau.net/forget-unit-tests/"/>
    <updated>2017-08-08T06:24:00+02:00</updated>
    <id>http://philippe.bourgau.net/forget-unit-tests</id>
    <content type="html"><![CDATA[<p>Don&rsquo;t worry if your unit tests go to the DB, that might not be so bad.</p>

<p>When I started writing unit tests, I did not know what these were. I read <a href="http://www.artima.com/weblogs/viewpost.jsp?thread=126923">the definition</a>, and strived to follow the recommandations :</p>

<ul>
<li>they should be independent from each other</li>
<li>they should not access the DB</li>
<li>they should not use the network</li>
<li>they should only cover a small scope of your code</li>
</ul>


<p>I started to write unit tests on my own and became <a href="http://wiki.c2.com/?TestInfected">test infected</a> pretty fast. Once I got convinced of the benefits of unit testing, I tried to spread the practice around me. I used to explain to people that it is very important to write real unit tests by the book. Otherwise, <em>Bad Things</em> would happen &hellip;</p>

<h2>How I changed my mind</h2>

<p>A few years ago, I spent a few years working on a <a href="http://rubyonrails.org/">Rails</a> side project called <a href="https://github.com/philou/mes-courses">mes-courses.fr</a>. I was using a small test gem to enforce that no unit tests were accessing the db. I had to write a lot of mocks around the code. I ended up hating mocks : they are too painful to maintain and provide a false sense of security. I&rsquo;m not alone in this camp, check <a href="https://www.youtube.com/watch?v=9LfmrkyP81M">DHH&rsquo;s keynote at RailsConf 2014</a>.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/9LfmrkyP81M" frameborder="0" allowfullscreen></iframe>


<p>At some point, the mock pain got so bad that I stopped all developments until I found another way. I found a pretty simple workaround : use in-memory SQLite. I got rid of all the DB access mocks. Not only were the tests easier to write and maintain, but they were as fast as before, and they covered more code.</p>

<p>That changed something fundamental in my understanding of testing</p>

<h2>It&rsquo;s all about speed baby</h2>

<p>The only thing that makes unit tests so important is that they run fast.</p>

<p>Unit tests as described in the literature run fast. Let&rsquo;s see what happens when you remove one of the recommandations for unit tests.</p>

<ul>
<li>If tests depend on each other, their outcome will change with the execution order. This wastes our time in analyzing the results. On top of that, independent unit tests are easy to run in parallel, providing an extra speedup. We lose this potential when our tests are dependent.</li>
<li>Tests that rely on an out-of-process DB run slower. Tests need to start the DB before anything else. Data needs to be setup and cleaned at every test. Accessing the DB implies using the network, which takes time as well. There&rsquo;s also a risk of making the tests dependent by sharing the same DB. A last issue is troubleshooting the DB process when things don&rsquo;t work.</li>
<li>Tests that use the network are slow too ! First, Network is slower than memory. Second, data serialization between processes is slow as well. Finally, these tests are likely to use some form of sleep or polling, which is slow, fragile, or both !</li>
<li>Finally, there is always a scope past which a test will be too slow.</li>
</ul>


<p>This means that not only unit tests are fast, but also that fast tests usually show the features of unit tests.</p>

<p>My guess is that &lsquo;unit tests&rsquo; were explicitly defined as a recipe for fast tests ! If you stick to the definition of unit tests, you&rsquo;ll get fast tests and all their benefits.</p>

<p><img src="../imgs/2017-08-08-forget-unit-tests/speedometer.jpg" alt="A speedometer" /></p>

<h2>Fast tests</h2>

<p>That also means that we should focus first on having fast tests rather than unit tests. Here is my real check to know if tests are fast enough :</p>

<ul>
<li>Is the build (and the tests and everything) less than 10 minutes ?</li>
<li>Can I continuously run my tests while coding and stay in the flow ?</li>
</ul>


<p>If both answers are yes, then I won&rsquo;t question myself too much whether my tests are unit, integration or end to end.</p>

<h2>So what ?</h2>

<p>I&rsquo;ve been experimenting with these heuristics for some time. Side projects are great for experimenting since you don&rsquo;t have a team to convince ! Here are my main takeaways :</p>

<ul>
<li>Stick to end to end tests at the beginning of your project. They are easy to refactor to finer grained tests later on.</li>
<li>In-memory DBs are great to speed tests up without wasting your time with mocking.  We can use a unique DB for every test to keep them independent.</li>
<li>Large scope tests are not an issue provided 2 things.

<ol>
<li>The code contains very few side effects.</li>
<li>It provides good exceptions and assertions messages</li>
</ol>
</li>
</ul>


<p>On the other side, there are things that I still recommend :</p>

<ul>
<li>Independent tests are easy to write from the beginning, difficult to fix later on. As they save a lot of headaches in diagnostic, I stick to them from the start.</li>
<li>Avoid network, it makes the tests slow, fragile and tricky to diagnostic. But please, read <a href="http://philippe.bourgau.net/how-not-to-use-mocks-my-talk-at-paris-rb/">this</a> before jumping to mocks.</li>
</ul>


<p>These rules have served me well, particularly in my side projects, where I don&rsquo;t have a lot of time. What about you ? Do you have your own testing rules ?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Don't stick to TDD's Red-Green-Refactor loop to the letter]]></title>
    <link href="http://philippe.bourgau.net/dont-stick-to-tdds-red-green-refactor-loop-to-the-letter/"/>
    <updated>2017-06-28T15:52:00+02:00</updated>
    <id>http://philippe.bourgau.net/dont-stick-to-tdds-red-green-refactor-loop-to-the-letter</id>
    <content type="html"><![CDATA[<p>As long as you are writing your tests before your code and doing regular refactoring, you are doing <a href="https://en.wikipedia.org/wiki/Test-driven_development">TDD</a> !</p>

<p>The Red &ndash; Green &ndash; Refactor loop is useful to introduce TDD to new developers. Different loops can be more effective in real world situation.</p>

<p>The Red &ndash; Green &ndash; Refactor loop is not a dogma !</p>

<p><a href="http://www.natpryce.com/"><img src="../imgs/2017-06-28-dont-stick-to-tdds-red-green-refactor-loop-to-the-letter/red-green-refactor.jpg" alt="The famous red, green, refactor TDD loop" /></a></p>

<h2>Refactor &ndash; Red &ndash; Green</h2>

<p>When I work on a story, I very often keep a TODO list next to my desk. I use it to keep track of the next steps, the edge cases to test, the code smells and refactorings to do.</p>

<p>When I get to the end of the story, all that remains of this list is a few refactorings. Very often, I don&rsquo;t do them !</p>

<p>With the feature working, doing these refactorings feels like violation of <a href="https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it">YAGNI</a>. Next time we&rsquo;ll have to work on this part of the code, we&rsquo;ll have a story to serve as guide to which refactorings to do.</p>

<p>The same thing is effective at the unit test scale. It&rsquo;s easier to refactor when you know the test you want to add. Refactor to make this test easy to write !</p>

<p>Here is an example with <a href="https://en.wikipedia.org/wiki/Fizz_buzz">Fizz Buzz</a></p>

<p>```java
static int fizzBuzz(int number) {
Â Â Â return number;
}</p>

<p>@Test public void
it_is_1_for_1() {
Â Â Â assertThat(fizzBuzz(1)).isEqualTo(1);
}</p>

<p>@Test public void
it_is_2_for_2() {
Â Â Â assertThat(fizzBuzz(2)).isEqualTo(2);
}
```</p>

<p>Here is the test I&rsquo;d like to add.Â </p>

<p><code>java
@Test public void
it_is_Fizz_for_3() {
Â Â Â assertThat(fizzBuzz(3)).isEqualTo("Fizz");
}
</code></p>

<p>Unfortunately, fizzBuzz needs to return a String instead of an integer for it to compile. That&rsquo;s when I would refactor before adding the new test.</p>

<p>```java
static String fizzBuzz(int number) {
Â Â Â return Integer.toString(number);
}</p>

<p>@Test public void
it_is_1_for_1() {
Â Â Â assertThat(fizzBuzz(1)).isEqualTo(&ldquo;1&rdquo;);
}</p>

<p>@Test public void
it_is_2_for_2() {
Â Â Â assertThat(fizzBuzz(2)).isEqualTo(&ldquo;2&rdquo;);
}
```</p>

<p>In the end, this loop is very like the classic TDD loop :</p>

<p><code>
red-green-refactor-red-green-refactor-red-green-refactor.............
..........refactor-red-green-refactor-red-green-refactor-red-green...
</code></p>

<p>A bit more YAGNI, that&rsquo;s all.</p>

<h2>Red &ndash; Better Red &ndash; Green &ndash; Refactor</h2>

<p>A few weeks ago, <a href="/speed-up-the-tdd-feedback-loop-with-better-assertion-messages/">I wrote about error messages in unit tests</a>. To summarize, extra work on error messages reduces the testing feedback loop.</p>

<p>We can translate this focus on error messages into an extra TDD step. Whatever the TDD loop you are using, you can add this step after the Red step.</p>

<h2><del>Red</del> &ndash; Green &ndash; Refactor &ndash; Red &ndash; Green</h2>

<p>Sometimes, it makes sense to refactor before fixing the test. The idea is to rely on the existing tests to prepare the code to fix the new test in one line.</p>

<p>Let&rsquo;s take our Fizz Buzz example again. Imagine we finished the kata, when we decide to tweak the rules and try Fizz Buzz Bang. We should now print Bang on multiples of 7.</p>

<p>Here is our starting point :</p>

<p>```java
static String fizzBuzz(int number) {
Â Â Â if (multipleOf(number, 3)) {
Â Â Â Â Â Â return &ldquo;Fizz&rdquo;;
Â Â Â }
Â Â Â if (multipleOf(number, 5)) {
Â Â Â Â Â Â return &ldquo;Buzz&rdquo;;
Â Â Â }
Â Â Â if (multipleOf(number, 3*5)) {
Â Â Â Â Â Â return &ldquo;FizzBuzz&rdquo;;
Â Â Â } Â Â 
Â Â Â return Integer.toString(number);
}</p>

<p>&hellip;</p>

<p>@Test public void
it_is_Bang_for_7() {
Â Â Â assertThat(fizzBuzz(7)).isEqualTo(&ldquo;Bang&rdquo;);
}
```</p>

<p>I could go through all the hoops, 7, 14, then 3<em>7, 5</em>7 and finally 3<em>5</em>7 &hellip; By now, I should know the music though !</p>

<p>What I would do in this case is :</p>

<ul>
<li>first to comment the new failing test to get back to green</li>
<li>refactor the code to prepare for the new code</li>
<li>uncomment the failing test</li>
<li>fix it</li>
</ul>


<p>In our example, here is the refactoring I would do</p>

<p>```java
static String fizzBuzz(int number) {
Â Â Â String result = &ldquo;&rdquo;;
Â Â Â result += multipleWord(number, 3, &ldquo;Fizz&rdquo;);
Â Â Â result += multipleWord(number, 5, &ldquo;Buzz&rdquo;);
Â Â Â if (result.isEmpty()) {
Â Â Â Â Â Â result = Integer.toString(number);
Â Â Â }
Â Â Â return result;
}</p>

<p>private static String multipleWord(int number, int multiple, String word) {
Â Â Â if (multipleOf(number, multiple)) {
Â Â Â Â Â Â return word;
Â Â Â }
Â Â Â return &ldquo;&rdquo;;
}</p>

<p>&hellip;</p>

<p>//@Test public void
//it_is_Bang_for_7() {
// Â Â assertThat(fizzBuzz(7)).isEqualTo(&ldquo;Bang&rdquo;);
//}
```</p>

<p>From there, fixing the test is dead simple.</p>

<p>In practice I find this loop very useful. At local scale as we saw but it&rsquo;s also a great way to refactor your architecture at larger scale.</p>

<p>One downsize is that if you are not careful, it might lead to over-engineering. Be warned, keep an eye on that !</p>

<p>Last caveat : not all TDD interviewers like this technique &hellip;</p>

<h2>Don&rsquo;t obsess</h2>

<p>It&rsquo;s not because you are not following the Red Green Refactor loop to the letter that you are not doing TDD.</p>

<p>An interesting point is that these variations to the TDD loop are combinable ! Experienced TDD practitioners can jump from one to the other without even noticing.</p>

<p><a href="https://blog.acolyer.org/2017/06/13/a-dissection-of-the-test-driven-development-process-does-it-really-matter-to-test-first-or-test-last/">This paper</a> argues that as long as you write the tests along (before or after) the code, you get the same benefit. That&rsquo;s not going to make me stop writing my tests first, but it is interesting. That would mean that even a Code &ndash; Test &ndash; Refactor loop would be ok if it is fast enough !</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Speed up the TDD feedback loop with better assertion messages]]></title>
    <link href="http://philippe.bourgau.net/speed-up-the-tdd-feedback-loop-with-better-assertion-messages/"/>
    <updated>2017-05-29T06:37:00+02:00</updated>
    <id>http://philippe.bourgau.net/speed-up-the-tdd-feedback-loop-with-better-assertion-messages</id>
    <content type="html"><![CDATA[<p>There is a rather widespread <a href="https://en.wikipedia.org/wiki/Test-driven_development">TDD</a> practice to have a <a href="https://softwareengineering.stackexchange.com/questions/7823/is-it-ok-to-have-multiple-asserts-in-a-single-unit-test">single assertion per test</a>. The goal is to have faster feedback loop while coding. When a test fails, it can be for a single reason, making the diagnostic faster.</p>

<p>The same goes with the test names. When a test fails, a readable test name in the report simplifies the diagnostic. Some testing frameworks allow the use of plain strings as test names. In others, people use <a href="https://en.wikipedia.org/wiki/Naming_convention_(programming">underscores</a>#Multiple-word_identifiers) instead of <a href="https://en.wikipedia.org/wiki/Camel_case">CamelCase</a> in test names.</p>

<p><img src="../imgs/2017-05-29-speed-up-the-tdd-feedback-loop-with-better-assertion-messages/rubymine.jpg" alt="RubyMine test report" /></p>

<h2>A 4th step in TDD: Fail, <em>Fail better</em>, Pass, Refactor</h2>

<h3>First, make it fail</h3>

<p>Everyone knows that Test Driven Development starts by making the test fail. Let me illustrate why.</p>

<p>A few years ago, I was working on a <a href="https://en.wikipedia.org/wiki/C_Sharp_%28programming_language%29">C#</a> project. We were using TDD and <a href="https://github.com/nunit/nunit">NUnit</a>. At some point, while working on a story, I forgot to make my latest test fail. I wrote some code to pass this test, I ran the tests, and they were green. When I was almost done, I tried to plug all the parts together, but nothing was working. I had to start the debugger to understand what was going wrong. At first, I could not understand why the exact thing I had unit tested earlier was now broken. After more investigation I discovered that I had forgotten to make my test public. NUnit only runs public tests &hellip;</p>

<p>If I had made sure my test was failing, I would have spotted straightaway that it was not ran.</p>

<h3>Then make it fail &hellip; better !</h3>

<p>I lived the same kind of story with wrong failures many times. The test fails, but for a bad reason. I move on to implement the code to fix it &hellip; but it still does not pass ! Only then do I check the error message and discover the real thing to fix. Again, it&rsquo;s a transgression to baby steps and to the <a href="https://en.wikipedia.org/wiki/You_aren%27t_gonna_need_it">YAGNI</a> principle. If the tests is small, that might not be too much of an issue. But it can be if the test is big, or if the real fix deprecates all the premature work.</p>

<h3>Strive for explicit error message</h3>

<p>The idea is to make sure to have good enough error messages before moving on to the &ldquo;pass&rdquo; step.</p>

<p><a href="https://www.amazon.com/Growing-Object-Oriented-Software-Guided-Tests/dp/0321503627/ref=sr_1_1?tag=pbourgau-20&amp;amp;s=books&amp;ie=UTF8&amp;qid=1495080583&amp;sr=1-1&amp;keywords=growing+object-oriented+software+guided+by+tests"><img src="../imgs/2017-05-29-speed-up-the-tdd-feedback-loop-with-better-assertion-messages/growing.jpg" alt="Cover of GOOSGT" /></a></p>

<p>There&rsquo;s nothing groundbreaking about this practice. It&rsquo;s not a step as explicit as the other 3 steps of TDD. The first place I read about this idea was in <a href="https://www.amazon.com/Growing-Object-Oriented-Software-Guided-Tests/dp/0321503627/ref=sr_1_1?tag=pbourgau-20&amp;amp;s=books&amp;ie=UTF8&amp;qid=1495080583&amp;sr=1-1&amp;keywords=growing+object-oriented+software+guided+by+tests">Growing Object Oriented Software Guided By Tests</a>.</p>

<h2>How to improve your messages</h2>

<h3>Readable code</h3>

<p>Some test frameworks print out the failed assertion code to the test failure report. Others, especially in dynamic languages, use the assertion code itself to deduce an error message. If your test code is readable enough, your error messages might be as well !</p>

<p>For example, with <a href="https://www.ruby-lang.org">Ruby</a> <a href="http://rspec.info/">RSpec</a> testing framework :</p>

<p><code>ruby
it "must have an ending" do
  expect(Vote.new(team: @daltons)).to be_valid
end
</code></p>

<p>Yield the following error :</p>

<p>```
expected #<Vote ...> to be valid, but got errors: Ending can&rsquo;t be blank</p>

<p>```</p>

<h3>Pass in a message argument</h3>

<p>Sometimes, readable code is not enough to provide good messages. All testing frameworks I know provide some way to pass in a custom error message. That&rsquo;s often a cheap and straightforward way to clarify your test reports.</p>

<p>```ruby
  it &ldquo;should not render anything&rdquo; do</p>

<pre><code>post_create
expect(response.code).to eq(HTTP::Status::OK.to_s),
                         "expected the post to succeed, but got http status #{response.code}"
</code></pre>

<p>  end
```</p>

<p>Yields</p>

<p><code>
expected the post to succeed, but got http status 204
</code></p>

<h3>Define your own matchers</h3>

<p>The drawback with explicit error message is that they harm code readability. If this becomes too much of an issue, one last solution is the use of <a href="https://objectpartners.com/2013/09/18/the-benefits-of-using-assertthat-over-other-assert-methods-in-unit-tests/">test matchers</a>. A test matcher is a class encapsulating assertion code. The test framework provides a fluent api to bind a matcher with the actual and expected values. Almost all test framework support some flavor of these. If not, or if you want more, there are libraries that do :</p>

<ul>
<li><a href="http://joel-costigliola.github.io/assertj/index.html">AssertJ</a> is a fluent assertion library for Java. You can easily extend it with your own assertions (ie. matchers)</li>
<li><a href="http://n-fluent.net/">NFluent</a> is the same thing for .Net.</li>
</ul>


<p>As an example, in a past side project, I defined an <a href="https://github.com/philou/mes-courses/blob/master/spec/support/include_all_matcher.rb">include_all</a> rspec matcher that verifies that many elements are present in a collection. It can be used that way :</p>

<p><code>ruby
expect(items).to include_all(["Tomatoes", "Bananas", "Potatoes"])
</code></p>

<p>It yields error messages like</p>

<p><code>
["Bananas", "Potatoes"] are missing
</code></p>

<p>A custom matcher is more work, but it provides both readable code and clean error messages.</p>

<h2>Other good points of matchers</h2>

<p>Like any of these 3 tactics, matchers provide better error messages. Explicit error messages, in turn, speed up the diagnostic on regression. In the end, faster diagnostic means easier maintenance.</p>

<p>But there&rsquo;s more awesomness in custom test matchers !</p>

<h3>Adaptive error messages</h3>

<p>In a custom matcher, you have to write code to generate the error message. This means we can add logic there ! It&rsquo;s an opportunity to build more detailed error messages.</p>

<p>This can be particularly useful when testing recursive (tree-like) structures. A few years ago, I wrote an rspec matcher library called <a href="https://github.com/philou/xpath-specs">xpath-specs</a>. It checks html views for the presence of recursive XPath. Instead of printing</p>

<p><code>
Could not find //table[@id="grades"]//span[text()='Joe'] in ...
</code></p>

<p>It will print</p>

<p><code>
Could find //table[@id="grades"] but not //table[@id="grades"]//span[text()='Joe'] in ...
</code></p>

<p>(BTW, I&rsquo;m still wondering if testing views this way is a good idea &hellip;)</p>

<h3>Test code reuse</h3>

<p>One of the purpose of custom test matchers is to be reusable. That&rsquo;s a good place to factorize assertion code. It is both more readable and more organized than extracting an assertion method.</p>

<h3>Better coverage</h3>

<p>I noticed that custom matcher have a psychological effect on test coverage ! A matcher is a place to share assertion code. Adding thorough assertions seems legitimate, contrary to repeating them inline.</p>

<h3>Avoids mocking</h3>

<p>We often resort to mocks instead of side effect tests because it&rsquo;s a lot shorter. A custom matcher encapsulates the assertion code. It makes it OK to use a few assertions to test for side effects, which is usually preferable to mocking.</p>

<p>For example, here is a matcher that checks that our remote service API received the correct calls, without doing any mocking.</p>

<p>```ruby
RSpec::Matchers.define :have_received_order do |cart, credentials|
  match do |api|</p>

<pre><code>not api.nil? and
api.login == credentials.email and
api.password == credentials.password and
cart.lines.all? do |cart_line|
  cart.content.include?(cart_line.item.remote_id)
end
</code></pre>

<p>  end</p>

<p>  failure_message do |api|</p>

<pre><code>"expected #{api.inspect} to have received order #{cart.inspect} from #{credentials}"
</code></pre>

<p>  end
end
```</p>

<h2>Care about error messages</h2>

<p>Providing good error messages is a small effort compared to unit testing in general. At the same time, it speeds up the feedback loop, both while coding and during later maintenance. Imagine how easier it would be to analyze and fix regressions if they all had clear error messages !</p>

<p>Spread the word ! Leave comments in code reviews, demo the practice to your pair buddy. Prepare a team coding dojo about custom assertion matchers. Discuss the issue in a retro !</p>

<p><img src="../imgs/2017-05-29-speed-up-the-tdd-feedback-loop-with-better-assertion-messages/just-do-it.jpg" alt="'Just Do It' written on a board" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[20 Bad Excuses For Not Writing Unit Tests]]></title>
    <link href="http://philippe.bourgau.net/20-bad-excuses-for-not-writing-unit-tests/"/>
    <updated>2017-05-23T06:08:00+02:00</updated>
    <id>http://philippe.bourgau.net/20-bad-excuses-for-not-writing-unit-tests</id>
    <content type="html"><![CDATA[<blockquote><p>I guess we always find excuses to keep on with our bad habits, don&rsquo;t we ? <em><a href="http://www.goodreads.com/quotes/797048-i-guess-we-always-find-excuses-to-keep-on-with">Stephen King</a></em></p></blockquote>

<ol>
<li>I don&rsquo;t have the time. <em>But you&rsquo;ll have the time to fix the bugs &hellip;</em></li>
<li>I don&rsquo;t know how to write tests. <em>No problem, anyone can <a href="/from-apprentice-to-master-how-to-learn-tdd-test-driven-development/">learn</a>.</em></li>
<li>I&rsquo;m sure the code is working now. <em><a href="https://www.brainyquote.com/quotes/quotes/e/edsgerdijk204340.html">The competent programmer is fully aware of the limited size of his own skull &hellip;</a></em></li>
<li>This code is not testable. <em>Learn or refactor.</em></li>
<li>It&rsquo;s (UI|DB) code, we don&rsquo;t test it. <em>Because it never crashes ?</em></li>
<li>Because I need to refactor first &hellip; and I need tests to refactor ! <em>Damn, you&rsquo;ve fallen into the test deadlock !</em></li>
<li>It&rsquo;s multithreaded code, it&rsquo;s impossible to test. <em>Because it&rsquo;s fully tederministic ?</em></li>
<li>The QA department is already testing the code. <em>Is that working well ?</em></li>
<li>I should not test my own code, I&rsquo;ll be biased. <em>Start testing other people&rsquo;s code right now then !</em></li>
<li>I&rsquo;m a programmer, not a tester. <em><a href="https://softwareengineering.stackexchange.com/questions/159572/as-a-professional-developer-is-it-acceptable-to-not-write-unit-tests">Professional programmers write tests</a>.</em></li>
</ol>


<p><img src="../imgs/2017-05-23-20-bad-excuses-for-not-writing-unit-tests/stronger_than_excuses_quote.jpg" alt="A quote 'Be Stronger Than Your Excuses'" /><div class="image-credits">From <a href="https://todayiwillbefit.com/2013/10/30/excuses-suck-top-10-bad-excuses-to-not-exercise/">todayiwillbefit.com</a></div></p>

<ol>
<li>I&rsquo;m using a REPL, it replaces unit tests. <em>Sure, and you&rsquo;re running your REPL buffers on the CI ? and keeping your them for the next time someone modifies your code.</em></li>
<li>My type system is strong enough to replace tests. <em>Does it detect when you use &lsquo;+&rsquo; instead of &lsquo;*&rsquo; ?</em></li>
<li>We don&rsquo;t have the tooling to write unit tests. <em><a href="https://en.wikipedia.org/wiki/List_of_unit_testing_frameworks">Get one</a>.</em></li>
<li>Tests aren&rsquo;t run automatically anyway. <em><a href="https://en.wikipedia.org/wiki/Comparison_of_continuous_integration_software">Install a Continuous Integration Server</a>.</em></li>
<li>I&rsquo;m domain expert developer, writing tests is not my job. <em>Creating bugs isn&rsquo;t either !</em></li>
<li>We&rsquo;d rather switch to the <a href="http://www.paulgraham.com/avg.html">Blub language</a> first ! <em>You&rsquo;re right, let&rsquo;s do neither then !</em></li>
<li>We don&rsquo;t test legacy code. <em>Specifically because it is <a href="https://en.wikipedia.org/wiki/Legacy_code">legacy code</a>.</em></li>
<li>Adding tests for every production code we write is insane ! <em>As shipping untested code is unprofessional.</em></li>
<li>I find more issues doing manual testing. <em><a href="/how-we-started-exploratory-testing/">Exploratory Testing</a> is a valuable testing, even more so on top of automated tests.</em></li>
<li>Because my teammates don&rsquo;t run them. <em>Time for a <a href="/most-scrum-teams-are-not-agile/">retrospective</a>.</em></li>
</ol>


<p><img src="../imgs/2017-05-23-20-bad-excuses-for-not-writing-unit-tests/just-do-it.jpg" alt="'Just Do It' written on a board" /></p>
]]></content>
  </entry>
  
</feed>
