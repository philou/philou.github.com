<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: lean | Philippe Bourgau's blog]]></title>
  <link href="http://philippe.bourgau.net/blog/categories/lean/atom.xml" rel="self"/>
  <link href="http://philippe.bourgau.net/"/>
  <updated>2015-12-04T05:10:42+00:00</updated>
  <id>http://philippe.bourgau.net/</id>
  <author>
    <name><![CDATA[Philippe Bourgau]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Silosis]]></title>
    <link href="http://philippe.bourgau.net/silosis/"/>
    <updated>2015-09-05T06:17:00+00:00</updated>
    <id>http://philippe.bourgau.net/silosis</id>
    <content type="html"><![CDATA[<p>I just invented the word, I found it funny :</p>

<blockquote><p>Silosis: an internal organization of an enterprise in which people are grouped by job titles. Although not obvious at first sight, it usually involves excessively high communication costs. Worst cases can create vicious circles where more &lsquo;communication people&rsquo; are added which in turn increase the overall communication costs &hellip;</p></blockquote>

<p><img class="center" src="/imgs/2015-09-05-silosis/silos.jpg" title="&ldquo;Silos under a grey sky&rdquo;" ></p>

<p>Here are other reasons I don&rsquo;t like silos :</p>

<ul>
<li>Silos create local optimums, which <a href="http://www.amazon.com/Principles-Product-Development-Flow-Generation/dp/B00CAYOX3O/ref=sr_1_sc_2?ie=UTF8&amp;qid=1441546057&amp;sr=8-2-spell&amp;keywords=reinsertsen+flow">lean theory</a> (and mathematics) taught us is bad</li>
<li>Silos create specialists, which bring the whole system to a halt when they leave</li>
<li>Silos create work queue, which increase cycle time</li>
<li>Silos create some form of black market in the organization, through which people can bypass the official communication channel and actually get the job done</li>
<li>Silos hides the big picture to anyone, thus removing autonomy from and <a href="http://www.amazon.com/Drive-Surprising-Truth-About-Motivates/dp/1594484805/ref=sr_1_1?ie=UTF8&amp;qid=1441546115&amp;sr=8-1&amp;keywords=daniel+pink+drive">demotivating people</a></li>
</ul>


<p>I could go one for hours &hellip; I just hate silos.</p>

<p><img class="center" src="/imgs/2015-09-05-silosis/no-silos.png" title="&ldquo;No Silos Logo&rdquo;" ></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Plan For Technical Debt (Lean Software Development Part 7)]]></title>
    <link href="http://philippe.bourgau.net/a-plan-for-technical-debt-lean-software-development-part-7/"/>
    <updated>2015-08-20T17:11:00+00:00</updated>
    <id>http://philippe.bourgau.net/a-plan-for-technical-debt-lean-software-development-part-7</id>
    <content type="html"><![CDATA[<p>The sad truth :</p>

<blockquote><p>The <a href="http://en.wikipedia.org/wiki/Technical_debt">technical debt</a> metaphor does not help me to fix it.</p></blockquote>

<p><img class="center" src="/imgs/2015-08-20-a-plan-for-technical-debt-lean-software-development-part-7/debt.jpg" title="&ldquo;A desperate man counting his debts&rdquo;" ></p>

<p>Here is my modest 2€ plan about how to try to get out of this.</p>

<h2>Why does the metaphor fall short ?</h2>

<p>The debt comparison effectively helps non programming people to understand that bad code costs money. Unfortunately, it does not tell you how much. As a consequence, deciding whether it&rsquo;s best to fix the technical debt or to live with it remains a gut feeling decision <del>(aka programmers want to stop the world and fix all of it while the product owner wants to live with it)</del>.</p>

<p>They are very good reason why we cannot measure how much the technical debt costs :</p>

<ul>
<li>It is purely subjective : bad code for someone might be good code for another. Even worse, as you become a better programmer, yesterday&rsquo;s master piece might become today&rsquo;s crap. More often, as a team gains insight on the domain, old code might suddenly appear completely wrong &hellip;</li>
<li>Tools such as Sonar only spot the a small part of the debt. The larger part (design, architecture and domain) remains invisible</li>
<li>Finally, non-remediation cost (the time wasted working on the bad code) is often overlooked and very difficult to measure : it depends on what you are going to work in the future !</li>
</ul>


<p>No surprise it&rsquo;s difficult to convince anyone else why fixing your debt is a good investment.</p>

<p><a href="http://www.dilbert.com"><img class="center" src="/imgs/2015-08-20-a-plan-for-technical-debt-lean-software-development-part-7/dilbert.jpg" title="&ldquo;A dilbert cartoon about a programmer killed by technical debt&rdquo;" ></a></p>

<h2>The Plan</h2>

<p>In the team, we usually try not to create debt in the first place. We have strong code conventions and working agreements. We are doing a lot of refactoring in order to keep our code base clean. But even with all this, debt creeps in :</p>

<ul>
<li>a pair worked on something and did not know that there is another part of the system that does roughly the same thing</li>
<li>we understand something new about the domain and some previously fine code becomes debt !</li>
<li>like all programmers, we are constantly in a hurry, and sometimes, we just let debt through</li>
<li>&hellip;</li>
</ul>


<p>If the required refactoring is small enough, we just slip it inside a user story and do it on the fly. The real problem comes larger refactorings.</p>

<p>The strategy to deal with those is to get estimations of both the remediation and non-remediation costs. This way, the technical debt becomes an investment ! Invest X$ now and receive Y$ every month up to the end of the life of product. Provided you have the Cost Of Delay of the product, you can estimate the cost of delay of this individual technical debt fix. For example :</p>

<ul>
<li>Let&rsquo;s define the product horizon as its expected remaining life span at any moment</li>
<li>Suppose the product has a 5 years (60 months) horizon</li>
<li>Suppose the Cost Of Delay of the full product is 150K€/month</li>
<li>Suppose that the technical debt costs 10 days (0.5 month) to fix</li>
<li>Suppose that that once fixed, you&rsquo;ll save 2 days (0.1 month) of work per month</li>
<li>By doing the fix now, at the end of the 5 years, you would have saved : <em>(60 &ndash; 0.5) * 0.1 &ndash; 0.5 = 5.45 months</em></li>
<li>Using CoD, this ammounts to : <em>5.45 * 159K = 817.5K €</em></li>
<li>Dividing by the number of months, we finaly get the CoD for this technical debt fix : <em>817.5K / 60 = 13 625 €/month</em></li>
</ul>


<p>This can be compared to the CoD of other backlog items, allowing us to prioritize large refactorings as we would of any feature or story.</p>

<p>One nice thing about this is that it not only helps to know if a refactoring is cost effective, but also when is the best moment to do it. As the CoD of the refactoring is proportional to inverse of the product horizon, a premature refactoring for a startup product might become a real bargain after the product has settled as a market leader. Here are examples of possible product horizons :</p>

<table>
<thead>
<tr>
<th>Context </th>
<th> Horizon</th>
</tr>
</thead>
<tbody>
<tr>
<td>Startup </td>
<td> 6 months</td>
</tr>
<tr>
<td>3 years old company </td>
<td> 3 years</td>
</tr>
<tr>
<td>Market leading product </td>
<td> 10 years</td>
</tr>
<tr>
<td>Aging System </td>
<td> 5 years</td>
</tr>
<tr>
<td>Legacy System </td>
<td> 2 years</td>
</tr>
</tbody>
</table>


<br/>


<p>Oh, and just one more thing &hellip; prioritizing technical debt fixes in your backlog will create some real time to focus on and only on refactoring, reducing task switching and saving even more time.</p>

<p>All this sounds great ! There&rsquo;s just one last little thing : how do we get estimations of both costs of the technical debt ?</p>

<h2>Idea 1 : Collective Estimations</h2>

<p>When I attended Donald Reinertsen&rsquo;s training, I asked him the question and he answered :</p>

<blockquote><p>I&rsquo;d gather the top programmers in a room and I&rsquo;d make them do an estimation of both costs.</p></blockquote>

<p>So I asked my team if they wanted to do the following :</p>

<ol>
<li>whenever you spot a large piece of debt, create a JIRA issue for it</li>
<li>at the end of your next sprint planning session, go through all your technical debt issues, and for each

<ol>
<li>estimate the remediation cost in story points</li>
<li>estimate the non-remediation cost on the coming sprint, taking the prioritized stories into account</li>
</ol>
</li>
<li>using the ROI horizon for every issues, collectively decide which one to tackle and add them to the sprint backlog</li>
</ol>


<p>To keep the story short, it did not stick. I bet it was just too boring.</p>

<h2>Idea 2 : Technical Debt Code Annotations</h2>

<p>During a retrospective, we discussed marking technical debt directly in the code to decide when to fix it. I created 2 code annotations so that this can be done. Here is an example of some identified technical debt :</p>

<p>```java
public final class Transformers {</p>

<p>   private Transformers() {
   }</p>

<p>   @TechnicalDebt(storyPoints = 8, description =</p>

<pre><code> "We need to find a way to do all the ast rewriting before staring the analysis", wastes = {
 @Waste(date = "2015/05/14", hours = 16, summary =
   "For union, we lost quite some time identifying which transformers were not copying the full tree")})
</code></pre>

<p>   public static AstNode analyzeAst(AstNode ast) {</p>

<pre><code> ...
</code></pre>

<p>```</p>

<p>The @TechnicalDebt annotation identifies areas of the code that could be improved.
The @Waste annotation is a way to log time wasted because of this bad code.</p>

<p>By comparing the time to fix the technical debt and the flow of extra work it incurs, we should be able to more easily justify and prioritize these in our backlog.</p>

<p>We are thinking of writing a sonar plugin to keep track of this technical debt right in our <a href="http://www.sonarqube.org/">Sonar dashboard</a>. It would :</p>

<ul>
<li>create a technical debt item in sonar for every @TechnicalDebt annotation found in the code</li>
<li>link it with a mirror technical debt issue in JIRA</li>
<li>use the story points we entered in the annotation as remediation cost</li>
<li>extrapolate the non remediation cost by the sum of wasted hours registered during the last month</li>
</ul>


<p>We just started using those, and I cannot give enough feedback for the moment. I bet not enough @Waste items will be entered though &hellip; again, it might just be too boring</p>

<p><a href="http://www.sonarqube.org/"><img class="center" src="/imgs/2015-08-20-a-plan-for-technical-debt-lean-software-development-part-7/sonar-sqale.jpg" title="&ldquo;A screenshot of Sonar Qube Sqale technical debt plugin&rdquo;" ></a></p>

<h2>Idea 3 : Sonar and IDE Plugins</h2>

<p>If it&rsquo;s too boring to add @Waste annotations in the code, it might be easier to have an IDE plugin with 1 big button to register some time wasted on the local @TechnicalDebt zone.</p>

<p>Pushing things a bit further, it might even be possible to estimate non remediation cost by having a look at what files are read the more, what files are triggering the more test failures when changed, etc.</p>

<p>Unfortunately, that&rsquo;s a long shot, we&rsquo;re definitely not there yet !</p>

<h2>Possible Improvements</h2>

<h3>The Mikado Method</h3>

<p>Whether you&rsquo;ve got these estimations or not, it&rsquo;s always a good practice to learn how to  use the <a href="https://mikadomethod.wordpress.com/2010/02/02/the-mikado-method-in-under-a-minute/">mikado method</a>. It&rsquo;s great to split a refactoring into smaller part and spread them over many sprints.</p>

<p>The pill is easier to swallow for everyone, and it keeps the code releasable at any given time.</p>

<h3>Decision Rule</h3>

<p>Provided you have :</p>

<ul>
<li>Product CoD</li>
<li>Top Features CoD</li>
<li>Product horizon</li>
</ul>


<p>You could easily come up with a <a href="/you-dont-have-to-ask-your-boss-for-a-fast-build-lean-software-development-part-6/">decision rule</a> to help us prioritizing technical debt more quickly, without the need for a formal planning.</p>

<h2>References</h2>

<ul>
<li><a href="http://fr.slideshare.net/zazworka/identifying-and-managing-technical-debt">Identifying and Managing Technical Debt</a></li>
<li><a href="http://www.amazon.com/Managing-Software-Debt-Inevitable-Development/dp/0321948610/ref=sr_1_2?ie=UTF8&amp;qid=1433246385&amp;sr=8-2&amp;keywords=managing+technical+debt">Managing Technical Debt</a></li>
</ul>


<p>This was part 7 of my <a href="/the-flow-book-summary-lean-software-development_part_1/">suite of article about Lean Software Development</a>, Part 6 was <a href="/you-dont-have-to-ask-your-boss-for-a-fast-build-lean-software-development-part-6/">You don&rsquo;t have to ask your boss for a fast build</a>, Part 8 will be &ldquo;Measure the value of the lean startup &lsquo;learning&rsquo;&rdquo;.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[You don't have to ask your boss for a fast build (Lean Software Development part 6)]]></title>
    <link href="http://philippe.bourgau.net/you-dont-have-to-ask-your-boss-for-a-fast-build-lean-software-development-part-6/"/>
    <updated>2015-06-03T04:43:00+00:00</updated>
    <id>http://philippe.bourgau.net/you-dont-have-to-ask-your-boss-for-a-fast-build-lean-software-development-part-6</id>
    <content type="html"><![CDATA[<p>A slow build costs money. I mean it costs <em>a whole lot of money all the time !</em></p>

<p>Spending some time to speed up the build is like an investment, you&rsquo;ll pay some money now, but then it&rsquo;s only a matter of time until you get a return on investment. Here is the trick, if you manage to get it quickly, no one will even notice that you spent some time making the build faster !</p>

<p>With a bit of maths, you can even get what <a href="http://www.amazon.com/Principles-Product-Development-Flow-Generation/dp/1935401009/ref=sr_1_1?ie=UTF8&amp;qid=1432909293&amp;sr=8-1&amp;keywords=reinertsen+flow">Reinertsen</a> calls a &ldquo;Decentralized Decision Rule&rdquo;, making it possible for anyone in the organization to figure out if he should spend some time on the build or not; without the need to ask the permission to anyone.</p>

<p><a href="http://fr.wikipedia.org/wiki/Balance_%28instrument%29"><img class="center" src="/imgs/2015-06-03-you-dont-have-to-ask-your-boss-for-a-fast-build-lean-software-development-part-6/Balance_ordinaire.jpg" title="A 2-pan balance (From Wikipedia)" ></a></p>

<h2>Our example</h2>

<p>Our team is constituted of 5 pairs, each running the build at least 10 times per day. Let&rsquo;s figure out the value of 1 minute build time speed-up</p>

<ul>
<li>The whole team would save : 1m x 5 pairs x 10 builds = 50 minutes per day</li>
<li>In a 2 weeks sprint, this would amount to around 1 day of work</li>
</ul>


<p>This means that if a pair spends half a day to get a 1 minute build speed-up, it would not change the output of the sprint, and it would in fact increase the throughput of the team for further sprints.</p>

<p>Anyone in our team that spots a potential 1 minute build time speed-up that would take less that 1 man.day to implement should do it right away, without asking the permission to anyone</p>

<h2>Other Benefits</h2>

<p>A direct benefit is that the issue will not have to be re-discussed every time someone spots a build time improvement. This will save some management time, and more build speed-up actions will eventually be undertaken.</p>

<p>The astute lean reader will have noticed that I completely ignored the second effect of fast feedback :</p>

<ul>
<li>if the build is faster</li>
<li>we will run it more often</li>
<li>we&rsquo;ll spot errors earlier</li>
<li>less errors will be submitted</li>
<li>the overall throughput will be increased even more</li>
</ul>


<p>Another hidden benefit concerns the <a href="http://en.wikipedia.org/wiki/Cost_of_delay">Cost of Delay</a> (the cost of not selling the product NOW). As Cost of Delay typically trumps the work costs, this means that any improvement to the build time will bring even greater ROI in the long term.</p>

<h2>Variations</h2>

<p><a href="http://en.wikipedia.org/wiki/Boeing_777"><img class="center" src="/imgs/2015-06-03-you-dont-have-to-ask-your-boss-for-a-fast-build-lean-software-development-part-6/Boeing_777.jpg" title="A Boeing 777 in flight over the mountains (From Wikipedia)" ></a></p>

<p>If your sponsor agrees, you can negotiate a longer return on investment period for your decision rule. For example, if he agreed to increase the horizon to 2 sprints, we could undertake more build time speed-up tasks. You might also prefer only to discuss really long ROI investments with him.</p>

<p>While designing the 777 Boeing used a similar decision rule to meet the required weight of the plan : any engineer could increase the production cost of 300$ provided it saved a pound of weight on the plane. This fixed issues they previously had with department weight budgets and escalation.</p>

<p>Finally, it would be great if we had the same rule for technical debt ! Imagine that you knew both the costs of fixing and not fixing your technical debt, you could then decided whether it makes sense to work on the debt right now or not. But that&rsquo;s for a later experiment.</p>

<p>This was part 6 of my <a href="/the-flow-book-summary-lean-software-development_part_1/">Lean Software Development Series</a>. Part 5 was <a href="/what-optimization-should-we-work-on-lean-software-development-part-5/">What optimization should we work on ?</a>, Part 7 will be <a href="/a-plan-for-technical-debt-lean-software-development-part-7/">A Plan for Technical Debt</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What Optimization Should We Work On (Lean Software Development Part 5)]]></title>
    <link href="http://philippe.bourgau.net/what-optimization-should-we-work-on-lean-software-development-part-5/"/>
    <updated>2015-03-26T20:30:00+00:00</updated>
    <id>http://philippe.bourgau.net/what-optimization-should-we-work-on-lean-software-development-part-5</id>
    <content type="html"><![CDATA[<p>At work, we are building a risk aggregation system. As it&rsquo;s dealing with a large bunch of numbers, it&rsquo;s a huge heap of optimizations. Once that its most standard features set is supported, our job mostly consists of making it faster.</p>

<p>That&rsquo;s were we are now doing.</p>

<p><img class="center" src="/imgs/2015-03-26-what-optimization-should-we-work-on-lean-software-development-part-5/turtle.jpg" title="A turtle with a rocket on the back" ></p>

<h1>How do we choose which optimization to work on ?</h1>

<p>The system still being young, we have a wide range of options to optimize it. To name just a few : caches, better algorithms, better low level hardware usage &hellip;</p>

<p>It turns out that we can use the speedup factor as a substitute for business value and use known techniques to help us to make the best decisions.</p>

<h2>Let&rsquo;s walk through an example</h2>

<h3>I. List the optimizations you are thinking of</h3>

<p>Let&rsquo;s suppose we are thinking of the following 3 optimizations for our engine</p>

<ul>
<li>Create better data structures to speed up the reconciliation algorithm</li>
<li>Optimize the reconciliation algorithm itself to reduce CPU cache misses</li>
<li>Minimize boxing and unboxing</li>
</ul>


<h3>II. Poker estimate the story points and speedup</h3>

<p>Armed with these stories, we can poker estimate them, by story points and by expected speedup.
As a substitute for WSJF, we will then be able to compute the speedup rate per story point.
We will then just have to work on the stories with the highest speedup rate first.</p>

<table>
<thead>
<tr>
<th>Title                   </th>
<th> Story Points  </th>
<th> /10    </th>
<th> /2     </th>
<th> -10%    </th>
<th> ~  </th>
<th> +10%   </th>
<th> x2      </th>
<th> x10     </th>
<th> Expected Speedup ratio* </th>
<th> Speedup rate / story point**</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data Structures     </td>
<td> 13        </td>
<td>        </td>
<td>        </td>
<td>             </td>
<td>        </td>
<td> 4 votes</td>
<td> 5 votes </td>
<td>         </td>
<td> x 1.533                 </td>
<td> x 1.033</td>
</tr>
<tr>
<td>Algorithm           </td>
<td> 13        </td>
<td>    </td>
<td> 1 vote </td>
<td> 1 vote      </td>
<td> 2 votes</td>
<td> 1 vote </td>
<td> 2 votes </td>
<td> 2 votes </td>
<td> x 1.799                 </td>
<td> x 1.046</td>
</tr>
<tr>
<td>Boxing                  </td>
<td> 8     </td>
<td>    </td>
<td>        </td>
<td>             </td>
<td>        </td>
<td> 9 votes</td>
<td>         </td>
<td>         </td>
<td> x 1.1                   </td>
<td> x 1.012</td>
</tr>
</tbody>
</table>


<p><sup><em>* Expected speedup ratio is the logarithmic average of the voted speedups</em></sup><br>
<sup><em>** Speedup rate is &ldquo;speedup<sup>(1/ story points)</sup>&rdquo;</em></sup></p>

<p>So based on speedup rate, here is the order in which we should perform the stories :</p>

<ol>
<li>Algorithm</li>
<li>Data Structures</li>
<li>Boxing</li>
</ol>


<h3>III. And what about the risks ?</h3>

<p><img class="center" src="/imgs/2015-03-26-what-optimization-should-we-work-on-lean-software-development-part-5/danger.jpg" title="A danger zone panel" ></p>

<p>This poker estimation tells us something else &hellip;</p>

<blockquote><p>We don&rsquo;t have a clue about the speedup we will get by trying to optimize the algorithm !</p></blockquote>

<p>The votes range from /2 to x10 ! This is the perfect situation for an XP spike.</p>

<table>
<thead>
<tr>
<th>Title </th>
<th> Story points </th>
<th> Expected Speedup rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>Algorithm spike : measure out of context CPU cache optimization speedup </td>
<td> 2 </td>
<td>   ?</td>
</tr>
</tbody>
</table>


<br>


<p>In order to compute the expected speedup rate, let&rsquo;s suppose that they are 2 futures, one where we get a high speedup and another where we get a low one.</p>

<p>They are computed by splitting the votes in 2 :</p>

<ul>
<li><em>low_speedup = 0.846</em></li>
<li><em>high_speedup = 3.827</em></li>
</ul>


<h4>If the spike succeeds</h4>

<p>We&rsquo;ll first work on the spike, and then on the algorithm story. In the end, we would get the speedup of the algorithm optimization.</p>

<ul>
<li><em>spike_high_speedup = high_speedup = 3.827</em></li>
</ul>


<h4>If the spike fails</h4>

<p>We&rsquo;ll also start by working on the spike. Afterwards, instead of the algorithm story, we&rsquo;ll tackle another optimization stories, yielding our average speedup rate for the duration of the algorithm story. The average speedup rate can be obtained from historical benchmark data, or by averaging the speedup rate of the other stories.</p>

<ul>
<li><em>average_speedup_rate = (1.033 * 1.011)<sup>&frac12;</sup> = 1.022</em></li>
<li><em>spike_low_speedup = average_speedup_rate<sup>story_points</sup> = 1.02213 = 1.326</em></li>
</ul>


<h4>Spike speedup rate</h4>

<p>We can now compute the average expected speedup rate for the full period &lsquo;spike &amp; algorithm&rsquo; stories. From this we will be able to get the speedup rate and finally, to prioritize this spike against the other stories in our backlog.</p>

<ul>
<li><em>spike_speedup = (spike_low_speedup * spike_high_speedup)<sup>&frac12;</sup> = 2.253</em></li>
<li><em>spike_speedup_rate = spike_speedup<sup>1/(spike_story_points + algorithm_story_points)</sup> = 2.253<sup>1/(2 + 13)</sup> = 1.056</em></li>
</ul>


<h3>IV. Putting it all together</h3>

<p>Here are all the speedup rate for the different stories.</p>

<table>
<thead>
<tr>
<th>Title </th>
<th> Speedup rate / story point</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data Structure  </td>
<td> x 1.033</td>
</tr>
<tr>
<td>Algorithm           </td>
<td> x 1.046</td>
</tr>
<tr>
<td>Boxing                  </td>
<td> x 1.012</td>
</tr>
<tr>
<td>Algorithm spike         </td>
<td> x 1.056</td>
</tr>
</tbody>
</table>


<br>


<p>Finally, here is the optimal order through which we should perform the stories :</p>

<ul>
<li>Algorithm spike</li>
<li>Algorithm (only if the spike proved it would work)</li>
<li>Data Structures</li>
<li>Boxing</li>
</ul>


<h2>Summary</h2>

<p>The math are not that complex, and a simple formula can be written to compute the spike speedup rate :</p>

<p><img class="center" src="/imgs/2015-03-26-what-optimization-should-we-work-on-lean-software-development-part-5/poc_speedup_rate.png" title="Formula for a spike speedup rate" ></p>

<p>I think most experienced engineers would have come to the same conclusion by gut feeling &hellip;</p>

<p>Nevertheless I believe that systematically applying the such method when prioritizing optimizations can lead to a greater speedup rate than the competition in the long run. This is a perfect example where taking measured risks can payoff !</p>

<p>This was part 5 of my <a href="/the-flow-book-summary-lean-software-development_part_1/">Lean Software Development Series</a>. Part 4 was <a href="/measure-the-business-value-of-your-spikes-and-take-high-payoff-risks-lean-software-development-part-4/">Measure the business value of your spikes and take high payoff risks</a>, Part 5 will be <a href="/you-dont-have-to-ask-your-boss-for-a-fast-build-lean-software-development-part-6/">You don&rsquo;t have to ask your boss for a fast build</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Measure The Business Value of Your Spikes and Take High Payoff Risks (Lean Software Development Part 4)]]></title>
    <link href="http://philippe.bourgau.net/measure-the-business-value-of-your-spikes-and-take-high-payoff-risks-lean-software-development-part-4/"/>
    <updated>2015-01-31T15:13:00+00:00</updated>
    <id>http://philippe.bourgau.net/measure-the-business-value-of-your-spikes-and-take-high-payoff-risks-lean-software-development-part-4</id>
    <content type="html"><![CDATA[<p>Lately <a href="http://www.murex.com">at work</a>, we&rsquo;ve unexpectedly been asked by other teams if they could use our product for something that we had not forseen. As we are not sure whether we&rsquo;ll be able to tune our product to their needs, we are thinking about doing a short study to know the answer. This looks like a great opportunity to try out <a href="http://en.wikipedia.org/wiki/Cost_of_delay">Cost of Delay</a> analysis about uncertain tasks.</p>

<p>Unfortunately, I cannot write the details of what we are creating at work in this blog, so let&rsquo;s assume that we are building a Todo List Software.</p>

<p>We have been targeting the enterprise market. Lately, we&rsquo;ve seen some interest from individuals planning to use our todo list system for themselves at home.</p>

<p>For individuals, the system would need to be highly available and live 24/7 over the internet, latency will also be critical to retain customers, but the product could get a market share with a basic feature set.</p>

<p>On the other side, enterprise customers need advanced features, absolute data safety, but they can cope with nightly restarts of the server.</p>

<p>In order to know if we can make our todo list system available and fast enough for the individuals market, we are planning to conduct a pre-study, so as not to waste time on an unreachable goal. In <a href="http://www.extremeprogramming.org/">XP</a> terms, this is a <a href="http://www.extremeprogramming.org/rules/spike.html">spike</a>, and it&rsquo;s a bunch of experiments rather than a theoretical study.</p>

<p><a href="/imgs/2015-01-31-measure-the-business-value-of-your-spikes-and-take-high-payoff-risks-lean-software-development-part-4/study.jpg"><img class="center" src="/imgs/2015-01-31-measure-the-business-value-of-your-spikes-and-take-high-payoff-risks-lean-software-development-part-4/study-petit.jpg" title="Photo of someone studying behind piles of books" ></a></p>

<h2>When should we prioritize this spike ?</h2>

<p>If we are using the <a href="http://www.scaledagileframework.com/wsjf/">Weighted Shortest Job First</a> metric to prioritize our work, we need to estimate the cost of delay of a task to determine its priority. Hereafter I will explain how we could determine the value of this spike.</p>

<h2>Computing the cost of delay</h2>

<p>The strategy to compute the cost of delay for such a risk mitigation task is to compute the difference in cost of delays with or without doing it.</p>

<h3>1. The products, the features, the MVP and the estimates</h3>

<p>As I explained in <a href="/how-to-measure-your-speed-with-your-business-value-lean-software-development-part-3/">a previous post</a>, for usual features, cost of delay is equivalent to it&rsquo;s value. Along with our gross estimates, here are the relative values that our product owner gave us for the different products we are envisioning.</p>

<table>
<thead>
<tr>
<th>Feature                   </th>
<th> $ Enterprise </th>
<th> $ Individuals </th>
<th> Estimated work</th>
</tr>
</thead>
<tbody>
<tr>
<td>Robustness            </td>
<td> 20*          </td>
<td> 20*           </td>
<td> 2</td>
</tr>
<tr>
<td>Availability              </td>
<td> 0            </td>
<td> 40*           </td>
<td> 2</td>
</tr>
<tr>
<td>Latency                   </td>
<td> 0            </td>
<td> 40*           </td>
<td> 1</td>
</tr>
<tr>
<td>Durability            </td>
<td> 40*          </td>
<td> 13            </td>
<td> 2</td>
</tr>
<tr>
<td>Multi user lists          </td>
<td> 20*          </td>
<td> 8             </td>
<td> 2</td>
</tr>
<tr>
<td>Labels                    </td>
<td> 20           </td>
<td> 13            </td>
<td> 2</td>
</tr>
<tr>
<td>Custom report tool        </td>
<td> 13           </td>
<td> 0             </td>
<td> 3</td>
</tr>
<tr>
<td>TOTAL Cost Of Delay of v1 </td>
<td> 80           </td>
<td> 100           </td>
<td></td>
</tr>
</tbody>
</table>


<p><small>Stared (*) features are required for the first version of the product. Features with a value of 0 are not required for the product. Eventually, unstared features with a non null business value would be great for a second release.</small><br></p>

<p>It seems that the individuals market is a greater opportunity, so it&rsquo;s worth thinking about it. Unfortunately for the moment, we really don&rsquo;t know if we&rsquo;ll manage to get the high availability that is required for such a product.</p>

<p>The availability spike we are envisioning would take 1 unit of time.</p>

<h3>2. Computing the cost of delay of this spike</h3>

<p>The cost of delay of a task involving some uncertainty is the probabilistic expected value of its cost of delay. We estimate that we have 50% of chances of matching the availability required by individuals. It means that CoD of the spike = 50% * CoD if we match the latency + 50% CoD if we don&rsquo;t match the availability.</p>

<h4>2.a. The Cost of Delay if we get the availability</h4>

<p>Let&rsquo;s consider the future in which we&rsquo;ll manage to reduce the latency.
The cost of delay of a spike task is the difference in Cost with and without doing the spike, per relevent months.</p>

<h5>2.a.i. The cost if we don&rsquo;t do the spike</h5>

<p>Unfortunately, at this point in this future, we don&rsquo;t yet know that we&rsquo;ll manage to get to the availability.</p>

<table>
<thead>
<tr>
<th>Feature                   </th>
<th> $ Enterprise </th>
<th> $ Individuals </th>
<th> $ Expected   </th>
<th> Estimated work </th>
<th> WSJF</th>
</tr>
</thead>
<tbody>
<tr>
<td>Latency                   </td>
<td> 0            </td>
<td> 40*           </td>
<td> 20           </td>
<td> 1              </td>
<td> 20</td>
</tr>
<tr>
<td>Durability            </td>
<td> 40*          </td>
<td> 13            </td>
<td> 26           </td>
<td> 2              </td>
<td> 13</td>
</tr>
<tr>
<td>Robustness            </td>
<td> 20*          </td>
<td> 20*           </td>
<td> 20           </td>
<td> 2              </td>
<td> 10</td>
</tr>
<tr>
<td>Availability              </td>
<td> 0            </td>
<td> 40*           </td>
<td> 20           </td>
<td> 2              </td>
<td> 10</td>
</tr>
<tr>
<td>Labels                    </td>
<td> 20           </td>
<td> 13            </td>
<td> 17           </td>
<td> 2              </td>
<td> 9</td>
</tr>
<tr>
<td>Multi user lists          </td>
<td> 20*          </td>
<td> 8             </td>
<td> 14           </td>
<td> 2              </td>
<td> 7</td>
</tr>
<tr>
<td>Custom report tool        </td>
<td> 13           </td>
<td> 0             </td>
<td> 8            </td>
<td> 3              </td>
<td> 3</td>
</tr>
</tbody>
</table>


<br>


<p>We&rsquo;ll resort to WSJF to prioritize our work. Here is what we&rsquo;ll be able to ship :</p>

<table>
<thead>
<tr>
<th>Product </th>
<th> Delay </th>
<th> CoD </th>
<th> Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td>Individuals                     </td>
<td> 7    </td>
<td> 100   </td>
<td>  700</td>
</tr>
<tr>
<td>Individuals Durability          </td>
<td> 7    </td>
<td> 13    </td>
<td>   91</td>
</tr>
<tr>
<td>Individuals Labels              </td>
<td> 9    </td>
<td> 13    </td>
<td>  117</td>
</tr>
<tr>
<td>Enterprise                      </td>
<td> 11   </td>
<td> 80    </td>
<td>  880</td>
</tr>
<tr>
<td>Enterprise labels               </td>
<td> 11   </td>
<td> 20    </td>
<td>  220</td>
</tr>
<tr>
<td>Individuals Multi user lists    </td>
<td> 13   </td>
<td> 8     </td>
<td>  104</td>
</tr>
<tr>
<td>Enterprise Custom reports       </td>
<td> 16   </td>
<td> 13    </td>
<td>  208</td>
</tr>
<tr>
<td>                                </td>
<td>      </td>
<td>       </td>
<td> 2320</td>
</tr>
</tbody>
</table>


<br>


<h5>2.a.ii. The cost if we do the spike</h5>

<p>In this case, we would start by the spike, and it would tell us that we can reach the individuals availability and so that we should go for this feature first. Here will be our planning</p>

<table>
<thead>
<tr>
<th>Feature                   </th>
<th> $ Enterprise </th>
<th> $ Individuals </th>
<th> Estimated work </th>
<th> Enterprise WSJF </th>
<th> Individuals WSJF</th>
</tr>
</thead>
<tbody>
<tr>
<td>Feasibility spike         </td>
<td>              </td>
<td>               </td>
<td> 1              </td>
<td>                 </td>
<td></td>
</tr>
<tr>
<td>Latency                   </td>
<td> 0            </td>
<td> 40*           </td>
<td> 1              </td>
<td>                 </td>
<td> 40</td>
</tr>
<tr>
<td>Availability              </td>
<td> 0            </td>
<td> 40*           </td>
<td> 2              </td>
<td>                 </td>
<td> 20</td>
</tr>
<tr>
<td>Robustness            </td>
<td> 20*          </td>
<td> 20*           </td>
<td> 2              </td>
<td> 10              </td>
<td> 10</td>
</tr>
<tr>
<td>Durability            </td>
<td> 40*          </td>
<td> 13            </td>
<td> 2              </td>
<td> 20              </td>
<td> 7</td>
</tr>
<tr>
<td>Multi user lists          </td>
<td> 20*          </td>
<td> 8             </td>
<td> 2              </td>
<td> 10              </td>
<td> 4</td>
</tr>
<tr>
<td>Labels                    </td>
<td> 20           </td>
<td> 13            </td>
<td> 2              </td>
<td> 10              </td>
<td> 7</td>
</tr>
<tr>
<td>Custom report tool        </td>
<td> 13           </td>
<td> 0             </td>
<td> 3              </td>
<td> 4               </td>
<td></td>
</tr>
</tbody>
</table>


<br>


<p>Here is how we will be able to ship :</p>

<table>
<thead>
<tr>
<th>Product </th>
<th> Delay </th>
<th> CoD </th>
<th> Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td>Individuals                     </td>
<td> 6    </td>
<td> 100   </td>
<td>  600</td>
</tr>
<tr>
<td>Individuals Durability          </td>
<td> 8    </td>
<td> 13    </td>
<td>  104</td>
</tr>
<tr>
<td>Individuals Multi user lists    </td>
<td> 10   </td>
<td> 8     </td>
<td>   80</td>
</tr>
<tr>
<td>Enterprise                      </td>
<td> 10   </td>
<td> 80    </td>
<td>  800</td>
</tr>
<tr>
<td>Individuals Labels              </td>
<td> 12   </td>
<td> 13    </td>
<td>  156</td>
</tr>
<tr>
<td>Enterprise Labels               </td>
<td> 12   </td>
<td> 20    </td>
<td>  240</td>
</tr>
<tr>
<td>Enterprise Custom reports       </td>
<td> 15   </td>
<td> 13    </td>
<td>  195</td>
</tr>
<tr>
<td>                                </td>
<td>      </td>
<td>       </td>
<td> 2175</td>
</tr>
</tbody>
</table>


<br>


<h5>2.a.iii. Cost of delay of the spike if we reach the availability</h5>

<p>By making the spike, we would save 2320 &ndash; 2175 = 145$</p>

<p>Without doing the spike, we would discover whether we would reach the availability when we try it, around time 7 (see 2.a.i).</p>

<p>So the cost of delay for the spike would be around 145/7 = 21 $/m</p>

<h4>2.b. The Cost of Delay if we don&rsquo;t get the availability</h4>

<p>Let&rsquo;s now consider the future in which we don&rsquo;t manage to increase the availability.</p>

<p>Using the same logic as before, let&rsquo;s now see what happens</p>

<h5>2.b.i. The cost if we don&rsquo;t do the spike</h5>

<p>Unfortunately, at this point in this future, we don&rsquo;t yet know that we&rsquo;ll not manage to get to the availability.</p>

<table>
<thead>
<tr>
<th>Feature                   </th>
<th> $ Enterprise </th>
<th> $ Individuals </th>
<th> $ Expected   </th>
<th> Estimated work </th>
<th> WSJF</th>
</tr>
</thead>
<tbody>
<tr>
<td>Latency                   </td>
<td> 0            </td>
<td> 40*           </td>
<td> 20           </td>
<td> 1              </td>
<td> 20</td>
</tr>
<tr>
<td>Durability            </td>
<td> 40*          </td>
<td> 13            </td>
<td> 26           </td>
<td> 2              </td>
<td> 13</td>
</tr>
<tr>
<td>Robustness            </td>
<td> 20*          </td>
<td> 20*           </td>
<td> 20           </td>
<td> 2              </td>
<td> 10</td>
</tr>
<tr>
<td>Availability              </td>
<td> 0            </td>
<td> 40*           </td>
<td> 20           </td>
<td> 2              </td>
<td> 10</td>
</tr>
<tr>
<td>Multi user lists          </td>
<td> 20*          </td>
<td> 8             </td>
<td> 14           </td>
<td> 2              </td>
<td> 7</td>
</tr>
<tr>
<td>Labels                    </td>
<td> 20           </td>
<td> 13            </td>
<td> 17           </td>
<td> 2              </td>
<td> 9</td>
</tr>
<tr>
<td>Custom report tool        </td>
<td> 13           </td>
<td> 0             </td>
<td> 8            </td>
<td> 3              </td>
<td> 3</td>
</tr>
</tbody>
</table>


<br>


<p>When we&rsquo;ll fail at the availability, we&rsquo;ll switch multi user lists and labels to be able to ship to enterprises as quickly as possible.
Here is what we&rsquo;ll ship.</p>

<table>
<thead>
<tr>
<th>Product </th>
<th> Delay </th>
<th> CoD </th>
<th> Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td>Enterprise                      </td>
<td>  9   </td>
<td> 80    </td>
<td>  720</td>
</tr>
<tr>
<td>Enterprise Labels               </td>
<td> 11   </td>
<td> 20    </td>
<td>  220</td>
</tr>
<tr>
<td>Enterprise Custom reports       </td>
<td> 14   </td>
<td> 13    </td>
<td>  182</td>
</tr>
<tr>
<td>                                </td>
<td>      </td>
<td>       </td>
<td> 1122</td>
</tr>
</tbody>
</table>


<br>


<h5>2.b.ii. The cost if we do the spike</h5>

<p>In this case, we would start by the spike, and it would tell us that we won&rsquo;t match the availability required for individuals and so that that there&rsquo;s no need to run after this now.</p>

<table>
<thead>
<tr>
<th>Feature            </th>
<th> $ Enterprise </th>
<th> Estimated work </th>
<th> WSJF</th>
</tr>
</thead>
<tbody>
<tr>
<td>Feasibility spike  </td>
<td>              </td>
<td> 1              </td>
<td></td>
</tr>
<tr>
<td>Durability     </td>
<td> 40*      </td>
<td> 2              </td>
<td> 13</td>
</tr>
<tr>
<td>Robustness     </td>
<td> 20*      </td>
<td> 2              </td>
<td> 10</td>
</tr>
<tr>
<td>Multi user lists   </td>
<td> 20*          </td>
<td> 2              </td>
<td> 7</td>
</tr>
<tr>
<td>Labels             </td>
<td> 20           </td>
<td> 2              </td>
<td> 9</td>
</tr>
<tr>
<td>Custom report tool </td>
<td> 13           </td>
<td> 3              </td>
<td> 3</td>
</tr>
</tbody>
</table>


<br>


<p>Here is how we will be able to ship :</p>

<table>
<thead>
<tr>
<th>Product </th>
<th> Delay </th>
<th> CoD </th>
<th> Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td>Enterprise                      </td>
<td>  7   </td>
<td> 80    </td>
<td>  560</td>
</tr>
<tr>
<td>Enterprise Labels               </td>
<td>  9   </td>
<td> 20    </td>
<td>  180</td>
</tr>
<tr>
<td>Enterprise Custom reports       </td>
<td> 12   </td>
<td> 13    </td>
<td>  156</td>
</tr>
<tr>
<td>                                </td>
<td>      </td>
<td>       </td>
<td>  896</td>
</tr>
</tbody>
</table>


<br>


<h5>2.b.iii. Cost of delay of the spike if we reach the availability</h5>

<p>By making the spike, we would save 1122 &ndash; 896 = 226$</p>

<p>As before, without doing the spike, we would discover whether we would get the availability when we try it, around time 7.</p>

<p>So the cost of delay for the spike is around 226/7 = 32 $/m</p>

<h4>2.c. Compute overall Cost of Delay of the Spike</h4>

<p><a href="/imgs/2015-01-31-measure-the-business-value-of-your-spikes-and-take-high-payoff-risks-lean-software-development-part-4/CoD.jpg"><img class="center" src="/imgs/2015-01-31-measure-the-business-value-of-your-spikes-and-take-high-payoff-risks-lean-software-development-part-4/CoD-petit.jpg" title="Bank notes going through an hourglass" ></a></p>

<p>Given that we estimate that there is a 50% chances of making the latency, the overall expected cost of delay is</p>

<p>50% * 21 + 50% * 32 = 26.5 $/m</p>

<p>Inject the spike in the backlog</p>

<p>With the Cost of Delay of the spike, we can compute it&rsquo;s WSJF and prioritize it against other features.</p>

<table>
<thead>
<tr>
<th>Feature </th>
<th> $ Enterprise </th>
<th> $ Individuals </th>
<th> Expected $ </th>
<th> Estimated work </th>
<th> WSJF</th>
</tr>
</thead>
<tbody>
<tr>
<td>Feasibility Spike         </td>
<td>              </td>
<td>               </td>
<td> 26.5         </td>
<td> 1              </td>
<td> 26.5</td>
</tr>
<tr>
<td>Latency                   </td>
<td> 0            </td>
<td> 40*           </td>
<td> 20           </td>
<td> 1              </td>
<td> 20</td>
</tr>
<tr>
<td>Durability            </td>
<td> 40*          </td>
<td> 13            </td>
<td> 26           </td>
<td> 2              </td>
<td> 13</td>
</tr>
<tr>
<td>Robustness            </td>
<td> 20*          </td>
<td> 20*           </td>
<td> 20           </td>
<td> 2              </td>
<td> 10</td>
</tr>
<tr>
<td>Availability              </td>
<td> 0            </td>
<td> 40*           </td>
<td> 20           </td>
<td> 2              </td>
<td> 10</td>
</tr>
<tr>
<td>Multi user lists          </td>
<td> 20*          </td>
<td> 8             </td>
<td> 14           </td>
<td> 2              </td>
<td> 7</td>
</tr>
<tr>
<td>Labels                    </td>
<td> 20           </td>
<td> 13            </td>
<td> 17           </td>
<td> 2              </td>
<td> 9</td>
</tr>
<tr>
<td>Custom report tool        </td>
<td> 13           </td>
<td> 0             </td>
<td> 8            </td>
<td> 3              </td>
<td> 3</td>
</tr>
</tbody>
</table>


<br>


<p>The spike comes at the top of our backlog. Which confirms our gut feeling.</p>

<h2>Conclusion</h2>

<p>Doing this long study confirmed classic rule of thumbs</p>

<ul>
<li>Don&rsquo;t develop many products at the same time</li>
<li>Do some Proof Of Concepts early before starting to work on uncertain features</li>
<li>Tackle the most risky features first</li>
</ul>


<p>By improving the inputs, we could get more quality results :</p>

<ul>
<li>If we had access to real sales or finance figures for the costs</li>
<li>If we did some sort of poker risk estimation instead of just guessing at 50% chances</li>
</ul>


<p>Obviously, the analysis itself is not perfect, but it hints to the good choices. And as <a href="http://reinertsenassociates.com/">Don Reinertsen</a> puts it, using an economical framework, the spread between people estimations goes down from <a href="http://leanmagazine.net/lean/cost-of-delay-don-reinertsen/">50:1</a> to 2:1 ! This seems a good alternative to the experience and gut feeling approach which :</p>

<ul>
<li>can trigger heated unfounded discussions</li>
<li>often means high dependence on the intuition of a single individual</li>
</ul>


<p>As everything is quantitative though, one could imagine that with other figures, we could have got to another conclusion, such as :</p>

<ul>
<li>The spike is not worth doing (it costs more than it might save)</li>
<li>The spike can wait a bit</li>
</ul>


<p><a href="http://dilbert.com/strip/2014-03-30"><img class="center" src="/imgs/2015-01-31-measure-the-business-value-of-your-spikes-and-take-high-payoff-risks-lean-software-development-part-4/dt140330.jpg" title="A dilbert strip about gut feeling at work" ></a></p>

<p>This was part 4 of my <a href="/the-flow-book-summary-lean-software-development_part_1/">Lean Software Development Series</a>. Part 3 was <a href="/how-to-measure-your-speed-with-your-business-value-lean-software-development-part-3/">How to measure your speed with your business value</a>, continue on Part 5 : <a href="/what-optimization-should-we-work-on-lean-software-development-part-5/">What optimization should we work on ?</a>.</p>
]]></content>
  </entry>
  
</feed>
